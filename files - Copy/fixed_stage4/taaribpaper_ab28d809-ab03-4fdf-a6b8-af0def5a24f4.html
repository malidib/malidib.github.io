```html
<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Leona Hennig, Tanja Tornede, Marius Lindauer">
  <title>نحو استغلال AutoML للتعلم العميق المستدام: نهج تحسين الفعالية متعددة الأهداف على شبكات العصبونات العميقة المتغيرة</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    html, body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 22px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.7;
    }
    header {
      background: linear-gradient(90deg, #3a8dde 0%, #6dd5ed 100%);
      color: #fff;
      padding: 2.5rem 1.5rem 1.5rem 1.5rem;
      text-align: center;
      border-bottom-left-radius: 40px;
      border-bottom-right-radius: 40px;
      box-shadow: 0 4px 16px rgba(58, 141, 222, 0.08);
      margin-bottom: 2.5rem;
    }
    h1.title {
      font-size: 2.3rem;
      font-weight: 700;
      margin-bottom: 1.2rem;
      letter-spacing: 0.5px;
      line-height: 1.3;
    }
    .author {
      font-size: 1.1rem;
      font-weight: 400;
      margin-top: 0.5rem;
      color: #e3f2fd;
    }
    h1, h2, h3 {
      color: #2b5d8c;
      font-weight: 700;
      margin-top: 2.2rem;
      margin-bottom: 1.1rem;
      line-height: 1.2;
    }
    h1 {
      font-size: 2rem;
      border-right: 6px solid #3a8dde;
      padding-right: 0.7rem;
      background: #e3f2fd;
      border-radius: 0 20px 20px 0;
      display: inline-block;
      margin-bottom: 1.5rem;
    }
    h2 {
      font-size: 1.4rem;
      border-right: 4px solid #6dd5ed;
      padding-right: 0.5rem;
      background: #f1f8fe;
      border-radius: 0 14px 14px 0;
      display: inline-block;
      margin-bottom: 1rem;
    }
    h3 {
      font-size: 1.1rem;
      color: #388e3c;
      margin-top: 2rem;
      margin-bottom: 0.7rem;
      border-right: 3px solid #a5d6a7;
      padding-right: 0.4rem;
      background: #e8f5e9;
      border-radius: 0 10px 10px 0;
      display: inline-block;
    }
    p {
      margin: 1.1rem 0;
      text-align: justify;
    }
    ol, ul {
      margin: 1.2rem 2.5rem 1.2rem 0;
      padding-right: 1.5rem;
    }
    li {
      margin-bottom: 0.7rem;
    }
    code, pre {
      font-family: 'Fira Mono', 'Consolas', 'Courier New', monospace;
      background: #f3f6fa;
      color: #1a237e;
      border-radius: 6px;
      padding: 0.2em 0.5em;
      font-size: 0.95em;
    }
    pre {
      display: block;
      padding: 1em;
      overflow-x: auto;
      background: #f3f6fa;
      border: 1px solid #e3eaf2;
      margin: 1.5em 0;
      border-radius: 10px;
    }
    .math.display, .math.inline {
      direction: ltr;
      unicode-bidi: embed;
      font-size: 1.05em;
      background: #f1f8fe;
      padding: 0.2em 0.5em;
      border-radius: 6px;
      margin: 0.5em 0;
      display: inline-block;
    }
    .nodecor {
      text-decoration: none !important;
      color: #1976d2 !important;
      font-weight: 600;
      font-family: inherit;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 2em 0;
      background: #fff;
      border-radius: 10px;
      overflow: hidden;
      box-shadow: 0 2px 8px rgba(58, 141, 222, 0.05);
    }
    th, td {
      border: 1px solid #e3eaf2;
      padding: 0.7em 1em;
      text-align: center;
    }
    th {
      background: #e3f2fd;
      color: #2b5d8c;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f8f9fa;
    }
    a {
      color: #1976d2;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover {
      color: #0d47a1;
    }
    @media (max-width: 800px) {
      html, body {
        font-size: 18px;
      }
      header {
        padding: 1.5rem 0.5rem 1rem 0.5rem;
      }
      h1.title {
        font-size: 1.3rem;
      }
      h1 {
        font-size: 1.1rem;
        padding-right: 0.4rem;
      }
      h2 {
        font-size: 1rem;
        padding-right: 0.3rem;
      }
      h3 {
        font-size: 0.9rem;
        padding-right: 0.2rem;
      }
      table, th, td {
        font-size: 0.95em;
      }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">نحو استغلال <span class="nodecor">AutoML</span> للتعلم العميق المستدام: نهج تحسين الفعالية متعددة الأهداف على شبكات العصبونات العميقة المتغيرة</h1>
  <p class="author"><span class="nodecor">Leona Hennig</span>, <span class="nodecor">Tanja Tornede</span>, <span class="nodecor">Marius Lindauer</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">مُلَخَّص</h1>
<p>ساهم التعلم العميق في تقدم العديد من المجالات من خلال القدرة على استخراج الأنماط المعقدة من مجموعات البيانات الضخمة. ومع ذلك، فإن المتطلبات الحسابية الكبيرة لنماذج التعلم العميق تفرض تحديات بيئية واقتصادية تتعلق بالموارد. تقدم شبكات العصبونات العميقة المتغيرة حلاً من خلال استغلال عمليات الإزاحة لتقليل التعقيد الحسابي أثناء الاستدلال. استناداً إلى الرؤى المستخلصة من الشبكات العصبية التقليدية، نركز على استثمار الإمكانات الكاملة لهذه الشبكات عبر تقنيات <span class="nodecor">AutoML</span>. ندرس تأثير تحسين المعلمات الفائقة لتعظيم أداء الشبكات المتغيرة مع تقليل استهلاك الموارد. ونظراً لأن هذا يأخذ في الاعتبار كلاً من الدقة واستهلاك الطاقة كأهداف متنافسة، نقترح الجمع بين تحسين المعلمات الفائقة متعدد الأوجه والتحسين متعدد الأهداف. تظهر النتائج التجريبية فعالية نهجنا، حيث تؤدي إلى نماذج تصل دقتها إلى أكثر من <span class="nodecor">80%</span> مع تكلفة حسابية منخفضة. بشكل عام، تُسرع طريقتنا عملية تطوير نماذج فعالة وتُمكّن تطبيقات الذكاء الاصطناعي المستدامة.</p>
<h1 id="مُقَدِّمَةِ">مقدمة</h1>
<p>يُعَد التعلم العميق من أكثر النهج الواعدة لاستخراج المعلومات من مجموعات البيانات الكبيرة ذات الهياكل المعقدة. ويشمل ذلك إجراء الحسابات في بيئات إنترنت الأشياء وعلى أجهزة الحافة (<span class="nodecor">DBLP:journals/network/LiOD18</span>, <span class="nodecor">DBLP:journals/pieee/ZhouCLZLZ19</span>). مع الزيادة المستمرة في حجم وأداء هذه النماذج نتيجة التقدم في العلوم والصناعة، ينتج عنها تكلفة حسابية مرتفعة (<span class="nodecor">DBLP:journals/pieee/SzeCYE17</span>). تقليل هذه التكلفة يؤثر مباشرة على الأثر البيئي للنموذج (<span class="nodecor">DBLP:journals/cacm/SchwartzDSE20</span>)، وبالتالي يتاح تحرير الموارد لاستخدامها في مهام أخرى، مثل التطبيقات محدودة الموارد (<span class="nodecor">DBLP:journals/corr/HowardZCKWWAA17</span>). يسهم نهجنا في توسيع استخدام التعلم العميق في هذه البيئات ذات الموارد المحدودة.</p>
<p>تقدم شبكات العصبونات العميقة المتغيرة إمكانات كبيرة في تقليل استهلاك الطاقة مقارنة بنماذج التعلم العميق التقليدية (<span class="nodecor">DBLP:conf/cvpr/ElhoushiCSTL21</span>). بدلاً من العمليات الحسابية العائمة، تستفيد هذه الشبكات من عمليات الإزاحة—وتحديداً إزاحة البت—كوحدة حسابية، مما يعزز الكفاءة عبر استبدال عمليات الضرب المكلفة في الشبكات التلافيفية. نعتقد أن تكوين هذه الشبكات له تأثير واضح على كل من الأداء والكفاءة الحسابية.</p>
<p>أحد التحديات الرئيسية مع شبكات العصبونات العميقة المتغيرة هو تحديد مستوى الدقة المناسب لعمليات الإزاحة لتقليل أخطاء الكمية دون زيادة العبء الحسابي بشكل مفرط. في هذه الدراسة، نستفيد من شبكات العصبونات المتغيرة إلى جانب التعلم الآلي المؤتمت لاكتشاف التكوين الأمثل في إطار التعلم الآلي الأخضر (<span class="nodecor">DBLP:journals/jair/TornedeTHMWH23</span>). نعتمد على تحسين المعلمات الفائقة باستخدام إطار العمل <span class="nodecor">SMAC3</span> الذي اقترح (<span class="nodecor">DBLP:journals/jmlr/LindauerEFBDBRS22</span>)، والذي يؤتمت البحث عن التكوينات المثلى. يسهّل دمج تقنيات التحسين متعدد الدقة ومتعدد الأهداف (<span class="nodecor">Belakaria2020-re</span>) استكشاف مساحة المعلمات الفائقة مع إعطاء الأولوية للأداء واستهلاك الطاقة في آن واحد (<span class="nodecor">Deb2014</span>). يؤدي تنفيذ <span class="nodecor">SMAC</span> للتحسين متعدد الأهداف إلى موازنة فعّالة بين تحقيق دقة تنبؤية عالية وتقليل استهلاك الطاقة. كما يسمح الجانب متعدد الدقة باستخدام الموارد الحسابية بكفاءة عبر تقييم التكوينات على مستويات متفاوتة من الدقة. ويوفر استخدام أدوات مثل <span class="nodecor">CodeCarbon</span> (<span class="nodecor">DBLP:journals/corr/abs-1910-09700</span>, <span class="nodecor">DBLP:journals/corr/abs-1911-08354</span>) خلال مراحل التدريب والتقييم رؤى في الوقت الفعلي عن استهلاك الطاقة وانبعاثات الكربون لكل تكوين نموذج.</p>
<p>بشكل عام، نقدم المساهمات التالية:</p>
<ol>
<li><p>فضاء تكوين مفصّل لشبكات العصبونات العميقة المتغيرة،</p></li>
<li><p>نهج تعلم آلي أخضر لبناء نماذج موجهة نحو الكفاءة،</p></li>
<li><p>رؤى حول قرارات التصميم لتحقيق توازن مثالي بين الدقة وكفاءة الطاقة، و</p></li>
<li><p>دمج تقنيات التحسين متعدد الدقة ومتعدد الأهداف في <span class="nodecor">SMAC</span> لتعزيز فعالية التحسين واستخدام الموارد الحسابية.</p></li>
</ol>
<h1 id="background">الخلفية</h1>
<p>يقدم هذا الفصل المفاهيم الأساسية لمنهجيتنا.</p>
<h2 id="شبكات-العصبونات-العميقة-المتغيرة">شبكات العصبونات العميقة المتغيرة</h2>
<p>شبكات العصبونات العميقة المتغيرة هي نهج جديد لتقليل المتطلبات الحسابية والطاقية للتعلّم العميق (<span class="nodecor">DBLP:conf/cvpr/ElhoushiCSTL21</span>). تحقق هذه الشبكات خفضاً كبيراً في زمن الانتظار عبر تبسيط هندسة الشبكة بحيث تستبدل عمليات الضرب التقليدية بعمليات الإزاحة البتّية وقلب الإشارة، مما يجعلها مناسبة لأجهزة الحوسبة ذات الموارد المحدودة. هناك طريقتان لتدريب هذه الشبكات: تقنية التكميم (Quantization) وتقنية القوى الثنائية والإشارة (Powers of Two and Sign). تتضمن الأولى تكميم الأوزان إلى أقرب قوة اثنين خلال المرورين الأمامي والخلفي، بينما تتيح التقنية الثانية تدريب كل من قيم التحويل وقلب الإشارة كمعاملات قابلة للتعلم.</p>
<p>في نهج التكميم، نحصل على مصفوفة الإشارة <span class="math inline">\(S\)</span> من مصفوفة الوزن المدربة <span class="math inline">\(W\)</span>: <span class="math inline">\( S = \mathit{sign}(W) \)</span>. ومصفوفة القوة <span class="math inline">\(P\)</span> هي لوغاريتم الأساس الثنائي للقيم المطلقة لـ<span class="math inline">\(W\)</span>، أي <span class="math inline">\( P = \log_{2}(|W|) \)</span>. بعد تقريب <span class="math inline">\(P\)</span> إلى أقرب قوة اثنين (<span class="math inline">\( P_{\mathit{r}} = \mathit{round}(P) \)</span>)، تُحسب الأوزان المكممة <span class="math inline">\(\tilde{W}_q\)</span> بتطبيق الإشارة: <span class="math inline">\(\tilde{W}_q = \mathit{flip}(2^{P_{\mathit{r}}}, S)\)</span>. في تقنية القوى الثنائية والإشارة، تُعدل قيم التحويل (<span class="math inline">\( \tilde{P} \)</span>) وقلب الإشارة (<span class="math inline">\( \tilde{S} \)</span>) مباشرة، حيث <span class="math inline">\( \tilde{P} = \mathit{round}(P) \)</span> و<span class="math inline">\( \tilde{S} = \mathit{sign}(\mathit{round}(S)) \)</span>، ثم تُحسب الأوزان <span class="math inline">\(\tilde{W}_{\mathit{ps}} = \mathit{flip}(2^{\tilde{P}}, \tilde{S})\)</span>.</p>
<h2 id="hpo">تحسين المعلمات الفائقة</h2>
<p>تزيد تعقيدات خوارزميات التعلم العميق من الحاجة إلى التحسين المؤتمت للمعلمات الفائقة (<span class="nodecor">HPO</span>) لتعزيز أداء النماذج (<span class="nodecor">DBLP:journals/widm/BischlBLPRCTUBBDL23</span>). لنفترض أننا نملك مجموعة بيانات <span class="math inline">\( \mathcal{D} = \{(x_i,y_i)\}_{i=1}^{N} \in \mathbb{D} \subset \mathcal{X} \times \mathcal{Y} \)</span>، حيث <span class="math inline">\( \mathcal{X} \)</span> هو فضاء المتغيرات و<span class="math inline">\( \mathcal{Y} \)</span> هو فضاء الأهداف، وفضاء تكوين المعلمات الفائقة <span class="math inline">\( \Lambda = \{\lambda_1, \ldots, \lambda_L\} \)</span>. في دراستنا، <span class="math inline">\( \mathcal{M} \)</span> يمثل فضاء النماذج الممكنة لـ <span class="nodecor">DSNN</span>. الخوارزمية <span class="math inline">\( \mathcal{A} : \mathbb{D} \times \Lambda \rightarrow \mathcal{M} \)</span> تُدرّب نموذجاً <span class="math inline">\( M \)</span> استناداً إلى تكوين من <span class="math inline">\( L \)</span> معلمات فائقة مأخوذة من <span class="math inline">\( \Lambda \)</span> على مجموعة التدريب <span class="math inline">\( \mathcal{D}_{\textit{train}} \)</span>. تُقسم البيانات إلى مجموعات التدريب والاختبار والتحقق <span class="math inline">\( \mathcal{D}_{\textit{train}}, \mathcal{D}_{\textit{val}}, \mathcal{D}_{\textit{test}} \)</span>. يُقيَّم أداء النموذج بدالة خسارة <span class="math inline">\( \mathcal{L} : \mathcal{M} \times \mathbb{D} \rightarrow \mathbb{R} \)</span>. هدف الـ <span class="nodecor">HPO</span> هو إيجاد التكوين <span class="math inline">\( \lambda^* \in \Lambda \)</span> الذي يقلل هذه الخسارة:</p>
<pre class="math display">\[\lambda^* \in \argmin_{\lambda \in \Lambda} \mathcal{L}\big(\mathcal{A}(\mathcal{D}_{\textit{train}},\lambda), \mathcal{D}_{\textit{val}}\big).\]</pre>
<p>بذلك يتم تعديل المعلمات الفائقة بناءً على أداء مجموعة التحقق، ثم يُختبر النموذج النهائي على <span class="math inline">\(\mathcal{D}_{\textit{test}}\)</span>.</p>
<h2 id="bo">التحسين البايزي</h2>
<p>التحسين البايزي استراتيجية عالمية لتحسين دوال الخسارة الصندوق الأسود <span class="math inline">\(\mathcal{L}:\mathcal{M}\times\mathbb{D}\to\mathbb{R}\)</span> المكلفة في التقييم (<span class="nodecor">DBLP:journals/jgo/JonesSW98</span>). يستخدم نموذجاً بديلاً احتماليًا <span class="math inline">\(\mathcal{S}\)</span>—عادةً عملية غاوسية أو غابة عشوائية—لتمثيل دالة الخسارة (<span class="nodecor">DBLP:books/lib/RasmussenW06</span>, <span class="nodecor">DBLP:conf/lion/HutterHL11</span>). توجه دالة الاستحواذ <span class="math inline">\(\mathcal{C}:\Lambda\to\mathbb{R}\)</span> البحث عن النقاط التالية الموازنة بين الاستكشاف والاستغلال بناءً على النقاط المُستعلم عنها سابقاً <span class="math inline">\(\{(\lambda_i,\mathcal{L}_i)\}_{i=1}^{m-1}\)</span>. ثم يحدث تحديث النموذج البديل عند تقييم <span class="math inline">\(\mathcal{L}\)</span> في تلك النقاط.</p>
<h2 id="multi-fidelity">تحسين متعدد الدقة</h2>
<p>لتجنب الكلفة العالية في تدريب جميع التكوينات حتى النهاية، نستخدم نهجاً متعدد الدقة (<span class="nodecor">MF</span>) (<span class="nodecor">DBLP:journals/jmlr/LiJDRT17</span>), الذي يوازن بين الأداء وخطأ التقريب (<span class="nodecor">DBLP:books/sp/HKV2019</span>). يخصص هذا النهج عددًا قليلًا من العصور لعدد كبير من التكوينات أولاً، ثم يوجه الميزانية للأفضل منها تدريجياً. رسمياً، نحدد مجموعة دقات <span class="math inline">\(\mathcal{F}\)</span> ونهدف إلى تقليل الوظيفة عالية الدقة <span class="math inline">\(F\in\mathcal{F}\)</span>:</p>
<pre class="math display">\[\min_{\lambda\in\Lambda} F(\lambda)\, .\]</pre>
<p>ثم نقرب <span class="math inline">\(F\)</span> بسلسلة من التقريبات الأقل دقة والأقل تكلفة <span class="math inline">\(\{f_1(\lambda),\ldots,f_j(\lambda)\}\)</span> عبر مستويات متعددة <span class="math inline">\(j\)</span>، حيث يُخصص لكل تقييم ميزانية بناءً على المستوى.</p>
<h2 id="التحسين-متعدد-الأهداف">التحسين متعدد الأهداف</h2>
<p>يعالج التحسين متعدد الأهداف مشكلات تتضمّن أهدافاً متعارضة، مثل تحسين الدقة وتقليل استهلاك الطاقة في شبكات العصبونات العميقة المتغيرة. يهدف إلى إيجاد حلول باريتو مثلى (<span class="nodecor">Deb2014</span>) عبر إضافة نقاط جديدة بناءً على الملاحظات الحالية <span class="math inline">\( \mathcal{D}_\mathit{obs} = \{(\lambda_i,\mathcal{L}(\lambda_i))\}_{i=1}^{n}\)</span>. تشكّل هذه النقاط السطح غير المهيمن <span class="math inline">\( D^\star_n \)</span>، بحيث لا يُمكن تحسين أي هدف دون الإضرار بآخر.</p>
<h1 id="approach">المنهج</h1>
<p>لتعزيز كفاءة شبكات العصبونات العميقة المتغيرة حسابياً عبر التعلم الآلي المؤتمت، نستخدم تحسين متعدد الدقة. نقدم مستويات دقة مختلفة لعملية التدريب بزيادة عدد طبقات التحويل في النموذج تدريجياً. تبدأ العملية بنماذج تحتوي على عدد محدود من طبقات التحويل، ثم يُسمح لها بالزيادة خلال التحسين. نفترض أن هذا يُوجه البحث نحو التكوينات الأعلى أداءً، حيث يوازن عدد أقل من الطبقات بين دقة التمثيل وخطأ التقريب. نحقق ذلك عبر توسيع خوارزمية HyperBand (<span class="nodecor">DBLP:journals/jmlr/LiJDRT17</span>)—التي تعتمد على الإنقاص التدريجي (<span class="nodecor">jamieson-aistats16a</span>)—في إطار متعدد الأهداف.</p>
<p>في قسم الخلفية، شرحنا الإنقاص التدريجي حيث يُدرَّب <span class="math inline">\(n_c\)</span> تكوينات على ميزانية أولية <span class="math inline">\(b_I\)</span>، ثم يُختار أفضل <span class="math inline">\(\nu/(\nu+1)\)</span> منها لميزانية جديدة <span class="math inline">\(\nu b_I/(\nu+1)\)</span>، وهكذا حتى نصل للتكوين الأفضل. تجمع HyperBand عدة أقواس من هذه العملية مع توزيع الميزانية الإجمالية عبرها.</p>
<p>نوسع ذلك ليشمل أيضاً تحسين متعدد الأهداف. نعالج هدفين معاً: دقة النموذج واستهلاك الطاقة. نعرف دالة هدف ثنائية الأبعاد:</p>
<p><span class="math display">\[f_{MO}:\Lambda\to\mathbb{R}^2,\quad f_{MO}(\lambda) = \big(f_{\text{loss}}(\lambda), f_{\text{emission}}(\lambda)\big),\]</span></p>
<p>حيث <span class="math inline">\(f_{\text{loss}}(\lambda)\)</span> تهدف إلى تقليل الخسارة لزيادة الدقة و<span class="math inline">\(f_{\text{emission}}(\lambda)\)</span> تسعى إلى تقليل استهلاك الطاقة أثناء التدريب والاستدلال. نرغب في حل:</p>
<p><span class="math display">\[\argmin_{\lambda\in\Lambda} f_{MO}(\lambda)\, .\]</span></p>
<p>يستخدم <span class="nodecor">SMAC3</span> استراتيجية ParEGO (<span class="nodecor">DBLP:journals/tec/Knowles06</span>) لدمج الأهداف المهتمة عبر معاملات وزن متغيرة في كل تكرار من HyperBand، مما يحول المشكلة متعددة الأهداف إلى سلسلة من مشاكل أحادية الهدف ويعزز تقريب واجهة باريتو.</p>
<h1 id="experiments">التجارب</h1>
<p>في القسم التالي، نعرض الإعداد والمنهجية المستخدمة لتقييم المنهج المقدم في قسم المنهج، مع التركيز على تحسين شبكات العصبونات العميقة المتغيرة عبر التحسين متعدد الدقة ومتعدد الأهداف. نناقش كيفية موازنتنا بين أداء النموذج والأثر البيئي.</p>
<h2 id="eval setup">إعداد التقييم</h2>
<p>قمنا بتدريب وتقييم النماذج على مجموعة بيانات Cifar10 (<span class="nodecor">krizhevsky2009learning</span>) باستخدام وحدات معالجة الرسومات NVIDIA A100. لتحسين المعلمات الفائقة، اعتمدنا على <span class="nodecor">SMAC3</span> (<span class="nodecor">DBLP:journals/jmlr/LindauerEFBDBRS22</span>). ولدمج الأثر البيئي، استخدمنا متتبع انبعاثات <span class="nodecor">CodeCarbon</span> (<span class="nodecor">DBLP:journals/corr/abs-1910-09700</span>, <span class="nodecor">DBLP:journals/corr/abs-1911-08354</span>) لرصد استهلاك الطاقة وانبعاثات الكربون بوحدة <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>. اخترنا شبكة ResNet20 المدربة مسبقاً (<span class="nodecor">DBLP:conf/cvpr/HeZRS16</span>).</p>
<h2 id="results">النتائج</h2>
<p>يوضح الشكل [fig:results] أعلى دقة اختبار لـ DSNN بالتكوين الافتراضي (الجدول [table:model_config]) مقارنةً بدقة DSNN المكوَّنة عبر <span class="nodecor">SMAC3</span> مع <span class="nodecor">MF</span> باستخدام HyperBand وطبقات التحويل المتفاوتة. الحل الأمثل باريتو رقم <span class="nodecor">1</span> من الجدول [table:model_config] تم تقييمه من بين خمسين تكويناً. رغم بعض التقلبات، يتجاوز أداء النموذج المحسّن دقة النموذج الافتراضي بما لا يقل عن ثلاثة بالمئة، مما يؤكد نجاح نهج <span class="nodecor">MF</span>.</p>
<p>يدعم ذلك افتراضنا بأن النماذج التي تُقيَّم تحت مستويات دقة مبسطة تحافظ على تحسن الأداء عند إضافة المزيد من طبقات التحويل.</p>
<p>يعرض الجدول [table:model_config] حلَين أمثلين باريتو من بين <span class="nodecor">33</span> تكويناً. الحل رقم <span class="nodecor">1</span> يحقق دقة اختبار <span class="nodecor">83.50%</span> مع <span class="nodecor">0.1661</span> <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>. والحل رقم <span class="nodecor">2</span> يحقق دقة <span class="nodecor">84.67%</span> مع <span class="nodecor">0.1673</span> <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>—تم تدريبهما لحقب زمنية أقل بسبب قيود الحساب. كلا التكوينين يوازن بين أداء عالٍ وانبعاثات منخفضة.</p>
<p>تتضمن المعلمات النوعية عدد بتات التنشيط والانزلاق العائم وعمق طبقات التحويل. في الحل رقم <span class="nodecor">1</span> عدد أقل من طبقات التحويل مع تعداد محدود من البتات لتمثيل الأوزان، مما يخفض الطلب الحسابي مع الحفاظ على انبعاثات مماثلة للحل رقم <span class="nodecor">2</span> الذي يستخدم عدداً أكبر من طبقات ولكن بتات أكثر. يظهر هذا التوازن أن التحكم في عدد طبقات التحويل وعدد البتات يؤدي إلى تكوينات متوازنة عالية الأداء وفعّالة في استهلاك الطاقة، مما يدعم جدوى نهج <span class="nodecor">MFMO</span>.</p>
<h1 id="conclusion">الخلاصة</h1>
<p>قدّمنا في هذا العمل منهجاً أخضر للتعلم الآلي المؤتمت يستهدف تحسين مستدام لشبكات العصبونات العميقة المتغيرة عبر إطار تحسين متعدد الدقة ومتعدد الأهداف. نسعى من خلاله لتحقيق التوازن الحرج بين قدرات التعلم العميق والاستدامة البيئية. بدمج أثر الطاقة كهدف، نجحنا في توجيه تحسين المعلمات الفائقة بين دقة عالية لاستهلاك منخفض للموارد.</p>
<p>أظهرت نتائجنا إمكانات النهج في تحسين شبكة عصبونات عميقة متغيرة لتحقيق دقة عالية وتقليل الانبعاثات. قدمنا فضاء تكوين شامل، ونهجاً أخضر لتطوير النموذج، ورؤى بشأن قرارات التصميم. في المستقبل، سنوسع التطبيق ليشمل معايير إضافية وهياكل متنوعة للتحقق من عمومية المنهج وضبط تداخل ParEGO وHyperBand بطرق أكثر فعالية، فضلاً عن دراسة أنواع الدقة المختلفة وخوارزميات متعددة الأهداف لتحقيق مزيد من التخفيضات في الانبعاثات.</p>
<h3 id="الشكر-والتقدير" class="unnumbered">الشكر والتقدير</h3>
<p>لقد دعم هذا العمل وزارة البيئة الاتحادية الألمانية لحماية الطبيعة والسلامة النووية وحماية المستهلك (مشروع GreenAutoML4FAS رقم <span class="nodecor">67KI32007A</span>).</p>
</body>
</html>
```
**ملاحظات:**
- تم تحسين الخطوط والألوان والخلفيات والعناوين والقوائم والجداول والرياضيات لتكون أكثر وضوحًا وجاذبية.
- تم الحفاظ على النص الأصلي بالكامل دون أي تغيير في الكلمات أو المحتوى.
- تم التأكد من خلو الكود من الأخطاء وأنه كامل ومتوافق مع HTML5.