<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Cevat V. Karadağ وَ Nezih Topaloğlu">
  <title>تَدْرِيبِ الشَبَكَةِ العَصَبِيَّةِ المُقَسَّمَةِ بِاِسْتِخْدامِ العَلاماتِ الوَسِيطَة الاِصْطِناعِيَّةِ</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">تَدْرِيبِ الشَبَكَةِ العَصَبِيَّةِ المُقَسَّمَةِ بِاِسْتِخْدامِ العَلاماتِ الوَسِيطَة الاِصْطِناعِيَّةِ</h1>
<p class="author"><span class="nodecor">Cevat V. Karadağ</span> وَ <span class="nodecor">Nezih Topaloğlu</span></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>الانتشار الواسع لهياكل الشبكات العصبية، وخاصة نماذج التعلم العميق، يمثل تحدياً من حيث التدريب المكثف للموارد. أصبحت قيود ذاكرة وحدة معالجة الرسومات عائقاً رئيسياً في تدريب هذه النماذج الكبيرة. تقدم الاستراتيجيات الحالية، بما في ذلك التوازي في البيانات، والتوازي في النموذج، والتوازي في الأنابيب، والتوازي الكامل في البيانات المجزأة، حلولاً جزئية. التوازي في النموذج، على وجه الخصوص، يمكّن من توزيع النموذج بالكامل عبر وحدات معالجة الرسومات متعددة، لكن التواصل بين هذه الأقسام يبطئ عملية التدريب. بالإضافة إلى ذلك، يُثقل عبء الذاكرة اللازم لتخزين المعلمات المساعدة على كل وحدة معالجة الرسومات المتطلبات الحسابية. بدلاً من استخدام النموذج بالكامل للتدريب، تقترح هذه الدراسة تقسيم النموذج عبر وحدات معالجة الرسومات وتوليد العلامات الوسيطة الاصطناعية لتدريب الأجزاء الفردية. تساعد هذه العلامات، التي تُنتج من خلال عملية عشوائية، في تخفيف العبء على الذاكرة والحمل الحسابي. يؤدي هذا النهج إلى عملية تدريب أكثر كفاءة تقلل من التواصل مع الحفاظ على دقة النموذج. للتحقق من هذه الطريقة، يتم تقسيم شبكة عصبية متصلة بالكامل مكونة من <span class="nodecor">6</span> طبقات إلى جزأين ويتم تقييم أدائها على مجموعة بيانات <span class="nodecor">MNIST</span> الموسعة. تشير النتائج التجريبية إلى أن النهج المقترح يحقق دقة اختبار مماثلة لطرق التدريب التقليدية، مع تقليل كبير في متطلبات الذاكرة والحساب. تساهم هذه الأعمال في التخفيف من كثافة الموارد اللازمة لتدريب الشبكات العصبية الكبيرة، مما يمهد الطريق لتطوير نماذج تعلم عميق أكثر كفاءة.</p>
<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>في السنوات الأخيرة، ظهرت الشبكات العصبية، وخاصة نماذج التعلم العميق، كأدوات قوية لحل المهام المعقدة في مختلف المجالات. لقد نمت هذه الشبكات بشكل كبير في الحجم والتعقيد، مما أتاح إنجازات في مجالات مثل التعرف على الصور، ومعالجة اللغات الطبيعية، وتوليد الكلام. ومع ذلك، فقد أدى هذا النمو أيضاً إلى تحدٍّ كبير للمستخدم العادي: تدريب الشبكات العميقة والكبيرة جداً. على سبيل المثال، تتطلب نماذج اللغة مثل GPT-3، التي تحتوي على مليارات المعاملات، موارد حسابية كبيرة وخبرة متخصصة لتدريبها بفعالية. بشكل خاص، تشكل متطلبات ذاكرة وحدة معالجة الرسومات (GPU) للشبكات الكبيرة عقبة في تدريبها.</p>
<p>مع الاعتراف بأهمية موارد الذاكرة المحدودة في التعلم العميق، كان الباحثون والممارسون يستكشفون بنشاط مناهج مختلفة لجعل هذه التكنولوجيا أكثر سهولة في متناول المستخدمين (<span class="nodecor">Sutton09,Shiram19,Fu2021,Yao2018,Meng2017TrainingDM</span>). إحدى الطرق البسيطة والفعالة هي تقليل دقة معاملات النموذج. بدلاً من استخدام تنسيق النقطة العائمة بدقة <span class="nodecor">32</span>-بت، يمكن استخدام <span class="nodecor">BFLOAT16</span>، الذي يستهلك <span class="nodecor">16</span> بت (<span class="nodecor">8</span> بت للأس و<span class="nodecor">7</span> بت للكسر) (<span class="nodecor">URL_BFLOAT16</span>). على الرغم من أن هذا النهج يقلل من استهلاك الذاكرة إلى النصف، إلا أنه يأتي بدقة أقل لمعاملات النموذج. بالإضافة إلى ذلك، فإن التحسين الذي يوفره محدود: استخدام أنواع البيانات ب<span class="nodecor">8</span>-بت، على سبيل المثال، سيؤدي إلى تدهور كبير في الدقة.</p>
<p>إحدى الطرق الواعدة التي تقلل من استهلاك ذاكرة وحدة معالجة الرسومات تتضمن تقسيم البيانات، أو النموذج، أو الأنابيب، وتسمى التوازي (<span class="nodecor">jia2018data</span>). يتضمن التوازي البياني (<span class="nodecor">DP</span>) تقسيم دفعات التدريب إلى مجموعات فرعية أصغر وتوزيعها عبر مستخدمين أو أجهزة متعددة، مما يسمح بالتدريب التعاوني دون الحاجة إلى بنية تحتية مركزية. من ناحية أخرى، يركز التوازي النموذجي (<span class="nodecor">MP</span>) على تقسيم النموذج عمودياً إلى مكونات أصغر يمكن تدريبها بشكل مستقل ثم دمجها لتشكيل شبكة أكبر. يتيح هذا النهج التدريب الموزع، حيث يمكن معالجة أجزاء مختلفة من الشبكة في وقت واحد بواسطة أجهزة أو أفراد مختلفين. التوازي الأنبوبي (<span class="nodecor">PP</span>)، من ناحية أخرى، يقسم الشبكة أفقياً على وحدات حوسبة مختلفة. لقد حظيت جميع طرق التوازي بالكثير من الاهتمام وتعتبر مواضيع ساخنة في سعيها لتقليل متطلبات الحوسبة والذاكرة لتدريب التعلم العميق، حيث تقدم حلولاً محتملة للتخفيف من التحديات التي تفرضها تدريب الشبكات العميقة والكبيرة جداً.</p>
<p>في التوازي البياني، يفترض أن النموذج يتناسب ضمن ذاكرة إحدى وحدات معالجة الرسومات. يتم نسخ النموذج ومعاملات التدريب إلى وحدات معالجة الرسومات متعددة ويتم تقسيم البيانات إلى عدد الوحدات المتاحة. ثمّ تُجرى العمليات الأمامية والخلفية بشكل متوازٍ في كل وحدة بمعزل عن غيرها. ثمّ تُزامن التدرجات المحسوبة في كل وحدة معالجة الرسومات، وتُحدّث معاملات النموذج فيها وفقاً لذلك. بينما هذه الطريقة فعالة من حيث الحساب، فإنها لا تقلل من متطلبات الذاكرة لكل جهاز. يتم استخدام التوازي البياني بنشاط في PyTorch ويعرف باسم التوازي البياني الموزع (<span class="nodecor">DDP</span>) (<span class="nodecor">URL_DDP</span>). يمكن العثور على تحقيق شامل لـ<span class="nodecor">DDP</span> في PyTorch في (<span class="nodecor">Li20</span>). على الرغم من أن <span class="nodecor">DDP</span> هو حل متعدد الاستخدامات عندما يتناسب النموذج داخل وحدة معالجة الرسومات، إلا أنه لا ينطبق بمفرده عندما يكون حجم النموذج أكبر من ذاكرة وحدة معالجة الرسومات.</p>
<p>يمكن أن يكون التوازي الأنبوبي مفيداً عندما يمكن تقسيم النموذج أفقياً، مثل في تطبيقات معالجة الصور. مثال شهير هو G-Pipe، الذي اقترحه Huang <span class="nodecor">et al</span>. (<span class="nodecor">huang2019gpipe</span>). اقترحوا استراتيجية توازي أنبوبي للدفعات الصغيرة، من خلال تقسيم الدفعة الكبيرة إلى دفعات صغيرة متساوية. في نهاية كل دفعة صغيرة، يتم تجميع التدرجات وتطبيقها لتحديث معاملات النموذج. أحد العيوب الرئيسية للتوازي الأنبوبي هو فقاعة الأنبوب، بسبب عدم التوازن في العمل أو التبعيات بين الأنابيب. بالإضافة إلى ذلك، تتطلب عملية التطبيع الدفعي عبر الأنابيب اهتماماً خاصاً.</p>
<p>التوازي النموذجي هو استراتيجية رئيسية عندما يكون حجم النموذج ومعاملات التدريب، مثل التدرجات، وحالات المحسن، والمتغيرات المؤقتة الأخرى أكبر من ذاكرة وحدة معالجة الرسومات (<span class="nodecor">Castello19,Wanwu2022</span>). اقترح Rajbhandari <span class="nodecor">et al</span>. طريقة لتقسيم النموذج ومعاملات التدريب، مدفوعة بتحقيق عدم تداخل البيانات بين وحدات معالجة الرسومات (<span class="nodecor">rajbhandari2020zero</span>). تم تحسين الطريقة مؤخراً كتوازي البيانات المقسمة بالكامل (<span class="nodecor">FSDP</span>)، مع تنفيذ في PyTorch (<span class="nodecor">zhao2023pytorch</span>). مرة أخرى، يتم تقسيم البيانات بين وحدات معالجة الرسومات، ولكن هذه المرة يتم أيضاً تقسيم النموذج ومعاملات التدريب، مع عدم وجود تداخل. بعد إجراء التمريرات الأمامية والخلفية بطريقة متوازية، تتم مزامنة التدرجات بشكل مركزي ويتم بعد ذلك تنفيذ تحديث النموذج. مؤخراً، اقترح Mlodozenie <span class="nodecor">et al</span>. التوازي النموذجي والبياني، حيث يتم تحسين كل قسم من النموذج لشرائح بيانات محددة (<span class="nodecor">mlodozeniec2023</span>). تم تعريف دالة خسارة بناءً على شرائح البيانات التي لم ترها الشبكة الفرعية. اقترح Akintoye <span class="nodecor">et al</span>. تقسيم النموذج طبقياً (<span class="nodecor">akintoye2022</span>)، لتقليل النفقات العامة للاتصال بين الأجهزة وتكلفة الذاكرة أثناء التدريب. يتم تقسيم الطبقات باستخدام وحدات معالجة الرسومات ثم دمجها. التقسيم النموذجي شائع أيضاً بين الشبكات العصبية الرسومية (<span class="nodecor">liao2018graph</span>) وضمن الأنظمة التي تشمل أجهزة الحافة، مثل شبكات الأشياء (<span class="nodecor">IoT</span>) (<span class="nodecor">Na22,Oliveira19,parthasarathy2023</span>).</p>
<p>بخلاف التوازي البياني والنموذجي والأنبوبي، اقترح الباحثون طرقاً أخرى لتقليل الطلب على الذاكرة لتدريب الشبكات العصبية. اقترح Jain <span class="nodecor">et al</span>. تشفير خرائط الميزات لتوفير الذاكرة (<span class="nodecor">Jain18</span>). اقترح Wang <span class="nodecor">et al</span>. الخلايا العصبية الفائقة التي تتميز بتحسين الذاكرة من خلال تخصيص الذاكرة بشكل ديناميكي لمساحات العمل التحويلية (<span class="nodecor">Wang18</span>). الطلب على الذاكرة لطريقة التحسين التكيفية كبير أيضاً. قامت بعض الدراسات بتقليل هذه البصمة الذاكرية من خلال تبسيط نماذج المحسن (<span class="nodecor">anil19memoryefficient,shazeer2018adafactor</span>). تحمل هذه الطرق خطر التأثير على تقارب النموذج.</p>
<p>في هذه الدراسة، تم تطوير منهجية تدريب شبكة عصبية جديدة تُسمى تدريب الشبكة العصبية المقسمة (<span class="nodecor">PNN</span>). يتم تقسيم الشبكة إلى شبكتين فرعيتين أو أكثر. بدلاً من تدريب الشبكة بالكامل، يتم تدريب هذه الأقسام بشكل منفصل باستخدام البيانات الاصطناعية وبيانات التدريب الأصلية. نظراً لأن كل قسم يتم تدريبه بشكل منفصل، يتم القضاء على التواصل لمخرجات التغذية الأمامية والتدرجات إلى أقسام أخرى أو جهاز مضيف. وبالتالي، يتم تقليل النفقات العامة للاتصال بشكل كبير مقارنة بطرق التوازي النموذجي الأخرى. بالإضافة إلى ذلك، يسمح التدريب المنفصل للأقسام بضبط معلمات التدريب الفائقة لكل قسم. يتيح هذا النهج تقليل الطلب الحسابي الإجمالي مع الحفاظ على الدقة.</p>
<h1 id="التدريب-المنفصل-لأقسام-النموذج">التدريب المنفصل لأقسام النموذج</h1>
<p>يساعد التدريب المنفصل لأقسام النموذج أيضاً في التخفيف من مشكلة تلاشي التدرجات، التي من المتوقع حدوثها أكثر في الشبكات العصبية العميقة (<span class="nodecor">Kolbusz2017vanishing</span>).</p>
<h1 id="الطريقة-المقترحة">الطريقة المقترحة</h1>
<p>تعتمد الطريقة على الملاحظة بأن أوزان الطبقات المتوسطة في الشبكة العصبية تظهر بعض العشوائية بطبيعتها. تنبع هذه العشوائية من التهيئة العشوائية الأولية للأوزان داخل الشبكة العصبية، بالإضافة إلى الطبيعة التكرارية لعملية التدريب (<span class="nodecor">Maennel20</span>). ونتيجة لذلك، تختلف قيمة الأوزان في الطبقة المتوسطة وستعتمد على الأوزان العشوائية الأولية (<span class="nodecor">franchi2021tradi</span>). لذا، نفترض أنه عند تقسيم شبكة، يمكن تدريب القطاعات الفردية بشكل مستقل باستخدام تسميات صناعية أو خرائط ميزات تم إنشاؤها من خلال عملية عشوائية.</p>
<p>لإظهار الطبيعة العشوائية لمعاملات الشبكة بعد التدريب، يتم تدريب شبكة متصلة بالكامل بشكل متكرر باستخدام مجموعة بيانات المعهد الوطني للمعايير والتكنولوجيا (<span class="nodecor">deng2012mnist</span>). تحتوي الشبكة على ثلاث طبقات، بعدد الخلايا العصبية في كل طبقة كما يلي: 100، 50 و10. في كل خطوة، يتم إعادة تهيئة الأوزان بشكل عشوائي من خلال إعادة إنشاء الشبكة. تهيئة الوزن هي التهيئة الافتراضية في PyTorch (<span class="nodecor">URL_torch_init_linear</span>). يتم التدريب في 15 دورة تدريبية بحجم دفعة قدره 256، ويتم حفظ النموذج بعد كل دورة. بعد إجراء هذا الإجراء التدريبي الكامل 300 مرة، يتم رسم الرسم البياني لثلاث معاملات: الحد الأقصى، الحد الأدنى والفرق بينهما من أوزان الطبقة المتوسطة. تظهر الرسومات أنه حتى بعد التدريب، لا تزال الأوزان تحتوي على بعض العشوائية، بسبب الأوزان الأولية العشوائية عند بدء الشبكة. وبالتالي، يجب أن تظهر نتائج التنشيط المتوسطة للشبكة أيضاً كمية كبيرة من العشوائية، ومن المتوقع أنه يمكن تقسيم الشبكة وتدريبها باستخدام تسميات وسيطة صناعية، يتم إنشاؤها بواسطة عملية عشوائية.</p>
<p>تبدأ الطريقة بتقسيم النموذج بالكامل إلى شبكتين فرعيتين، تعرفان بالقسم الأيسر والقسم الأيمن. على الرغم من أن التقسيم إلى أكثر من شبكتين فرعيتين ممكن، إلا أن الطريقة موضحة على افتراض أنها مقسمة إلى شبكتين فرعيتين للبساطة. يتم تدريب القسم الأيسر أولاً دون استخدام القسم الأيمن. لتحقيق ذلك، يتم إنشاء التسميات بشكل صناعي للقسم الأيسر. تُسمى هذه التسميات بالتسميات الوسيطة الصناعية (SIL). بافتراض أن عدد الخلايا العصبية في الطبقة النهائية للقسم الأيسر هو <span class="math inline">\(N_P\)</span> وعدد الفئات هو <span class="math inline">\(M\)</span>، يتم إنشاء <span class="math inline">\(M\)</span> متجهات بحجم <span class="math inline">\([N_P\times1]\)</span>. يمكن تمثيل التسميات الوسيطة الصناعية بواسطة مصفوفة N-by-M (<span class="math inline">\(SIL \in \mathbb{R}^{N_P\times M} \)</span>)، حيث يمثل كل عمود فئة. يتم إنشاء عناصر المصفوفة بشكل عشوائي باستخدام التوزيع الموحد <span class="math inline">\((0,1)\)</span> وتتم معايرتها باستخدام معامل <span class="math inline">\(\kappa\)</span>. في الصياغة الرياضية: <span class="math display">\[\label{eq:SIL}
    SIL_{i,j} \sim \kappa \, U(0,1)\]</span> حيث <span class="math inline">\(U(0,1)\)</span> هو التوزيع الموحد <span class="math inline">\((0,1)\)</span> و<span class="math inline">\(i \in {1,2,...,N_P}\)</span> و<span class="math inline">\(j \in {1,2,...,M}\)</span> هما مؤشرا الصف والعمود.</p>
<p>باستخدام المدخلات الأصلية لمجموعة البيانات التدريبية وSIL، يتم تدريب القسم الأيسر على مدى <span class="math inline">\(N_L\)</span> دورات تدريبية دون خلط المدخلات. في هذه المرحلة، لا يتم استخدام القسم الأيمن إطلاقاً. بعد <span class="math inline">\(N_L\)</span> دورات تدريبية، يُنهى تدريب القسم الأيسر ويُخزن الإخراج النهائي له (الاستجابة في الدورة الأخيرة).</p>
<p>في المرحلة الثانية، يُدرَّب القسم الأيمن مدخلاً عليه الإخراج النهائي للقسم الأيسر المخزون مسبقاً، مع استخدام تسميات مجموعة البيانات الأصلية. يتم التدريب على مدى <span class="math inline">\(N_R\)</span> دورات تدريبية، ثم تكتمل عملية التدريب ويصبح بالإمكان دمج الأقسام لاستخدام الشبكة.</p>
<p>يمكن توسيع الطريقة بسهولة للحالات التي يتم فيها تقسيم النموذج إلى أكثر من شبكتين فرعيتين. يتم توضيح هذا الخوارزم الشامل في الرسم البياني. في هذا السيناريو، من الضروري وجود تسمية وسيطة صناعية مميزة لكل طبقة وسيطة. تظل فوائد الطريقة المقترحة قابلة للتطبيق.</p>
<p>يمكن أيضاً الانفصال عن الطابع التسلسلي للتدريب وتدريب كل شبكة فرعية بشكل متزامن باستخدام التسميات الوسيطة الصناعية كمدخلات وتسميات. تتضمن هذه الهندسة أقساماً وسيطة يتم تدريبها بواسطة المدخلات والتسميات التي تم إنشاؤها بواسطة عملية عشوائية. أُثبت سابقاً أنه من الممكن تدريب شبكة عصبية باستخدام بيانات عشوائية بخسارة تدريب صفرية، شريطة أن يكون عدد الدورات التدريبية وعدد المعاملات كافيين (<span class="nodecor">zhang2017understanding</span>). ومع ذلك، تتطلب هذه الطريقة العديد من الدورات لكل شبكة فرعية لتحقيق مستوى مقبول من الدقة، مما يزيد بشكل كبير من الحمل الحسابي، مما يجعلها غير عملية.</p>
<h2 id="المزايا-مقارنة-بالتوازي-النموذجي-القياسي">المزايا مقارنة بالتوازي النموذجي القياسي</h2>
<p>يتطلب التوازي النموذجي التقليدي مقداراً كبيراً من الاتصالات بين الأجهزة. على سبيل المثال، في عقدة تحتوي على وحدات معالجة الرسومات مترابطة، تكون الاتصالات من الجهاز إلى المضيف ومن المضيف إلى الجهاز ضرورية لنقل الاستجابات والتدرجات ومعاملات النموذج المحدثة (<span class="nodecor">Jain20</span>, <span class="nodecor">zhuang2022optimizing</span>). مع زيادة عدد وحدات معالجة الرسومات، يزداد الحمل الزائد للاتصالات، مما يفرض حداً أعلى على الأداء الكلي (<span class="nodecor">rajbhandari2020zero</span>).</p>
<p>يسهل النهج المقترح تدريب كل قسم داخل وحدة معالجة الرسومات الخاصة به. يقتصر الحمل الزائد للاتصالات أثناء التدريب على نقل مخرجات القسم السابق، التي تعمل كمدخلات للقسم الحالي قيد التدريب. تساعد هذه الاستراتيجية في تقليل الحمل الزائد للاتصالات، مما يقدم نهجاً فعالاً ودقيقاً للتدريب.</p>
<p>تنطبق حالة مماثلة على ذاكرة الكاش L3 مؤخراً، نظراً لتوفر سعات كبيرة من ذاكرة الكاش L3 على معالجات مثل AMD-EPYC-9684X بذاكرة كاش <span class="nodecor">1152</span> ميغابايت (<span class="nodecor">URL_epyc</span>). بخلاف التوازي النموذجي، فإن الطريقة المقترحة تسلسلية. يمكن تطبيق هذه الطريقة حتى عندما يكون جهاز واحد فقط متاحاً بذاكرة أصغر من حجم النموذج. في المقابل، سيكون التوازي النموذجي القياسي بطيئاً بشكل ملحوظ في مثل هذه الظروف.</p>
<p>يسمح التدريب المنفصل لكل قسم بتخصيص معايير تدريب خاصة بكل منهما، مثل حجم الدفعة وعدد الدورات التدريبية ومعدل التعلم. على سبيل المثال، بدلاً من إخضاع الشبكة بالكامل لـ<span class="nodecor">40</span> دورة تدريبية، يمكن تخصيص <span class="nodecor">5</span> دورات فقط للقسم الأيسر و<span class="nodecor">80</span> دورة للقسم الأيمن. يوضح القسم [sec:results] أن دقة القسم الأيسر تتقارب خلال عدد محدود من الدورات، مما يبرز ميزة هذه الاستراتيجية.</p>
<h1 id="تنفيذ-على-الشبكات-المتصلة-بالكامل">تنفيذ على الشبكات المتصلة بالكامل</h1>
<p>يتم تطبيق الخوارزمية على شبكة تصنيف متصلة بالكامل. تُستخدم مجموعة بيانات الأحرف الموسعة المتوازنة (EMNIST) (<span class="nodecor">cohen2017emnist</span>)، والتي تشمل <span class="nodecor">47</span> فئة، بما في ذلك الأرقام والحروف الكبيرة والصغيرة. المدخلات عبارة عن صور بالأبيض والأسود بحجم <span class="math inline">\(28\times28\)</span>، وتُسطح إلى متجهات بحجم <span class="math inline">\(784\times 1\)</span>.</p>
<p>الشبكة الأساسية (غير المقسمة) هي شبكة متصلة بالكامل مكونة من ست طبقات مع تحيز. تبدأ بطبقة المدخلات بحجم <span class="nodecor">784</span>، ويكون عدد الخلايا العصبية في كل طبقة <span class="nodecor">80</span>، <span class="nodecor">60</span>، <span class="nodecor">60</span>، <span class="nodecor">60</span> و <span class="nodecor">47</span>. تتم عملية التقسيم عند الطبقة الثالثة. وبالتالي، فإن القسم الأيسر يحتوي على <span class="nodecor">140</span> خلية عصبية بينما يحتوي القسم الأيمن على <span class="nodecor">167</span> خلية عصبية. ومع ذلك، نظراً لأن حجم المدخلات هو <span class="nodecor">784</span>، فإن عدد المعاملات في القسم الأيسر أكبر بكثير: <span class="math inline">\((784+1)\times80 + (80+1)\times60 = 67660\)</span> معلمة في القسم الأيسر و <span class="math inline">\((60+1)\times60 + (60+1)\times60 + (60+1)\times47 = 10187\)</span> معلمة في القسم الأيمن. يتناسب عدد عمليات الضرب والتجميع (MACs) مع عدد المعاملات في شبكة متصلة بالكامل. باستخدام مكتبة عداد MACs (<span class="nodecor">ptflops</span>)، يتم حساب MACs للقسمين الأيسر والأيمن على التوالي بـ <span class="nodecor">67800</span> و <span class="nodecor">10307</span>. وبالتالي، فإن تدريب القسم الأيسر أكثر كثافة من الناحية الحسابية مقارنة بتدريب الجزء الأيمن.</p>
<p>يتم توليد التسميات الوسيطة الاصطناعية باستخدام المعادلة [eq:SIL]، مع <span class="math inline">\(\kappa=10\)</span>. حجم التسميات الوسيطة الاصطناعية هو <span class="math inline">\(60\times47\)</span> حيث <span class="nodecor">47</span> هو عدد التسميات و <span class="nodecor">60</span> هو عدد الخلايا العصبية في طبقة التقسيم. يوجد <span class="nodecor">112800</span> صورة في مجموعة بيانات التدريب. التسمية لكل صورة مدخلة هي متجه بحجم <span class="math inline">\(60\times1\)</span> مأخوذ من مصفوفة التسميات الوسيطة الاصطناعية. نظراً لأن المدخلات لا يتم خلطها أثناء التدريب، فإن التسميات المطلوبة لتدريب القسم الأيسر (<span class="nodecor">112800</span> تسمية بحجم <span class="math inline">\(60\times1\)</span>) تُرتب وتُحمَّل إلى وحدة معالجة الرسومات كدفعات. تُستخدم طريقة التدرج العشوائي مع معدل تعلم <span class="nodecor">0.01</span> وزخم <span class="nodecor">0.9</span> للتحسين. الدالة التنشيطية في كل طبقة هي وحدة الخط المستقيم المعدلة (ReLU)، باستثناء الطبقة النهائية حيث تُستخدم دالة الهوية. يتم تحديد حجم الدفعة بـ <span class="nodecor">1410</span>. تم تنفيذ العمليات على بطاقة الرسومات AMD Radeon RX 7600.</p>
<h1 id="sec:results">النتائج والمناقشة</h1>
<p>في هذا القسم، يتم فحص دقة الطريقة المقترحة كدالة للحساب، مقارنة بالتدريب الأساسي. بالنسبة للتدريب الأساسي، يتم استخدام نفس مجموعة البيانات لتدريب الشبكة ذات الطبقات الست غير المقسمة، مع استخدام نفس معايير التحسين وحجم الدفعات.</p>
<p>يتم حساب الحمل الحسابي باستخدام عمليات الضرب والجمع المتراكمة للأقسام الأيسر والأيمن والنموذج غير المقسم. تقدم الطريقة المقترحة تكاليف اتصال إضافية نظراً لنقل العلامة الوسيطة الاصطناعية إلى وحدة معالجة الرسومات ونقل مخرجات القسم الأيسر كمدخلات للقسم الأيمن. يحدث كلا النقلين مرة واحدة فقط وبالتالي تم استثناؤهما من التحليل.</p>
<p>تظهر النتائج لسيناريو محدد في الشكل [fig:general_result]. عدد الدورات التدريبية لكل من القسم الأيسر <span class="nodecor"><span class="math inline">\(N_L = 5\)</span></span>، والقسم الأيمن <span class="nodecor"><span class="math inline">\(N_R = 160\)</span></span>، والنموذج الأساسي <span class="nodecor"><span class="math inline">\(N_B = 40\)</span></span>. يتم ضبط معامل الضرب <span class="nodecor"><span class="math inline">\(\kappa\)</span></span> على <span class="nodecor">10</span>. تم تنفيذ التدريب لكل من النموذج الأساسي والشبكة العصبية المقسمة <span class="nodecor">10</span> مرات، مع توضيح المتوسط بالعلامات الدائرية ونطاق الانحراف المعياري (<span class="nodecor"><span class="math inline">\(68\%\)</span></span>) بشرائط الخطأ.</p>
<p>تظهر النتائج بوضوح أن الطريقة المقترحة تحقق دقة اختبار مماثلة للتدريب التقليدي (<span class="nodecor">71.5%</span> للشبكة العصبية المقسمة و <span class="nodecor">76.2%</span> للنموذج الأساسي) مع استخدام قوة حسابية أقل بشكل ملحوظ. على الرغم من أن القسم الأيمن خضع لـ<span class="nodecor">160</span> دورة تدريبية، فإن دقته تتقارب خلال عدد قليل من الدورات. نقطة أخرى جديرة بالملاحظة هي أن القسم الأيسر، الذي تم تدريبه لمدة <span class="nodecor">5</span> دورات فقط، يساهم بشكل كبير في تقليل الحمل الحسابي الإجمالي.</p>
<p>تشير النتائج إلى إمكانية ضبط معلمات مثل عدد الدورات التدريبية ومعامل الضرب <span class="nodecor"><span class="math inline">\(\kappa\)</span></span> لتحسين كفاءة ودقة التدريب. تأثير <span class="nodecor"><span class="math inline">\(N_L\)</span></span> موضح في الشكل [fig:effect_of_NL]. يتم رسم دقة الاختبار للشبكة العصبية المقسمة كدالة لـ <span class="nodecor"><span class="math inline">\(N_L\)</span></span> للقيمتين <span class="nodecor"><span class="math inline">\(\kappa = 2\)</span></span> و <span class="nodecor"><span class="math inline">\(\kappa = 10\)</span></span>، بينما تظل المعلمات الأخرى ثابتة. تؤكد النتائج أن حوالي <span class="nodecor">5</span> دورات للقسم الأيسر كافية لتحقيق نتائج متقاربة، ويزداد تأثير <span class="nodecor"><span class="math inline">\(N_L\)</span></span> عند زيادة <span class="nodecor"><span class="math inline">\( \kappa\)</span></span>. يمثل التدريب غير المتناسق للقسمين (<span class="nodecor"><span class="math inline">\(N_L = 5\)</span></span> و <span class="nodecor"><span class="math inline">\(N_R = 160\)</span></span>) ميزة مميزة للشبكة العصبية المقسمة. من خلال تحسين عدد الدورات والمعلمات الأخرى لكل قسم، يمكن تقليل الوقت الحسابي الإجمالي بشكل كبير، وهو ما لا يمكن تحقيقه في التوازي النموذجي التقليدي.</p>
<h1 id="تعزيز-الدقة-من-خلال-مراحل-التعافي">تعزيز الدقة من خلال مراحل التعافي</h1>
<p>على الرغم من أن الطريقة المقترحة توفر مزايا كبيرة، إلا أن النتائج تشير إلى أن دقة الاختبار أقل قليلاً من دقة النموذج الأساسي. يمكن التخفيف من هذا العيب جزئياً من خلال مواصلة التدريب بعد الانتهاء من طريقة التدريب المقترحة. في هذه المرحلة، يتم تدريب القسم الأيسر، الذي تلقى عدداً قليلاً فقط من الدورات، لعدة دورات إضافية بينما تُجمَّد أوزان القسم الأيمن. تشبه هذه الطريقة نوعاً من التعلم النقلي، وتحسن الدقة.</p>
<p>توضح النتائج دقة الاختبار عند تطبيق مرحلة التعافي لمدة <span class="nodecor">10</span> دورات إضافية بالتكوين نفسه (المبيّن في الشكل المحذوف). بلغت دقة الاختبار المتوسطة <span class="nodecor">72%</span> بعد نهاية تدريب القسم الأيمن (الموضحة بالمنطقة البنية)، وارتفعت إلى <span class="nodecor">77.5%</span> بعد مرحلة التعافي (الموضحة بالمنطقة الوردية). بهذه الطريقة، لا توفر الطريقة فوائد كفاءة أولية فحسب، بل توفر أيضاً مساراً لتحسين الدقة من خلال التدريب الممتد.</p>
<h1 id="الاستنتاجات-والآفاق-المستقبلية">الاستنتاجات والآفاق المستقبلية</h1>
<p>في الختام، يقدم هذا البحث منهجية جديدة رائدة لمواجهة التحديات التي تفرضها تدريب الشبكات العصبية الضخمة. من خلال الجمع بين التوازي في النموذج والتسميات الوسيطة الاصطناعية، تم تطوير نهج يعزز كفاءة التدريب بشكل كبير دون التضحية بدقة النموذج.</p>
<p>تؤكد التجارب على مجموعة بيانات <span class="nodecor">EMNIST</span> فعالية النهج المقترح. تقسيم شبكة عصبية مكونة من <span class="nodecor">6</span> طبقات إلى أجزاء، مقترناً بالتسميات الوسيطة الاصطناعية، يحافظ على دقة الاختبار مقارنة بالطرق التقليدية مع تقليل الحاجة إلى الذاكرة والمتطلبات الحسابية. يحمل هذا التقدم آفاقاً بعيدة المدى، حيث يقدم حلاً عملياً لقيود الموارد في التعلم العميق الحديث. من خلال تحسين عمليات التدريب، يمهد هذا النهج الطريق لتطوير نماذج شبكات عصبية أكثر سهولة في الوصول وكفاءة.</p>
<p>بالإضافة إلى مساهماته الحالية، من المتوقع أن تستكشف الأبحاث المستقبلية تطبيق هذه المنهجية على نطاق أوسع يتجاوز الشبكات العصبية المتصلة بالكامل. تحمل قابلية تكييف هذا النهج وعوداً لتوسيعه إلى الشبكات العصبية الالتفافية (<span class="nodecor">CNNs</span>)، والشبكات العصبية المتكررة (<span class="nodecor">RNNs</span>)، وهياكل المحولات. من خلال تخصيص الطريقة لهذه الأنواع المتنوعة من الشبكات، يمكن للباحثين التحقق من تعميمها وفعاليتها عبر نطاق أوسع من نماذج التعلم العميق، مما يدفع المجال نحو تطوير نماذج أكثر سلاسة ويسراً في الوصول.</p>
</body>
</html>