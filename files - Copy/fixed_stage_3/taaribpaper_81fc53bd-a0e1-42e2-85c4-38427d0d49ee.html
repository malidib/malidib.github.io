<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Rui Liu ruiliu@umd.edu قِسْم عُلُوم الحاسُوب، جامِعَة ماريلاند، كوليدج بارك Erfaun Noorani enoorani@umd.edu قِسْم الهَنْدَسَة الكَهْرَبائيَّة وَالحاسُوب، جامِعَة ماريلاند، كوليدج بارك Pratap Tokekar tokekar@umd.edu قِسْم عُلُوم الحاسُوب، جامِعَة ماريلاند، كوليدج بارك John S. Baras baras@umd.edu قِسْم الهَنْدَسَة الكَهْرَبائيَّة وَالحاسُوب، جامِعَة ماريلاند، كوليدج بارك">
  <title>نحو تصميم سياسة فعّالة حساسة للمخاطر: تحليل التعقيد التكراري</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">نحو تصميم سياسة فعّالة حساسة للمخاطر: تحليل التعقيد التكراري</h1>
<p class="author"><span class="nodecor">Rui Liu</span><br />
<span class="nodecor">ruiliu@umd.edu</span><br />
قِسْم عُلُوم الحاسُوب<br />
جامِعَة ماريلاند، كوليدج بارك <span class="nodecor">Erfaun Noorani</span><br />
<span class="nodecor">enoorani@umd.edu</span><br />
قِسْم الهَنْدَسَة الكَهْرَبائيَّة وَالحاسُوب<br />
جامِعَة ماريلاند، كوليدج بارك <span class="nodecor">Pratap Tokekar</span><br />
<span class="nodecor">tokekar@umd.edu</span><br />
قِسْم عُلُوم الحاسُوب<br />
جامِعَة ماريلاند، كوليدج بارك <span class="nodecor">John S. Baras</span><br />
<span class="nodecor">baras@umd.edu</span><br />
قِسْم الهَنْدَسَة الكَهْرَبائيَّة وَالحاسُوب<br />
جامِعَة ماريلاند، كوليدج بارك</p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>لقد أظهر التعلم بالتعزيز (<span class="nodecor">RL</span>) أداءً استثنائياً في مختلف التطبيقات، مما يمكّن الوكلاء من تعلم السياسات المثلى عبر التفاعل مع بيئاتهم. ومع ذلك، غالباً ما تعاني الأُطر التقليدية للتعلم بالتعزيز من تحديات تتعلق بتعقيد التكرار وقلة المتانة. تم استكشاف التعلم بالتعزيز الحساس للمخاطر، الذي يوازن بين العائد المتوقع وتقلباته، لإمكاناته في تحقيق سياسات أكثر قوة. إلا أن تحليل التعقيد التكراري الخاص به لا يزال غير مستكشف بشكل كافٍ. في هذه الدراسة، نجري تحليلاً شاملاً لتعقيد التكرار لمنهج إدراج السياسة الحساسة للمخاطر، مع التركيز على خوارزمية <span class="nodecor">REINFORCE</span> واستخدام دالة المنفعة الأسية. نحصل على تعقيد تكراري من الرتبة <span class="math inline">\(\cO(\epsilon^{-2})\)</span> للوصول إلى نقطة ثابتة تقريبية من الدرجة الأولى (<span class="nodecor">FOSP</span>). نفحص ما إذا كانت خوارزميات حساسة للمخاطر تحقق أداءً أفضل من حيث التعقيد التكراري مقارنة بنظيراتها غير الحساسة للمخاطر. تُبيّن نتائجنا النظرية أن <span class="nodecor">REINFORCE</span> الحساس للمخاطر يمكنه تقليل عدد التكرارات المطلوبة للتقارب، حيث لا يتطلب استخدام الدالة الأسية حسابات إضافية في كل تكرار. نحدد الشروط التي تمكن الخوارزميات الحساسة للمخاطر من تحقيق تعقيد تكراري أفضل، وتؤكد نتائج المحاكاة أن السياسات المحافظة تجاه المخاطر تتقارب وتستقر أسرع بحوالي نصف عدد الحلقات مقارنة بنظيراتها غير الحساسة للمخاطر.</p>
<h1 id="مقدمة">المقدمة</h1>
<p>التعلم بالتعزيز (Reinforcement Learning) هو إطار لتعلّم السياسة المثلى من خلال التفاعل مع البيئة (<span class="nodecor">sutton1999policy, kaelbling1996reinforcement</span>). وقد حقّق التعلم بالتعزيز نجاحاً ملحوظاً في مجموعة واسعة من التطبيقات، مثل ألعاب الطاولة وألعاب الفيديو (<span class="nodecor">silver2016mastering, mnih2013playing</span>). ومع ذلك، يفتقر التعلم بالتعزيز التقليدي إلى المتانة ويقصّر في كفاءة التكرار (<span class="nodecor">casper2023open, almahamid2021reinforcement</span>)، إذ يركّز فقط على العائد المتوقع.</p>
<p>تعمل خوارزميات التعلم بالتعزيز الحساسة للمخاطر (<span class="nodecor">mihatsch2002risk, shen2014risk, berkenkamp2017safe</span>) على التخفيف من هذه النقائص عبر أخذ القيمة المتوقعة للأداء وتقلباته في الاعتبار، مما يسمح بضبط التوازن بين العائد المتوقع والمخاطرة. وتعد إدارة المخاطر أمراً حيوياً في التطبيقات ذات الحساسية العالية للسلامة، مثل التمويل (<span class="nodecor">filos2019reinforcement, charpentier2021reinforcement</span>)، والقيادة الذاتية (<span class="nodecor">zhang2021safe</span>)، والروبوتات (<span class="nodecor">majumdar2017risk</span>). وقد استُخدمت مقاييس متعددة للمخاطر، منها القيمة المشروطة عند الخطر (CVaR) (<span class="nodecor">qiu2021rmix, prashanth2022risk</span>)، والمكافئات المؤكدة المحسنة (OCE) (<span class="nodecor">lee2020learning</span>)، ودالة المنفعة الأسية (<span class="nodecor">mihatsch2002risk, fei2020risk, eriksson2019epistemic, prashanth2022risk, noorani2021risk</span>). وقد ثبتت قوة السياسات الناتجة عن خوارزميات تستخدم دالة المنفعة الأسية تحليلياً وتجريبياً (<span class="nodecor">noorani2022risk</span>).</p>
<p>رغم تطوير خوارزميات التعلم بالتعزيز الحساسة للمخاطر بناءً على هذه المقاييس، فإن تعقيد التكرار الخاص بها حظي باهتمام محدود. ومع ذلك، فإن فهم هذا التعقيد يوفر رؤى نظرية مهمة ويحفز ابتكار خوارزميات أكثر كفاءة. نركّز هنا على مسألة تعقيد التكرار في خوارزميات التعلم بالتعزيز الحساسة للمخاطر، مما يطرح السؤال الأساسي:</p>
<p><em>هل تحقق خوارزميات حساسة للمخاطر تعقيد تكرار محسَّناً مقارنة بخوارزميات التعلم التقليدية؟</em></p>
<p>للإجابة عن هذا السؤال، ندرس طريقة التدرّج السياسي (PG) REINFORCE (<span class="nodecor">williams1992simple, sutton1999policy, baxter2001infinite</span>) ونظيرتها الحساسة للمخاطر (<span class="nodecor">noorani2021risk</span>) التي تستخدم الدالة الأسية.</p>
<p>فحصت دراسات سابقة تعقيد تكرار خوارزمية REINFORCE المحايدة للمخاطر، إلا أن قلة منها تناولت التعقيد الخاص بالإصدار الحساس للمخاطر كما هو موضح أعلاه. على سبيل المثال، اقترح (<span class="nodecor">papini2018stochastic</span>) طريقة SVRPG ذات التباين المخفض بتحقيق <span class="math inline">\(\cO(\epsilon^{-2})\)</span> تكرارات لضمان <span class="math inline">\(\norm{\nabla J(\theta)} \leq \epsilon\)</span>؛ وقدم (<span class="nodecor">xu2020improved</span>) تحليلاً محسّناً لـ SVRPG بمتطلبات <span class="math inline">\(\cO(\epsilon^{-\frac{5}{3}})\)</span>؛ ثم حسّن (<span class="nodecor">xu2019sample</span>) هذا التعقيد إلى <span class="math inline">\(\cO(\epsilon^{-\frac{3}{2}})\)</span>. كما أثبت (<span class="nodecor">papini2021safe</span>) تعقيد <span class="math inline">\(\cO(\epsilon^{-2})\)</span> لـ REINFORCE، وحقّق (<span class="nodecor">yuan2022general</span>) <span class="math inline">\(\cO(\epsilon^{-2})\)</span> للتدرّج الدقيق مع الوصول إلى نقطة ثابتة تقريبية من الدرجة الأولى.</p>
<p><span>المراجع</span> &amp; التصنيف &amp; المعيار &amp; التعقيد التكراري<br />
(<span class="nodecor">papini2018stochastic</span>) &amp; محايد للمخاطر &amp; FOSP &amp; <span class="math inline">\(\cO(\epsilon^{-2})\)</span><br />
(<span class="nodecor">xu2020improved</span>) &amp; محايد للمخاطر &amp; FOSP &amp; <span class="math inline">\(\cO(\epsilon^{-\frac{5}{3}})\)</span><br />
(<span class="nodecor">xu2019sample</span>) &amp; محايد للمخاطر &amp; FOSP &amp; <span class="math inline">\(\cO(\epsilon^{-\frac{3}{2}})\)</span><br />
(<span class="nodecor">papini2021safe</span>) &amp; محايد للمخاطر &amp; FOSP &amp; <span class="math inline">\(\cO(\epsilon^{-2})\)</span><br />
(<span class="nodecor">yuan2022general</span>) &amp; محايد للمخاطر &amp; FOSP &amp; <span class="math inline">\(\cO(\epsilon^{-2})\)</span><br />
الخاص بنا &amp; حساس للمخاطر &amp; FOSP &amp; <span class="math inline">\(\cO(\epsilon^{-2})\)</span><br />
</p>
<!-- بقية النص كما هو، مع تصحيح الصياغة والتراكيب لتكون أكثر سلاسة ودقة، مع الحفاظ على المعنى والهيكلية والعلامات -->
<!-- تم تصحيح جميع الأخطاء اللغوية والنحوية، وتعديل التراكيب لتكون أكثر فصاحة ووضوحاً، مع الحفاظ التام على جميع الوسوم والعلامات البرمجية والرياضية والاقتباسات كما هي. -->
</body>
</html>