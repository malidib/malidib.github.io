<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Mukul Gagrani">
  <meta name="author" content="Raghavv Goel">
  <meta name="author" content="Wonseok Jeon">
  <meta name="author" content="Junyoung Park">
  <meta name="author" content="Mingu Lee">
  <meta name="author" content="Christopher Lott">
  <title>في الترميز التخميني لنماذج اللغة الكبيرة متعددة الوسائط</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">في الترميز التخميني لنماذج اللغة الكبيرة متعددة الوسائط</h1>
<p class="author"><span class="nodecor">Mukul Gagrani</span></p>
<p class="author"><span class="nodecor">Raghavv Goel</span></p>
<p class="author"><span class="nodecor">Wonseok Jeon</span></p>
<p class="author"><span class="nodecor">Junyoung Park</span></p>
<p class="author"><span class="nodecor">Mingu Lee</span></p>
<p class="author"><span class="nodecor">Christopher Lott</span></p>
</header>
<h1 id="ملخص">مُلخَّص</h1>
<p>الاستدلال باستخدام نماذج اللغة الكبيرة متعددة الوسائط (<span class="nodecor">MLLMs</span>) بطيء؛ ذلك بسبب أن عمودها الفقري – نموذج اللغة الكبير – يعاني من عنق زجاجة في عرض النطاق الترددي للذاكرة، ويولد الرموز بشكل تلقائي تراكمي. في هذه الورقة، نستكشف تطبيق الترميز التخميني لتعزيز كفاءة الاستدلال في نماذج <span class="nodecor">MLLMs</span>، وبشكلٍ خاص نموذج <span class="nodecor">LLaVA 7B</span>. نُظهر أن نموذج اللغة النصي وحده يمكن أن يكون نموذجًا مسودًا مناسبًا للترميز التخميني مع <span class="nodecor">LLaVA 7B</span>، متجاوزًا الحاجة إلى رموز الصور ومكونات المعالجة المرتبطة بها. تبين تجاربنا عبر ثلاث مهام مختلفة أن الترميز التخميني يمكن أن يحقق تسريعًا محدودًا في استهلاك الذاكرة يصل إلى <span class="nodecor">2.37<span class="math inline">\(\times\)</span></span> عند استخدام نموذج لغوي بحجم <span class="nodecor">115M</span> معامل قمنا بتدريبه من الصفر. بالإضافة إلى ذلك، نقدم نموذجًا مسودًا مدمجًا من <span class="nodecor">LLaVA</span> يتضمّن محول الصور، والذي يظهر مكاسب أداء طفيفة في وصف الصور مع الحفاظ على نتائجٍ مماثلة في المهام الأخرى.</p>
<h1 id="مقدمة">مقدمة</h1>
<p>أصبحت نماذج اللغة الكبيرة (Large Language Models) واسعة الانتشار في مختلف المجالات بفضل أدائها المميز. ومع ذلك، فهي تقتصر على استقبال استفسارات نصية فقط، بينما تأتي البيانات في العالم الحقيقي على شكل وسائط متعددة تشمل معلومات بصرية. توفر نماذج اللغة الكبيرة متعددة الوسائط (<span class="nodecor">MLLMs</span>) (<span class="nodecor">awadalla2023openflamingo</span>, <span class="nodecor">liu2024visual</span>, <span class="nodecor">tsimpoukelli2021multimodal</span>, <span class="nodecor">zhu2023minigpt</span>) قدرات فهم الصور عبر دمج الرموز البصرية والنصية لتحقيق تفاعل أكثر فائدة مع المستخدمين. تتكون هذه النماذج من مشفّر رؤية لمعالجة معلومات الصورة، ومحور لتحويل ترميزات الصور إلى فضاء تضمين نموذج اللغة، بالإضافة إلى عمودها الفقري الذي يرث منه توليد الرموز تلقائيًا تراكميًا ويعاني من عنق زجاجة في عرض النطاق الترددي للذاكرة، مما يؤدي إلى بطء الاستدلال (<span class="nodecor">shazeer2019fast</span>).</p>
<p>اقترح فك الترميز التخميني (<span class="nodecor">speculative decoding</span>) (<span class="nodecor">leviathan2023fast</span>, <span class="nodecor">chen2023accelerating</span>, <span class="nodecor">sun2023spectr</span>, <span class="nodecor">miao2023specinfer</span>, <span class="nodecor">jeon2024recursive</span>) كحل لتسريع عملية الاستدلال في نماذج اللغة الكبيرة دون التضحية بالدقة، حيث يتنبأ نموذج مسودة أصغر بعدة رموز مستقبلية يتم التحقق منها في استدعاء واحد للنموذج الكبير. ونظرًا إلى أن نماذج اللغة الكبيرة متعددة الوسائط تعتمد في عمودها الفقري على نموذج لغة كبير، يمكن تطبيق الترميز التخميني لجعل استدلالها أكثر كفاءة. تناولت العديد من الأعمال الحديثة استخدام الترميز التخميني ومتغيراته (<span class="nodecor">kim2023big</span>, <span class="nodecor">fu2023lookahead</span>, <span class="nodecor">medusa</span>, <span class="nodecor">santilli2023accelerating</span>, <span class="nodecor">sun2023spectr</span>, <span class="nodecor">jeon2024recursive</span>) لنماذج اللغة الكبيرة، لكن لا توجد دراسات سابقة في سياق النماذج متعددة الوسائط حسب علمنا.</p>
<p>في هذه الورقة، نطبق الترميز التخميني على نموذج <span class="nodecor">LLaVA 7B</span> (الذي يستخدم <span class="nodecor">LLaMA 7B</span> كعمود فقري للغة) لجعل الاستدلال أكثر كفاءة. ونظرًا لغياب نماذج أصغر متاحة علنًا من عائلتي LLaVA وLLaMA بأقل من 7 مليارات معامِلات، قمنا بتدريب نموذج لغوي من الصفر بحجم <span class="nodecor">115M</span> معامل لاستخدامه كنموذج مسودة. نُظهر أن نموذجًا لغويًا يتجاهل رموز الصور (وبالتالي لا يحتاج إلى مشفّر الرؤية والمحور) يمكن أن يكون نموذج مسودة جيدًا لـ<span class="nodecor">LLaVA 7B</span>. أجرينا تجارب على ثلاث مهام تشمل أسئلة وأجوبة على صور في مجموعة بيانات LLaVA Instruct 150K (<span class="nodecor">liu2024visual</span>)، ووضع العناوين على صور من مجموعة بيانات COCO (<span class="nodecor">lin2014microsoft</span>) ومجموعة بيانات ScienceQA (<span class="nodecor">lu2022learn</span>) باستخدام نماذج مسودة بمستويات تدريب وصقل متفاوتة. تظهر نتائجنا أننا نستطيع تحقيق تسريع محدود في استهلاك الذاكرة يصل إلى <span class="nodecor">2.37×</span> باستخدام نموذج لغوي فقط كنموذج مسودة. كما أنشأنا نموذجًا مسودًا مدمجًا من <span class="nodecor">LLaVA</span> يضم محول الرؤية إلى جانب نموذج اللغة المدرب، وأظهر تحسينًا طفيفًا في مهمة التعليق على COCO ومهمة ScienceQA مع أداء مماثل في بقية المهام.</p>
<h1 id="الطريقة">الطريقة</h1>
<h1 id="الخلفية">الخلفية</h1>
<h2 id="التفكيك-التخميني">الترميز التخميني</h2>
<p>يتضمن الترميز التخميني (<span class="nodecor">Speculative Decoding</span>) (<span class="nodecor">chen2023accelerating</span>, <span class="nodecor">leviathan2023fast</span>) استخدام نموذج مسودة أصغر لتوليد عدة رموز يتم التحقق منها بالتوازي من قبل النموذج اللغوي الكبير المستهدف. بناءً على سياق الإدخال <span class="math inline">\(X_{1:n}:=[X_{1}, \dots, X_{n}]\)</span>، يولد نموذج المسودة تسلسلًا من الرموز <span class="math inline">\(\hat{X}_{n+1:n+L}\)</span> بطريقة تلقائية تراكميًا، <span class="math inline">\(\hat{X}_{n+j}\sim p(\cdot\mid X_{1:n},\hat{X}_{n+1:n+j-1})\)</span>. ثم يتم التحقق من هذه الرموز في استدعاء واحد للنموذج الكبير (<span class="math inline">\(q\)</span>) باستخدام آلية أخذ العينات بالرفض لضمان مطابقة التوزيع الأصلي. على وجه التحديد، يُقبل الرمز <span class="math inline">\(\hat{X}_{n+j}\)</span> بالاحتمالية <span class="math display">\[\min\left\{1, \frac{q(\hat{X}_{n+j}\mid X_{1:n},\hat{X}_{n+1:n+j-1})}{p(\hat{X}_{n+j}\mid X_{1:n},\hat{X}_{n+1:n+j-1})}\right\}.\]</span> إذا رُفض رمز المسودة <span class="math inline">\(\hat{X}_{n+j}\)</span>، يُؤخذ عينة جديدة من التوزيع المتبقي <span class="math inline">\(p_{res}(x)=\max(0,q(x)-p(x))\)</span>.</p>
<h2 id="نماذج-اللغة-الكبيرة-متعددة-الوسائط">نماذج اللغة الكبيرة متعددة الوسائط</h2>
<p>يتكون نموذج اللغة الكبير متعدد الوسائط المعتمد على الصور من <em>1) مشفّر رؤية</em> لتشفير الصورة المدخلة، <em>2) محول</em> لتحويل ترميزات الصور إلى تضمينات نموذج اللغة، و<em>3) العمود الفقري لنموذج اللغة</em>. نصف إطار عمل <span class="nodecor">LLaVA</span> بالتفصيل؛ بالنظر إلى صورة مدخلة <span class="math inline">\(I\)</span> واستعلام نصي <span class="math inline">\(Q\)</span>، تُحوَّل الصورة إلى تسلسل <span class="math inline">\(H_1,H_2,\ldots,H_m\)</span> من الترميزات، ويُحوَّل النص إلى تسلسل من تضمينات الرموز <span class="math inline">\(X_1,X_2,\ldots,X_n\)</span>. ثم يحوّل المحول <span class="math inline">\(g_\theta\)</span> هذا التسلسل إلى تضمينات صورة <span class="math inline">\(V_i=g_\theta(H_i)\)</span> في فضاء نموذج اللغة. أخيرًا، يولّد نموذج اللغة الرموز التالية بناءً على تضمينات الصورة والنص كما في: <span class="math display">\[X_{n+1}\sim q(\cdot\mid V_{1:m},X_{1:n})\]</span></p>
<h1 id="تحليل-spd-لنماذج-mllm">تحليل SPD لنماذج MLLM</h1>
<p>لتحقيق مكاسب أكبر مع الترميز التخميني، نحتاج إلى نموذج مسودة أصغر كثيرًا ومتوافق جيدًا مع نموذجنا الهدف <span class="nodecor">LLaVA-7B</span>. الخيار الشائع في الأدبيات هو استخدام نموذج مسودة مُدرَّب مسبقًا من نفس العائلة أو تدريب نموذج أصغر بنفس بنية الهدف (<span class="nodecor">miao2023specinfer</span>). ونظرًا إلى عدم توفر نموذج أصغر علنًا في عائلة LLaVA، دربنا نموذج مسودة من الصفر. اخترنا بنية مشابهة لهيكل LLaVA بحيث يكون نموذج المسودة إمّا: 1) محول رؤية أصغر مع نموذج لغة مسودة، أو 2) نموذج مسودة نصي فقط يولّد الرموز استنادًا إلى النص. بالنظر إلى تضمينات الصورة <span class="math inline">\(V_{1:m}\)</span> وتضمينات النص <span class="math inline">\(X_{1:n}\)</span>، يولّد نموذج المسودة تسلسل الرموز <span class="math inline">\(\hat{X}_{n+1:n+L}\)</span> حيث <span class="math display">\[\hat{X}_{n+j}\sim p(\cdot\mid X_{1:n},\hat{X}_{n+1:n+j-1}).\]</span> يتحقق نموذج الهدف LLaVA من هذه الرموز اعتمادًا على تضمينات الصورة والنص باحتمالية <span class="math display">\[\min\left\{1,\frac{q(\hat{X}_{n+j}\mid V_{1:m},X_{1:n},\hat{X}_{n+1:n+j-1})}{p(\hat{X}_{n+j}\mid X_{1:n},\hat{X}_{n+1:n+j-1})}\right\}.\]</span> يُعد نموذج المسودة النصي فقط أكثر كفاءة لأنه 1) لا يحتاج إلى محول إضافي لتضمينات الصورة، و2) لا يتطلب تدريب المحول.</p>
<h1 id="التجارب">التجارب</h1>
<p>نقوم بتشغيل التجارب على ثلاث مهام إرشاد بصري باستخدام SPD مع النموذج الهدف <span class="nodecor">LLaVA-7B</span> (<span class="nodecor">liu2023improved</span>) الذي يعتمد على LLaMA-7B. تمتلك جميع نماذج المسودة حجمًا ثابتًا للغة يبلغ <span class="math inline">115M</span> معامل.</p>
<h4 id="مرشحو-نموذج-المسودة.">مرشحو نماذج المسودة:</h4>
<p>دربنا نموذج مسودة بحجم <span class="math inline">115M</span> وفق هيكلية LLaMA-2 من الصفر، ثم صقلناه على بيانات تعليمات بالخسارة TVD++ (<span class="nodecor">goel2024direct</span>) ومجموعة فرعية من LLaVA Instruct 150K (<span class="nodecor">liu2024visual</span>). نعتبر المراحل التالية:</p>
<ol>
<li><em>نموذج LLaMA الأساسي</em>: بعد التدريب المسبق على <span class="math inline">600B</span> رمز إنجليزي.</li>
<li><em>نموذج دردشة LLaMA</em>: صقل تعليمي لنموذج LLaMA الأساسي (<span class="nodecor">goel2024direct</span>).</li>
<li><em>LLaVA المصقول</em> (<span class="nodecor">ft-llava</span>): صقل كامل مع تهيئة محول الرؤية من LLaVA-7B باستخدام تقنية التقسيم الفرعي (<span class="nodecor">samragh2023weight</span>) ونموذج اللغة من دردشة LLaMA.</li>
<li><em>LLaVA نصيًا مصقول</em> (<span class="nodecor">ft-llava-text</span>): يستخدم جزء نموذج اللغة فقط من <em>ft-llava</em>.</li>
</ol>
<p>عندما يعتمد نموذج المسودة على الصورة، يُشارك مشفّر الرؤية (CLIP ViT-L/14) مع الهدف لتجنب إعادة حساب التضمينات. تفاصيل المعلمات في الملحق [app:model_config].</p>
<h4 id="مهام-التقييم.">مهام التقييم:</h4>
<p>نركّز على توليد نص مفتوح نهائي والإجابة متعددة الخيارات مع التفكير المتسلسل (CoT) لتعزيز طول التوليد. نقيّم على: 1) مجموعة بيانات LLaVA Instruct 150K (<span class="nodecor">liu2024visual</span>), 2) وصف COCO (<span class="nodecor">lin2014microsoft</span>), و3) ScienceQA مع CoT (<span class="nodecor">lu2022learn</span>). إعدادات المطالبات في الملحق [app:sys_prompts].</p>
<h4 id="المقاييس.">المقاييس:</h4>
<p>نقيس فعالية SPD عبر: 1) <strong>كفاءة الكتلة</strong> (<span class="math inline">\(\tau\)</span>): متوسط عدد الرموز المولدة لكل استدعاء للنموذج الهدف لحجم كتلة <span class="math inline">\(\gamma\)</span>. 2) <strong>التسريع المحدود بالذاكرة</strong> (<span class="nodecor">MBSU</span>): <span class="math inline">\(\mathrm{MBSU}(x)=\frac{c\,\tau(x)}{c\,\gamma+1}\)</span> حيث <span class="math inline">\(c\)</span> نسبة معامِلات نموذج المسودة إلى الهدف. 3) <strong>معدل الرموز</strong>: إجمالي عدد الرموز المولدة مقسومًا على زمن التوليد. نجري القياسات بحجم كتلة <span class="math inline">\(\gamma\in\{3,5\}\)</span>.</p>
<h4 id="فك-التشفير.">فك الترمیز:</h4>
<p>نستخدم التفكيك الجشع لكل التجارب، بحيث يكون التوليد مطابقًا للتوليد التلقائي التراكمي للنموذج الهدف. نترك استكشاف طرق تفكيك قائمة على العينات (تغيير درجة الحرارة، top-<span class="math inline">\(p\)</span>, top-<span class="math inline">\(k\)</span>) كعمل مستقبلي.</p>
<h4 id="النتائج.">النتائج:</h4>
<p>تُظهر نتائجنا أن SPD مع نموذج الهدف LLaVA <span class="nodecor">7B</span> يمنح تسريعًا كبيرًا في التوليد. وعند استخدام نموذج مسودة نصي فقط، يقدم SPD تسريعًا تنافسيًا مقارنةً بنموذج مسودة يستفيد من معلومات الصورة.</p>
<p>من الشكل [fig:result] (العلوي والوسطي)، نرى أن SPD يحقق مكاسب تزيد على <span class="nodecor">2<span class="math inline">\(\times\)</span></span> من حيث كفاءة الكتلة وMBSU. يتجه الأداء للارتفاع عند زيادة حجم الكتلة من <span class="nodecor">3</span> إلى <span class="nodecor">5</span> في جميع المهام، باستثناء SQA حيث يتفوّق نموذج المسودة base-llama عند <span class="nodecor">5</span>. في تقييم LLaVA، يتصدّر ft-llava-text ثم ft-llava لكلا الحجمين. في COCO، يتصدّر ft-llava ثم ft-llava-text. في SQA، عند <span class="nodecor">3</span> يتفوّق ft-llava ثم ft-llava-text، وعند <span class="nodecor">5</span> يتفوّق ft-llava ثم base-llama. كما حسّنت جميع نماذج المسودة معدل الرموز مقارنة بالتوليد التلقائي التراكمي—ويُعد حجم الكتلة 3 أفضل من 5—مما رفع عدد الرموز في الثانية.</p>
<p>نعرض أيضًا نتائج نوعية لتوليد تعليقات COCO باستخدام نموذج المسودة ft-llava-text في الشكل [fig:qualitative_example]، حيث التوكنات باللون الأزرق والمسطرة مقبولة. نرى أن نموذج المسودة يتنبأ بالكلمات الشائعة والإكمالات دون معلومات الصورة؛ على سبيل المثال، يتنبأ بـ"tables" من "vege"، وفي المثال الثاني من "app" يتنبأ بـ"liances". عمومًا، تحتوي النصوص المفتوحة على العديد من التوكنات الشائعة والإكمالات التي لا تتطلب تضمينات بصرية، لذا يقدم نموذج المسودة النصي أداءً تنافسيًا. يمكنه أيضًا تكرار التوكنات بعد توليدها—مثل "counter" و"bowls" في المثال الثاني. أخيرًا، نترك صقلًا أكثر صرامة لنموذج متعدد الوسائط صغير كعمل مستقبلي.</p>
<p>بناءً على نوع الرموز المقبولة، افترضنا أن نموذج LLaVA الأولي قد لا يستخدم معلومات الرؤية بالكامل (أي لا يحسن كفاءة الكتلة)، ربما لأن محول الرؤية الأولي لم يرمِّز الرموز بشكل كامل. لذلك جرّبنا SPD مع نماذج أولية لا تستخدم رموز الصور، لمراقبة تأثير المحول المتدرج في تحسين الكتلة أو MBSU. من الشكل <span class="nodecor">fig:avg_token_all</span> و<span class="nodecor">fig:mbsu_all</span> نرى أن LLaVA المصقول نصيًا والنسخة الكاملة يقدمان أداءً متقاربًا، مما يدعم فرضيتنا تجريبيًا.</p>
<p>أضفنا أيضًا نماذج مسودة نصية أخرى لمراقبة التسريع باستخدام النص فقط. ولدهشتنا، أظهر حتى نموذجَي LLaMA الأساسي ودردشة LLaMA تسريعًا يزيد عن <span class="nodecor">2</span> مرات في المتوسط (<span class="nodecor">fig:mbsu_all</span>).</p>
<p>أداء SPD مع نموذج مسودة نصي قريب نسبيًا من نموذج LLaVA، ويتفوق أحيانًا في ScienceQA. بناءً على ذلك، حلّلنا متوسط قبول التوكنات في النماذج المختلفة ودرجة الاهتمام المُخصصة لرموز الصورة في هدف LLaVA (انظر الملحق).</p>
<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<h1 id="الخلاصة">الخلاصة</h1>
<p>في هذه الورقة، نقدّم الجهد الأول نحو استخدام الترميز التخميني لتسريع الاستدلال في نماذج اللغة الكبيرة متعددة الوسائط، مع التركيز على الصور والنصوص. نُظهر أن نموذج مسودة نصي فقط يحقق أداءً تنافسيًا مقارنةً بنموذج مسودة يستفيد من ميزات الصورة. أجرينا تجارب على مهام توليد نص مفتوح ونموذج توليد مع تفكير متسلسل باستخدام نماذج مسودة نصية ونص–صورة، وحققنا تسريعًا يصل إلى <span class="nodecor"><span class="math inline">\(2.37\times\)</span></span> للنموذج النصي وحده وتسريعًا أفضل قليلًا للنموذج نص–صورة، مما يُظهر تجريبيًا فعالية الترميز التخميني في MLLMs.</p>
<p>يفتح عملنا آفاقًا مستقبلية متعددة ضمن الإطار العام المقدم. يمكن توسيعه ليشمل نماذج أخرى مثل (<span class="nodecor">li2023blip</span>)، (<span class="nodecor">zhu2023minigpt</span>)، (<span class="nodecor">awadalla2023openflamingo</span>)، وكذلك وسائط أخرى كال صوت (<span class="nodecor">chu2023qwen</span>) التي تعاني من نفس قيود التوليد التلقائي التراكمي. علاوة على ذلك، يمكن تبني أساليب ترميز تخميني قائمة على الشجرة (<span class="nodecor">sun2023spectr</span>, <span class="nodecor">miao2023specinfer</span>, <span class="nodecor">medusa</span>, <span class="nodecor">jeon2024recursive</span>) لزيادة سرعة التوليد أكثر.</p>
<h1 id="الملحق">الملحق</h1>
<h2 id="app:model_config">تكوينات النموذج</h2>
<p>يستخدم نموذج LLaVA-7B: (i) مشفّر رؤية، (ii) محول/مشروع الصور المبني على شبكة عصبية متعددة الطبقات، و(iii) نموذج اللغة LLaMA 7B. المشفّر البصري هو CLIP ViT-L/14 مع تفاصيل في (<span class="nodecor">radford2021learning</span>)، ومحولات الصور تحتوي على طبقتين خطيتين بأبعاد <span class="math inline">\(1024\times4096\)</span> و<span class="math inline">\(4096\times4096\)</span>. أما في سيناريو النماذج الأولية مع محول الصور، فتكون الأبعاد <span class="math inline">\(1024\times1024\)</span> و<span class="math inline">\(1024\times1024\)</span>.</p>
<p>تكوينات جزء نموذج اللغة الهدف والمسودة (LLaMA) كما يلي:</p>
<table>
<caption>تكوينات النموذج المسودة والهدف</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">الهدف (7B)</th>
<th style="text-align: right;">المسودة (115M)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">الطبقات</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">رؤوس الانتباه</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">البعد الوسيط</td>
<td style="text-align: right;"><span class="nodecor">11,008</span></td>
<td style="text-align: right;"><span class="nodecor">2,816</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">البعد الخفي</td>
<td style="text-align: right;"><span class="nodecor">2,048</span></td>
<td style="text-align: right;"><span class="nodecor">1,024</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">التنشيط</td>
<td style="text-align: right;">SiLU</td>
<td style="text-align: right;">SiLU</td>
</tr>
</tbody>
</table>
<p>[tab:model_config]</p>
<h2 id="app:sys_prompts">مطالبات النظام</h2>
<p>نستخدم المطالبات التالية لكل مهمة. يُستخدم الرمز <em><span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span></em> لتمثيل بيانات الصورة.</p>
<p><strong>تقييم LLaVA.</strong> نتبع منهجية (<span class="nodecor">liu2024visual</span>)، حيث يعرض المساعد أسئلة وأجوبة متعددة.</p>
<p><em><span class="math inline">\(&lt;\)</span>s<span class="math inline">\(&gt;\)</span> دردشة بين مستخدم فضولي ومساعد ذكاء اصطناعي. يقدم المساعد إجابات مفصلة ومهذبة. المستخدم: <span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span>  \\\\ السؤال <span class="math inline">\(Q_{1}\)</span> المساعد: <span class="math inline">\(R_{1}\)</span>. المستخدم: السؤال <span class="math inline">\(Q_{2}\)</span> …</em></p>
<p><strong>وصف COCO.</strong> بما أن COCO لا يتضمّن أسئلة، استخدمنا مطلَبًا مشابهًا:</p>
<p><em><span class="math inline">\(&lt;\)</span>s<span class="math inline">\(&gt;\)</span> دردشة بين مستخدم فضولي ومساعد ذكاء اصطناعي. يقدم المساعد إجابات مفيدة ومفصلة. المستخدم: <span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span>  \\\\ قدّم وصفًا مفصلاً للصورة المساعدة:</em></p>
<p><strong>أسئلة العلوم.</strong> نتبع (<span class="nodecor">lu2022learn</span>) مع مثال واحد للسؤال، الخيارات، الإجابة والتعليل للسماح بالتفكير المتسلسل. نستخدم عينات الاختبار المرتبطة بصورة.</p>
<p><span class="math display">\[\begin{aligned}
    &amp; \text{السؤال: } I_{i}^{ques} \\
    &amp; \text{الخيارات: (0) } I_{i1}^{opt} \,(1)\,I_{i2}^{opt}\,(2)\,I_{i3}^{opt} \\
    &amp; \text{السياق: } I_{i}^{cont} \\
    &amp; \text{الإجابة: } I_{i}^{ans} \text{، لأن: } I_{i}^{lect} \text{. التفسير: } I_{i}^{exp} \\
    &amp; <image> \\
    &amp; \text{السؤال: } I_{test}^{ques} \\
    &amp; \text{الخيارات: (0) } I_{test,1}^{opt}\,(1)\,I_{test,2}^{opt}\,(2)\,I_{test,3}^{opt} \\
    &amp; \text{السياق: } I_{test}^{cont} \\
    &amp; \text{الإجابة:} 
\end{aligned}\]</span></p>
<p>يشير <span class="math inline">\(i\)</span> إلى العينة داخل السياق. في SQA، وُفِّر حقل السياق عبر تسمية الصور المولدة تلقائيًا، غير أنها كانت بسيطة. لذا استخدمنا حقل “التلميح” من البيانات. مثال السياق لا يتضمّن صورة متعددة لتجنب تعقيد الاستهداف. نترك SPD مع أكثر من مثال في السياق كعمل مستقبلي.</p>
<h2 id="درجة-الانتباه-لرموز-الصورة">درجة الانتباه لرموز الصورة</h2>
</body>
</html>