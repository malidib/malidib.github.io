<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque, University of Montreal &amp; Mila firstname.lastname@umontreal.ca Shunichi Akatsuka, Hitachi, Ltd. shunichi.akatsuka.bo@hitachi.com Aaron Courville University of Montreal &amp; Mila aaron.courville@umontreal.ca">
  <title>تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</h1>
<p class="author"><span class="nodecor">Milad Aghajohari</span>, <span class="nodecor">Tim Cooijmans</span>, <span class="nodecor">Juan Agustin Duque</span>,<br />
<span class="nodecor">University of Montreal &amp; Mila</span><br />
<code>firstname.lastname@umontreal.ca</code><br />
<strong><span class="nodecor">Shunichi Akatsuka</span></strong>,<br />
<span class="nodecor">Hitachi, Ltd.</span><br />
<span class="nodecor">shunichi.akatsuka.bo@hitachi.com</span><br />
<strong><span class="nodecor">Aaron Courville</span></strong><br />
<span class="nodecor">University of Montreal &amp; Mila</span><br />
<code>aaron.courville@umontreal.ca</code><br /></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>نَسْتَكْشِف تَحَدِّي تَعَلُّم التَّعْزِيز العَمِيق مُتَعَدِّد الوُكَلاء في بِيئات تَنافُسِيَّة جُزْئِيَّة، حيث تواجِه الطُّرُق التَّقْلِيدِيَّة صُعُوبات في تَعْزِيز التَّعاوُن القائم على المُعامَلَة بالمِثْل. يَتَعَلَّم وُكَلاء <span class="nodecor">LOLA</span> و<span class="nodecor">POLA</span> سِياسات تَعاوُنِيَّة قائمة على المُعامَلَة بالمِثْل من خلال التَّفاضُل عبر عدد قليل من خُطُوات التَّحْسِين المُسْتَقْبَلِيَّة لِلخَصْم. ومع ذلك، هناك قَيْد رئيسي في هذه التِّقْنيات. نظراً لأنها تأخذ في الاعتبار عدد قليل من خُطُوات التَّحْسِين، فقد يستغل خَصْم يَتَعَلَّم ويأخذ العديد من الخُطُوات لتحسين عائده هذه التِّقْنيات. رداً على ذلك، نُقَدِّم نَهْجاً جديداً، تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ (<span class="nodecor">BRS</span>)، الذي يُقَرِّب من خلال خَصْم يَقْتَرِب من الاِسْتِجَابَةِ الأَمْثَلِ، المُسَمَّى "المُحَقِّق". لِتَكْيِيف المُحَقِّق على سِياسة الوَكِيل في الألعاب المُعَقَّدة، نَقْتَرِح آلية تَكْيِيف قابلة للتَّفاضُل تعتمد على الحالة، مُيَسَّرة بطريقة الإِجَابَة على الأَسْئِلَة التي تَسْتَخْرِج تَمْثِيلاً للوَكِيل بناءً على سُلُوكه في حالات بِيئية مُحَدَّدة. لِلتَّحَقُّق من صِحَّة طريقتنا تَجْرِيبياً، نَعْرِض أداءها المُحَسَّن ضد خَصْم <span class="nodecor">Monte Carlo Tree Search (MCTS)</span>، الذي يعمل كَتَقْرِيب للاِسْتِجَابَةِ الأَمْثَلِ في لعبة العُمْلَة. يُوَسِّع هذا العمل تطبيق تَعَلُّم التَّعْزِيز مُتَعَدِّد الوُكَلاء في البِيئات التَّنافُسِيَّة الجُزْئِيَّة ويُوَفِّر مَساراً جديداً نحو تحقيق رَفَاهِيَّة اجتماعية مُحَسَّنة في الألعاب ذات المَجْمُوع العام.</p>
<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>مَكَّنَت خوارزميات تَعَلُّم التَّعْزِيز الوُكَلاء من الأداء بشكل جيد في ألعاب مُعَقَّدة ذات أبعاد عالية مثل لعبة الذَّهاب (<span class="nodecor">alphago</span>) وستاركرافت (<span class="nodecor">alphastar</span>). الهَدَف النِّهائي من تَعَلُّم التَّعْزِيز هو تَدْرِيب وُكَلاء يُمْكِنهم مُسَاعَدَة البَشَر في حَلّ المُشْكِلات الصَّعْبَة. لا مَحالة، سيحتاج هؤلاء الوُكَلاء إلى الاِنْدِماج في سِينارِيُوهات الحياة الواقعية التي تتطلب التَّفاعُل مع البَشَر ووُكَلاء تَعَلُّم آخرين. بينما يَتَفَوَّق تَدْرِيب تَعَلُّم التَّعْزِيز مُتَعَدِّد الوُكَلاء في البِيئات التَّعاوُنِيَّة أو التَّنافُسِيَّة بالكامل، فإنه غالباً ما يفشل في إيجاد تَعاوُن قائم على المُعامَلَة بالمِثْل في البِيئات التَّنافُسِيَّة الجُزْئِيَّة. مثال على ذلك هو فشل وُكَلاء تَعَلُّم التَّعْزِيز مُتَعَدِّد الوُكَلاء في تَعَلُّم سِياسات مثل المُعامَلَة بالمِثْل في مُعْضِلَة السَّجِين المُتَكَرِّرَة (<span class="nodecor">LOLA</span>).</p>
<p>على الرغم من الطابع اللعبي لألعاب المَجْمُوع العام الشائعة مثل مُعْضِلَة السَّجِين، فإن هذه الأنواع من المُشْكِلات مُنْتَشِرة في المجتمع والطبيعة على حد سواء. فكِّر في سِينارِيو حيث تسعى دولتان (وُكَلاء) لتعظيم الإنتاج الصناعي لهما مع ضمان مناخ مناسب للإنتاج من خلال الحد من الاِنْبِعاثات الكربونية. من ناحية، تود كل دولة أن ترى الدولة الأخرى تفي بالتزاماتها للحد من الاِنْبِعاثات الكربونية. ومن ناحية أخرى، يُحَفِّز كل منهما إصدار المزيد من الكربون لتحقيق عوائد صناعية أعلى. ستجبر مُعاهدة المناخ الفعالة كل دولة - على الأرجح من خلال تهديد بالعقوبات - على الاِلْتِزام بالحدود المتفق عليها للاِنْبِعاثات الكربونية. إذا فشل هؤلاء الوُكَلاء في تطوير اِسْتراتِيجِيَّات مثل المُعامَلَة بالمِثْل، فمن المُحْتَمَل أن يتجهوا نحو تصعيد متبادل مؤسف للاِسْتِهلاك والاِنْبِعاثات الكربونية.</p>
<p>اقترحت (<span class="nodecor">LOLA</span>) خوارزمية تَعَلُّم مع وعي بتَعَلُّم الخَصْم، وهي خوارزمية نجحت في تَعَلُّم سلوك المُعامَلَة بالمِثْل في إعداد مُعْضِلَة السَّجِين المُتَكَرِّرَة من خلال التَّفاضُل عبر خطوة بسيطة مُفْتَرَضَة يتخذها الخَصْم. بناءً على ذلك، قدمت (<span class="nodecor">POLA</span>) تحديث سِياسة الخَصْم القريب، والذي يُعَزِّز خوارزمية تَعَلُّم مع وعي بتَعَلُّم الخَصْم من خلال افتراض تحديث سِياسة قريب للخَصْم. يسمح هذا التَّحْسِين بتَدْرِيب سِياسات الشبكة العصبية في ألعاب أكثر تعقيداً، مثل لعبة العُمْلَة (<span class="nodecor">LOLA</span>). حسب علمنا، تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب هو الأسلوب الوحيد الذي يُدَرِّب بشكل موثوق وُكَلاء تَعاوُنِيِّين قائمين على المُعامَلَة بالمِثْل في لعبة العُمْلَة.</p>
<p>على الرغم من نجاحه في لعبة العُمْلَة، فإن تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب له قيوده. بينما يتعلم تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب مع وعي بتَعَلُّم الخَصْم، فإن نمذجته لتَعَلُّم الخَصْم محدودة ببضع خُطُوات تَحْسِين مُسْتَقْبَلِيَّة. يجعل هذا تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب عُرْضَة للاِسْتِغْلال من قِبَل خُصُوم يشاركون في تَحْسِين إضافي. على وجه الخصوص، تُظْهِر تحليلاتنا لوُكَلاء تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب المُدَرَّبِين على لعبة العُمْلَة أن تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب عُرْضَة للاِسْتِغْلال من قِبَل الخَصْم الذي يستجيب بشكل أفضل. عندما يتم تَدْرِيب الخَصْم خصيصاً لتعظيم عائده الخاص ضد سِياسة ثابتة تم تَدْرِيبها بواسطة تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب، فإن الأول يستغل الأخير. أيضاً، يمكن أن تعيق هذه القيود قابلية توسع تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب؛ إذ لا يمكنه التَّفاضُل عبر جميع خُطُوات تَحْسِين الخَصْم. هذه مشكلة خاصة إذا كان الخَصْم شبكة عصبية معقدة، حيث تكون العديد من خُطُوات التَّحْسِين مطلوبة لتقريب تَعَلُّمها.</p>
<p>في هذه الورقة، نقدم نهجاً جديداً يُسَمَّى تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ. يعتمد أسلوبنا على بناء خَصْم يَقْتَرِب من سِياسة الاِسْتِجَابَةِ الأَمْثَلِ ضد وَكِيل معين. نشير إلى هذا الخَصْم باسم "المُحَقِّق". يتم تصوير المفهوم العام في الشكل [fig:cobalt]: يخضع المُحَقِّق للتدريب ضد وُكَلاء مأخوذين من توزيع متنوع. لتدريب الوَكِيل، نُفَاضِل عبر خَصْم المُحَقِّق. على عكس الأساليب مثل تَعَلُّم مع وعي بتَعَلُّم الخَصْم وتَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب، التي تفترض بضع خُطُوات تَحْسِين مُسْتَقْبَلِيَّة، يعتمد أسلوبنا على إصدار المُحَقِّق للاِسْتِجَابَةِ الأَمْثَلِ للوَكِيل الحالي من خلال تَكْيِيف السِياسة.</p>
<p>نُحَقِّق تَجْرِيبياً من أسلوبنا في مُعْضِلَة السَّجِين المُتَكَرِّرَة ولعبة العُمْلَة. نظراً للاعتماد على سِياسة الخَصْم لنتائج الوَكِيل، فإنه ليس من السهل دائماً تقييم ومقارنة سِياسات وُكَلاء مختلفين في الألعاب. هذا صحيح بشكل خاص في الألعاب غير المتساوية التي تُظْهِر جوانب تَعاوُنِيَّة وتَنافُسِيَّة. في هذه الورقة، ندافع عن أن نقطة المقارنة المعقولة هي نتيجة الوَكِيل عند مواجهة خَصْم يستجيب بشكل أفضل، والذي نُقَرِّبه بواسطة بحث شجرة مونت كارلو. نُظْهِر أنه بينما لا يتعاون بحث شجرة مونت كارلو بالكامل مع وُكَلاء تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب، فإنهم يتعاونون بالكامل مع وَكِيل تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ لدينا.</p>
<p><strong>المُسَاهَمَات الرَئِيسِيَّة</strong>: نُلَخِّص مُسَاهَمَاتنا الرَئِيسِيَّة أدناه:</p>
<ul>
<li><p>نُحَدِّد أن الخَصْم الذي يستجيب بشكل أفضل، كما هو مُقَرَّب بواسطة بحث شجرة مونت كارلو، لا يتعاون مع وُكَلاء تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب. يستغل بحث شجرة مونت كارلو وُكَلاء تَعَلُّم مع وعي بتَعَلُّم الخَصْم القريب لتحقيق عائد أعلى مما كان سيحققه من خلال التعاون الكامل.</p></li>
<li><p>لمعالجة هذا الضعف، نقدم أسلوب تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ، الذي يُدَرِّب وَكِيلاً من خلال التَّفاضُل عبر خَصْم يَقْتَرِب من الاِسْتِجَابَةِ الأَمْثَلِ (يُشار إليه باسم "خَصْم المُحَقِّق"). نُحَقِّق تَجْرِيبياً من أسلوبنا ونُظْهِر أن الاِسْتِجَابَةِ الأَمْثَلِ لوُكَلاء تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ هي بالفعل التعاون الكامل كما هو موضح في الشكل [fig:coin_main_compare].</p></li>
<li><p>بالإضافة إلى ذلك، نقترح آلية تَكْيِيف قابلة للتَّفاضُل وواعية بالحالة لخَصْم المُحَقِّق، مما يُمَكِّنه من التَّكَيُّف مع سِياسة الوَكِيل.</p></li>
</ul>
<h1 id="sec:background">الخَلْفِيَّة</h1>
<h2 id="تعلم-التعزيز-المتعدد-العوامل">تَعَلُّم التَّعْزِيز المُتَعَدِّد العَوامِل</h2>
<p>تُعَرَّف لعبة ماركوف المُتَعَدِّدَة العَوامِل بالرمز <span class="math inline">\(\bm{(} N, \mathcal{S},\left\{\mathcal{A}^i\right\}_{i=1}^N, \mathbb{P},\left\{r^i\right\}_{i = 1}^N, \gamma \bm{)}\)</span>. هنا، <span class="math inline">\(N\)</span> تُمَثِّل عدد العَوامِل، <span class="math inline">\(\mathcal{S}\)</span> فضاء الحالات للبيئة، و<span class="math inline">\(\mathcal{A}:=\mathcal{A}^1 \times \cdots \times \mathcal{A}^N\)</span> مجموعة الأفعال لكل عامل. احتمالات الاِنْتِقال ممثلة بـ <span class="math inline">\(\mathbb{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\)</span> ودالة المُكافَأَة بـ <span class="math inline">\(r^i: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span>. أخيراً، <span class="math inline">\(\gamma \in [0,1]\)</span> هو عامل الخصم. في مشكلة تَعَلُّم التَّعْزِيز المُتَعَدِّد العَوامِل، كل عامل يحاول تعظيم عائده <span class="math inline">\(R^i = \sum_{t=0}^\infty \gamma^t r^i_t\)</span>. سِياسة العامل <span class="math inline">\(i\)</span> ممثلة بـ <span class="math inline">\(\pi^{i}_{\theta_{i}}\)</span> حيث <span class="math inline">\(\theta_i\)</span> هي معاملات السِياسة. في تَعَلُّم التَّعْزِيز العميق، هذه السِياسات هي شبكات عصبية. يتم تَدْرِيب هذه السِياسات عبر مُقَدِّرات التدرج مثل REINFORCE (<span class="nodecor">reinforce</span>).</p>
<h2 id="المعضلات-الاجتماعية-ومعضلة-السجين-المتكررة">المُعْضِلات الاِجْتِمَاعِيَّة ومُعْضِلَة السَّجِين المُتَكَرِّرَة</h2>
<p>في سياق الألعاب ذات المَجْمُوع العام، تظهر المُعْضِلات الاِجْتِمَاعِيَّة عندما يسعى الوُكَلاء الأفراد لتحسين مكافآتهم الشخصية لكنهم بذلك يُقَوِّضُون النتيجة الجماعية أو الرَّفَاهِيَّة الاِجْتِمَاعِيَّة. هذه الظاهرة تكون أكثر وضوحاً عندما تكون النتيجة الجماعية أدنى من النتيجة التي كان يمكن تحقيقها من خلال التعاون الكامل. توضح الدراسات النظرية، مثل مُعْضِلَة السَّجِين، السيناريوهات التي يكون فيها كل مشارك، رغم أنه يكون في وضع أفضل عند الاِعْتِراف، يحقق مكافأة جماعية أقل مقارنة بالبقاء صامتاً.</p>
<p>ومع ذلك، في مُعْضِلَة السَّجِين المُتَكَرِّرَة (<span class="nodecor">IPD</span>)، لم يعد التخلي غير المشروط هو الاِسْتراتِيجِيَّة السائدة. على سبيل المثال، في مواجهة خَصْم يتبع اِسْتراتِيجِيَّة الرَّد بالمِثْل (<span class="nodecor">TFT</span>)، يؤدي التعاون المستمر إلى عائد أعلى للوَكِيل. قد يتوقع المرء أن التَّعَلُّم الآلي المُعَزَّز للوُكَلاء المتعددين (<span class="nodecor">MARL</span>)، المصمم لتعظيم عائد كل وَكِيل، سيكتشف اِسْتراتِيجِيَّة <span class="nodecor">TFT</span>، حيث تعزز كلاً من العوائد الجماعية والفردية، ولا توفر حافزاً لتغيير السِياسة، مجسدة توازن ناش. ومع ذلك، تكشف الملاحظات التجريبية أن الوُكَلاء القياسيين في التَّعَلُّم المُعَزَّز، المُدَرَّبِين لتعظيم عائدهم الخاص، يميلون عادة إلى التخلي غير المشروط.</p>
<p>هذا يمثل أحد التحديات الرئيسية للتَّعَلُّم المُعَزَّز للوُكَلاء المتعددين في الألعاب ذات المَجْمُوع العام: خلال التدريب، غالباً ما يتجاهل الوُكَلاء حقيقة أن الوُكَلاء الآخرين أيضاً في عملية التَّعَلُّم. لمعالجة هذه المشكلة، وإذا كانت الرَّفَاهِيَّة الاِجْتِمَاعِيَّة هي الاِعْتِبَار الرئيسي، يمكن مشاركة المُكافَآت بين الوُكَلاء أثناء التدريب. على سبيل المثال، تدريب كلا الوَكِيلَيْن في إعداد <span class="nodecor">IPD</span> لتعظيم العائد الجماعي سيؤدي إلى تعاون مستمر. ومع ذلك، هذا النهج غير كافٍ إذا كان الهدف هو تعزيز التعاون المبني على المُعامَلَة بالمِثْل. تتطلب سِياسة تحفز الخَصْم على التعاون من أجل تعظيم عائده الخاص. بينما <span class="nodecor">TFT</span> هي إحدى هذه السِياسات، فإن تصميم سِياسات مماثلة لـ <span class="nodecor">TFT</span> يدوياً في مجالات أخرى ليس مرغوباً فيه ولا يمكن تحقيقه، مما يبرز الحاجة إلى تطوير خوارزميات تدريب جديدة يمكنها اكتشاف هذه السِياسات.</p>
<h1 id="sec:relatedworks">الأَعْمَال ذات الصِّلَة</h1>
<p>تحاول (<span class="nodecor">LOLA</span>) تشكيل الخَصْم من خلال أخذ التدرج للقيمة بالنظر إلى خطوة واحدة للأمام من معاملات الخَصْم. بدلاً من النظر في العائد المتوقع تحت زوج معاملات السِياسة الحالية، <span class="math inline">\(V^1(\theta_i^1, \: \theta_i^2)\)</span>، تقوم (<span class="nodecor">LOLA</span>) بتحسين <span class="math inline">\(V^1(\theta_i^1, \: \theta_i^2 + \Delta \theta_i^2)\)</span> حيث يشير <span class="math inline">\(\Delta \theta_i^2\)</span> إلى خطوة تَعَلُّم بسيطة للخَصْم. لإجراء حساب التدرج للتحديث <span class="math inline">\(\Delta \theta_i^2\)</span>، تعتبر (<span class="nodecor">LOLA</span>) القيمة البديلة المعطاة بواسطة تقريب تايلور من الدرجة الأولى لـ <span class="math inline">\(V^1(\theta_i^1, \: \theta_i^2 + \Delta \theta_i^2)\)</span>. نظراً لأنه لا يمكن حساب القيمة الدقيقة تحليلياً في معظم الألعاب، يقدم المؤلفون صيغة تدرج السِياسة التي تعتمد على تدحرجات البيئة لتقريبها. تمكنت هذه الطريقة من إيجاد اِسْتراتِيجِيَّات الرَّد المتبادل في لعبة السَّجِين المُتَكَرِّرَة.</p>
<p>تقدم (<span class="nodecor">POLA</span>) نسخة مثالية من (<span class="nodecor">LOLA</span>) لا تتأثر بمعاملات السِياسة. للقيام بذلك، يحاول كل لاعب زيادة احتمال الأفعال التي تؤدي إلى عوائد أعلى مع معاقبة التباين كولباك-لايبلر في فضاء السِياسة بالنسبة لسِياساتهم في الخطوة الزمنية السابقة. مشابهة لطريقة النقطة القريبة، تشكل كل خطوة من خطوات (<span class="nodecor">POLA</span>) مشكلة تحسين يتم حلها تقريباً من خلال التنازل التدريجي. مثل (<span class="nodecor">LOLA</span>)، تُستخدم (<span class="nodecor">POLA</span>) تدحرجات المسار لتقدير قيمة كل لاعب وتطبق مُقَدِّر التَّعْزِيز لحساب التدرجات. تحقق (<span class="nodecor">POLA</span>) تعاوناً غير قابل للاِسْتِغْلال بفعالية في لعبة السَّجِين المُتَكَرِّرَة ولعبة العُمْلَة مع تحسين العيوب في سابقتها.</p>
<p>يُعْتَبَر (<span class="nodecor">mfos</span>) لعبة فوقية حيث يتم لعب لعبة كاملة في كل خطوة فوقية والمكافأة الفوقية هي عائد تلك اللعبة. العامل هو سِياسة فوقية تتعلم التأثير على سلوك الخَصْم خلال هذه التدحرجات. يغير (<span class="nodecor">mfos</span>) اللعبة ولا يمكن مقارنته بطريقتنا التي تعتبر تَعَلُّم سِياسة واحدة. يغير (<span class="nodecor">rusp</span>) هيكل اللعبة حيث يشارك كل عامل في المكافأة مع عملاء آخرين. يدرك العملاء هذا التجميع للمكافآت من خلال نسخة مشوشة من مصفوفة مشاركة المكافآت. في وقت الاختبار، يتم ضبط مصفوفة التمثيل على عدم مشاركة المكافآت ولا يتم إضافة ضوضاء إلى هذه المصفوفة.</p>
<p>تدور ألعاب (<span class="nodecor">colman1998stackelberg</span>) حول اختيار الفعل الأول للقائد يليه حركة الأتباع اللاحقة. يقدم إطار العمل الممثل-الناقد ثنائي المستويات (<span class="nodecor">zhang2020bi</span>) نهجاً مبتكراً لتدريب كل من القائد والأتباع في نفس الوقت خلال فترة التدريب مع الحفاظ على قابلية التنفيذ المستقلة، مما يجعله مناسباً لمعالجة تحديات التنسيق في التَّعَلُّم المتعدد للعملاء. على عكس إعدادنا، حيث يعمل المُحَقِّق كأداة تدريب يتم التخلص منها بعد التدريب، يختلف (<span class="nodecor">zhang2020bi</span>) من خلال نشر كل من القائد والأتباع معاً خلال وقت الاختبار (حيث الاِهْتِمام الرئيسي هو التنسيق بين القائد والأتباع). تعكس التفاعلات بين العامل والمُحَقِّق الإعداد الأساسي لـ (<span class="nodecor">colman1998stackelberg</span>)، حيث يلعب العامل دور القائد والمُحَقِّق دور الأتباع.</p>
<p>يُدَرِّب (<span class="nodecor">balaguer2022good</span>) أفضل اِسْتِجَابَة لعامل تَعَلُّم، معكوساً فكرة أفضل اِسْتِجَابَة لأفضل اِسْتِجَابَة. يقدم المؤلفون طريقتين للتدريب ضد هذه الاِسْتِجَابَة المثلى. أولاً، من خلال إنشاء رسم بياني حسابي واسع لتحسين العامل. ثانياً، باستخدام اِسْتراتِيجِيَّات تطورية. لا تتسم أي من هذه الطرق بالقابلية للتوسع. إنشاء رسم بياني حسابي كامل لكل خطوة تحسين للعامل غير فعال للغاية. علاوة على ذلك، تتطلب الاِسْتراتِيجِيَّات التطورية تدريب الخَصْم ضد نقاط بيانات جديدة في كل مرة. تتجنب طريقتنا هذه المشكلة باستخدام شبكة عصبية لتقليل عملية التحسين. يُوَحِّد (<span class="nodecor">lanctot2017unified</span>) العديد من إطارات تدريب التَّعَلُّم المتعدد للعملاء مثل التَّعَلُّم المستقل، الاِسْتِجَابَة المثلى المتكررة، واللعب الذاتي الخيالي. تمدد طرق عائلة (<span class="nodecor">PSRO</span>) مجموعة من السِياسات السابقة بشكل تكراري، من خلال إضافة أفضل اِسْتِجَابَة لمزيج من تلك السِياسات السابقة. على عكس (<span class="nodecor">BRS</span>), لا تُفَاضِل (<span class="nodecor">PSRO</span>) عبر أفضل اِسْتِجَابَة.</p>
<h1 id="تشكيل-الاستجابة-الأمثل">تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</h1>
<p>تُدَرِّب خوارزمية تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ (Best Response Shaping) العامل من خلال التَّفاضُل عبر تقريب للخَصْم ذو الاِسْتِجَابَةِ الأَمْثَلِ (كما وُصِف في القسم [sec:method:bestresponse]). هذا الخَصْم، المُسَمَّى <em>المُحَقِّق</em>، يعتمد على سِياسة العامل من خلال آلية الإِجَابَة على الأَسْئِلَة لاختيار أفعاله (القسم [sec:method:detective]). بعد ذلك، نقوم بتدريب العامل من خلال التَّفاضُل عبر المُحَقِّق باستخدام مُقَدِّر التدرج REINFORCE (<span class="nodecor">reinforce</span>) (القسم [sec:method:agent]). كما نقترح اللعب الذاتي كطريقة تنظيمية لتشجيع السلوك التعاوني، مما يشجع العامل على استكشاف السِياسات التعاونية. نُثْبِت أيضاً أن هذا اللعب الذاتي يعادل اللعب الذاتي مع مشاركة المكافآت. يتم توفير الشفرة الزائفة لـ BRS في الخوارزمية [algo:cobalt].</p>
<h2 id="sec:method:bestresponse">وَكِيل الاِسْتِجَابَةِ الأَمْثَلِ لِلخَصْم ذو الاِسْتِجَابَةِ الأَمْثَلِ</h2>
<p>نتبع تعريفاتنا ورموزنا من (<span class="nodecor">Agarwal2021</span>)، نرمز لـ <span class="math inline">\(\tau\)</span> كمسار توزعه، <span class="math inline">\(\text{Pr}_\mu^{\theta_1, \theta_2}(\tau)\)</span>، بتوزيع الحالة الابتدائية <span class="math inline">\(\mu\)</span>، يُعطى بواسطة <span class="math display">\[\begin{aligned}
    \text{Pr}_\mu^{\theta_1, \theta_2}(\tau) = \mu(s_0) \pi_{\theta_1}(a_0 | s_0)\pi_{\theta_2}(b_0 | \pi_{\theta_1}, s_0) P(s_1 | s_0, a_0, b_0) \hdots\end{aligned}\]</span> هنا <span class="math inline">\(a\)</span> تدل على الفعل الذي يتخذه الوَكِيل و<span class="math inline">\(b\)</span> الفعل الذي يتخذه الخَصْم. الخَصْم ذو الاِسْتِجَابَةِ الأَمْثَلِ هو السِياسة التي تحصل على أعلى عائد متوقع ضد وَكِيل معين. بشكل رسمي، بالنظر إلى <span class="math inline">\(\theta_1\)</span>، فإن سِياسة الخَصْم ذو الاِسْتِجَابَةِ الأَمْثَلِ <span class="math inline">\(\theta_2^*\)</span> تحل للمعادلة التالية: <span class="math display">\[\begin{aligned}
    \theta_2^* = \argmax_{\theta_2} \mathbb{E}_{\tau \sim 
     \text{Pr}_\mu^{\theta_1, \theta_2}} \left[ R^2(\tau)\right]\end{aligned}\]</span></p>
<p>بعد ذلك، نقوم بتدريب سِياسة الوَكِيل للحصول على أعلى عائد متوقع ضد وَكِيل الاِسْتِجَابَةِ الأَمْثَلِ. يتم حل تدريب سِياسة الوَكِيل للمعادلة التالية: <span class="math display">\[\begin{aligned}
\theta_1^{**} = \argmax_{\theta_1}  \mathbb{E}_{\tau \sim 
 \text{Pr}_\mu^{\theta_1, \theta_2^*}} \left[R^1(\tau)\right] \end{aligned}\]</span> لاحظ أن هذه مشكلة تحسين ثنائية المستوى. نفترض أن الوَكِيل <span class="math inline">\(\pi_{\theta_1}^{**}\)</span> يُظْهِر خصائص وَكِيل غير قابل للاِسْتِغْلال، حيث يتعلم اِسْتراتِيجِيَّات الاِنْتِقام رداً على خَصْم متخلف، مما يخلق حوافز لخَصْم عقلاني للتعاون.</p>
<h2 id="sec:method:detective">تَدْرِيب الخَصْم الكاشِف</h2>
<p>في التَّعَلُّم المُعَزَّز العميق، يعتمد تدريب الوُكَلاء على استخدام التحسين المبني على التدرج. ونتيجة لذلك، نحتاج إلى خَصْم قابل للتَّفاضُل يَقْتَرِب من أفضل اِسْتِجَابَة ممكنة. نُسَمِّي هذا الخَصْم "الكاشِف". تعتمد سِياسة الكاشِف على سِياسة الوَكِيل بالإضافة إلى حالة البيئة، والتي نرمز لها بـ <span class="math inline">\(\pi_{\theta_2} (a|\pi_{\theta_1}, s)\)</span>. نقوم بتدريب الكاشِف ليحقق أقصى عائد له ضد وُكَلاء مختلفين. من الناحية الرسمية، يتم تدريب الكاشِف بواسطة الخطوة التدرجية التالية: <span class="math display">\[\begin{aligned}
    \nabla_{\theta_2} \underset{\theta_1 \sim 
\mathcal{B}}{\mathbb{E}}\underset{\tau \sim 
 \text{Pr}_\mu^{\theta_1, \theta_2}}{\mathbb{E}} \left[R^2(\tau)\right]\end{aligned}\]</span> حيث يُمثل <span class="math inline">\(\mathcal{B}\)</span> توزيعاً لسِياسات متنوعة للوَكِيل <span class="math inline">\(1\)</span>. يجب الإشارة إلى أن الكاشِف يتم تدريبه عبر الإنترنت ويتم تحديث الذاكرة المؤقتة، <span class="math inline">\(\mathcal{B}\)</span>، بمعاملات الوَكِيل الحالية.</p>
<h3 id="التكييف-على-سياسة-الوكيل">التَّكْيِيف على سِياسة الوَكِيل</h3>
<p>يستعلم المُحَقِّق عن سلوك الوَكِيل في حالات مختلفة من اللعبة. للقيام بذلك، يقوم بتقييم احتمالات أفعال الوَكِيل (الإجابات) على حالة من اللعبة (الأسئلة). بشكل رسمي، لنفترض أن <span class="math inline">\(\mathcal{Q}_{\psi}(\theta_1, s)\)</span> هي الدالة التي يستخدمها المُحَقِّق لاستخراج تمثيل واعٍ بالحالة للوَكِيل. نُسَمِّي <span class="math inline">\(\mathcal{Q}\)</span> دالة الإجابة على الأسئلة (QA) إذا كان يمكن التعبير عن <span class="math inline">\(\mathcal{Q}\)</span> بأنها تملك الوصول فقط إلى دالة السِياسة، أي <span class="math inline">\(\mathcal{Q}_{\psi}(\pione, s)\)</span>. هناك العديد من الطرق الممكنة لتصميم دالة QA. بعد ذلك، نحدد طريقة أظهرت نجاحاً في لعبة العُمْلَة.</p>
<h3 id="sec:method:detective:qa">الإجابة على الأسئلة بناءً على المحاكاة</h3>
<p>سلوك الوَكِيل في استمراريات محتملة للعبة بدءاً من الحالة <span class="math inline">\(s\)</span> يحمل معلومات قيمة. على وجه التحديد، يمكننا تقييم سلوك الوَكِيل مقابل وَكِيل عشوائي يبدأ من حالة اللعبة <span class="math inline">\(s\)</span>. رسمياً، ليكن <span class="math inline">\(\delta_A\)</span> معرفاً كما يلي حيث <span class="math inline">\(\tau\)</span> هو مسار يبدأ من الحالة <span class="math inline">\(s\)</span> في الوقت <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
\delta_A := \underset{\tau \sim 
  \text{Pr}_\mu^{\theta_1, \theta_r}}{\mathbb{E}} \left[R^r(\tau) | s_t = s\right]\end{aligned}\]</span></p>
<p>حيث <span class="math inline">\(\pi_{\theta_r}\)</span> هو خَصْم يختار الفعل <span class="math inline">\(A\)</span> في الوقت <span class="math inline">\(t\)</span> وبعد ذلك يأخذ عينات من توزيع موحد على جميع الأفعال الممكنة: <span class="math display">\[\begin{aligned}
    \pi_{\theta_r}(a_i = A| s_i) = 
    \begin{cases}
        \frac{1}{|\mathcal{A}|} &amp; \text{if } i &gt; t \\ 
        \mathbbm{1}_{\{a_i = A\}} &amp; \text{if } i = t \\
    \end{cases}\end{aligned}\]</span> يُقَدَّر المُحَقِّق <span class="math inline">\(\delta_A\)</span> بواسطة تدحرجات مونت كارلو للعبة إلى طول معين بين الوَكِيل والخَصْم العشوائي، <span class="math inline">\(\pi_{\theta_r}\)</span>. نرمز لتقدير <span class="math inline">\(\delta_A\)</span> بـ <span class="math inline">\(\hat{\delta}_{A}\)</span>. ثم نعرف <span class="math inline">\(\mathcal{Q}^{\text{simulation}}\)</span> = <span class="math inline">\([\hat{\delta}_{A_1}, \hat{\delta}_{A_2}, \cdots, \hat{\delta}_{A_{|\mathcal{A}|}}]\)</span>. يعتبر عدد العينات المستخدمة لتقدير عوائد اللعبة وطول الألعاب المحاكاة معاملات فائقة لـ <span class="math inline">\(\mathcal{Q}^{\text{simulation}}\)</span> الإجابة على الأسئلة. لاحظ أنه يمكن تمييز <span class="math inline">\(\mathcal{Q}^{\text{simulation}}\)</span> بالنسبة لمعاملات سِياسة الوَكِيل عبر مُصْطَلَح التَّعْزِيز (<span class="nodecor">reinforce</span>). على وجه التحديد، نستخدم عامل النرد (<span class="nodecor">LOLA</span>).</p>
<p>[sec:method:agent]</p>
<h3 id="التمييز-من-خلال-المحقق">التَّفاضُل عبر المُحَقِّق</h3>
<p>تم تدريب سِياسة الوَكِيل لتعظيم عائده ضد خَصْم المُحَقِّق من خلال مُقَدِّر التدرج REINFORCE. ومع ذلك، نظراً لأن سِياسة المُحَقِّق تأخذ سِياسة الوَكِيل كمدخلات، فإن مُصْطَلَح REINFORCE سيشمل مُصْطَلَحاً إضافياً للاِنتشار العكسي للمُحَقِّق فوق المُصْطَلَح المعتاد لـ REINFORCE: <span class="math display">\[\begin{aligned}
\underset{\tau \sim \text{Pr}_\mu^{\theta_1, \theta_2}}{\mathbb{E}} \left[R^{1}(\tau) \sum_{t=1}^{T} \left[\nabla_{\theta_1} \log(\pione(a_{t}|s_t)) + \underbrace{\nabla_{\theta_1}\log(\pitwo(b_{t}|\pi_{\theta_1}, s_t))}_{\text{مُصْطَلَح الاِنتشار العكسي للمُحَقِّق}}\right]\right]\end{aligned}\]</span></p>
<p>يمكن اعتبار هذا المُصْطَلَح الإضافي كالاِتجاه في فضاء السِياسات الذي يشجع فيه تغيير معاملات الوَكِيل المُحَقِّق على اتخاذ إجراءات تزيد من عائد الوَكِيل الخاص.</p>
<h3 id="تنظيم-التعاون-من-خلال-اللعب-الذاتي-مع-مشاركة-المكافآت">تَنْظِيم التَّعاوُن من خلال اللعب الذاتي مع مشاركة المكافآت</h3>
<p>العوامل التي تتدرب ضد خُصُوم عقلانيين تميل إلى الاِعتماد على الافتراض بأن العامل المعارض متساهل تجاه أفعالهم غير التعاونية. يسمح هذا الاِعتماد على السلوك العقلاني لهم بأن يستغلوا الخَصْم إلى حد ما. ونتيجة لذلك، قد لا يتعلمون بفعالية كيفية التعاون مع أنفسهم. في السيناريوهات التي يكون الهدف فيها هو تعزيز السلوك التعاوني، وخاصة تشجيع العامل على التعاون مع نفسه، فإن النهج المباشر هو تدريب العامل في إعداد اللعب الذاتي، مع الافتراض بأن سِياسة الخَصْم تعكس سِياسة العامل. من الناحية الرسمية، نقوم بتحديث العامل باستخدام قاعدة التحديث التالية: <span class="math display">\[\begin{aligned}
\label{eqn:selfplayrewardsharing}
\nabla_{\theta_1} \underset{\tau \sim \text{Pr}_\mu^{\theta_1, \theta_1}}{\mathbb{E}} \left[ R^1(\tau)\right]\end{aligned}\]</span> نُثْبِت أنه في الألعاب المتماثلة مثل لعبة السجناء التكرارية ولعبة العُمْلَة، هذا يعادل تدريب عامل مع اللعب الذاتي مع مشاركة المكافآت (انظر البرهان في §[app:self-play]). يُبْرِز هذا التدريب العنصر التعاوني لألعاب المَجْمُوع العام. في الألعاب ذات المَجْمُوع الصفري، لن يكون لهذا التحديث أي تأثير حيث سيكون التدرج صفراً (انظر البرهان في §[app:self-play]). نشير إلى هذا المُصْطَلَح الخسارة التنظيمية باسم اللعب الذاتي مع مشاركة المكافآت طوال الورقة. نقوم أيضاً بإجراء تجربة على BRS-NOSP حيث نتخطى خسارة اللعب الذاتي لدراسة تأثيرها.</p>
<!-- ... باقي النص كما هو ... -->
</body>
</html>
