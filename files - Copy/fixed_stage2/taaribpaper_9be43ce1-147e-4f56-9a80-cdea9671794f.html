<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="John Doe">
  <meta name="author" content="Jane Smith">
  <title>نموذج TextHawk لتحليل النصوص</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">نموذج <span class="nodecor">TextHawk</span> لتحليل النصوص</h1>
<p class="author"><span class="nodecor">John Doe</span></p>
<p class="author"><span class="nodecor">Jane Smith</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">الملخص</h1>
<p>في هذه الورقة، نقدم نموذج <span class="nodecor">TextHawk</span>، وهو نظام جديد لتحليل النصوص يستخدم تقنيات متقدمة في معالجة اللغات الطبيعية. يهدف النموذج إلى تحسين فهم النصوص وتحليلها بدقة وفعالية أكبر. نستعرض في هذه الدراسة الأساليب التي يستخدمها النموذج ونقارن أداءه بالنماذج الأخرى المتاحة.</p>
<h1 id="مقدمة">مقدمة</h1>
<p>تُعد معالجة اللغات الطبيعية (<span class="nodecor">Natural Language Processing</span>) من المجالات الحيوية في علم الحاسوب، ولها تطبيقات متعددة تشمل ترجمة اللغات، والتعرف على الكلام، وتحليل النصوص. النموذج الذي نقدمه، <span class="nodecor">TextHawk</span>, يمثل خطوة مهمة نحو تحسين قدرات الأنظمة الحاسوبية على فهم اللغة البشرية ومعالجتها بشكل أكثر فعالية.</p>
<h1 id="الأساس-النظري">الأساس النظري</h1>
<p>يعتمد <span class="nodecor">TextHawk</span> على مجموعة من الخوارزميات المتطورة التي تم تطويرها لفهم النصوص وتحليلها. تشمل هذه الخوارزميات تقنيات التعلم العميق (<span class="nodecor">Deep Learning</span>)، والتي تمكن النموذج من تحليل النصوص بدقة عالية.</p>
<h1 id="التطبيقات">التطبيقات</h1>
<p>يمكن تطبيق نموذج <span class="nodecor">TextHawk</span> في مجموعة واسعة من المجالات مثل البحث العلمي، والتعليم، والصناعات التي تعتمد على معالجة البيانات النصية. بفضل قدراته المتقدمة، يساهم النموذج في تسريع وتحسين عمليات التحليل النصي.</p>
<h1 id="الخلاصة">الخلاصة</h1>
<p>يقدم نموذج <span class="nodecor">TextHawk</span> مساهمة قيمة في مجال معالجة اللغات الطبيعية، حيث يوفر أدوات قوية لتحليل النصوص وفهمها بشكل أفضل. نأمل أن يفتح هذا النموذج الباب أمام تطوير تقنيات جديدة في هذا المجال الحيوي.</p>
<h1 id="الملخص">الملخص</h1>
<p>أظهرت نماذج اللغة الكبيرة متعددة الوسائط (MLLMs) نتائج مثيرة للإعجاب في مهام متعددة الوسائط المختلفة. ومع ذلك، فإن معظم نماذج اللغة الكبيرة متعددة الوسائط الحالية لا تناسب المهام الموجهة نحو الوثائق، والتي تتطلب إدراكاً دقيقاً للصور وضغط المعلومات. في هذه الورقة، نقدم <span class="nodecor">DocuPercept</span>، نموذج لغة كبير متعدد الوسائط مصمم خصيصاً للمهام الموجهة نحو الوثائق، مع الحفاظ على القدرات العامة لنماذج اللغة الكبيرة متعددة الوسائط. يهدف <span class="nodecor">DocuPercept</span> إلى استكشاف الإدراك الدقيق الفعال من خلال تصميم أربعة مكونات مخصصة. أولاً، يتم اقتراح وحدة إعادة العينة وإعادة الترتيب (ReSA) لتقليل الفائض في نصوص الوثائق وخفض تكلفة الحساب لنموذج اللغة الكبير متعدد الوسائط. نستكشف ترميز مواقع كل ميزة محلية من خلال تقديم التضمينات الموضعية القابلة للتوسع (SPEs)، والتي يمكن أن تحافظ على قابلية التوسع لأحجام الصور المختلفة. ثم يتم تبني شبكة اقتراح الاستعلام (QPN) لتهيئة الاستعلامات بشكل ديناميكي بين الصور الفرعية المختلفة. لتعزيز القدرة الإدراكية البصرية الدقيقة لنموذج اللغة الكبير متعدد الوسائط، نصمم آلية الانتباه المتقاطع متعدد المستويات (MLCA) التي تلتقط البنية الهرمية والعلاقات الدلالية لصور الوثائق. بالإضافة إلى ذلك، نقوم بإنشاء مجموعة بيانات جديدة لضبط التعليمات للمهام الموجهة نحو الوثائق من خلال إثراء بيانات الوثائق متعددة الوسائط مع Gemini Pro. نجري تجارب واسعة على معايير نماذج اللغة الكبيرة متعددة الوسائط العامة والموجهة نحو الوثائق، ونظهر أن <span class="nodecor">DocuPercept</span> يتفوق على الطرق الحديثة، مما يدل على فعاليته وتفوقه في إدراك الوثائق الدقيق والقدرات العامة. صفحة المشروع: <a href="https://github.com/yuyq96/TextHawk" class="uri">https://github.com/yuyq96/TextHawk</a>.</p>
<h1 id="الكلمات-المفتاحيه">الكلمات المفتاحية</h1>
<p>نماذج اللغة الكبيرة متعددة الوسائط، فهم الوثائق، الإجابة على الأسئلة البصرية</p>
<h1 id="sec:intro">مقدمة</h1>
<p>لقد حظيت نماذج اللغة الكبيرة متعددة الوسائط (<span class="nodecor">MLLMs</span>) (<span class="nodecor">blip2</span>, <span class="nodecor">instructblip</span>, <span class="nodecor">llava</span>) باهتمام كبير وأحرزت تقدماً ملحوظاً مؤخراً. تُستخدم هذه النماذج نماذج اللغة الكبيرة (<span class="nodecor">LLMs</span>) كجوهر لها وتمدد قدرات نماذج اللغة الكبيرة القوية إلى وسائط أخرى، مثل الوسائط البصرية. بفضل مجموعة واسعة من سيناريوهات التطبيق لفهم صور الوثائق، فإن لها موقعاً محورياً في مجال الإدراك البصري. تعد قدرة فهم صور الوثائق إحدى القدرات الأساسية لنماذج اللغة الكبيرة متعددة الوسائط، مما يجعل تحقيق التطبيقات الرائدة أمراً سهلاً، مثل وكلاء التطبيقات الذكية المبنية على نماذج اللغة الكبيرة متعددة الوسائط، والقراءة المساعدة بالنصوص الغنية، وغيرها. ومع ذلك، تطرح صور الوثائق تحديات فريدة لنماذج اللغة الكبيرة متعددة الوسائط، حيث إنها تختلف عن الصور الطبيعية في عدة جوانب. تتميز صور الوثائق عادةً بدقة أعلى وكثافة معلومات أكبر من الصور الطبيعية، مما يعني أن نماذج اللغة الكبيرة متعددة الوسائط تحتاج إلى التغلب على صعوبتين رئيسيتين عند معالجتها. الصعوبة الأولى هي تحقيق إدراك بصري دقيق لمحتوى الوثيقة. الصعوبة الثانية هي ضغط معلومات صورة الوثيقة بكفاءة.</p>
<p>لقد حاولت الأعمال السابقة حول نماذج اللغة الكبيرة متعددة الوسائط الموجهة للوثائق حل الصعوبات المذكورة أعلاه. لتحقيق قدرات إدراك بصري دقيق أقوى، زادت <span class="nodecor">Qwen-VL</span> (<span class="nodecor">qwen-vl</span>) دقة الإدخال لمشفر الرؤية من <span class="math inline">\(224\times224\)</span> إلى <span class="math inline">\(448\times448\)</span> وقدمت <span class="nodecor">UReader</span> (<span class="nodecor">ureader</span>) وحدة قص متكيفة مع الشكل. لضغط المعلومات الوثائقية، استخدمت <span class="nodecor">mPLUG-DocOwl</span> (<span class="nodecor">mplugdocowl</span>) مجرداً بصرياً واستخدمت <span class="nodecor">Qwen-VL</span> محول الرؤية-اللغة. لقد ساهمت هذه الطرق المصممة بشكل جيد بشكل كبير في تطوير نماذج اللغة الكبيرة متعددة الوسائط الموجهة للوثائق. ومع ذلك، لا يزال هناك مجال لمزيد من الاستكشاف والتحسين في الإدراك البصري الدقيق وضغط المعلومات الوثائقية. بالإضافة إلى ذلك، تجد معظم نماذج اللغة الكبيرة متعددة الوسائط الحالية صعوبة في تحقيق التوازن بين القدرات العامة والوثائقية. على وجه التحديد، عادةً ما لا تركز نماذج اللغة الكبيرة متعددة الوسائط العامة على تحسين الإدراك البصري الدقيق وضغط المعلومات، بينما قد تفتقر نماذج اللغة الكبيرة متعددة الوسائط الموجهة للوثائق إلى القدرات العامة في تصميمها.</p>
<p>في هذه الورقة، نقترح <span class="nodecor">DocuPercept</span>، نموذج كبير متعدد الوسائط يتفوق في المهام الوثائقية المعقدة ويظهر قدرات عامة متميزة عبر مجالات الرؤية واللغة، كما هو موضح في الشكل [fig:radar]. نظراً لأن مجرد تكبير حجم الصور المدخلة لا يمكن أن يتناسب مع الدقة المتنوعة لصور الوثائق، نتبع <span class="nodecor">Ureader</span> (<span class="nodecor">ureader</span>) لقص الصور إلى صور فرعية بشكل تكيفي وفقاً لأشكال الصور. استناداً إلى ذلك، نبتكر وحدة إعادة العينة وإعادة الترتيب (<span class="nodecor">ReSA</span>) التي تضغط وتعيد ترتيب المعلومات البصرية، مما يقلل بشكل كبير من عدد الرموز البصرية، كما هو موضح في الشكل [fig:tokens]. نظراً لإدخال الصور الفرعية، نقترح تضمينات المواقع القابلة للتوسع (<span class="nodecor">SPEs</span>) لتشفير مواقع الصور الفرعية مع الحفاظ على القابلية للتوسع عبر أحجام الصور المختلفة. بالنظر إلى الاختلافات بين الصور الفرعية، يتم بعد ذلك اعتماد شبكة اقتراح الاستعلام (<span class="nodecor">QPN</span>) لتهيئة الاستعلامات بشكل ديناميكي بين الميزات المحلية. بالإضافة إلى ذلك، نقدم وحدة الانتباه المتقاطع متعددة المستويات (<span class="nodecor">MLCA</span>) التي تستفيد من الهيكل الهرمي والعلاقات الدلالية لصور الوثائق لتعزيز قدرة الإدراك البصري الدقيق. يمكن لذلك مشفر الرؤية لدينا استخراج المعلومات التفصيلية من صور الوثائق الكثيفة. بالإضافة إلى ذلك، نثري بيانات الوثائق متعددة الوسائط مع <span class="nodecor">Gemini Pro</span>، محرك نموذج اللغة الكبيرة متعدد الوسائط التجاري، للتخفيف من مشكلة عدم كفاية بيانات ضبط التعليمات.</p>
<p>نتناول تحديات الإدراك البصري الدقيق وضغط المعلومات البصرية لنماذج اللغة الكبيرة متعددة الوسائط الموجهة للوثائق ونقترح نموذجاً جديداً لنماذج اللغة الكبيرة متعددة الوسائط، يُسمى <span class="nodecor">DocuPercept</span>، يمكنه التعامل مع المهام الموجهة للوثائق ومهام الرؤية-اللغة العامة بأداء عالٍ. تتمثل مساهماتنا فيما يلي:</p>
<ol>
<li><p>نصمم <span class="nodecor">ReSA</span> لضغط المعلومات البصرية مما يقلل بشكل كبير من عدد الرموز البصرية.</p></li>
<li><p>نقترح <span class="nodecor">SPEs</span> و<span class="nodecor">QPN</span> لتناسب تمثيلات الصور الفرعية وتعزيز إدراك النموذج الدقيق.</p></li>
<li><p>نقدم <span class="nodecor">MLCA</span> التي يمكن أن تحسن قدرة الإدراك البصري الدقيق من خلال التقاط المعلومات العالمية والمحلية واستغلال الهيكل الهرمي.</p></li>
<li><p>نثري بيانات ضبط التعليمات متعددة الوسائط لمهام موجهة للوثائق مختلفة مع <span class="nodecor">Gemini Pro</span>. يمكن لهذه البيانات تسهيل ضبط الدقة لـ <span class="nodecor">DocuPercept</span> وتعود بالفائدة على مجتمع البحث.</p></li>
<li><p>نظهر أن <span class="nodecor">DocuPercept</span> يحقق نتائج رائدة في كل من معايير الوثائق والمعايير العامة، مما يظهر قدراته البصرية الدقيقة المتفوقة وقدراته العامة في مجال الرؤية-اللغة.</p></li>
</ol>
<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<h2 id="نماذج-اللغة-الكبيرة-متعددة-الوسائط">نماذج اللغة الكبيرة متعددة الوسائط</h2>
<p>نماذج اللغة الكبيرة متعددة الوسائط هي فئة من النماذج التي يمكنها معالجة وتوليد المعلومات متعددة الوسائط، والتي تشمل بشكل رئيسي اللغة الطبيعية والمعلومات البصرية. لقد أظهرت هذه النماذج أداءً ملحوظاً في مهام متنوعة، مثل التعليق على الصور، والإجابة على الأسئلة البصرية، والحوار البصري. تتكون نماذج اللغة الكبيرة متعددة الوسائط الحالية عادةً من مشفر بصري، ومحول بصري-لغوي، ونموذج لغة كبير.</p>
<p>(<span class="nodecor">blip2</span>) اقترح محول استعلام لربط مشفر الصور المجمد ونموذج اللغة الكبير المجمد. بدأ أولاً بتعلم تمثيل اللغة البصرية من مشفر صورة مجمد ثم طبق التعلم التوليدي من اللغة إلى البصر من نموذج لغة مجمد. (<span class="nodecor">instructblip</span>) قام بتنفيذ تعديل تعليمات اللغة البصرية بناءً على النموذج المدرب مسبقاً (<span class="nodecor">blip2</span>) من خلال تقديم محول استعلام مدرك للتعليمات. (<span class="nodecor">llava</span>) اتبع هندسة مماثلة مع استخدام طبقة خطية بسيطة لربط الرؤية واللغة. لقد حول أزواج الصور والنصوص إلى تنسيق يتبع التعليمات مع ChatGPT/GPT-4 لتحسين نتائج التنعيم الدقيق. (<span class="nodecor">minigpt4</span>) اعتمد محول Q المجمد وطبقة إسقاط خطية واحدة لمحاذاة الوضع البصري واللغوي. (<span class="nodecor">llava-1.5</span>) هو نسخة محسنة من (<span class="nodecor">llava</span>)، التي اعتمدت مشفر رؤية بصور مدخلات أكبر وطبقة MLP ذات طبقتين لتحسين الأداء. (<span class="nodecor">mplugowl</span>) اقترح نمط تدريب جديد سمح بتدريب مشفر الرؤية والمجرد البصري في مرحلة التدريب المسبق ومكن LoRA مع نموذج اللغة الكبير في مرحلة تعديل التعليمات. (<span class="nodecor">mplugowl2</span>) صمم وحدة تكيفية للوضعية بناءً على (<span class="nodecor">mplugowl</span>) ومكن جميع الوحدات للتدريب. (<span class="nodecor">qwen-vl</span>) استخدم خط أنابيب تدريب من ثلاث مراحل، بما في ذلك التدريب المسبق مع أزواج الصور والنصوص، والتدريب المسبق متعدد المهام مع البيانات متعددة المهام والمتداخلة، والتنعيم الدقيق تحت الإشراف مع بيانات VL المتداخلة في الدردشة.</p>
<p>يمكن لهذه الطرق فهم صور النصوص إلى حد ما، ولكن لديها إدراك بصري محدود للوثائق الكثيفة، خاصة تلك التي تحتوي على صور عالية الدقة.</p>
<h2 id="نماذج-اللغة-الكبيرة-المتعددة-الوسائط-الموجهة-للوثائق">نماذج اللغة الكبيرة متعددة الوسائط الموجهة للوثائق</h2>
<p>نماذج اللغة الكبيرة متعددة الوسائط الموجهة للوثائق هي نماذج لغوية كبيرة يمكنها فهم النصوص من أنواع مختلفة من الوثائق، مثل الرسوم البيانية، الجداول، صفحات الويب، والأوراق العلمية. عادةً ما تتضمن هذه النماذج بعض التكييفات المحددة لصور الوثائق استناداً إلى نماذج اللغة الكبيرة متعددة الوسائط العامة.</p>
<p>(<span class="nodecor">mplugdocowl</span>) تبع نموذج (<span class="nodecor">mPLUG-Owl</span>) وأضاف بعض بيانات تعليمات الوثائق، بما في ذلك الوثيقة، الجدول، صفحة الويب، والرسم البياني. (<span class="nodecor">ureader</span>) اقترح وحدة قص متكيفة مع الشكل للحصول على قدرة إدراك بصري دقيق أفضل لصور الوثائق، استناداً إلى نموذج (<span class="nodecor">mPLUG-Owl</span>) المدرب مسبقاً. (<span class="nodecor">unidoc</span>) كان مجهزاً بمهام كشف النص والتعرف على النص في تعليماته لتحسين قدرة فهم النص. (<span class="nodecor">monkey</span>)، نموذج اللغة الكبيرة متعددة الوسائط مع تصاميم خاصة لصور الوثائق، دعم دقة أعلى وقدم بيانات وصف متعددة المستويات استناداً إلى نموذج (<span class="nodecor">Qwen-VL</span>) المدرب مسبقاً.</p>
<p>تركز نماذج اللغة الكبيرة متعددة الوسائط الموجهة للوثائق الحالية بشكل رئيسي على التكيف مع دقة الصور الأعلى واستغلال المزيد من بيانات التحسين المحددة للوثائق. يركز نموذجنا المقترح أيضاً على الإدراك البصري الدقيق لصور الوثائق عالية الدقة وتوليد بيانات الوثائق، مع تصاميمنا الجديدة. بالإضافة إلى ذلك، نولي اهتماماً لضغط المعلومات والحفاظ على القدرات العامة.</p>
<h1 id="الطريقة">الطريقة</h1>
<p>تم تصميم نموذجنا بهدفين: لمعالجة المدخلات البصرية بدقة متفاوتة بفعالية ولضغط الرموز البصرية.</p>
<h2 id="ssec:arch">الهندسة المعمارية</h2>
<p>تتكون هندسة <span class="nodecor">DocuPercept</span> من مشفر بصري مجمد، وجهاز إعادة تشكيل، ونموذج لغوي كبير مع <span class="nodecor">LoRA</span> ورأس كشف.</p>
<h3 id="المشفر-البصري.">المشفر البصري.</h3>
<p>لتسريع تشفير الصورة، نفضل استخدام مشفر بصري خفيف بدلاً من نموذج ضخم أو هائل. (<span class="nodecor">siglip</span>)، وهو متغير من (<span class="nodecor">clip</span>) الذي يعتمد خسارة السيجمويد للتدريب المسبق على الرؤية-اللغة بدلاً من التعلم التبايني مع تطبيع سوفتماكس، يحقق دقة أفضل في مهام متعددة دون تدخل مسبق مقارنة بمنافسيه. لذلك، نستخدم محول الرؤية (<span class="nodecor">ViT</span>) من نموذج (<span class="nodecor">SigLIP-SO</span>) الفعال كمشفر بصري لدينا للعرض، والذي يحتوي على تكوينات مختلفة لطبقات المحول ولكن بتكلفة حسابية مماثلة لنموذج (<span class="nodecor">ViT-L</span>) القياسي. ومع ذلك، يجب أن تكون جميع أنواع المشفرات البصرية قابلة للتطبيق في إطار عملنا، بما في ذلك النماذج المدربة مسبقاً بأساليب مختلفة أو المبنية بمعماريات مختلفة.</p>
<h3 id="إعادة-المعايرة.">إعادة المعايرة.</h3>
<p>بشكل مشابه لـ Q-Former (<span class="nodecor">blip2</span>)، يتألف إعادة معايرة الرموز البصرية لدينا غالباً من مفكك غير سببي يعتمد مجموعة من الأوزان القابلة للتعلم كاستعلامات أولية ويقلل بشكل طبيعي من طول الميزات البصرية عدة مرات. من أجل مرونة هيكلية، نقوم بتهيئة إعادة المعايرة بشكل عشوائي بدلاً من تهيئتها من نموذج BERT المدرب مسبقاً أو إعادة معايرة موجودة من نماذج التعلم العميق متعددة المهام الأخرى. بديهياً، نحافظ على بعد الخفاء لطبقات إعادة المعايرة المتوسطة مساوياً لذلك في طبقات المشفرات البصرية. تحتوي إعادة المعايرة على 8 طبقات ويتم إزالة الانتباه الذاتي في الطبقة الأولى. من أجل تعزيز الوعي بمعلومات الموضع أثناء الانتباه المتقاطع، نستخدم ترميزات الموضع الجيبية والتضمينات الموضعية المتعلمة لمخرجات المشفرات البصرية والاستعلامات على التوالي في كل طبقة من طبقات الانتباه المتقاطع.</p>
<h3 id="نموذج-اللغة-الكبير.">نموذج اللغة الكبير.</h3>
<p>لتسهيل التدريب المسبق والاستفادة من التدريب المتداخل بين الرؤية واللغة، نقوم بتهيئة نموذج اللغة الكبير الخاص بنا بسعة <span class="nodecor">7B</span> بأوزان (<span class="nodecor">xcomposer</span>). يعتمد (<span class="nodecor">xcomposer</span>)، مشابهاً لـ (<span class="nodecor">BLIP-2</span>)، على أداة إعادة تنظيم الرموز البصرية تُسمى أداة الإدراك لتوفير الجسر بين المشفر البصري ونموذج اللغة الكبير، لكنه مرتكز على نموذج لغة كبير متعدد اللغات آخر يُسمى (<span class="nodecor">internlm</span>). تقريباً، تكون هندسة (<span class="nodecor">internlm</span>) مماثلة لـ (<span class="nodecor">LLaMA</span>) باستثناء الاحتفاظ بالتحيزات في وحدات الانتباه. على وجه التحديد، يتم تدريب (<span class="nodecor">xcomposer</span>) على مرحلتين: المرحلة الأولى هي التدريب المسبق للغة الرؤية، والذي يشمل أزواج الصور والنصوص بالإضافة إلى البيانات المتداخلة للصور والنصوص. يتم تحديث كل من أداة الإدراك ونموذج اللغة الكبير في هذه المرحلة. المرحلة الثانية هي التنقيح الدقيق متعدد المهام تحت إشراف، حيث يتم تحديث أداة الإدراك ووحدات (<span class="nodecor">LoRA</span>) فقط. لتجنب تسرب البيانات المحتمل من مجموعات بيانات التنقيح الدقيق لـ (<span class="nodecor">xcomposer</span>)، نحتفظ فقط بأوزان نموذج اللغة الكبير من مرحلة التدريب المسبق الأولى ونتخلى عن جميع الأوزان من المشفر البصري، أداة الإدراك، ووحدات (<span class="nodecor">LoRA</span>).</p>
<h2 id="ssec:efgp">الإدراك الدقيق الفعال</h2>
<h3 id="القص-التكيفي-للشكل.">القص التكيفي للشكل.</h3>
<p>يقوم المشفر البصري المدرب مسبقاً بتوحيد دقة الصورة إلى حجم ثابت وأقل، دون مراعاة النسبة الأصلية للأبعاد. تؤدي هذه المعالجة إلى تقليل القدرة على إدراك المحتوى الدقيق في الصور عالية الدقة وتقديم تشوهات ملحوظة في نسبة الأبعاد. باتباع (<span class="nodecor">ureader</span>)، نقوم بتعزيز ViT المجمد بدمج استراتيجية قص ديناميكية، مما يمكن من التعامل الفعال مع الصور ذات النسب العشوائية للأبعاد والدقة. على وجه التحديد، سيتم قص صورة مدخلة <span class="math inline">\(\varv\)</span> بشكل <span class="math inline">\((h\times w)\)</span> إلى عدة صور فرعية لتتوافق مع أحد الشبكات المحددة مسبقاً <span class="math inline">\(\{\varg=(r\times c)|r,c\in\{1,2,\dots,l\},r\cdot c\leq n\}\)</span>، حيث <span class="math inline">\(r\)</span> و <span class="math inline">\(c\)</span> تدلان على الصفوف والأعمدة للشبكة <span class="math inline">\(\varg\)</span>، <span class="math inline">\(l\)</span> تدل على الحد الأقصى لـ <em>طول الجانب</em> (عدد الصور الفرعية في صف أو عمود واحد)، و <span class="math inline">\(n\)</span> تدل على الحد الأقصى لـ <em>المساحة</em> (عدد الصور الفرعية في الصورة بالكامل). يتم تنظيم محاذاة الشبكة بواسطة قياسات تقاطع الاتحاد المنتظمة والموجهة حسب الشكل (IoU). دعونا نعرف صندوق الصورة كـ <span class="math inline">\(\text{box}(\varv)=(0,0,h,w)\)</span>، وصندوق الشبكة كـ <span class="math inline">\(\text{box}(\varg)=(0,0,rH,cW)\)</span>، وصندوق الشكل الموجه كـ <span class="math inline">\(\text{box}_\text{s}(\varv,\varg)=(0,0,\frac{wr}{h}H,cW)\)</span>، حيث <span class="math inline">\((H\times W)\)</span> هو شكل الإدخال لـ ViT. تعرف قيم IoU كما يلي: <span class="math display">\[\begin{aligned}
        S_\text{r}(\varv,\varg)&amp;=\text{IoU}(\text{box}(\varv),\text{box}(\varg)),\\
        S_\text{s}(\varv,\varg)&amp;=\text{IoU}(\text{box}_\text{s}(\varv,\varg),\text{box}(\varg)),\\
        S(\varv,\varg)&amp;=S_\text{r}(\varv,\varg)+S_\text{s}(\varv,\varg).
    \end{aligned}\]</span> نختار الشبكة النهائية بأعلى قيمة IoU مجمعة <span class="math inline">\(S\)</span>، من أعلى <span class="math inline">\(k\)</span> شبكات بأعلى قيم IoU منتظمة <span class="math inline">\(S_\text{r}\)</span>.</p>
<h3 id="إعادة-العينة-وإعادة-الترتيب-resa.">إعادة العينة وإعادة الترتيب (ReSA).</h3>
<p>بعد تمكين المشفر البصري من قبول مدخلات بدقة متغيرة، يمكن أن ينمو عدد رموز الصورة بشكل أسي مع دقة الصورة. بدون ضغط الرموز، يصل العدد الأقصى للرموز لصورة واحدة إلى <span class="math inline">\(nHW/p^2\)</span> بالنظر إلى حجم البقعة <span class="math inline">\(p\)</span>. بشكل محدد، ستستهلك صورة وثيقة قياسية محاذاة مع شبكة <span class="math inline">\(5\times4\)</span> حتى <span class="nodecor">5120</span> رمزاً. عادةً ما تظهر نماذج اللغة الكبيرة متعددة الوسائط المفتوحة المصدر ذات الإدراك الدقيق قدرة على ضغط رموز الصورة بنسبة <span class="nodecor">4</span>. على سبيل المثال، تقلل Qwen-VL وMonkey عدد رموز الصورة من <span class="nodecor">1024</span> إلى <span class="nodecor">256</span> لكل صورة فرعية بحجم <span class="math inline">\(448\times448\)</span>، بينما يضغط UReader العدد من <span class="nodecor">256</span> إلى <span class="nodecor">64</span> لكل صورة فرعية بحجم <span class="math inline">\(224\times224\)</span>. في هذه الحالة، لا يزال استهلاك رموز الصورة كبيراً. لاستكشاف إمكانية نسبة ضغط أعلى، نقترح طريقة تجمع بين مزايا إعادة العينة وإعادة الترتيب، والتي أطلقنا عليها اسم ReSA. كما هو موضح في الشكل، وبشكل مماثل لنماذج اللغة الكبيرة متعددة الوسائط السابقة، يقوم ReSA أولاً بإعادة أخذ عينات من ميزات الصورة باستخدام آلية الانتباه المتقاطع. تعكس البعد الخفي لمخرجات الانتباه المتقاطع البعد الخفي لمخرجات المشفر البصري، والذي يكون عادةً أصغر بعدة مرات من البعد الخفي لنماذج اللغة الكبيرة. مستفيدين من هذه الخاصية، نقدم خطوة إعادة ترتيب إضافية لتكثيف عدد رموز الصورة بشكل أكبر. بعد إعادة العينة، يتم دمج الرموز المعاد أخذ عينات منها في رمز واحد ثم تحويلها إلى الفضاء الكامن لنماذج اللغة الكبيرة من خلال إسقاط خطي. في تجاربنا، تمتلك كل خطوة من خطوات ReSA نسبة ضغط تبلغ <span class="nodecor">4</span>، مما يؤدي إلى نسبة ضغط أعلى بشكل ملحوظ تبلغ <span class="nodecor">16</span>.</p>
<h3 id="الانتباه-المتقاطع-متعدد-المستويات-mlca.">الانتباه المتقاطع متعدد المستويات (MLCA).</h3>
<p>كما ذكر في الأعمال السابقة (<span class="nodecor">blip2</span>, <span class="nodecor">llava</span>)، يتم تدريب مشفرات الصور مسبقاً على مهام محددة وبالتالي قد تركز الميزات من طبقاتها الأخيرة أكثر على تلك المهام. لقد ثبت أن الميزات من الطبقة قبل الأخيرة تعطي أداءً أفضل من الطبقة الأخيرة (<span class="nodecor">llava</span>). بالإضافة إلى ذلك، من الممكن دمج الميزات من عدة طبقات. في مجال الكشف عن الأجسام، شبكة هرم الميزات (<span class="nodecor">fpn</span>) معروفة بدمج الميزات متعددة المستويات، مما يحسن القدرة على الإدراك للأجسام الدقيقة. أما بالنسبة لنماذج اللغة متعددة المستويات، فقد أثبتت (<span class="nodecor">comm</span>) أن دمج الميزات العميقة والسطحية مفيد لتقليل الهلوسة وتحسين الأداء في المهام الدقيقة، حتى عند عدم وجود هيكل هرمي. مستلهمين من شبكة هرم الميزات، نقترح استراتيجية دمج الميزات متعددة المستويات تُسمى MLCA. كما هو موضح في الشكل (<span class="nodecor">fig:arch</span>) (ب)، تمكن MLCA جهاز إعادة العينات من امتصاص الميزات من طبقات مشفر الصور العميقة والسطحية مع جدول توجيه محدد مسبقاً. طالما أن العدد الإجمالي لطبقات جهاز إعادة العينات لم يتغير، فإن MLCA لا يتطلب تكلفة حسابية إضافية مقارنة بالانتباه المتقاطع القياسي. من خلال التجربة، نعتمد أربع مراحل لمشفر الصور، استخراج الميزات من طبقات المشفر الـ <span class="nodecor">14</span>، <span class="nodecor">18</span>، <span class="nodecor">22</span>، و <span class="nodecor">26</span> على التوالي.</p>
<h3 id="التضمينات-الموضعية-القابلة-للتوسع-spes.">التضمينات الموضعية القابلة للتوسع (SPEs).</h3>
<p>العلاقات الموضعية النسبية بين الصور الفرعية غير واضحة بدون إضافة تضمينات موضعية إضافية. للتعامل مع عدد متغير من قطع الصور، اقترحت الأعمال السابقة (<span class="nodecor">pix2struct</span>, <span class="nodecor">ureader</span>) تعلم تضمينات موضعية مطلقة ثنائية الأبعاد أو مفككة تغطي الفهرس الموضعي الأقصى المقدم في بيانات التدريب. لا تفتقر هذه التضمينات فقط إلى الفعالية في التوسع إلى أشكال خارج نطاق التدريب، ولكن من المؤكد أن التضمينات المتعلمة تظهر أيضاً عدم ملاءمة بسبب التوزيع غير المتساوي لأشكال الإدخال التدريبية. للتغلب على العقبات المذكورة، نقترح طريقة جديدة تُسمى SPEs، تمديد التضمينات الموضعية المفككة (حيث يتم تحليل الصف والعمود) إلى أشكال تعسفية. للتوضيح، يتم التعامل مع تضمينات الصف والعمود بنفس الطريقة في SPEs، ولذلك يتم حذف مواصفاتها في الجزء التالي.</p>
<p>أفترض أن التضمينات الموضعية المتعلمة مبدئياً من توزيع طبيعي <span class="math inline">\(\calN(0, 1)\)</span>. كل تضمين موضعي <span class="math inline">\(\vare\in\bbR^d\)</span> هو متجه بمعيار <span class="math inline">\(\ell_2\)</span>-norm <span class="math inline">\(\sqrt{d}\)</span>، مما يشير إلى أن التضمينات الموضعية موزعة عبر سطح كرة فائقة الأبعاد. في الممارسة العملية، يظل معيار <span class="math inline">\(\ell_2\)</span>-norm للتضمينات الموضعية المتعلمة ضمن نطاق ضيق خلال عملية التدريب بأكملها، محافظاً على خصائص توزيع الكرة الفائقة. التكامل الخطي الكروي (Slerp)، وهي تقنية شائعة الاستخدام في الرسومات الحاسوبية، تقوم بتكامل أي متجه وسيط بين متجهين وحدويين، وتظهر كبديل محتمل لطرق التكامل التقليدية للتضمينات الموضعية.</p>
<p>لتلبية متطلبات Slerp بدقة، نطبق التطبيع والتحجيم قبل التكامل لكل رأس انتباه، مما يضمن معيار <span class="math inline">\(\ell_2\)</span>-norm موحد عبر جميع التضمينات الموضعية: <span class="math display">\[\begin{aligned}
    \vare_i&amp;=s\frac{\tilde{\vare}_i}{\|\tilde{\vare}_i\|},\end{aligned}\]</span> حيث <span class="math inline">\(\tilde{\vare}_i\)</span> <span class="math inline">\((i\in\{0,1\})\)</span> يشير إلى تضمينين موضعيين نهائيين قابلين للتعلم، و<span class="math inline">\(s\)</span> هو عامل تحجيم قابل للتعلم مبدئياً كـ <span class="math inline">\(\sqrt{d}\)</span>.</p>
<p>كما هو موضح في الشكل [fig:spe_qpn] (a)، نستخدم Slerp لتوليد تضمينات موضعية تعسفية تمتد بين النقاط النهائية: <span class="math display">\[\begin{aligned}
    \theta&amp;=\arccos\frac{\vare_0\vare_1}{\|\vare_0\|\|\vare_1\|},\\
    \vare(t)&amp;=\frac{\sin(\theta-t\theta)}{\sin\theta}\vare_0+\frac{\sin(t\theta)}{\sin\theta}\vare_1,
\end{aligned}\]</span> حيث <span class="math inline">\(t\in[0,1]\)</span> هو الموضع الكسري، والذي يمكن أن يكون الموضع النسبي لصورة فرعية أو قطعة صورة.</p>
<h3 id="شبكة-اقتراح-الاستعلامات.">شبكة اقتراح الاستعلامات.</h3>
<p>على الرغم من الأداء المرضي الذي أظهره النموذج Q-Former على نماذج اللغة متعددة المستويات ذات الدقة الثابتة، فإن طريقة تهيئة استعلامات إعادة العينة من عدد ثابت من المعلمات المتعلمة تفتقر إلى المرونة تحت إعدادات الدقة المتغيرة. قد يؤدي إعادة استخدام الاستعلامات الأولية على صور فرعية مختلفة إلى الإفراط وأنماط انتباه غير مرغوب فيها، حيث تظهر رموز الصور المعاد تجميعها المقابلة لصور فرعية متميزة ولكن باستعلامات معادة متطابقة تشابهات قوية وتتلقى درجات انتباه أعلى بشكل غير مناسب. للقضاء على الآثار الجانبية للاستعلامات الأولية المشتركة، نقترح وحدة خفيفة تُسمى شبكة اقتراح الاستعلامات لتوليد الاستعلامات بشكل ديناميكي. كما هو موضح في الشكل، يتكون هيكل شبكة اقتراح الاستعلامات من شبكة عصبية متعددة الطبقات ذات طبقتين مع تنشيط GELU وطبقة تجميع الحد الأقصى وطبقة إسقاط خطية. يتم تغذية مخرجات المشفر البصري إلى شبكة اقتراح الاستعلامات ويتم التحكم في عدد الاستعلامات المقترحة بواسطة خطوة طبقة تجميع الحد الأقصى. للمقارنة العادلة، تعتمد تجاربنا خطوة بمقدار <span class="math inline">\(2\times2\)</span> بحيث يظل معدل الضغط <span class="nodecor">4</span>. تم ضبط بعد مخرجات طبقات الشبكة العصبية متعددة الطبقات وبعد مدخلات طبقة الإسقاط على أربعة أضعاف البعد الخفي للمشفر البصري.</p>
<h3 id="رأس-الكشف.">رأس الكشف.</h3>
<p>أظهرت الأعمال السابقة (<span class="nodecor">shikra</span>, <span class="nodecor">qwen-vl</span>, <span class="nodecor">llava-1.5</span>) في تطبيق نماذج اللغة متعددة الطبقات لتحديد مواقع الأهداف أنها تعتمد بشكل أساسي على النصوص العادية لتمثيل الإحداثيات، وهذا منطقي نظراً لأن النماذج المدربة مسبقاً تعمل بشكل جيد مع سلاسل النصوص العادية. ومع ذلك، فإن الإحداثيات المبنية على النصوص العادية تستهلك الكثير من الرموز، مما يقلل من كفاءة التدريب والاستدلال. نقترح توسيع قاموس نماذج اللغة متعددة الطبقات برموز خاصة للإحداثيات المعيارية. على وجه التحديد، يستخدم سلسلة نصية عادية لوصف مربع التحديد ما مجموعه <span class="math inline">\(2+4\times5+3=25\)</span> رمزاً، يشمل علامتين محفزتين، وأربعة أعداد عشرية، وثلاثة فواصل. ومع ذلك، من خلال استبدال العديد من رموز الأرقام لكل عدد عشري برمز إحداثي فريد والاحتفاظ بفاصلة واحدة فقط، يمكننا تقليل عدد الرموز إلى <span class="math inline">\(2+4+1=7\)</span> فقط.</p>
<p>ومع ذلك، فإن تدريب التضمينات الكلمية المضافة حديثاً بخسارة نمذجة اللغة على كمية صغيرة من البيانات ليس فعالاً. في تجاربنا، ينهار النموذج أحياناً، مما ينتج إحداثيات بلا معنى. للتخفيف من مشكلة تدريب رموز الإحداثيات بشكل غير فعال، نهدف إلى تقديم هدف تدريب مساعد. مستوحاة من (<span class="nodecor">detr</span>)، ندمج شبكة عصبية متعددة الطبقات بسيطة مكونة من طبقتين مع وظيفة تنشيط ReLU وطبقة إسقاط خطية كرأس كشف مساعد، والتي تعمل بالتوازي مع طبقة الإخراج الأصلية لنموذج اللغة. يتم تعيين إخراج رأس الكشف بواسطة وظيفة التنشيط Sigmoid. نقيم الخطأ بين التنبؤ والحقيقة الأرضية بواسطة خسارة <span class="math inline">\(\ell_1\)</span>: <span class="math display">\[\begin{aligned}
    \calL_\text{box}&=\frac{1}{|\mathcal{B}|}\sum_{i\in \calB}\|b_i-b^*_i\|_1,\end{aligned}\]</span> حيث <span class="math inline">\(b_i\)</span> و<span class="math inline">\(b^*_i\)</span> هما التنبؤات والحقيقة الأرضية لإحداثيات مربع التحديد المعيارية عند الموضع <span class="math inline">\(i\)</span> على التوالي، و<span class="math inline">\(\mathcal{B}\)</span> هي مجموعة مواضع رموز الإحداثيات في تسلسل الإخراج.</p>
<h3 id="دالة-الخسارة.">دالة الخسارة.</h3>
<p>تنظم جميع البيانات في محادثات متعددة الأدوار، حيث يتم تنسيق كل دور على النحو التالي: <span class="math display">\[\begin{aligned}
    \text{المستخدم: &lt;s&gt;}\calI^t\text{&lt;/s&gt;المساعد: &lt;s&gt;}\calR^t\text{&lt;/s&gt;}\end{aligned}\]</span> حيث يشير &lt;s&gt; و &lt;/s&gt; إلى الرموز الخاصة التي تعلن بداية ونهاية رسائل المحادثة. <span class="math inline">\(\mathcal{I}^t\)</span> و <span class="math inline">\(\mathcal{R}^t\)</span> هما رموز التعليمات ورموز الاستجابة في الدور <span class="math inline">\(t\)</span>. على عكس تعديل التعليمات اللغوية الذي يشمل فقط رموز النص، قد يتكون <span class="math inline">\(\mathcal{I}^t\)</span> من رموز نصية، صورية، أو كلا النمطين. يعتمد تدريب نماذج اللغة متعددة المهام بشكل أساسي على خسارة نمذجة اللغة على رموز الاستجابة: <span class="math display">\[\begin{aligned}
    \calL_\text{lm}=-\frac{1}{\sum \alpha_i}\sum_{i\in \calM}\alpha_i\log(p(x_i|\varx_{&lt;i})),\quad
    \alpha_i=\left\{
    \begin{aligned}
        &amp;1\quad&amp;i\notin\calB,\\
        &amp;\alpha&amp;i\in\calB,
    \end{aligned}
    \right.\end{aligned}\]</span> حيث <span class="math inline">\(\calM\)</span> هي مجموعة مواقع الاستجابة، <span class="math inline">\(\alpha\)</span> هو وزن محدد مسبقاً لرموز الإحداثيات، و <span class="math inline">\(\varx_{&lt;i}\)</span> هي رموز التعليمات والاستجابات متعددة الوسائط التي ظهرت قبل الرمز <span class="math inline">\(i\)</span>.</p>
<p>الخسارة النهائية هي مجموع موزون لخسارة نمذجة اللغة وخسارة مربع الحدود المذكورة أعلاه: <span class="math display">\[\begin{aligned}
    \calL=\calL_\text{lm} + \lambda\calL_\text{box},\end{aligned}\]</span> حيث <span class="math inline">\(\lambda\)</span> هو وزن محدد مسبقاً لخسارة مربع الحدود.</p>
<h1 id="التجارب">التجارب</h1>
<h2 id="مجموعات-البيانات">مجموعات البيانات</h2>
<h3 id="تجميع-البيانات.">تجميع البيانات.</h3>
<p>لإنشاء دفعات بيانات تحتوي على تسلسلات بأطوال متفاوتة، يلزم إجراء عملية تعبئة، مما يؤدي إلى هدر الرموز. للتقليل من هذا الهدر وزيادة كفاءة التدريب، نقوم بدمج عينات أصلية متعددة في عينة تدريب واحدة. على وجه التحديد، نختار ونجمع العينات من مجموعة البيانات بشكل عشوائي حتى يصل طول التسلسل المجمع إلى قيمة قصوى محددة مسبقاً. من الجدير بالذكر أننا نقوم بتغطية العينات الأصلية بعناية بحيث تكون <em>غير مرئية بالتبادل</em> من بعضها البعض.</p>
<h3 id="التعليق-التصوري.">التعليق التصوري.</h3>
<p>لتحقيق القدرة الأساسية على الإدراك وكذلك لمواءمة المفهوم بين المشفر البصري ونموذج اللغة الكبير، تم جمع (<span class="nodecor">96M</span>) زوجاً من الصور والنصوص من مجموعات بيانات التعليق على الصور، بما في ذلك (<span class="nodecor">CC3M</span>)، (<span class="nodecor">CC12M</span>)، (<span class="nodecor">SBU</span>) ومجموعة فرعية من (<span class="nodecor">LAION-400M</span>). في هذه المهمة، يولد النموذج تعليقاً قصيراً للصورة المعطاة، كما يتطلب الأمر “<em>وصف الصورة باختصار</em>”.</p>
<h3 id="التعليق-التأسيسي.">التعليق التأسيسي.</h3>
<p>لتمكين نموذج التعلم المتعدد اللغات بقدرات تأسيسية أساسية، تم اعتماد مجموعة فرعية من مجموعة البيانات GrIT (<span class="nodecor">kosmos2</span>) تشمل <span class="nodecor">16</span> مليون زوج من الصور والنصوص. في هذه المهمة، يولد النموذج تعليقاً قصيراً بالإضافة إلى صناديق الحدود المعيارية للأشياء المشار إليها في الصورة، كما يتطلبه التوجيه “<em>وصف الصورة بإيجاز، مع التركيز على الكائنات الرئيسية مع صناديق الحدود المعيارية</em>”.</p>
<h3 id="التعرف-الضوئي-على-الحروف.">التعرف الضوئي على الحروف.</h3>
<p>باستثناء الصور الطبيعية، نحن مهتمون بشكل خاص بالصور الموجهة للوثائق. لتعزيز قدرة الإدراك لنموذج التعلم المتعدد المستويات للحروف الضوئية، تم جمع <span class="nodecor">1.28</span> مليون صورة من (<span class="nodecor">IIT-CDIP</span>). ثلاثة أنواع من الاستعلامات، “اذكر محتوى النص في الصورة”، “اذكر مربعات النص المحيطة في الصورة” و “اذكر محتوى النص مع مربعاته المحيطة في الصورة”، تُستخدم لحث النموذج على توليد محتوى النص، مربعات النص، أو كليهما لصورة معينة، حيث يتم جمع التصنيفات الخشنة بواسطة نظام التعرف الضوئي على الحروف التجاري.</p>
<h3 id="تحويل-الصيغة.">تحويل الصيغة.</h3>
<p>مستوحى من (<span class="nodecor">nougat</span>)، نجمع <span class="nodecor">1.28</span> مليون صفحة PDF ومحتوى تحويل الصيغة الخاص بالأوراق العلمية من ملفات مصدر arXiv، والتي تحتوي على معلومات تخطيط أكثر مثل ترتيب القراءة مقارنة ببيانات التعرف الضوئي على الحروف العادية. نستخدم تعليمات بسيطة، “<em>انقل محتوى صورة الوثيقة</em>”، لطلب من النموذج تحويل صفحة PDF لوثيقة علمية إلى تحويل الصيغة.</p>
<h3 id="التعليمات.">التعليمات.</h3>
<p>أثر اتباع <span class="nodecor">LLaVA-1.5</span>، قمنا ببناء بيانات التنعيم الخاصة بنا استناداً إلى مجموعات البيانات الموجودة لتعزيز قدرة <span class="nodecor">MLLMs</span> على اتباع التعليمات والتفاعل في محادثات تتعلق بالطبيعة والمستندات. على وجه التحديد، نعتمد عدة مجموعات بيانات تشمل (<span class="nodecor">vqav2</span>)، (<span class="nodecor">okvqa</span>)، (<span class="nodecor">gqa</span>)، (<span class="nodecor">aokvqa</span>)، (<span class="nodecor">textcaps</span>)، (<span class="nodecor">ocrvqa</span>)، (<span class="nodecor">refcoco</span>)، (<span class="nodecor">pointqa</span>)، (<span class="nodecor">flickr</span>)، (<span class="nodecor">docvqa</span>)، (<span class="nodecor">chartqa</span>)، (<span class="nodecor">infovqa</span>)، (<span class="nodecor">tabfact</span>)، (<span class="nodecor">wtq</span>)، (<span class="nodecor">vg</span>)، (<span class="nodecor">visualmrc</span>)، و(<span class="nodecor">slidevqa</span>). تم اعتماد نفس الأوامر من <span class="nodecor">LLaVA-1.5</span> لتنظيم أسلوب الاستجابة لـ <span class="nodecor">MLLMs</span>. لكل مجموعة بيانات، نقوم بدمج جميع أزواج الأسئلة والأجوبة المتعلقة بنفس الصورة التدريبية لإنشاء محادثات متعددة الأدوار وتحسين كفاءة البيانات. بالإضافة إلى المهام الأصلية، نقدم أيضاً مهاماً متعددة لمساعدة <span class="nodecor">MLLMs</span> على التعرف على النصوص وفهم تخطيط المستندات، بما في ذلك مهمة التعرف الضوئي على الحروف لـ (<span class="nodecor">DocVQA</span>)، (<span class="nodecor">InfoVQA</span>)، (<span class="nodecor">VisualMRC</span>) و(<span class="nodecor">SlideVQA</span>)، مهمة التحويل من الرسم البياني إلى الجدول لـ (<span class="nodecor">ChartQA</span>)، ومهمة التحويل من الصورة إلى <span class="nodecor">markdown</span> لـ (<span class="nodecor">TabFact</span>) و(<span class="nodecor">WTQ</span>). لتطوير <span class="nodecor">MLLM</span> للأغراض العامة، نستفيد من عدة مجموعات بيانات حوارية تشمل (<span class="nodecor">ShareGPT</span>)، (<span class="nodecor">ShareGPT-4V</span>)، (<span class="nodecor">ALLaVA</span>)، (<span class="nodecor">LLaVA</span>)، (<span class="nodecor">SVIT</span>)، و(<span class="nodecor">Shikra</span>).</p>
<h3 id="دوك-جيميني.">دوك جيميني.</h3>
<p>لمعالجة ندرة مجموعات البيانات الحوارية الموجهة للوثائق ذات الجودة العالية، نستفيد من القدرات البصرية الأصلية لـ Gemini-Pro لتعزيز البيانات. لكل عينة تدريب من DocVQA، ChartQA، وInfoVQA، نقدم لـ Gemini-Pro الصورة وأزواج الأسئلة والأجوبة الأصلية مع استعلام لتوليد: (<span class="nodecor">1</span>) ملخص موجز لمواضيع الوثيقة؛ (<span class="nodecor">2</span>) أزواج أسئلة وأجوبة قصيرة إضافية، حتى <span class="nodecor">10</span>؛ (<span class="nodecor">3</span>) رؤى وراء كل إجابة. باختصار، تتكون مجموعة البيانات المولدة <em>دوك جيميني</em> من <span class="nodecor">30</span> ألف صورة و <span class="nodecor">195</span> ألف زوج من الأسئلة والأجوبة مع الرؤى.</p>
<h2 id="sec:train">التدريب</h2>
<p>لجميع مراحل التدريب، نعتمد على AdamW كمحسن، مع <span class="math inline">\(\beta_1=0.9\)</span>، <span class="math inline">\(\beta_2=0.95\)</span>، وتحلل الوزن <span class="nodecor">0.05</span>.</p>
<h3 id="التدريب-المسبق-بدقة-ثابتة.">التدريب المسبق بدقة ثابتة.</h3>
<p>مستوحى من <span class="nodecor">BLIP-2</span>، نعتمد مجموعات بيانات التعليق التصوري على نطاق واسع لمواءمة مشفر بصري مدرب مسبقاً ومجمد مع <span class="nodecor">LLM</span>. على وجه التحديد، يتم استخدام <span class="nodecor">96M</span> زوج صورة-نص في هذه المرحلة. كل تعليق تصوري هو وصف موجز يلخص المعلومات العامة المصورة في صورة، نادراً ما يكون متعلقاً بالتفاصيل الدقيقة. لتسريع التدريب، تخضع جميع الصور لإعادة تحجيم إلى <span class="math inline">\(224\times224\)</span>. الحد الأقصى لطول التسلسل هو <span class="nodecor">4,096</span> وحجم الدفعة هو <span class="nodecor">96</span>، مما يؤدي إلى حجم دفعة فعال يقارب <span class="nodecor">8,000</span> بعد تجميع البيانات. نقوم بتدريب النموذج مسبقاً لـ <span class="nodecor">12,000</span> خطوة، ما يعادل تقريباً دورة واحدة عبر مجموعة البيانات. خلال التدريب المسبق، نجمد المشفر البصري و<span class="nodecor">LLM</span> وندرب المعيد العشوائي المبدئي ووحدات <span class="nodecor">LoRA</span>. معدل التعلم يزداد تدريجياً إلى <span class="math inline">\(3e^{-4}\)</span> في أول <span class="nodecor">3%</span> من الخطوات، يليه انحدار خطي إلى <span class="math inline">\(1e^{-5}\)</span> في الخطوات المتبقية. يستغرق الأمر يوماً واحداً لإنهاء التدريب على <span class="nodecor">48</span> وحدة معالجة رسومات <span class="nodecor">NVIDIA V100</span>.</p>
<h3 id="التدريب-المسبق-بدقة-مختلطة.">التدريب المسبق بدقة مختلطة.</h3>
<p>في هذه المرحلة، نقوم بتكييف جهاز إعادة العينات ليتناسب مع دقة الإدخال المتغيرة. يتم استخدام الصور ذات الأحجام الأصلية المختلفة ونسب العرض إلى الارتفاع من مجموعات بيانات التعليق التوضيحي، والتعرف الضوئي على الحروف، وتخفيض الأسعار. يتم تحديد حجم كل صورة فرعية بـ <span class="math inline">\(224\times224\)</span>. يتم تحديد المساحة القصوى <span class="math inline">\(n\)</span> بـ <span class="nodecor">36</span> ويتم تحديد الطول الأقصى للجانب <span class="math inline">\(l\)</span> بـ <span class="nodecor">12</span>. لتسريع مطابقة الشبكة للقطع المتكيفة مع الشكل، يتم تحديد <span class="math inline">\(k\)</span> بـ <span class="nodecor">9</span>. حجم الدفعة الفعال تقريباً <span class="nodecor">1500</span> وعدد خطوات التدريب <span class="nodecor">12000</span>، ما يعادل تقريباً دورة واحدة عبر مجموعة البيانات بالكامل. باستثناء جهاز إعادة العينات و LoRA، يتم تهيئة رأس الكشف عشوائياً وتحديثه في هذه المرحلة. يتم تحديد وزن <span class="math inline">\(\alpha\)</span> لرموز الإحداثيات بـ <span class="math inline">\(0.25\)</span> (أربعة رموز لكل مربع تحديد) ويتم تحديد وزن <span class="math inline">\(\lambda\)</span> لخسارة <span class="math inline">\(\ell_1\)</span> بـ <span class="nodecor">1</span>. يتم الاحتفاظ بتجميد المشفر البصري والنموذج اللغوي الكبير. معدل التعلم يزداد تدريجياً إلى <span class="math inline">\(1.5e^{-4}\)</span> في أول <span class="nodecor">3</span>% من الخطوات، يليه تضاؤل تدريجي إلى <span class="math inline">\(5e^{-6}\)</span>. يستغرق الأمر <span class="nodecor">3</span> أيام لإنهاء التدريب على <span class="nodecor">40</span> وحدة معالجة رسومات من نوع NVIDIA V100.</p>
<h3 id="الضبط-الدقيق-بإشراف-مختلط-الدقة.">الضبط الدقيق بإشراف مختلط الدقة.</h3>
<p>خلال عملية الضبط الدقيق، ندمج أوزان LoRA مع نموذج اللغة الكبير وندرب محول الدقة والنموذج اللغوي الكبير ورأس الكشف معاً، مع الحفاظ على تجميد مشفر الصورة. تورث المعلمات الفائقة للقص المتكيف مع الشكل ورأس الكشف من التدريب المسبق بدقة مختلطة. الطول الأقصى للتسلسل هو <span class="nodecor">2048</span>. نقوم بتدريب النموذج على بيانات اتباع التعليمات لفترة واحدة مع حجم دفعة يبلغ <span class="nodecor">64</span>. معدل التعلم يزداد تدريجياً إلى <span class="math inline">\(2e^{-5}\)</span> في أول <span class="nodecor">3%</span> من الخطوات، يليه انحدار زاوي إلى <span class="math inline">\(0\)</span>. يستغرق التدريب يوماً واحداً لإنهاء التدريب على <span class="nodecor">32</span> وحدة معالجة رسومات من نوع NVIDIA V100.</p>
<h2 id="النتائج-على-المعايير-القياسية">النتائج على المعايير القياسية</h2>
<p>لإظهار فعالية طرقنا، نجري مقارنة بين <span class="nodecor">DocuPercept</span>، ونموذجين متخصصين لمهام موجهة للوثائق، وأحدث نماذج اللغات متعددة المستويات على مجموعة واسعة من المعايير. تستهدف كل معيار مجموعة من المهام العامة أو المهام المفصلة. أولاً، نقوم بتقييم النماذج على معايير شاملة تشمل (<span class="nodecor">MME</span>)، (<span class="nodecor">MMBench</span>)، (<span class="nodecor">SEED-Bench</span>)، و(<span class="nodecor">GQA</span>). نظراً لأن دقة الصور في هذه المعايير منخفضة نسبياً، نقوم بتقييم قدرة الإدراك المفصل على فهم الوثائق ومهام الإشارة، بما في ذلك (<span class="nodecor">DocVQA</span>)، (<span class="nodecor">ChartQA</span>)، (<span class="nodecor">InfoVQA</span>)، (<span class="nodecor">TabFact</span>)، (<span class="nodecor">WTQ</span>)، و(<span class="nodecor">RefCOCO</span>).</p>
<p>كما هو موضح في الجدول (<span class="nodecor">tab:benchmark</span>)، يتفوق <span class="nodecor">DocuPercept</span> في كل من المعايير العامة والموجهة للوثائق، حيث يحتل المرتبة الأولى في <span class="nodecor">6</span> من <span class="nodecor">9</span> معايير. في جميع المعايير العامة، لا يتفوق <span class="nodecor">DocuPercept</span> على (<span class="nodecor">LLaVA-1.5-7B</span>) فحسب، بل يحقق أيضاً نتائج مماثلة مع (<span class="nodecor">InternLM-XComposer</span>)، على الرغم من أن الأخير يشارك نفس النموذج الأساسي للغة ولكن يستخدم مشفراً بصرياً أكبر. عند المقارنة بنماذج اللغات متعددة المستويات السابقة الموجهة للوثائق، مثل (<span class="nodecor">Ureader</span>) و(<span class="nodecor">TextMonkey</span>)، يظهر <span class="nodecor">DocuPercept</span> أداءً متفوقاً في المعايير الموجهة للوثائق. على وجه التحديد، يحقق <span class="nodecor">DocuPercept</span> مكاسب في الأداء بنسبة <span class="nodecor">11.0%</span>، <span class="nodecor">7.3%</span>، <span class="nodecor">8.4%</span>، <span class="nodecor">3.5%</span>، و<span class="nodecor">5.3%</span> على (<span class="nodecor">DocVQA</span>)، (<span class="nodecor">ChartQA</span>)، (<span class="nodecor">InfoVQA</span>)، (<span class="nodecor">TabFact</span>)، و(<span class="nodecor">WTQ</span>)، على التوالي، مقارنة بـ(<span class="nodecor">Ureader</span>). بشكل ملحوظ، يتفوق <span class="nodecor">DocuPercept</span> حتى على (<span class="nodecor">TextMonkey</span>)، الذي يستخدم مشفراً بصرياً أكبر، في معايير (<span class="nodecor">DocVQA</span>) و(<span class="nodecor">WTQ</span>). من الجدير بالذكر أن إدخال بياناتنا (<span class="nodecor">DocGemini</span>) يمكن أن يحسن أكثر من الأداء في المعايير الموجهة للوثائق. بالإضافة إلى ذلك، يحقق <span class="nodecor">DocuPercept</span> نتائج تنافسية على مجموعة بيانات (<span class="nodecor">RefCOCO</span>)، مما يظهر قدراته الجيدة في مهمة الإشارة.</p>
<h2 id="دراسة-الاستئصال">دراسة الاستئصال</h2>
<p>نعتمد تكوينين أسرع للتدريب من أجل دراسة الاستئصال. التدريب بالدقة الثابتة هو نفسه تماماً كما هو موصوف في القسم [sec:train]. بعد ذلك، يتم تنقيح نماذج الدقة الثابتة على بيانات التدريب الخاصة بـ <span class="nodecor">LLaVA-1.5</span> لمدة دورة تدريبية واحدة، بينما يتم تنقيح نماذج الدقة المتغيرة على بيانات التدريب الخاصة بـ <span class="nodecor">LLaVA-1.5</span>، <span class="nodecor">DocVQA</span>، <span class="nodecor">ChartQA</span>، <span class="nodecor">InfoVQA</span>، <span class="nodecor">TabFact</span>، و <span class="nodecor">WTQ</span>.</p>
<h3 id="إعادة-العينة-وإعادة-الترتيب-resa.-1">إعادة العينة وإعادة الترتيب (ReSA).</h3>
<p>لإظهار فعالية إعادة العينة وإعادة الترتيب، نجري تجارب بدقة ثابتة مع تكوينات ضغط مختلفة، وتظهر النتائج في الجدول [tab:resa]. مقارنة باستراتيجية إعادة العينة فقط، فإن دمج إعادة العينة وإعادة الترتيب الذي يقسم إجراء الضغط إلى مرحلتين يحسن الأداء في جميع المعايير، خاصة في RefCOCO حيث تظهر مهمة الفهم المرجعي طلباً كبيراً للحفاظ على معلومات أكثر دقة.</p>
<h3 id="الانتباه-المتقاطع-متعدد-المستويات-mlca.-1">الانتباه المتقاطع متعدد المستويات (MLCA).</h3>
<p>من الناحية التجريبية، تلتقط الطبقات العميقة داخل مشفرات الرؤية المعلومات الدلالية العالمية بشكل أساسي، بينما تميل الطبقات السطحية إلى الاحتفاظ بالتفاصيل المحلية المعقدة. لاستكشاف تأثير استراتيجية التوجيه لـ MLCA، نجري تجارب مع جداول توجيه مختلفة، كما هو موضح في الجدول [tab:mlca]. من أجل البساطة، نستخدم R1 إلى R5 للإشارة إلى جداول التوجيه المختلفة. R1 هو حالة خاصة تشمل فقط مرحلة التشفير 3، مما يؤدي إلى العودة إلى إعدادات الانتباه المتقاطع التقليدية. عند مقارنة R1 و R2، يمكننا أن نجد أن الأخير يحسن الأداء بشكل كبير في المهام المفصلة، بينما ينخفض قليلاً في الأداء في المعايير العامة. عند مقارنة R2 و R3/R4، يمكننا أن نجد أن توجيه الميزات من طبقات التشفير الأقل عمقاً إلى طبقات إعادة العينات الأعمق يظهر دقة أعلى على RefCOCO، مقارنة بتوجيهها إلى طبقات إعادة العينات المتوسطة. من بين جميع الإعدادات التجريبية، يحقق R5 توازناً جيداً بين المهام العامة والمهام المفصلة، وبالتالي نعتمده كجدول توجيه افتراضي.</p>
<h3 id="شبكة-اقتراح-الاستعلامات-qpn.">شبكة اقتراح الاستعلامات (QPN).</h3>
<p>للتحقق من أهمية استعلامات إعادة العينة عالية الجودة، قمنا بمقارنة تهيئة الاستعلامات من المعلمات المتعلمة وتوليد الاستعلامات باستخدام شبكة اقتراح الاستعلامات، كما هو موضح في الجدول [tab:qpn]. للمقارنة العادلة، يبلغ عدد الاستعلامات <span class="nodecor">64</span> في كلتا التجربتين. يمكننا أن نجد أن دمج شبكة اقتراح الاستعلامات يحسن أداء النموذج في معظم المعايير القياسية، وخاصة في RefCOCO.</p>
<h3 id="التضمينات-الموضعية-القابلة-للتوسع-spes.-1">التضمينات الموضعية القابلة للتوسع (SPEs).</h3>
<p>لاستكشاف تأثير التضمينات الموضعية الإضافية، نجري تجارب مع إعدادات دقة متغيرة. تظهر النتائج على المعايير الدقيقة في الجدول [tab:pe]. من الواضح أن غياب التضمينات الموضعية الإضافية يؤدي إلى تدهور الأداء في معظم المعايير. مقارنة بالتضمينات الموضعية المطلقة المستخدمة في الأعمال السابقة، تحسن SPEs الأداء الدقيق بشكل أكبر. في الوقت نفسه، يمكن توسيع دقة SPEs من الخلية إلى بقعة الصورة دون زيادة عدد المعاملات. تم التأكيد على أن استخدام التضمينات الموضعية الأدق والأكثر سلاسة على مستوى بقعة الصورة يحسن الأداء العام بشكل أكبر.</p>
<h3 id="رأس-الكشف.-1">رأس الكشف.</h3>
<p>كلا من رأس نمذجة اللغة الأصلي ورأس الكشف الإضافي قادران على توليد الإحداثيات. عندما ينتج الأول رمز إحداثي، يمكننا استبداله بسلاسة بالناتج من الأخير. في الجدول [tab:head]، نقارن نتائج الرؤوس المختلفة على RefCOCO. من الواضح أن رأس الكشف يظهر دقة أعلى في جميع الأقسام، مما يثبت تفوقه في مهام التأصيل.</p>
<h1 id="القيود">القيود</h1>
<p>المشفر البصري في <span class="nodecor">DocuPercept</span> مجمد أثناء التدريب، مما يعني أنه لا يتعلم من بيانات التدريب. قد يحد هذا من قدرة النموذج على التكيف مع بيانات بصرية جديدة أو غير مرئية تختلف بشكل كبير عن البيانات التي تم تدريبه عليها في البداية. في المستقبل، سنقوم بتدريب المشفر البصري لتحسين قدرات الإدراك بشكل أكبر.</p>
<h1 id="الخلاصة-1">الخلاصة</h1>
<p>في هذه الورقة، قدمنا <span class="nodecor">DocuPercept</span>، نموذج لغوي كبير متعدد الوسائط (<span class="nodecor">Multimodal Large Language Model</span>) مصمم خصيصاً لمواجهة التحديات الفريدة التي تطرحها المهام الموجهة نحو الوثائق. يقدم <span class="nodecor">DocuPercept</span> عدة مكونات مبتكرة. تعمل هذه المكونات بتناغم لتعزيز قدرات النموذج على الإدراك البصري الدقيق وضغط المعلومات، مما يمكنه من التعامل مع الدقة العالية وكثافة المعلومات المميزة لصور الوثائق. تظهر تجاربنا الواسعة على معايير النماذج اللغوية الكبيرة متعددة الوسائط الموجهة للوثائق والعامة أن <span class="nodecor">DocuPercept</span> يتفوق على الطرق الحديثة، مما يبرز قدراته المتفوقة في إدراك الوثائق الدقيق وقدراته العامة في الرؤية واللغة.</p>
</body>
</html>