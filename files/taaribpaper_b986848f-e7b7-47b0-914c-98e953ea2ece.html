<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Sekeun Kim">
  <meta name="author" content="Hui Ren">
  <meta name="author" content="Peng Guo">
  <meta name="author" content="Abder-Rahman Ali">
  <meta name="author" content="Patrick Zhang">
  <meta name="author" content="Kyungsang Kim">
  <meta name="author" content="Quanzheng Li">
  <meta name="author" content="Xiang Li">
  <title>نموذج عالمي مُوَجَّه بالمُوجِّهات لتقسيم صور تخطيط صدى القلب مستقلّ عن المنظر</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 20px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.8;
    }
    header {
      background: linear-gradient(90deg, #3a8dde 0%, #6ed0cb 100%);
      color: #fff;
      padding: 40px 0 20px 0;
      text-align: center;
      box-shadow: 0 2px 8px rgba(58,141,222,0.08);
      margin-bottom: 40px;
    }
    h1.title {
      font-size: 2.2em;
      font-weight: 700;
      margin-bottom: 10px;
      letter-spacing: 0.5px;
    }
    .author {
      display: inline-block;
      margin: 0 10px;
      font-size: 1.05em;
      font-weight: 400;
      color: #eaf6fb;
    }
    h1, h2, h3 {
      color: #3a8dde;
      font-weight: 700;
      margin-top: 2.2em;
      margin-bottom: 0.7em;
      line-height: 1.3;
    }
    h1 {
      font-size: 1.9em;
      border-bottom: 2px solid #6ed0cb;
      padding-bottom: 0.2em;
      margin-bottom: 1.2em;
    }
    h2 {
      font-size: 1.35em;
      border-right: 4px solid #3a8dde;
      padding-right: 10px;
      margin-bottom: 0.8em;
    }
    h3 {
      font-size: 1.08em;
      color: #2b6777;
      margin-bottom: 0.5em;
    }
    p {
      margin: 0 0 1.2em 0;
      text-align: justify;
    }
    ul, ol {
      margin: 1em 2em 1em 0;
      padding: 0 1.5em 0 0;
    }
    li {
      margin-bottom: 0.5em;
    }
    em {
      color: #3a8dde;
      font-style: normal;
      font-weight: 600;
    }
    strong {
      color: #2b6777;
      font-weight: 700;
    }
    code, pre {
      background: #f1f3f6;
      color: #c7254e;
      font-family: 'Cairo', 'Consolas', 'monospace';
      font-size: 0.95em;
      border-radius: 4px;
      padding: 2px 6px;
    }
    pre {
      padding: 12px;
      overflow-x: auto;
      margin: 1.2em 0;
    }
    table {
      width: 80%;
      margin: 1.6em auto;
      border-collapse: collapse;
      background: #fff;
      box-shadow: 0 2px 8px rgba(58,141,222,0.07);
      border-radius: 8px;
      overflow: hidden;
    }
    caption {
      caption-side: bottom;
      text-align: center;
      padding: 8px;
      color: #2b6777;
      font-weight: 600;
    }
    th, td {
      border: 1px solid #e3e6ea;
      padding: 12px 18px;
      text-align: center;
      font-size: 1em;
    }
    th {
      background: #eaf6fb;
      color: #3a8dde;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f6fafd;
    }
    tr:hover {
      background: #f0f7fa;
    }
    .math.inline {
      font-family: 'Cairo', 'Consolas', 'monospace';
      background: none;
      color: #2b6777;
      font-size: 1em;
      padding: 0;
    }
    .math.display {
      display: block;
      margin: 1.5em auto;
      text-align: center;
      font-size: 1.08em;
      color: #2b6777;
    }
    .nodecor {
      text-decoration: none !important;
      color: inherit !important;
    }
    ul {
      list-style-type: square;
    }
    @media (max-width: 900px) {
      body { font-size: 18px; }
      table { width: 98%; }
      header { padding: 30px 0 15px 0; }
      h1.title { font-size: 1.8em; }
    }
    @media (max-width: 600px) {
      body { font-size: 16px; }
      header { padding: 20px 0 10px 0; }
      h1.title { font-size: 1.2em; }
      table { font-size: 0.95em; }
    }
    .latex-note {
      background: #fffbe6;
      border-right: 5px solid #ffe066;
      color: #b59f3b;
      padding: 10px 18px;
      margin: 1.5em 0;
      border-radius: 6px;
      font-size: 1.05em;
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">نموذج عالمي مُوَجَّه بالمُوجِّهات لتقسيم صور تخطيط صدى القلب مستقلّ عن المنظر</h1>
  <div>
    <span class="author">Sekeun Kim</span>
    <span class="author">Hui Ren</span>
    <span class="author">Peng Guo</span>
    <span class="author">Abder-Rahman Ali</span>
    <span class="author">Patrick Zhang</span>
    <span class="author">Kyungsang Kim</span>
    <span class="author">Quanzheng Li</span>
    <span class="author">Xiang Li</span>
  </div>
</header>

<div class="latex-note">ملاحظة: تُعرَض المعادلات باستخدام MathJax؛ لقد أُبقيَت صِيَغ LaTeX كما هي مع تصحيح الأخطاء الواضحة فقط عند اللزوم.</div>

<h1 id="ملخص">مُلَخَّص</h1>
<p>تُعَدّ عمليّة تقسيم صور تخطيط صدى القلب (الإيكو) مُستهلِكة للوقت وتتطلّب موارد حوسبيّة كبيرة؛ ويَعود ذلك إلى التبايُن في جودة الصور والحاجة إلى معالجة الفحوص ضمن مَناظِر تصوير قياسيّة متعددة. وعلى الرغم من أنّ الطُرُق الآليّة الحاليّة تُظهِر أداءً واعدًا، فإنّها عادةً ما تُدرَّب على مناظر محدّدة، ما يستلزم نموذجًا منفصلًا لكلّ منظر. ومع ازدياد عدد المَناظِر القياسيّة، يتضاعف عدد النماذج المطلوبة، وهو أمر غير عملي. لمعالجة ذلك، نقدّم طريقةً عالميّة مُوَجَّهة بالمُوجِّهات لتقسيم صور الإيكو بغضّ النظر عن منظر التصوير. ومع أخذ اختلاف التوزيعات بين المَناظِر القياسيّة بالحُسبان، نُقدِّم أوّلًا آليّة تُسمّى مطابقة المُوجِّهات، تتعلّم مُوجِّهاتٍ خاصّة بكلّ منظر عبر مطابقة المُوجِّهات مع استعلامات تضمينات الإدخال باستخدام نموذج رؤية مُدرَّب مُسبقًا. ثم نستخدم نموذج لغة طبّي مُدرَّبًا مُسبقًا لمواءمة المعلومات النصيّة مع بيانات البكسل بهدف تحقيق تقسيمٍ أدقّ. وقد أظهرت تجاربٌ واسعة على ثلاثة مَناظِر قياسيّة تفوّق نهجنا بوضوح على الطُرُق العالميّة الحديثة، وحقّق أداءً مماثلًا أو أفضل من النماذج المتخصّصة المُدرَّبة على المناظر نفسها.</p>

<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>يُعَدّ التصوير بالموجات فوق الصوتيّة أكثر أساليب تصوير القلب شيوعًا، إذ يسمح بتقييم وظيفته عبر فحص مجموعة من المَناظِر القياسيّة. ونظرًا لتعقيد تحليل هذه الصور وارتفاع عبء العمل على الفنيّين، تصاعد الاهتمام بتطوير أساليب آليّة للتقسيم في التصوير بالموجات فوق الصوتيّة (<span class="nodecor">kim2022fully,kim2021automatic,leclerc2020lu</span>). وقد أظهرت الطرق الحاليّة قدرةً جيّدة على تحديد البُنى التشريحيّة بدقّة ضمن المَناظِر التي تمّ التدريب عليها. ومع ذلك، يتطلّب معظم هذه الأساليب خطوةً أولى لتحديد منظر التصوير المناسب لكلّ مريض قبل بدء التحليل، مما يُضيف عبئًا إضافيًا في اختيار اللقطات الملائمة (<span class="nodecor">charton2023multi,jeon2023improving</span>). وحتى الآن، لم يُستكشَف بالكامل تطوير نموذجٍ عامّ يَقدر على تنفيذ مهام التقسيم عبر جميع المَناظِر القياسيّة بشكلٍ مستقل.</p>
<p>في المقابل، يعتمد النهج التقليدي على تدريب <em>N</em> نماذج منفصلة لكلٍّ من <em>N</em> المَناظِر القياسيّة. ومع تزايد عدد المَناظِر، يتضاعف عدد النماذج، ما يجعل هذا الأسلوب غير مرِن وصعب التطبيق. قد يبدو تبسيط الفكرة بتدريب شبكة واحدة على بيانات جميع المَناظِر حلًا، غير أنّ ذلك غالبًا ما يؤدّي إلى تدهور الأداء نظرًا لاختلاف الخصائص البصريّة المميِّزة لكلّ منظر (<span class="nodecor">kim2021automatic,mitchell2019guidelines</span>). ويواجه التصوير بالموجات فوق الصوتيّة تحدّيات إضافيّة مثل تغيّر مجال الرؤية بين المَناظِر وتفرُّق الملصقات عبر الإطارات. وعلى الرغم من جهود تطوير نماذج عالميّة مماثلة في مجالات أخرى (<span class="nodecor">zhang2021dodnet,butoi2023universeg,liu2023clip,ye2023uniseg</span>)، فإنّ بعضها يَعترضه قصور عند نقله إلى الأُطر الطبّية. فعلى سبيل المثال، يعتمد نموذج الشبكة الديناميكيّة (<span class="nodecor">zhang2021dodnet</span>) على بنية مُشفِّر–مُفكِّك مع ضوابط ديناميكيّة، بينما يُوسِّع نموذج CLIP المُغذّى بالنصّ (<span class="nodecor">liu2023clip</span>) الفكرة باستخدام نموذجٍ نصّي مُدرَّب مُسبقًا لإدارة رؤوس التقسيم دلاليًّا. ورغم نجاحهما في تقسيم أعضاء الجسم في التصوير المقطعي المحوسب، إلا أنّ الفجوة بين اللغة الطبيعيّة واللغة الطبّية تحدّ من فعاليتهما طبّيًا. كما يُقدِّم نموذج UniSeg (<span class="nodecor">ye2023uniseg</span>) إطارًا لتعلّم المُوجِّهات عبر بيانات تشريحيّة متقاربة، لكنه يواجه صعوبة مع تحوّلات المنظر كما يتّضح من الأداء في الجدول [table2].</p>
<p>لمعالجة هذه التحدّيات، نقترح نموذجًا عالميًّا مُوَجَّهًا بالمُوجِّهات يُتيح تقسيم البُنى القلبيّة بدقّة عالية بغضّ النظر عن منظر التصوير. يَدمج نموذجُنا آليّة تعلّم مُوجِّهات استرشاديّة مع المعرفة المُسبَقة لنموذج لغة مُدرَّب مُسبقًا عبر مواءمة تمثيلات النصّ والبكسل. أولًا، نعتمد طريقةً لتعلّم المُوجِّهات من بنك مُوجِّهات مُحدَّد تمكّن النموذج من استيعاب التنوّع في بيانات المَناظِر القياسيّة والتكيُّف معها ديناميكيًّا. ثانيًا، نستخدم خرائط الدرجات لربط المعلومات النصّيّة بالتمثيلات البكسليّة، مما يُتيح الإفادة الكاملة من دلالات اللغة في مهام التقسيم القلبي. وبحسب علمنا، يُعَدّ هذا العمل الأوّل من نوعه في تقديم نموذج موحَّد يمكنه أداء تقسيم صور الإيكو عبر مختلف المَناظِر دون الحاجة إلى خطوة تحديد منظرٍ مُسبقة. وقد أظهرت التجارب على ثلاثة مَناظِر قياسيّة، عبر مجموعات بيانات مختلفة، أداءً واعدًا يفوق الطُرُق العالميّة الحالية.</p>
<p>يمكن تلخيص مساهماتنا على النحو التالي:<br />
• نُقدِّم نموذجًا عالميًّا مُوَجَّهًا بالمُوجِّهات؛ يَضمّ بنك مُوجِّهات مُصمَّمًا لاستيعاب المَناظِر القياسيّة المختلفة، ويستفيد من مواءمة نص–بكسل ومن المعرفة المُسبَقة لنموذج لغة مُدرَّب مُسبقًا لإجراء تقسيم دقيق لصور الإيكو مستقلّ عن المنظر.<br />
• تُبسِّط الطريقة المُقترحة عمليّة التحليل القلبي بتقليل الحاجة إلى خطوة تحديد المنظر يدويًّا عند استرجاع لقطات المريض.<br />
• نُثبِت عبر تجارب واسعة على مجموعات بيانات متنوِّعة أنّ نموذجنا يحقّق أداءً مُتقدِّمًا في مهام تقسيم صور الإيكو مقارنةً بالنهج العالميّة السابقة.<br /></p>

<h1 id="الطريقة">الطَّرِيقَة</h1>
<p>كما هو موضَّح، يتكوّن نهجنا من العناصر الرئيسيّة التالية: مُوجِّه نصّي، مُشفِّر فيديو، بنك مُوجِّهات قابل للتدريب يضمّ مفاتيح وقِيَمًا، طبقة شبكة عصبيّة متعدِّدة الطبقات، ومُفكِّك فيديو. نعتمد على نموذج ClinicalBERT (<span class="nodecor">alsentzer2019publicly</span>) لتحسين استخراج تمثيلات النصوص الطبّية. ويهدف نموذجنا إلى تقسيم البُنى عبر جميع الإطارات والأحجام المأخوذة من مَناظِر تصوير مختلفة. لتحقيق ذلك، نُقدِّم مكوّنين أساسيّين: <span class="nodecor">1</span>) آليّة مواءمة كثيفة بين تمثيلات النصّ والبكسل لسدّ الفجوة بين نموذج اللغة المُدرَّب مُسبقًا وخصائص البكسل لمهام التنبّؤ الكثيف، و<span class="nodecor">2</span>) تقنية مطابقة المُوجِّه التي تستفيد من بنك المُوجِّهات لاختيار المُوجِّه الأنسب لكلّ مهمّة.</p>

<h2 id="تعريف-المشكلة">تَعْرِيف المُشْكِلَة</h2>
<p>لنفترض أنّ لدينا <span class="nodecor">N</span> من مجموعات البيانات <span class="nodecor">D</span> = <span class="math inline">\(\{D_1, D_2, \ldots, D_N\}\)</span>، حيث تُكتَب كلّ مجموعة بيانات <span class="math inline">\(D_i = \{X_{ij}, Y_{ij}\}_{j=1}^{n_i}\)</span>؛ ويمثّل <span class="math inline">\(X_{ij}\)</span> فيديوً مكوَّنًا من <span class="math inline">\(F\)</span> إطارات، بينما يدلّ <span class="math inline">\(Y_{ij}\)</span> على قناع الحقيقة الأرضيّة المقابل لـ <span class="math inline">\(X_{ij}\)</span>، مع عدد عينات قدره <span class="math inline">\(n_i\)</span>. وينتمي كلّ فيديو <span class="math inline">\(X_{ij}\)</span> إلى مجالِ منظرٍ معيّن <span class="math inline">\(V_k\)</span>، حيث توجد <span class="nodecor">K</span> مَناظِر في المجموع <span class="math inline">\(\{V_1, V_2, \ldots, V_K\}\)</span>. وإذا كانت جميع الإطارات مُوسَّمة في <span class="math inline">\(Y_{ij}\)</span>، تُعدّ <span class="math inline">\(D_i\)</span> مجموعة بيانات مُوسومة بالكامل، وإلاّ فتُسمّى مُوسومة جزئيًّا. وعليه، نسعى لتدريب نموذج <span class="nodecor">F(·)</span> باستخدام مجموعات البيانات المُوسومة جزئيًّا <span class="math inline">\(D\)</span> بحيث يتمكّن من إجراء تنبؤات كثيفة لجميع <span class="nodecor">K</span> الفئات عبر كلّ الإطارات.</p>

<h2 id="محاذاة-كثيفة-بين-النص-والبكسل">مُحاذاة كَثِيفَة بَيْن النَّصّ والبكسل</h2>
<p>في مجال الرؤية الحاسوبيّة، ظهرت نماذج متعدّدة تجمع بين الرؤية واللغة. أمّا في المجال الطبّي، فقد جرى تكييف تضمينات CLIP لتطبيقاتٍ طبّية (<span class="nodecor">qin2022medical</span>, <span class="nodecor">liu2023clip</span>). ومع ذلك، فإنّ تدريب CLIP على أزواج من الصور والنصوص الطبيعيّة يُضعِف دلالة المُوجِّهات النصّيّة طبّيًّا، كما يظهر في الجدول [table3]. للاستفادة الكاملة من المعرفة المُشفَّرة في نماذج اللغة الطبّية المُدرَّبة مُسبقًا، اعتمدنا ClinicalBERT (<span class="nodecor">alsentzer2019publicly</span>) في مهام التنبّؤ الكثيف. نولِّد تضمينات النصّ بتحويل عدد الفئات <span class="math inline">\(N\)</span> إلى جُمل مُوجِّهة بصيغة “تخطيط صدى القلب لـ [الفئة]”، مما يُنتِج مصفوفة تضمينات <span class="math inline">\(\mathcal{F}(c) \in \mathbb{R}^{N \times D}\)</span>. ويُحوِّل مُشفِّر الفيديو الإطارات إلى مصفوفة تضمينات محلّيّة <span class="math inline">\(\mathcal{G}(x) \in \mathbb{R}^{T_i H_i W_i \times D}\)</span>، حيث تمثّل <span class="math inline">\(T_i\)</span>، <span class="math inline">\(H_i\)</span>، <span class="math inline">\(W_i\)</span>، و<span class="math inline">\(D\)</span> عدد الإطارات، والارتفاع، والعرض، وبُعد التضمين. بعدها نحسب خريطة الدرجات عبر مواءمة تضمينات النصّ والبكسل وفق المعادلة 
<span class="math display">\[
\mathcal{S} = \bar{\mathcal{G}(x)}\,\bar{\mathcal{F}(c)}^{T}
\]</span>
حيث يدلّ التطبيع <span class="math inline">\(\bar{\cdot}\)</span> على التطبيع على طول بُعد القناة، و <span class="math inline">\(^{T}\)</span> على النقل. تُستخدم هذه الخريطة <span class="math inline">\( \mathcal{S} \)</span> في خسارة نص–بكسل المُساعِدة. وأخيرًا، ندمج خريطة نص–بكسل مع التضمينات المحلّيّة <span class="math inline">\(f\)</span> لإعادة وزن الأولويّات النصّيّة. ولتقليل التعقيد، استخدمنا فئة الحجرة فقط كمُوجِّه نصّي دون تضمين معلومات إضافيّة عن المنظر.</p>

<h2 id="تطابق-الأوامر-وتوليد-المعاملات-بناء-على-النص">تَطابُق المُوجِّهات وتوليد المُعامِلات المشروطة بالنَّصّ</h2>
<p>لتوضيح الإجراء، لنفترض أنّ المدخل <span class="math inline">\(x \in \mathbb{R}^{T \times H \times W \times C}\)</span>، وأنّ <span class="math inline">\( \mathfrak{Q} \)</span> هو مُحوِّل رؤية مُدرَّب مُسبقًا (ViT) كما في Segment Anything (<span class="nodecor">kirillov2023segment</span>). نُجزِّئ الإطار الأوّل من الفيديو إلى رُقَع بحجم <span class="math inline">\(S^2\)</span> لكلّ قناة، ثم نُحوِّلها إلى تضمينات <span class="math inline">\( \mathfrak{Q}: \mathbb{R}^{L\times (S^2 C)} \to \mathbb{R}^{L \times D} \)</span>، حيث تشير <span class="math inline">\(L\)</span> إلى عدد الرُّقَع، و<span class="math inline">\(D\)</span> إلى بُعد التضمين. يتكوّن بنك المُوجِّهات من <span class="math inline">\(M\)</span> أزواج من مفاتيح ومُوجِّهات قابلة للتعلّم <span class="math inline">\( \{(k_i, P_i)\}_{i=1}^M\)</span>، حيث <span class="math inline">\(k_i \in \mathbb{R}^D\)</span> و<span class="math inline">\(P_i \in \mathbb{R}^{L \times D}\)</span>. في إعدادنا، يُساوي <span class="math inline">\(M\)</span> عدد المَناظِر مضروبًا في عدد خانات المُوجِّه المخصّصة لكلّ منظر (ثلاثة مُوجِّهات). نسعى إلى تقريب تضمينات الاستعلام ومفاتيح المُوجِّهات ضمن كلّ منظر عبر تعظيم التشابُه الجيبي (<span class="math inline">\( \mathcal{L}_{pr}\)</span>) أثناء التدريب. كما نعتمد طبقة التجميع المتوسط العالمي (GAP) لاستخراج تمثيلٍ عالمي لمقطع الفيديو، ثم نُركِّب تضمينات النصّ مع قِيَم المُوجِّهات والتضمين العالمي لتوليد معاملات رؤوس فكّ التشفير <span class="math inline">\( \theta_N\)</span>. تُستخدم هذه المعاملات في مُفكِّك الفيديو لإنتاج تنبؤٍ ثنائي لكلٍّ من <span class="math inline">\(N\)</span> فئات (<span class="nodecor">tian2020conditional</span>)، مما يضمن حياد المنظر مع الاحتفاظ بمعلوماته أثناء الاختبار.</p>

<h2 id="دالة-الخسارة">دَالَّة الخَسَارَة</h2>
<h3 id="الانتشار-العكسي-المقنع-للفيديو">الاِنْتِشَار العَكْسِي المُقَنَّع لِلْفِيدْيُو</h3>
<p>نظرًا لندرة التسميات عبر الإطارات في بياناتنا خلافًا لأعمالٍ سابقة (<span class="nodecor">liu2023clip</span>)، طوّرنا تقنية الانتشار العكسي المُقنَّع للفيديو لمعالجة هذا الخلل. نُخفي الإطارات التي تفتقر إلى تسميات فئويّة، ونُطبِّق الانتشار العكسي للخسارة فقط على الإطارات المُوسَّمة. تُمكِّن هذه الطريقة نموذجَنا من استغلال البيانات المُوسومة جزئيًّا لتحقيق تقسيمٍ دقيق عبر مقاطع الفيديو.</p>
<h3 id="الخسارة-الكلية">الخَسَارَة الكُلِّيَّة</h3>
<p>نهدف إلى تحسين التقسيم عبر تقليل مكوّنين في دالّة الخسارة: خسارة التقسيم وخسارة مطابقة المُوجِّه، وفق:
<span class="math display">\[
\mathcal{L}_{seg} = \lambda_{1}\mathcal{L}_{pixel-text} + \lambda_{2}\mathcal{L}_{BCE}, \quad \mathcal{L}_{pr} = \langle \mathfrak{Q}(X_{i0}), P_{key}\rangle
\]</span>
<span class="math display">\[
\mathcal{L}_{total} = (1 - \lambda(t))\,\mathcal{L}_{seg} - \lambda(t)\,\mathcal{L}_{pr}
\]</span>
حيث <span class="math inline">\(\mathcal{L}_{seg}\)</span> هي خسارة التقسيم التي تضمّ خسارة الانتروبيا المتقاطعة على خريطة الدرجات (<span class="math inline">\(\mathcal{L}_{pixel-text}\)</span>) وخسارة الانتروبيا المتقاطعة الثنائيّة (<span class="math inline">\(\mathcal{L}_{BCE}\)</span>). ونُثبِّت <span class="math inline">\(\lambda_{1}\)</span> و<span class="math inline">\(\lambda_{2}\)</span> على قيَمٍ متساوية طوال التدريب. أمّا <span class="math inline">\(\mathcal{L}_{pr}\)</span> فتقيس التشابُه الجيبي بين تضمين الاستعلام ومفتاح المُوجِّه المناسب لكلّ منظر. ويتمّ جدولة وزن <span class="math inline">\(\lambda(t)\)</span> عبر دالّةٍ غاوسيّة زمنيّة <span class="math inline">\(\lambda(t) = \exp\bigl(-5(1 - t/t_{max})^2\bigr)\)</span> بحيث يقلّ الاعتماد على مطابقة المُوجِّهات في المراحل المبكِّرة ريثما تتقارب المفاتيح.</p>

<h1 id="التجارب-والنتائج">التَّجارِب وَالنَّتَائِج</h1>
<p><strong>البيانات.</strong> قيَّمنا الطريقة المُقترحة باستخدام ثلاث مجموعات بيانات متاحة للعموم (<span class="nodecor">leclerc2019deep</span>, <span class="nodecor">reddy2023video</span>, <span class="nodecor">ouyang2020video</span>). تتألّف هذه المجموعات من لقطات ثنائيّة الأبعاد B-mode مُعنونة لبُنى القلب المختلفة في مرحلتَي نهاية الانبساط (ED) ونهاية الانقباض (ES). تشمل الفئات بطانة البطين الأيسر الشِّغافيّة (LV<span class="math inline">$_{endo}$</span>) وغشاء البطين الأيسر النِّخابي (LV<span class="math inline">$_{epi}$</span>) في المَناظِر: القِمّي ثنائيّ الحجرات (A2C)، والقِمّي رباعيّ الحجرات (A4C)، والمحور القصِّي القصير (PSAX). اتّبعنا تقسيمة البيانات الموضّحة في الجدول [tab1].<br />
<strong>التنفيذ ومقاييس التقييم.</strong> لضمان عدالة المقارنة في جميع التجارب، وحَّدنا إعدادات التدريب والاختبار. أُجرِيت التجارب باستخدام PyTorch، بحجم دفعة ثابت مقداره 5 وعلى مدى 100 عصر باستخدام وحدة Nvidia A100. اعتمدنا بنية U-Net كعمود فقري لدمج مكوّناتنا، واستخدمنا المُحسِّن MADGRAD (<span class="nodecor">defazio2022adaptivity</span>) بمعدّل تعلّم 1e-4. حُوِّلت الصور إلى حجم 224×224 بكسل مع 16 إطارًا، ونُظِّمت لتحقيق متوسّط صفري وتباين واحد. ولزيادة المتانة، طُبِّقت تحويلات عشوائيّة تشمل القصّ التلقائي، والتدوير ضمن [-30°, +30°]، والقصّ على طول المحورين x و y. اعتمدنا مقياس تشابُه دايس (DSC) لتقييم الأداء، مع مقارنة النتائج عبر ثلاثة مَناظِر رئيسيّة: A4C وA2C وPSAX، في مرحلتَي ED وES عند توافر التسميات.<br />
<strong>دراسة المقارنة.</strong> نعرض أداء طريقتنا لتقسيم القلب عبر مَناظِر تصوير مختلفة. وإلى حدّ علمنا، تُعدّ طريقتنا الأولى التي تُغطي تقسيم صور الإيكو عبر مدخلٍ غير محدَّد المنظر. أجرينا المقارنة في وضعين: (1) تدريب واختبار النموذج على المنظر نفسه (نهج مُتخصّص)، و(2) تدريب على جميع المَناظِر واختبار على جميعها (نهج شامل). للاقتباس، اخترنا نماذج SwinUNETR (<span class="nodecor">hatamizadeh2021swin</span>) و U-Transformer (<span class="nodecor">petit2021u</span>) كأساسٍ للمقارنة اعتمادًا على دراسة سابقة (<span class="nodecor">kim2023medivista</span>). بالإضافة إلى ذلك، قارَنّا طريقتنا مع نماذج عالميّة أخرى مثل DoDNet (<span class="nodecor">reddy2023video</span>)، والنموذج العالمي المُوجَّه بـ CLIP (<span class="nodecor">liu2023clip</span>)، وUniSeg (<span class="nodecor">ye2023uniseg</span>)، وUniverSeg (<span class="nodecor">butoi2023universeg</span>) لمهام تقسيم القلب عبر ثلاث مجموعات بيانات.</p>
<p>كما يوضّح الجدول [table2]، تُنتج طريقتنا نتائج تفوق النماذج المُتخصّصة في معظم الحالات، وفي أحايين أخرى تتقارب معها. فعلى سبيل المثال، تتفوّق على U-Transformer المُتخصّص مع استثناءات طفيفة لفئة LV<span class="math inline">$_{endo}$</span> (93.2 مقابل 93.3) وLV<span class="math inline">$_{epi}$</span> (88.3 مقابل 88.5) في مَنظَرَي A2C وA4C على التوالي. وعند دمج المُوجِّهات لتضمين معلومات المنظر تكيُّفيًّا، يتفوّق نموذجنا على جميع النماذج العالميّة في تحديد مناطق الاهتمام (ROI) عبر المَناظِر كافة. ويُعزى ذلك إلى قدرة نموذجنا على التكيّف مع مُوجِّهات المنظر المختلفة، ما يُحسِّن نتائج التقسيم. كما لاحظنا تحسّنًا في المتوسّط مقارنةً بطريقة التقسيم القائم على أمثلة قليلة (89.64 مقابل 81.7). وتُؤكِّد هذه النتائج استفادة نهجنا الفعّالة من المُوجِّهات التكيُّفيّة لإنتاج تقسيمٍ مُتفوق.</p>
<p><strong>دراسة الاستئصال.</strong> لتقييم إسهام كلّ مكوّن في نموذجنا، أجرينا دراسة استئصال. أوّلًا، قارَنّا أداء النموذج مع وبدون مسار ترميز النصّ؛ وعند حذف مسار النصّ الذي يضمّ مواءمة نص–بكسل، انخفض متوسّط DSC من 89.6 إلى 85.6، ما يُؤكِّد أهميّة هذه المواءمة. كما قارَنّا مُشفِّرات نصّ متعددة، فتبيّن أنّ CLIP أقلّ فاعليّة في تمثيل النصّ الطبّي مقارنةً بـ ClinicalBERT (88.8 مقابل 89.6). ثانيًا، درسنا تأثير اختيار مفتاح المُوجِّه عبر تصويت الأغلبيّة المحدّد بـ 
<span class="math display">\[
\arg\max_{G \in \{A,B,C\}} \sum_{i=1}^{3} \mathbb{I}(x_i \in G)
\]</span>
يُظهر تقييم T-SNE (<span class="nodecor">van2008visualizing</span>) في الشكل [figure3] دقّة 0.96 في تمييز المَناظِر القِمّية عن الجانبية، بينما كانت الدقّة لتفريق A2C وA4C أقلّ (0.54 و0.60 على التوالي) نتيجة تداخل بشري في تحديد هذين المَنظرين. ويبيّن الجدول [table4] أثر توفير معلومات المنظر على الأداء؛ إذ انخفض المعدّل من 89.6 إلى 89.4 عند اعتماد معلومات المنظر بدلًا من اختيار المُوجِّهات ذاتيًّا.</p>

<table>
  <thead>
    <tr class="header">
      <th style="text-align: center;">متوسّط DSC (دون معلومات المنظر)</th>
      <th style="text-align: center;"><span class="nodecor">89.6</span></th>
    </tr>
  </thead>
  <tbody>
  </tbody>
  <caption>الجدول [table4]: تأثير تمرير معلومات المنظر صراحةً.</caption>
</table>
<p>[table4]</p>

<h1 id="الخلاصة">الخُلاصَة</h1>
<p>قدّمنا نموذجًا مُبتكرًا لتقسيم صور تخطيط صدى القلب مُوجَّهًا بالنصّ، قادرًا على تعلّم التقسيم عبر مَناظِر تصوير قياسيّة متعدّدة باستخدام بيانات مُوسومة جزئيًّا. يَدمج النموذجُ المعرفة المُسبَقة من نماذج اللغة عبر مواءمة تمثيلاتٍ نصّيّة مع بياناتٍ بصريّة على مستوى البكسل. كما اقترحنا تقنية مطابقة المُوجِّهات باستخدام بنك مُوجِّهات خاصّ لتحقيق تقسيمٍ مستقلّ عن المنظر. اختبرنا نهجنا على ثلاثة مَناظِر قياسيّة، وأثبتنا إمكانيّة تعميمه لتغطية مَناظِر إضافيّة مستقبلًا، ما يُمثّل خطوةً نحو نموذجٍ عالمي لتقسيم الإيكو. تُسهم هذه الطريقة في تبسيط عمليّة التحليل بإلغاء الحاجة إلى خطوة تحديد المنظر يدويًّا، ما يُقلِّل التباين البشري ويُعزِّز موثوقيّة النتائج. وأظهرت التجارب على معايير التقسيم في مَناظِر متعددة أنّ نهجنا لا يُحسِّن الأداء فحسب، بل يُثبت أيضًا فاعليّته العالية.</p>
</body>
</html>