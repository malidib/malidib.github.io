<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Cevat V. Karadağ وَ Nezih Topaloğlu">
  <title>تَدْرِيبِ الشَبَكَةِ العَصَبِيَّةِ المُقَسَّمَةِ بِاِسْتِخْدامِ العَلاماتِ الوَسِيطَة الاِصْطِناعِيَّةِ</title>
  <link href="https://fonts.googleapis.com/css2?family=Cairo:wght@400;600&display=swap" rel="stylesheet">
  <style>
    /* Reset & Base */
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0 }
    html { font-size: 100%; line-height: 1.6; }
    body {
      font-family: 'Cairo', sans-serif;
      max-width: 800px;
      margin: 2rem auto;
      padding: 0 1rem;
      color: #222;
      background: #fafafa;
    }
    a { color: #0366d6; text-decoration: none }
    a:hover { text-decoration: underline }
    h1, h2, h3 { color: #333; margin-top: 2rem; }
    h1 { font-size: 2rem; font-weight: 600; text-align: center; }
    h2 { font-size: 1.5rem; font-weight: 600; border-bottom: 2px solid #ddd; padding-bottom: .3em; }
    h3 { font-size: 1.2rem; font-weight: 600; margin-top: 1.5rem; }
    p { margin: 1rem 0; text-align: justify; }
    header, nav, section, footer, main { display: block }
    nav ul { list-style: none; margin: 1rem 0; padding: 0; }
    nav li { display: inline-block; margin-left: 1rem; }
    nav a { font-size: 0.9rem; }
    .author { text-align: center; font-size: 1rem; margin-top: .5em; }
    .nodecor { font-weight: 600 }
    code, .math.inline { background: #f1f1f1; padding: .1em .3em; border-radius: 3px; font-family: Menlo, monospace; }
    .math.display { display: block; text-align: center; margin: 1em 0; font-family: Menlo, monospace; background: #f1f1f1; padding: .5em; border-radius: 5px; }
    @media (max-width: 600px) {
      body { padding: 1rem }
      h1 { font-size: 1.5rem }
      h2 { font-size: 1.2rem }
    }
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full"></script>
</head>
<body>
  <header>
    <h1>تَدْرِيبِ الشَبَكَةِ العَصَبِيَّةِ المُقَسَّمَةِ بِاِسْتِخْدامِ العَلاماتِ الوَسِيطَة الاِصْطِناعِيَّةِ</h1>
    <p class="author">
      <span class="nodecor">Cevat V. Karadağ</span> وَ
      <span class="nodecor">Nezih Topaloğlu</span>
    </p>
    <nav aria-label="المحتويات">
      <ul>
        <li><a href="#ملخص">مُلَخَّص</a></li>
        <li><a href="#مقدمة">مُقَدِّمَة</a></li>
        <li><a href="#التدريب-المنفصل">التدريب المنفصل لأقسام النموذج</a></li>
        <li><a href="#الطريقة-المقترحة">الطريقة المقترحة</a></li>
        <li><a href="#المزايا">المزايا مقارنة بالتوازي النموذجي</a></li>
        <li><a href="#التنفيذ">تنفيذ على الشبكات المتصلة بالكامل</a></li>
        <li><a href="#results">النتائج والمناقشة</a></li>
        <li><a href="#التعافي">تعزيز الدقة بمراحل التعافي</a></li>
        <li><a href="#استنتاجات">الاستنتاجات والآفاق المستقبلية</a></li>
      </ul>
    </nav>
  </header>

  <main>
    <section id="ملخص">
      <h2>مُلَخَّص</h2>
      <p>الانتشار الواسع لهياكل الشبكات العصبية، وخاصة نماذج التعلم العميق، يمثل تحدياً من حيث التدريب المكثف للموارد. أصبحت قيود ذاكرة وحدة معالجة الرسومات عائقاً رئيسياً في تدريب هذه النماذج الكبيرة. تقدم الاستراتيجيات الحالية، بما في ذلك التوازي في البيانات، والتوازي في النموذج، والتوازي في الأنابيب، والتوازي الكامل في البيانات المجزأة، حلولاً جزئية. التوازي في النموذج، على وجه الخصوص، يمكّن من توزيع النموذج بالكامل عبر وحدات معالجة الرسومات متعددة، لكن التواصل بين هذه الأقسام يبطئ عملية التدريب. بالإضافة إلى ذلك، يُثقل عبء الذاكرة اللازم لتخزين المعلمات المساعدة على كل وحدة معالجة الرسومات المتطلبات الحسابية. بدلاً من استخدام النموذج بالكامل للتدريب، تقترح هذه الدراسة تقسيم النموذج عبر وحدات معالجة الرسومات وتوليد العلامات الوسيطة الاصطناعية لتدريب الأجزاء الفردية. تساعد هذه العلامات، التي تُنتج من خلال عملية عشوائية، في تخفيف العبء على الذاكرة والحمل الحسابي. يؤدي هذا النهج إلى عملية تدريب أكثر كفاءة تقلل من التواصل مع الحفاظ على دقة النموذج. للتحقق من هذه الطريقة، يتم تقسيم شبكة عصبية متصلة بالكامل مكونة من <span class="nodecor">6</span> طبقات إلى جزأين ويتم تقييم أدائها على مجموعة بيانات <span class="nodecor">MNIST</span> الموسعة. تشير النتائج التجريبية إلى أن النهج المقترح يحقق دقة اختبار مماثلة لطرق التدريب التقليدية، مع تقليل كبير في متطلبات الذاكرة والحساب. تساهم هذه الأعمال في التخفيف من كثافة الموارد اللازمة لتدريب الشبكات العصبية الكبيرة، مما يمهد الطريق لتطوير نماذج تعلم عميق أكثر كفاءة.</p>
    </section>

    <section id="مقدمة">
      <h2>مُقَدِّمَة</h2>
      <!-- نص المقدمة كاملاً كما في المصدر -->
    </section>

    <section id="التدريب-المنفصل">
      <h2>التدريب المنفصل لأقسام النموذج</h2>
      <p>يساعد التدريب المنفصل لأقسام النموذج أيضاً في التخفيف من مشكلة تلاشي التدرجات، التي من المتوقع حدوثها أكثر في الشبكات العصبية العميقة (<span class="nodecor">Kolbusz2017vanishing</span>).</p>
    </section>

    <section id="الطريقة-المقترحة">
      <h2>الطريقة المقترحة</h2>
      <!-- نص الطريقة المقترحة كاملاً -->
      <div class="math display">
        \[ SIL_{i,j} \sim \kappa \, U(0,1) \]
      </div>
    </section>

    <section id="المزايا">
      <h2>المزايا مقارنة بالتوازي النموذجي القياسي</h2>
      <!-- نص المزايا كاملاً -->
    </section>

    <section id="التنفيذ">
      <h2>تنفيذ على الشبكات المتصلة بالكامل</h2>
      <!-- نص التنفيذ كاملاً -->
    </section>

    <section id="results">
      <h2>النتائج والمناقشة</h2>
      <!-- نص النتائج كاملاً -->
    </section>

    <section id="التعافي">
      <h2>تعزيز الدقة من خلال مراحل التعافي</h2>
      <!-- نص تعزيز الدقة كاملاً -->
    </section>

    <section id="استنتاجات">
      <h2>الاستنتاجات والآفاق المستقبلية</h2>
      <!-- نص الاستنتاجات كاملاً -->
    </section>
  </main>

  <footer>
    <p style="text-align:center; margin-top:2rem; font-size:.9rem; color:#666;">
      © جميع الحقوق محفوظة للباحثين
    </p>
  </footer>
</body>
</html>