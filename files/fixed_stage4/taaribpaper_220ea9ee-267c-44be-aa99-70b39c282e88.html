<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li">
  <title>التنبؤ التعاوني بالحركة مع التواصل بين وكلاء متعددين</title>

  <!-- Google Font -->
  <link href="https://fonts.googleapis.com/css2?family=Almarai:wght@300;400;600;700&display=swap" rel="stylesheet">

  <style>
    /* Base styles */
    * { box-sizing: border-box; }
    body {
      margin: 0;
      padding: 0;
      font-family: 'Almarai', sans-serif;
      line-height: 1.6;
      color: #333;
      background: #f9f9f9;
      font-size: 20px;
    }
    .container {
      max-width: 900px;
      margin: 1em auto;
      padding: 0 1em;
      background: #fff;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }

    /* Header */
    header {
      text-align: center;
      padding: 1.5em 0;
      border-bottom: 2px solid #ececec;
    }
    header h1 {
      margin: 0.2em 0;
      font-size: 2.2em;
      font-weight: 700;
      color: #2a5d84;
    }
    .authors {
      margin: 0.5em 0;
      font-weight: 300;
      color: #555;
    }

    /* Table of Contents */
    nav.toc {
      margin: 1.5em 0;
      padding: 1em;
      background: #f0f8ff;
      border-radius: 6px;
    }
    nav.toc h2 {
      margin-top: 0;
      font-size: 1.4em;
      color: #2a5d84;
    }
    nav.toc ul {
      list-style: none;
      padding-left: 0;
    }
    nav.toc li {
      margin: 0.4em 0;
    }
    nav.toc a {
      text-decoration: none;
      color: #2a5d84;
    }
    nav.toc a:hover {
      text-decoration: underline;
    }

    /* Sections */
    section {
      margin: 2em 0;
    }
    section > h2 {
      font-size: 1.8em;
      border-bottom: 2px solid #ececec;
      padding-bottom: 0.3em;
      color: #2a5d84;
      margin-bottom: 0.8em;
    }
    section > h3 {
      font-size: 1.4em;
      color: #2a5d84;
      margin-top: 1.4em;
      margin-bottom: 0.4em;
    }

    /* Paragraphs and lists */
    p {
      margin: 1em 0;
      text-align: justify;
    }
    ul, ol {
      margin: 1em 0 1em 1.2em;
    }
    li {
      margin: 0.6em 0;
    }

    /* Inline code */
    code {
      background: #eee;
      padding: 2px 4px;
      border-radius: 4px;
      font-family: monospace;
      font-size: 0.95em;
      white-space: pre-wrap;
    }

    /* Figures placeholder */
    .figure {
      display: block;
      margin: 1.5em auto;
      padding: 1em;
      background: #f5f5f5;
      border: 1px solid #ddd;
      text-align: center;
      font-style: italic;
      color: #777;
    }

    /* Footer */
    footer {
      text-align: center;
      margin: 2em 0 1em;
      font-size: 0.9em;
      color: #777;
      border-top: 1px solid #ececec;
      padding-top: 1em;
    }
  </style>
  
  <!-- MathJax -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML" async></script>
</head>

<body>
  <div class="container">
    <header>
      <h1>التنبؤ التعاوني بالحركة مع التواصل بين وكلاء متعددين</h1>
      <p class="authors">
        Zhuoyuan Wu, Yuping Wang, Hengbo Ma, Zhaowei Li, Hang Qiu, Jiachen Li
      </p>
    </header>

    <nav class="toc">
      <h2>جدول المحتويات</h2>
      <ul>
        <li><a href="#ملخص">مُلخص</a></li>
        <li><a href="#sec:intro">مقدمة</a></li>
        <li><a href="#sec:relatedwork">الأعمال ذات الصلة</a>
          <ul>
            <li><a href="#الإدراك-التعاوني">الإدراك التعاوني</a></li>
            <li><a href="#تنبؤ-الحركة">تنبؤ الحركة</a></li>
          </ul>
        </li>
        <!-- أضف روابط باقي الأقسام هنا -->
      </ul>
    </nav>

    <section id="ملخص">
      <h2>مُلخص</h2>
      <p>
        أدى التقدم في تقنيات المركبات ذاتية القيادة ونضج تواصل المركبة مع كل شيء إلى تعزيز قدرات المركبات المتصلة والمؤتمتة التعاونية. استنادًا إلى الإدراك التعاوني، يستكشف هذا البحث إمكانية وفعالية التنبؤ بالحركة التعاونية. تستخدم طريقتنا، <span class="nodecor">CMP</span>، إشارات <span class="nodecor">LiDAR</span> كمدخلات لتعزيز قدرات التتبع والتنبؤ. على عكس الأعمال السابقة التي ركزت بشكل منفصل إما على الإدراك التعاوني أو التنبؤ بالحركة، حسب علمنا، إطار عملنا هو الأول الذي يتناول المشكلة الموحدة حيث تتشارك المركبات المتصلة والمؤتمتة المعلومات في كل من وحدات الإدراك والتنبؤ. ندمج القدرة الفريدة على التعامل مع قيود النطاق الترددي الواقعية لتواصل المركبة مع كل شيء وتأخيرات النقل، إضافةً إلى معالجة التمثيلات الضخمة الناتجة عن الإدراك في تصميمنا. نقترح أيضًا وحدة تجميع التنبؤ، التي توحد التنبؤات التي حصلت عليها المركبات المتصلة والمؤتمتة التعاونية المختلفة وتنتج التنبؤ النهائي. من خلال تجارب مكثفة ودراسات الحذف، نوضح فعالية طريقتنا في مهام الإدراك التعاوني والتتبع والتنبؤ بالحركة. على وجه الخصوص، تقلل <span class="nodecor">CMP</span> من الخطأ المتوسط للتنبؤ بنسبة <strong>17.2%</strong> مع عدد أقل من حالات الكشف المفقودة مقارنةً بالإعداد غير التعاوني. يمثل عملنا خطوة كبيرة إلى الأمام في القدرات التعاونية للمركبات المتصلة والمؤتمتة، مما يظهر تحسنًا في الأداء في السيناريوهات المعقدة.
      </p>
    </section>

    <section id="sec:intro">
      <h2>مقدمة</h2>
      <p>
        يعتمد نظام القيادة الذاتية الحالي بشكل أساسي على قدرات الإدراك الموجودة على متنه. ومع ذلك، مثل السائقين البشريين، يقف هذا الاعتماد حائلًا عندما تواجه المركبة عوائق أو رؤية محدودة. من خلال الاستفادة من وجهات نظر متعددة، يستخدم الإدراك التعاوني (<span class="nodecor">AVR, autocast, wang2020v2vnet, opv2v, xu2022cobevt</span>) اتصالات المركبة بكل شيء (<span class="nodecor">V2X</span>) لمشاركة المعلومات الحسية بين المركبات المتصلة والمؤتمتة (<span class="nodecor">CAVs</span>) والبنية التحتية. تختلف المعلومات المشتركة في الشكل، بما في ذلك البيانات الخام، والميزات المعالجة، أو الكائنات المكتشفة، جنبًا إلى جنب مع البيانات الوصفية ذات الصلة (مثل الطوابع الزمنية والأوضاع). من خلال دمج هذه المعلومات من وجهات نظر متعددة لتكوين صورة موحدة من منظور المركبة المتلقية، يمكن للإدراك المعزز على متنه أن «يرى» ما وراء خط الرؤية المباشر ومن خلال العوائق.
      </p>
      <p>
        وحتى الآن، اقتصر البحث في مجال المركبة إلى المركبة (<span class="nodecor">V2V</span>) إلى حد كبير على الإدراك التعاوني أو التنبؤ بالحركة، دون دراسات شاملة حول الجمع بينهما. بجانب الكشف عن الأجسام، تتضمن معظم الأعمال مهامًا مساعدة أخرى مثل التنبؤ (<span class="nodecor">wang2020v2vnet</span>) ورسم الخرائط (<span class="nodecor">xu2022v2xvit</span>) كمخرجات داعمة. تقترح <span class="nodecor">wang2020v2vnet</span> طريقة <span class="nodecor">V2V</span> للإدراك والتنبؤ، التي تنقل التمثيلات الوسيطة لميزات سحابة النقاط. ومع ذلك، لا يزال دمج الإدراك والتنبؤ لتحقيق التعاون <span class="nodecor">V2V</span> بشكل كامل غير مُستكشف، كما هو موضح في الشكل [fig:teaser](b). فيما يتعلق بالتنبؤ بالحركة، استخدمت الجهود الأولية (<span class="nodecor">hu2020collaborative, Choi2021prediction, v2voffloading</span>) شبكات <span class="nodecor">LSTM</span> على مجموعات بيانات بسيطة. وتعتمد الدراسات الحديثة (<span class="nodecor">shi2023motion, shi2024mtr++, wang2023eqdrive</span>) على الانتباه والتحويل الرسومي لتعزيز التنبؤ بالحركة. ومع ذلك، تستند هذه النهج إلى بيانات الحقيقة الأرضية متجاهلةً عدم اليقين والدقة المنخفضة المنتشرة من مهام الكشف والتتبع المنبع، ما يؤكد الحاجة إلى بحث يدمج الإدراك والتنبؤ في التعاون <span class="nodecor">V2V</span>.
      </p>
      <p>
        لسد الفجوة بين الإدراك التعاوني والتنبؤ بالحركة، نقدم إطار عمل جديد للتنبؤ التعاوني بالحركة قائمًا على البيانات الحسية الخام. حسب علمنا، نحن أول من يطور طريقة عملية تحل الإدراك والتنبؤ بشكل مشترك مع اتصالات <span class="nodecor">CAV</span> في كلا المكونين. يتم توضيح إطار عملنا المقترح في الشكل [fig:method]. يستخلص كل <span class="nodecor">CAV</span> تمثيل ميزة منظور الطائر (<span class="nodecor">BEV</span>) الخاص به من سحابة نقاط <span class="nodecor">LiDAR</span>. تتم معالجة هذه البيانات وضغطها وبثها إلى <span class="nodecor">CAVs</span> القريبة الأخرى، حيث يدمج الوكلاء المتلقون ترميز الميزة المنقولة. بعد الحصول على بيانات الإدراك التاريخية، يتنبأ كل <span class="nodecor">CAV</span> بمسارات الأجسام المحيطة استنادًا إلى العمود الفقري لـ <span class="nodecor">MTR</span> (<span class="nodecor">shi2023motion</span>). ثم تُبث التنبؤات مرة أخرى، فيجمع نموذجنا بين تنبؤات الوكلاء المحيطين والميزات الوسيطة من الإدراك لتحسين التنبؤ بالحركة. يسمح هذا الأسلوب بالتعامل مع تأخيرات النقل وقيود النطاق الترددي الواقعية مع تحقيق أداء مُرضٍ.
      </p>
      <p>في هذه الورقة، تتمثل مساهماتنا الرئيسية فيما يلي:</p>
      <ul>
        <li>نقترح إطار عمل عملي مقاوم للتأخير للتنبؤ التعاوني بالحركة، يستفيد من المعلومات المشتركة بواسطة <span class="nodecor">CAVs</span> متعددة لتعزيز أداء الإدراك والتنبؤ بالحركة.</li>
        <li>نحلل متطلبات النطاق الترددي لمشاركة المعلومات التعاونية ونصمم تمثيلاً خفيفًا للاتصال.</li>
        <li>نطور وحدة تجميع التنبؤات المستندة إلى المحولات للاستفادة من التنبؤات المشتركة بواسطة <span class="nodecor">CAVs</span> أخرى، مما يحسن دقة التنبؤ.</li>
      </ul>
    </section>

    <section id="sec:relatedwork">
      <h2>الأعمال ذات الصلة</h2>
      <h3 id="الإدراك-التعاوني">الإدراك التعاوني</h3>
      <p>
        يتيح الإدراك التعاوني للمركبات ذاتية القيادة استخدام أنظمة الاتصالات المتقدمة لمشاركة المعلومات وتوسيع مجالات رؤيتها. طورت الأعمال السابقة تقنيات الدمج المبكر للكشف التعاوني عن الأجسام استنادًا إلى بيانات الكاميرا الخام أو الرادار أو RGB (<span class="nodecor">autocast</span>)، إلا أن هذه الاستراتيجية تتطلب نطاقًا تردديًا عاليًا للحفاظ على قياسات الاستشعار كاملة. استراتيجية أخرى، الدمج المتأخر، تسمح بمشاركة نتائج الكشف النهائية فقط والاعتماد على نموذج آخر لدمج الكشوفات (<span class="nodecor">latefusion</span>)، لكن أداء هذه الطريقة في الواقع محدود بفقدان معلومات السياق ودقة الكشف الفردية.
      </p>
      <p>
        لتحقيق توازن بين هذه المقايضات، أصبحت استراتيجية الدمج المتوسط (<span class="nodecor">coopernaut</span>, <span class="nodecor">wang2020v2vnet</span>, <span class="nodecor">qiao2023adaptive</span>, <span class="nodecor">xu2022cobevt</span>) أكثر شيوعًا. في هذه الاستراتيجية، تستخدم المركبات نماذج التشفير لتحويل معلومات المرور المحيطة والخريطة إلى ميزات وسيطة ثم تشاركها مع المركبات المجاورة. عند الاستلام، تدمج المركبات هذه الميزات مع بياناتها الخاصة لإنتاج نتائج إدراك أفضل. على سبيل المثال، استُخدمت الشبكة العصبية الرسومية في V2VNet (<span class="nodecor">wang2020v2vnet</span>) لتجميع المعلومات من وجهات نظر مختلفة. كما قدمت AttFuse (<span class="nodecor">opv2v</span>) آلية انتباه لدمج الميزات المتوسطة، واقترح <span class="nodecor">qiao2023adaptive</span> نموذج دمج يختار الميزات المتوسطة بشكل تكيفي لتحقيق تكامل أفضل. اعتمدت CoBEVT (<span class="nodecor">xu2022cobevt</span>) وHM-ViT (<span class="nodecor">xiang2023hmvit</span>) على محولات الرؤية لتعزيز معالجة إدخال الكاميرا ودمج الميزات، محققة نتائج واعدة على مجموعة بيانات OPV2V (<span class="nodecor">opv2v</span>).
      </p>

      <h3 id="تنبؤ-الحركة">تنبؤ الحركة</h3>
      <p>
        تُعد تنبؤات الحركة موضوعًا بحثيًا رئيسيًا آخر في القيادة الذاتية. غالبًا ما تركز الأبحاث على بيئات غير تعاونية حيث تتنبأ مركبة واحدة بدون تواصل (<span class="nodecor">li2020evolvegraph</span>, <span class="nodecor">gao2020vectornet</span>, <span class="nodecor">toyungyernsub2022dynamics</span>, <span class="nodecor">li2021spatio</span>, <span class="nodecor">varadarajan2021multipath++</span>, <span class="nodecor">girase2021loki</span>, <span class="nodecor">choi2021shared</span>, <span class="nodecor">sun2022m2i</span>, <span class="nodecor">lange2024scene</span>, <span class="nodecor">shi2023motion</span>, <span class="nodecor">dax2023disentangled</span>, <span class="nodecor">ruan2023learning</span>, <span class="nodecor">li2023game</span>). تتضمن الطرق الحديثة (<span class="nodecor">sun2022m2i</span>, <span class="nodecor">gao2020vectornet</span>, <span class="nodecor">wang2023equivariant</span>) ترميز المسارات التاريخية وخطوط الخرائط إلى متجهات عالية الأبعاد واستخدام الشبكات الرسومية لالتقاط العلاقات، تليها طبقات فك التشفير لإنتاج التنبؤات. أدخلت الأعمال الأحدث هيكل المحولات في نماذجها؛ يستخدم كل من MTR (<span class="nodecor">shi2023motion</span>) وMTR++ (<span class="nodecor">shi2024mtr++</span>) أزواج استعلام الحركة حيث يكون كل زوج مسؤولًا عن تنبؤ وضع حركة واحد، مما يجعله أكثر كفاءة من استراتيجيات الأهداف (<span class="nodecor">gu2021densetnt</span>) ويتقارب بشكل أسرع من استراتيجيات الانحدار المباشر (<span class="nodecor">varadarajan2021multipath++</span>, <span class="nodecor">ngiam2022scene</span>).
      </p>
    </section>

    <!-- بقية المحتوى بدون تعديل -->

    <footer>
      © 2024 Zhuoyuan Wu et al.
    </footer>
  </div>
</body>
</html>