<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Rui Wang, Dengpan Ye, Long Tang, Yunming Zhang, Jiacheng Deng">
  <title>تحسين كشف التزييف العميق باستخدام الدمج السمعي البصري واستراتيجيات التوزين الديناميكي</title>
  <!-- MathJax for rendering المعادلات -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    /* عام */
    body {
      margin: 0;
      padding: 0;
      background: #f5f5f5;
      color: #333;
      font-family: "Segoe UI", "Tahoma", "Noto Sans Arabic", sans-serif;
      line-height: 1.6;
      -webkit-font-smoothing: antialiased;
    }
    .container {
      max-width: 900px;
      margin: 2em auto;
      background: #fff;
      padding: 2em;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    header {
      text-align: center;
      margin-bottom: 2em;
    }
    header h1 {
      margin: 0;
      font-size: 2.4em;
      line-height: 1.2;
      color: #222;
    }
    .authors {
      margin-top: 0.5em;
      color: #555;
      font-size: 1em;
    }
    /* فقرات */
    p {
      text-align: justify;
      margin: 1em 0;
    }
    a.uri {
      color: #0066cc;
      text-decoration: none;
    }
    a.uri:hover {
      text-decoration: underline;
    }
    /* العناوين */
    section > h2 {
      margin-top: 2em;
      margin-bottom: 0.5em;
      padding-bottom: 0.3em;
      border-bottom: 2px solid #e0e0e0;
      font-size: 1.6em;
      color: #222;
    }
    h3 {
      margin-top: 1.5em;
      margin-bottom: 0.4em;
      font-size: 1.3em;
      color: #333;
    }
    /* القوائم */
    ul {
      margin: 0.8em 0 1.2em 1.5em;
      padding: 0;
    }
    ul li {
      margin: 0.5em 0;
    }
    /* المعادلات */
    .math.inline { background: #f0f0f0; padding: 0 0.2em; border-radius: 2px; font-family: serif; }
    .math.display { margin: 1em 0; text-align: center; }
    /* أكواد */
    pre {
      background: #f9f9f9;
      padding: 1em;
      overflow-x: auto;
      font-family: monospace;
      border-radius: 4px;
      margin: 1em 0;
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1>تحسين كشف التزييف العميق باستخدام الدمج السمعي البصري واستراتيجيات التوزين الديناميكي</h1>
      <p class="authors">
        <span class="nodecor">Rui Wang</span>, 
        <span class="nodecor">Dengpan Ye</span>, 
        <span class="nodecor">Long Tang</span>, 
        <span class="nodecor">Yunming Zhang</span>, 
        <span class="nodecor">Jiacheng Deng</span>
      </p>
    </header>
    <main>
      <section id="ملخص">
        <h2>مُلَخَّص</h2>
        <p>مع التحسينات المستمرة في تقنيات التزييف العميق، تطوَّر المحتوى المزيف من أحادي الوسائط إلى تعدد وسائط مدمج، مما طرح تحديات جديدة أمام خوارزميات الكشف التقليدية. في هذه الورقة، نقترح <strong>AVT<span class="math inline">\(^2\)</span>-DWF</strong>، إطار سمعي–بصري مزدوج يعتمد على <strong>التوزين الديناميكي</strong> يهدف إلى تضخيم الإشارات المزيفة محلياً وعبر الوسائط لتعزيز دقة الكشف. يستند AVT<span class="math inline">\(^2\)</span>-DWF إلى نهج ثنائي المراحل لالتقاط الخصائص المكانية والديناميات الزمنية لتعابير الوجه من خلال مشفّر المحول البصري مع استراتيجية ترميز الإطارات <span class="math inline">\(n\)</span> ومشفّر المحول السمعي. ثم يُطبّق المحول متعدد الوسائط بالتوزين الديناميكي لمواجهة تحدي دمج المعلومات المشتركة بين الصوت والصورة. تشير التجارب على مجموعات DeepfakeTIMIT، FakeAVCeleb، وDFDC إلى أن AVT<span class="math inline">\(^2\)</span>-DWF يحقق نتائج رائدة في الكشف داخل مجموعات البيانات وخارجها. يتوفر الكود المصدري على <a href="https://github.com/raining-dev/AVT2-DWF" class="uri">https://github.com/raining-dev/AVT2-DWF</a>.</p>
      </section>

      <section id="مقدمة">
        <h2>مُقَدِّمَة</h2>
        <p>مع التقدم المستمر في تقنيات الذكاء الاصطناعي لإنتاج المحتوى، لم يعد الإنتاج مقتصراً على وسيط واحد. فقد استخدمت مؤخراً أداة “HeyGen” لإنشاء مقطع فيديو يصوّر المغنية تايلور سويفت تتحدث الصينية عبر مزج حركات شفاه اصطناعية وصوت مزيف. تشكّل هذه التجارب المعقدة ومتعددة الوسائط تحدياً كبيراً لأساليب الكشف الحالية، مما يستدعي تطوير تقنيات أكثر تقدماً لرصد هذه الفيديوهات العميقة.</p>
        <p>ركزت الطرق السابقة (<span class="nodecor">verdoliva2020media</span>، <span class="nodecor">rossler2019faceforensics++</span>) على كشف التزييف في وسيط واحد، معتمدةً على تقنيات التلاعب المعروفة لرصد البصمات البصرية. ومع ذلك، تبيّن أن أدائها يتراجع عند الانتقال عبر مجموعات بيانات مختلفة. حاولت بعض الأساليب الأحدث استخدام إشارات مكان-زمان على مستوى البقع لتعزيز متانة النموذج وقدرته على التعميم (<span class="nodecor">zhang2022deepfake</span>، <span class="nodecor">heo2023deepfake</span>). تعتمد هذه الطرق على تقسيم الفيديو إلى بقع تتم معالجتها بواسطة محول بصري، كما هو موضح في الشكل العلوي، لكن ذلك يقطع الارتباط الطبيعي بين مكونات الوجه ويحدّ من قدرة الكشف على اكتشاف التباينات المكانية. علاوة على ذلك، مع إمكانية تزوير المحتوى الصوتي، قد يؤدي التركيز الحصري على المعالجة البصرية إلى تحيّز. لذلك، حظي الكشف السمعي–البصري متعدد الوسائط باهتمام متزايد في الأبحاث الحديثة.</p>
        <p>تشمل الأساليب الحالية للكشف عن التزييف متعدد الوسائط EmoForen (<span class="nodecor">mittal2020emotions</span>) الذي يركز على اكتشاف التباين العاطفي، وMDS (<span class="nodecor">chugh2020not</span>) بمقياس التنافر الوسيطي لقياس التوافق السمعي–البصري، وVFD (<span class="nodecor">cheng2023voice</span>) بآلية مطابقة بين الصوت والوجه. يستفيد AVA-CL (<span class="nodecor">zhang2023joint</span>) من الانتباه السمعي–البصري والتعلم التبايني لتعزيز دمج السمات من كلا الوسائط بفعالية. إلا أن هذه الأبحاث ركّزت أساساً على دمج السمات عبر الوسائط، متجاهلة تحسين استخراج السمات داخل كل وسيط. وللتعامل مع هذا القصور، يقترح بحثنا استراتيجية بقع إطار-<span class="math inline">\(n\)</span> لتحسين استخراج السمات الموضعية، إلى جانب وحدة DWF لموازنة دمج أدلة التزييف عبر الوسائط.</p>
        <p>في هذه الدراسة، نقدم محولاً سمعي–بصرياً متعدد الوسائط يُعرف بـ<strong>AVT<span class="math inline">\(^2\)</span>-DWF</strong>، يستهدف التقاط السمات المميزة لكل وسيط وتحقيق تناغم فعّال بينها. لتعزيز قدرات التمثيل الزمني والمكاني، نعتمد استراتيجية ترميز بقع الإطار-<span class="math inline">\(n\)</span> داخل مشفّر المحول البصري لاستخراج ملامح الوجه الدقيقة، وبالتوازي نطبق معالجة مماثلة في المجال السمعي لاستخراج السمات الصوتية. بعدها نستخدم وحدة دمج الأوزان الديناميكي (<strong>DWF</strong>) في المحول متعدد الوسائط، حيث تتنبأ هذه الآلية بأوزان لكل من الوسائط السمعية والبصرية ديناميكياً، مما يعزّز اندماج أدلة التزييف والسمات المشتركة ويدعم دقة الكشف.</p>
        <p>باختصار، تشمل مساهماتنا:</p>
        <ul>
          <li>استراتيجية ترميز بقع الإطار-<span class="math inline">\(n\)</span> لتعزيز استخراج ملامح الوجه ضمن الإطارات، بما في ذلك تفاصيل التعابير وحركات الوجه الدقيقة.</li>
          <li>محول متعدد الوسائط مزوّد بوحدة دمج الأوزان الديناميكي (DWF) لتحسين دمج المعلومات المتباينة من الوسائط السمعية والبصرية.</li>
          <li>إطار AVT<span class="math inline">\(^2\)</span>-DWF الذي يجمع بين هاتين الطريقتين، مع تقييم شامل باستخدام معايير معترف بها لإظهار الفعالية العالية للإطار.</li>
        </ul>
      </section>

      <section id="الطريقة">
        <h2>الطريقة</h2>
        <p>يهدف نهجنا إلى تضخيم إشارات التزييف داخل كل وسيط وعبر الوسائط، لتعزيز قدرات الكشف بمعلومات أكثر دقة. يتألف إطار AVT<span class="math inline">\(^2\)</span>-DWF من ثلاثة مكونات رئيسية: مشفّر المحول البصري للوجه، مشفّر المحول السمعي، ووحدة دمج الأوزان الديناميكي (DWF). أولاً، يستخلص المشفّران السمات البصرية والصوتية لاكتشاف دلائل التزييف داخل كل وسيط. ثم تُدمج مخرجاتهما وتُمرَّر إلى وحدة DWF التي تتعلم الأوزان المناسبة لدمج الوسائط وتحسين نتائج الكشف.</p>

        <h3 id="مشفر-تحويل-الوجه">مشفّر المحول البصري للوجه</h3>
        <p>يتميّز مشفّر المحول البصري للوجه عن الأبحاث السابقة (<span class="nodecor">zhang2022deepfake</span>, <span class="nodecor">heo2023deepfake</span>) باستخدام استراتيجية ترميز تغطي <span class="math inline">\(n\)</span> إطارات كما في الشكل <span class="nodecor">1</span>. توجه هذه الاستراتيجية تركيز النموذج نحو المعلومات الزمانية–المكانية الجوهرية عبر إطارات متعددة ضمن الفيديو. لأي فيديو <span class="math inline">\(V\)</span>، نستخرج كتلة الوجه <span class="math inline">\(\mathbf{F} \in \mathbb{R}^{T \times C \times H \times W}\)</span> حيث تمثل <span class="math inline">\(T\)</span> طول التسلسل، و<span class="math inline">\(C\)</span> عدد القنوات، و<span class="math inline">\(H \times W\)</span> دقة الإطار. تُعاد تنظيم الإطارات زمنياً إلى تمثيل <span class="math inline">\(C \times (T \times H) \times W\)</span>. كما في ViT (<span class="nodecor">dosovitskiy2020image</span>)، يُدمج رمز فئة قابل للتعلّم <span class="math inline">\(\mathbf{F}_{class}\)</span> في السلسلة مع تضمينات الموضع <span class="math inline">\(\mathbf{E}_{p}\)</span>. ثم تُسقَط ميزات كل بقعة خطياً إلى فضاء بابعاد <span class="math inline">\(D\)</span> قبل إدخالها في مشفّر التحويل الذي يشمل انتباهاً ذاتياً متعدد الرؤوس (MSA)، مع تطبيق تطبيع الطبقة (LN) والاتصالات المتبقية (RC) في كل طبقة. تعبر العملية رسمياً عن نفسها كما يلي:</p>
        <pre><code class="math display">\[
\begin{aligned}
  \mathbf{F}_0 &= [ \mathbf{F}_{class}\mathbf{E}_{p};\, \mathbf{f}_1 \mathbf{E}_{p};\, \cdots; \mathbf{f}_T \mathbf{E}_{p} ],   \\
  \mathbf{F}_\ell &= \mathrm{MSA}(\mathrm{LN}(\mathbf{F}_{\ell-1})) + \mathbf{F}_{\ell-1},\quad \ell = 1, \dots, L .
\end{aligned}
\]</code></pre>
        <p>حيث <span class="math inline">\(\mathbf{f} \in \mathbb{R}^{(H \times W\times C) \times D}\)</span> هي الميزة البصرية و<span class="math inline">\(\mathbf{E}_{p} \in \mathbb{R}^{(T + 1) \times D}\)</span> تضمين الموضع القابل للتعلّم.</p>

        <h3 id="مشفر-تحويل-الصوت">مشفّر المحول السمعي</h3>
        <p>للتعامل مع الصوت، يستخدم النموذج بنية مشابهة لمشفّر الوجه مع آلية الانتباه الذاتي لالتقاط الاعتماديات طويلة المدى داخل الإشارة الصوتية. نبدأ باستخراج معالم MFCC من الإشارة، لتنتج <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{T \times M}\)</span> حيث <span class="math inline">\(T\)</span> الزمن و<span class="math inline">\(M\)</span> عناصر التردد، ثم تُسقَط خطياً إلى فضاء بابعاد <span class="math inline">\(D\)</span>. يُدمج رمز فئة متنقل <span class="math inline">\(\mathbf{A}_{\text{class}}\)</span> وتُضاف تضمينات موضعية <span class="math inline">\(\mathbf{E}_{p}\)</span> قبل إدخالها في مشفّر التحويل، وفق المعادلات:</p>
        <pre><code class="math display">\[
\begin{aligned}
  \mathbf{A}_0 &= [ \mathbf{A}_{class} \mathbf{E}_{p};\, \mathbf{a}_1 \mathbf{E}_{p};\, \cdots; \mathbf{a}_T \mathbf{E}_{p} ],  \\
  \mathbf{A}_\ell &= \mathrm{MSA}(\mathrm{LN}(\mathbf{A}_{\ell-1})) + \mathbf{A}_{\ell-1},\quad \ell = 1, \dots, L  .
\end{aligned}
\]</code></pre>
        <p>حيث <span class="math inline">\(\mathbf{a} \in \mathbb{R}^{(H \times W \times C) \times D}\)</span> تمثل الميزة الصوتية و<span class="math inline">\(\mathbf{E}_{p} \in \mathbb{R}^{(T + 1) \times D}\)</span> تضمين الموضع القابل للتعلّم.</p>

        <h3 id="المحول-متعدد-الوسائط-مع-دمج-الاوزان-الديناميكي">المحول متعدد الوسائط مع دمج الأوزان الديناميكي</h3>
        <p>بعد استخراج ميزات <span class="math inline">\(\mathbf{A}_{class}\)</span> و<span class="math inline">\(\mathbf{F}_{class}\)</span>، تولّد وحدة دمج الأوزان الديناميكي (DWF) أوزاناً <span class="math inline">\(W_A\)</span> و<span class="math inline">\(W_F\)</span> لكل وسيط باستخدام كتلة انتباه متقاطع متعدد الرؤوس MHCA مستلهمة من (<span class="nodecor">chen2023meaformer</span>). يحسب الرأس <span class="math inline">\(i\)</span> استفسارات <span class="math inline">\(Q_{f/a}^{(i)}\)</span>، مفاتيح <span class="math inline">\(K_{f/a}^{(i)}\)</span>، وقيم <span class="math inline">\(V_{f/a}^{(i)}\)</span> عبر مصفوفات <span class="math inline">\(W_q^{(i)}, W_k^{(i)}, W_v^{(i)}\)</span>. يتم حساب نواتج MHCA لكل وسيط ثم تجمع الأوزان كما يلي:</p>
        <pre><code class="math display">\[
\begin{gathered}
\mathrm{MHCA}(\mathbf{F}_{class}) = \mathrm{Concat}(W^i_F V_f)\,W_o,\quad
\mathrm{MHCA}(\mathbf{A}_{class}) = \mathrm{Concat}(W^i_A V_a)\,W_o,  \\
W^i_F = \bar{\beta}^{(i)}_{ff} + \bar{\beta}^{(i)}_{fa},\quad
W^i_A = \bar{\beta}^{(i)}_{aa} + \bar{\beta}^{(i)}_{af},  \\
W_F = \frac{1}{N_h}\sum_{i=1}^{N_h}W^i_F,\quad
W_A = \frac{1}{N_h}\sum_{i=1}^{N_h}W^i_A,
\end{gathered}
\]\end{code></pre>
        <p>حيث <span class="math inline">\(W_o \in \mathbb{R}^{d \times d}\)</span> و<span class="math inline">\(\bar{\beta}^{(i)}_{*}\)</span> أوزان الانتباه. يُحسب التفاعل المشترك <span class="math inline">\(\bar{\beta}^{(i)}_{fa}\)</span> كالآتي:</p>
        <pre><code class="math display">\[
\bar{\beta}^{(i)}_{fa}=\frac{\exp(Q_fK_a^{\top}/\sqrt{d_h})}{\sum_{n\in\{f,a\}}\exp(Q_fK_n^{\top}/\sqrt{d_h})},
\]\end{code></pre>
        <p>وبالمثل تُحسب بقية الأوزان مع <span class="math inline">\(d_h=d/N_h\)</span>. تُطبّق LN وRC بعد كل طبقة:</p>
        <pre><code class="math display">\[
\begin{aligned}
h_v &= \mathrm{LN}(\mathrm{MHCA}(\mathbf{F}_{\ell-1})+\mathbf{F}_{\ell-1}),\\
h_a &= \mathrm{LN}(\mathrm{MHCA}(\mathbf{A}_{\ell-1})+\mathbf{A}_{\ell-1}),
\end{aligned}
\]\end{code></pre>
        <p>ثم يُمرَّران إلى الطبقات التالية في وحدة DWF.</p>
        <p><strong>دمج الوسائط.</strong> لتعظيم الاستفادة من الميزات بين الوسائط، نضرب <span class="math inline">\(\mathbf{F}_{class}\)</span> و<span class="math inline">\(\mathbf{A}_{class}\)</span> بالأوزان <span class="math inline">\(W_F\)</span> و<span class="math inline">\(W_A\)</span> قبل الدمج:</p>
        <pre><code class="math display">\[
V = W_F\,\mathbf{F}_{class}\oplus W_A\,\mathbf{A}_{class}.
\]\end{code></pre>
      </section>

      <section id="التجربة">
        <h2>التجربة</h2>
        <h3 id="مجموعة-البيانات">مجموعة البيانات</h3>
        <p>شملت تجاربنا ثلاث مجموعات بيانات رئيسية: (<span class="nodecor">korshunov1812deepfakes</span>)، (<span class="nodecor">dolhansky2020deepfake</span>)، و(<span class="nodecor">khalid2021fakeavceleb</span>). ونظراً لعدم التوازن بين الفيديوهات الحقيقية والمزيفة، استخدمنا استراتيجيات لموازنة العينات. يوضح الجدول [tab:tab0] نسب البيانات قبل وبعد المعالجة. في مجموعة (<span class="nodecor">korshunov1812deepfakes</span>) جُمعت فيديوهات حقيقية من (<span class="nodecor">sanderson2002vidtimit</span>) وإطارات متتابعة جزئية من (<span class="nodecor">dolhansky2020deepfake</span>). لتعويض عدم التوازن في (<span class="nodecor">khalid2021fakeavceleb</span>)، أضفنا 19,000 فيديو حقيقي من (<span class="nodecor">chung2018voxceleb2</span>). قسمت البيانات بنسبة 7:1:2 بين تدريب، تحقق، واختبار مع توازن 1:1 في مجموعة الاختبار. جرت جميع التقييمات على هذه المجموعة حصرياً.</p>
        <h3 id="التنفيذ">التنفيذ</h3>
        <p>أثناء التدريب، نقسم الفيديوهات إلى كتل بطول <span class="math inline">\(T\)</span> (افتراضياً 30). نستخدم كاشف الوجوه Single Shot Scale-invariant Face Detector (S<span class="math inline">\(^3\)</span>FD) (<span class="nodecor">zhang2017s3fd</span>) لاكتشاف الوجوه ومحاذاتها، ثم نحفظها كصور <span class="math inline">\(224\times224\)</span>. في الجانب الصوتي، نحسب ميزات MFCC باستخدام نافذة Hanning مدتها 15 مللي ثانية بتداخل 4 مللي ثوان لضبط التحليل الطيفي. جرت جميع التجارب تحت إعدادات موحدة لضمان مقارنة عادلة.</p>
        <h3 id="مقارنات-مع-الأحدث-في-المجال">المقارنة مع أحدث الأساليب في المجال</h3>
        <p>في سلسلة من التجارب الشاملة، قارنّا AVT<span class="math inline">\(^2\)</span>-DWF بعدد من النماذج الرائدة باستخدام مقاييس الدقة (<span class="nodecor">Accuracy</span>) ومساحة تحت المنحنى (<span class="nodecor">AUC</span>). قسمت النماذج إلى فئتين: بصرية (<span class="nodecor">V</span>) ومتعددة الوسائط (<span class="nodecor">AV</span>). أظهرت النتائج في الجدول [tab:tab1] تفوّق AVT<span class="math inline">\(^2\)</span>-DWF في معظم الحالات. في مجموعة DF-TIMIT منخفضة الجودة (LQ)، حقق كل من AVT<span class="math inline">\(^2\)</span>-DWF وAVA-CL دقة 99.99% و100% على التوالي، بينما في FakeAVCeleb المعقدة قدّمت AVA-CL المدعومة بالتعلم التبايني أداءً مقارباً، لكن إطارنا سجّل موثوقية أعلى بفضل توازن بيانات الاختبار. في مجموعة DFDC الواسعة، تفوّق AVT<span class="math inline">\(^2\)</span>-DWF على جميع الأساليب، محققاً دقة 88.02% وAUC 89.20%.</p>
        <h3 id="تقييم-البيانات-المتقاطعة">تقييم البيانات المتقاطعة</h3>
        <p>ركز هذا التقييم على متانة AVT<span class="math inline">\(^2\)</span>-DWF عبر مجموعات بيانات مختلفة، مقارنةً بأربعة نماذج بارزة: Xception (<span class="nodecor">rossler2019faceforensics++</span>)، CViT (<span class="nodecor">wodajo2021deepfake</span>)، LipForensics (<span class="nodecor">haliassos2021lips</span>)، وMDS (<span class="nodecor">mittal2020emotions</span>). أجريت التجارب عبر FakeAVCeleb، DFDC، وDF-TIMIT. تلخص النتائج في الجدول [tab:tab2]، حيث يُبرز AVT<span class="math inline">\(^2\)</span>-DWF تفوّقه في مواجهة طرق تزييف جديدة، متفوقاً على CViT المدعوم بالمحول.</p>
        <h3 id="دراسة-الاستئصال">دراسة الاستئصال</h3>
        <h4 id="فائدة-وحدة-dwf">فائدة وحدة DWF</h4>
        <p>في تحليل استئصالي لوحدة DWF، قارنا ثلاث تكوينات: مشفّر بصري فقط، دمج ميزات صوتية وبصرية بسيط (AV) بدون DWF، والإطار الكامل (VA-DWF). تظهر نتائج DFDC وFakeAVCeleb في الجدول [tab:tab3] أثر DWF. في DFDC، حيث يبقى الصوت غير مزوَّر، أدى الدمج البسيط إلى تراجع الدقة. أما في FakeAVCeleb التي تضمنت تزييفاً صوتياً مع وجه حقيقي، فرفع DWF نسبة الكشف بمقدار 11.55% و12.89%، مما يدل على قيمته في التقاط الاتساق بين الوسائط.</p>
        <h4 id="فائدة-ترميز-الإطارات-n">فائدة ترميز الإطارات <span class="math inline">\(n\)</span></h4>
        <p>لتقييم تأثير استراتيجية ترميز الإطارات <span class="math inline">\(n\)</span>، استخرجنا بقعاً عشوائية غير متكررة من تسلسل إطارات الوجه ودمجناها لتكوين صور إدخال كاملة. تظهر نتائج DFDC وFakeAVCeleb في الجدول [tab:tab4] تحسناً بنسبة 22.45% و3.74% على التوالي مقارنةً بأسلوب البقع التقليدي، مما يؤكد فعالية ترميز الإطارات <span class="math inline">\(n\)</span> في الحفاظ على المعلومات المستمرة لملامح الوجه.</p>
      </section>

      <section id="الخلاصة">
        <h2>الخلاصة</h2>
        <p>قدّمنا إطار AVT<span class="math inline">\(^2\)</span>-DWF لمعالجة الفروق المكانية الدقيقة والاتساق الزمني داخل محتوى الفيديو. نسلط الضوء على السمات الفريدة لكل وسيط عبر مشفّرَي محول الصوت والوجه باستخدام استراتيجية ترميز الإطار <span class="math inline">\(n\)</span>، ثم نطبق آلية الدمج الديناميكي (DWF) لاستخراج الخصائص المشتركة. تشير نتائج تجاربنا إلى أن AVT<span class="math inline">\(^2\)</span>-DWF يتفوق على الأساليب الحالية، سواء داخل البيانات نفسها أو عبر مجموعات بيانات مختلفة، مما يؤكد أهمية التناغم المتعدد الوسائط للكشف الفعّال عن التزييف العميق في السيناريوهات الواقعية.</p>
      </section>
    </main>
  </div>
</body>
</html>