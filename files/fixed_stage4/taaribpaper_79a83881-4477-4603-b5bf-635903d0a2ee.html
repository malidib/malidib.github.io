<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Gonçalo Paulo, Thomas Marshall, Nora Belrose">
  <title>هل تنتقل قابلية تفسير المحوّلات إلى الشبكات العصبية المتكررة؟</title>

  <!-- Google font for Arabic typography -->
  <link href="https://fonts.googleapis.com/css2?family=Noto+Naskh+Arabic&display=swap" rel="stylesheet">

  <!-- MathJax for equations -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

  <!-- Modern CSS reset and typography -->
  <style>
    /* Reset */
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }

    /* Base typography */
    body {
      font-family: 'Noto Naskh Arabic', serif;
      font-size: 1rem;        /* 16px */
      line-height: 1.6;
      color: #333;
      background: #fafafa;
      padding: 1.5rem;
    }
    a { color: #1a73e8; text-decoration: none; }
    a:hover { text-decoration: underline; }

    /* Header */
    header {
      text-align: center;
      margin-bottom: 2rem;
    }
    h1.title {
      font-size: 2.5rem;
      color: #222;
      margin-bottom: 0.5rem;
    }
    .author {
      font-size: 1rem;
      color: #555;
    }
    .nodecor {
      font-weight: bold;
      color: #1a73e8;
      text-decoration: none;
    }

    /* Main article */
    main {
      max-width: 900px;
      margin: 0 auto;
      background: #fff;
      padding: 2rem;
      border-radius: 8px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.05);
    }
    section {
      margin-top: 2rem;
    }
    h2, h3, h4 {
      color: #2a2a2a;
      margin-bottom: 0.75rem;
      line-height: 1.3;
    }
    h2 { font-size: 1.75rem; border-bottom: 2px solid #e0e0e0; padding-bottom: 0.3rem; }
    h3 { font-size: 1.4rem; margin-top: 1.2rem; }
    h4 { font-size: 1.2rem; margin-top: 1rem; }

    /* Paragraphs and lists */
    p {
      margin: 0 0 1rem 0;
      text-align: justify;
    }
    ol, ul {
      margin: 0 0 1rem 1.5rem;
      line-height: 1.6;
    }
    ol li, ul li {
      margin-bottom: 0.5rem;
    }

    /* Tables */
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 1rem 0;
    }
    th, td {
      border: 1px solid #ddd;
      padding: 0.75rem;
      vertical-align: top;
    }
    th {
      background: #f0f0f0;
      text-align: left;
    }

    /* Footnotes */
    .footnotes {
      margin-top: 2rem;
      padding-top: 1rem;
      border-top: 1px solid #ddd;
      font-size: 0.9rem;
      color: #555;
    }
    .footnotes ol {
      list-style: decimal;
      margin-left: 1rem;
    }
    .footnotes li {
      margin-bottom: 0.5rem;
    }

    /* Math display */
    .math.display {
      display: block;
      text-align: center;
      margin: 1rem 0;
    }
    code { white-space: pre; background: #f5f5f5; padding: 0.2rem 0.4rem; border-radius: 4px; }

  </style>
</head>
<body>

  <header>
    <h1 class="title">
      هل تنتقل قابلية تفسير <span class="nodecor">Transformer</span> إلى <span class="nodecor">RNNs</span>؟
    </h1>
    <p class="author">
      <span class="nodecor">Gonçalo Paulo</span>, 
      <span class="nodecor">Thomas Marshall</span>, 
      <span class="nodecor">Nora Belrose</span>
    </p>
  </header>

  <main>
    <section id="ملخص">
      <h2>مُلَخَّص</h2>
      <p>شهدنا في الآونة الأخيرة تقدماً في هندسة الشبكات العصبية المتكررة، مثل <span class="nodecor">Mamba</span> و<span class="nodecor">RWKV</span>، مما مكّن هذه الشبكات من مطابقة أداء نماذج <span class="nodecor">Transformer</span> ذات الحجم المماثل، أو حتى التفوق عليها في تعقيد نمذجة اللغة وتقييمات المهام اللاحقة، وهو ما يُشير إلى إمكانية بناء الأنظمة المستقبلية على هذه البنى الجديدة كلياً. في هذه الورقة، نفحص ما إذا كانت طرق التفسير المصممة أصلاً لنماذج لغة <span class="nodecor">Transformer</span> قابلة للتطبيق على هذه البنى المتكررة الجديدة. على وجه التحديد، نركز على: توجيه مخرجات النموذج عبر إضافة التنشيط التبايني، واستخراج التنبؤات الكامنة باستخدام العدسة المعدلة، واستنباط المعرفة الكامنة من النماذج المضبوطة لإصدار مخرجات خاطئة في ظل ظروف معينة. تُظهر نتائجنا أن معظم هذه التقنيات فعّالة عند تطبيقها على <span class="nodecor">RNNs</span>، كما نوضح أنه يمكن تعزيز بعضها بالاستفادة من الحالة المضغوطة لدى هذه الشبكات.</p>
    </section>

    <section id="مقدمة">
      <h2>مُقَدِّمَة</h2>
      <p>لقد حلّت هندسة المحوّلات (<span class="nodecor">vaswani2017attention</span>) محل الشبكات العصبية المتكررة (<span class="nodecor">RNN</span>) في معالجة اللغات الطبيعية في السنوات الأخيرة، بسبب قدرتها المبهرة على التعامل مع التبعيات طويلة المدى وإمكانية تدريبها بشكل متوازٍ عبر البعد الزمني. ومع ذلك، تعاني آلية الانتباه الذاتي—القلب النابض للمحوّل—من تعقيد زمني تربيعي، مما يجعل تطبيقها على تسلسلات طويلة جداً مكلفاً حسابياً.</p>
      <p>مامبا (<span class="nodecor">gu2023mamba</span>) و<span class="nodecor">RWKV</span> (<span class="nodecor">peng2023rwkv</span>) هما نموذجان متكراران يسمحان بالتدريب المتوازي عبر البعد الزمني من خلال تقييد العلاقة التكرارية الكامنة لتكون <em>قابلة للتوازي</em> (<span class="nodecor">martin2017parallelizing</span>, <span class="nodecor">blelloch1990prefix</span>). من الناحية التجريبية، تظهر هذه البنى تعقيداً حسابياً وأداءً منخفضين مقارنة بالمحوّلات ذات الأبعاد المماثلة، مما يجعلها بديلاً جذاباً للعديد من حالات الاستخدام.</p>
      <p>في هذه الورقة، نقيم مدى انطباق أدوات التفسير الشائعة المصممة أصلاً للمحوّل على هذه النماذج الجديدة من الشبكات العصبية المتكررة. على وجه الخصوص، نعيد إنتاج النتائج التالية من أدبيات تفسير المحوّل:</p>
      <ol>
        <li>
          <strong>إضافة التنشيط التبايني (CAA):</strong> وجد (<span class="nodecor">rimsky2023steering</span>) أنه يمكن التحكم في نماذج لغة المحوّل باستخدام "متجهات التوجيه"، المحسوبة بأخذ متوسط الفرق في تنشيطات تيار البقايا بين أزواج من الأمثلة الإيجابية والسلبية لسلوك معين، مثل الاستجابات الواقعية مقابل الاستجابات الهلوسية.
        </li>
        <li>
          <strong>العدسة المعدلة:</strong> وجد (<span class="nodecor">belrose2023eliciting</span>) أنه يمكن استخلاص تنبؤات الرمز التالي القابلة للتفسير من الطبقات المتوسطة للمحوّل باستخدام مسابير خطية، وأن دقة هذه التنبؤات تزداد تدريجياً مع العمق.
        </li>
        <li>
          <strong>النماذج "الغريبة":</strong> وجد (<span class="nodecor">mallen2023eliciting</span>) أن طرق الاستقصاء البسيطة يمكن أن تستخلص معرفة المحوّل بالإجابة الصحيحة على سؤال، حتى عندما يتم ضبطه لإخراج إجابة خاطئة. كما وجدوا أن هذه المسابير تعمم على مشكلات أصعب من تلك التي تم تدريب المسبار عليها.
        </li>
      </ol>
      <p>نُقدّم أيضاً <em>توجيه الحالة</em>، وهو تعديل لـCAA يعمل على الحالة المضغوطة للشبكة العصبية المتكررة بدلاً من تيارها المتبقي.</p>
    </section>

    <section id="الهندسات-المعمارية">
      <h2>البنى المِعْمَارِيَّة</h2>
      <p>نركّز في هذه الورقة على بنية مامبا (<span class="nodecor">gu2023mamba</span>) و<span class="nodecor">RWKV</span> v5، حيث تتوفر نماذج مدرّبة مسبقاً مجاناً على HuggingFace Hub. قررنا استبعاد نموذج الضبع المخطط 7B (<span class="nodecor">stripedhyena2023</span>) لأنه يتضمن كتل انتباه ذات تعقيد زمني تربيعي، وبالتالي لا يصنّف ضمن الشبكات العصبية المتكررة حسب تعريفنا.</p>

      <section id="مامبا">
        <h3>مامبا</h3>
        <p>تعتمد هندسة مامبا على آليتين مختلفتين لتوجيه المعلومات بين مواقع الرموز: كتلة التلافيف السببية، ونموذج الحالة الفضائية الانتقائي (<span class="nodecor">SSM</span>). يُعد نموذج الحالة الفضائية الانتقائي الابتكار الرئيسي لـ(<span class="nodecor">gu2023mamba</span>)، حيث تُسمح معاملات <span class="nodecor">SSM</span> بأن تعتمد على المدخلات، مما يعزّز قدرة النموذج التعبيرية.</p>
      </section>

      <section id="rwkv">
        <h3>RWKV</h3>
        <p>القيمة الرئيسية الموزونة بالاستجابة (RWKV) هي بنية شبكة عصبية متكررة قدمها (<span class="nodecor">peng2023rwkv</span>). خضعت RWKV لسلسلة من التعديلات؛ في هذه الورقة نركّز على الإصدارين <span class="nodecor">4</span> و<span class="nodecor">5</span>. تستخدم بنى RWKV وحدات المزج الزمني المتناوب ومزج القنوات، اللتين تشكلان معاً طبقة واحدة. يكمن الفرق الرئيسي بين الإصدار <span class="nodecor">4</span> والإصدار <span class="nodecor">5</span> في أن الأول يحتوي على حالة ذات قيمة متجهية، بينما يحتوي الثاني على حالة ذات قيمة مصفوفة "متعددة الرؤوس" (<span class="nodecor">peng2024eagle</span>).</p>
      </section>

    </section>

    <!-- باقي الأقسام تلتزم بنفس النمط -->

  </main>

  <footer>
    <section class="footnotes">
      <ol>
        <li id="fn1">
          على عكس (<span class="nodecor">rimsky2023steering</span>), اخترنا عدم تطبيع متجهات التوجيه لأن معايير التنشيط تختلف بشكل كبير بين النماذج، ولا يحقق متجه بموحد معيار التأثير نفسه في جميعها.
          <a href="#fnref1">↩</a>
        </li>
        <li id="fn2">
          استخدمنا نسخة معدلة من شفرتهم متاحة على 
          <a href="https://github.com/AlignmentResearch/tuned-lens">https://github.com/AlignmentResearch/tuned-lens</a>.
          <a href="#fnref2">↩</a>
        </li>
        <li id="fn3">
          يمكن الاطلاع على الكود الأصلي في 
          <a href="https://github.com/EleutherAI/elk-generalization">https://github.com/EleutherAI/elk-generalization</a>.
          <a href="#fnref3">↩</a>
        </li>
      </ol>
    </section>
  </footer>

</body>
</html>