<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Zhenwen Dai, Federico Tomasi, Sina Ghiassian">
  <title>استكشاف السياق والاستغلال في تعلم التعزيز</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-CHTML-full" async></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    /* ====== Base ====== */
    body {
      font-family: "Segoe UI", "Tahoma", "Arial", sans-serif;
      font-size: 20px;
      line-height: 1.6;
      color: #222;
      background: #fafafa;
      margin: 0;
      padding: 0;
    }
    .container {
      max-width: 900px;
      margin: auto;
      padding: 20px;
      background: #fff;
      box-shadow: 0 2px 8px rgba(0,0,0,0.1);
    }
    a { color: #0066cc; text-decoration: none; }
    a:hover { text-decoration: underline; }

    /* ====== Typography ====== */
    h1.title {
      margin-bottom: 0.2em;
      font-size: 2.4em;
      font-weight: bold;
      text-align: center;
    }
    header .author {
      margin-bottom: 1.2em;
      text-align: center;
      font-size: 1em;
      color: #555;
    }
    header .author .nodecor {
      font-style: normal;
      color: #333;
    }
    h2 {
      margin-top: 1.6em;
      margin-bottom: 0.6em;
      font-size: 1.8em;
      border-bottom: 2px solid #eee;
      padding-bottom: 0.2em;
    }
    p {
      margin-bottom: 1em;
      text-align: justify;
    }
    code, .nodecor {
      white-space: pre-wrap;
      font-family: Menlo, Consolas, monospace;
      background: #f0f0f0;
      padding: 2px 6px;
      border-radius: 4px;
      color: #d6336c;
    }

    /* ====== Sections ====== */
    section {
      margin-bottom: 2em;
    }

    /* ====== RTL adjustments ====== */
    html[dir="rtl"] h1.title,
    html[dir="rtl"] h2,
    html[dir="rtl"] p {
      text-align: right;
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <h1 class="title">استكشاف السياق والاستغلال في تعلم التعزيز</h1>
      <p class="author">
        <span class="nodecor">Zhenwen Dai</span>, 
        <span class="nodecor">Federico Tomasi</span>, 
        <span class="nodecor">Sina Ghiassian</span>
      </p>
    </header>

    <section id="ملخص">
      <h2>مُلخّص</h2>
      <p>التعلّم في السياق هو نهج واعد لتعلّم السياسات عبر الإنترنت لأساليب التعلم المعزز (<span class="nodecor">RL</span>) في وضع عدم الاتصال، والذي يمكن تنفيذه في وقت الاستدلال دون الحاجة إلى تحسين تدريجي. ومع ذلك، تعوق هذه الطريقة تكاليف حسابية كبيرة ناتجة عن جمع مجموعات ضخمة من مسارات التدريب والحاجة إلى تدريب نماذج <span class="nodecor">Transformer</span> واسعة النطاق. نعالج هذا التحدي من خلال تقديم خوارزمية استكشاف السياق والاستغلال (<span class="nodecor">ICEE</span>) المصممة لتحسين كفاءة تعلم السياسات في السياق. على عكس النماذج الحالية، تحقق <span class="nodecor">ICEE</span> توازناً بين الاستكشاف والاستغلال في وقت الاستدلال داخل نموذج <span class="nodecor">Transformer</span>، دون الحاجة إلى استدلال <span class="nodecor">Bayesian</span> صريح. ونتيجة لذلك، يمكن لـ<span class="nodecor">ICEE</span> حل مشاكل التحسين <span class="nodecor">Bayesian</span> بكفاءة مماثلة لتلك الخاصة بأساليب تعتمد على العمليات <span class="nodecor">Gaussian</span>، ولكن بزمن أقل بكثير. من خلال التجارب في بيئات عالم الشبكة، نُظهر أن <span class="nodecor">ICEE</span> يمكنها تعلم حل مهام التعلم المعزز الجديدة باستخدام عشرات الحلقات فقط، مما يمثل تحسناً كبيراً مقارنة بالمئات من الحلقات التي تحتاجها طريقة التعلم في السياق السابقة.</p>
    </section>

    <section id="مقدمة">
      <h2>مقدمة</h2>
      <p>تمثل النماذج المحولة نهجاً فعالاً للغاية في نمذجة التسلسل، مع تطبيقات تمتد عبر مجالات متعددة مثل النصوص والصور والصوت. في مجال التعلم المعزز (<span class="nodecor">Reinforcement Learning (RL)</span>)، اقترح (<span class="nodecor">NEURIPS2021_7f489f64</span>) و(<span class="nodecor">NEURIPS2021_099fe6b0</span>) نموذج تعلم معزز في وضع عدم الاتصال كمشكلة تنبؤ تسلسلي باستخدام المحول. لقد أثبت هذا الأسلوب نجاحه في التعامل مع مجموعة من المهام باستخدام تقنيات نمذجة التسلسل على نطاق واسع فقط (<span class="nodecor">NEURIPS2022_b2cac94f, Reed2022-lj</span>). يكمن العيب الرئيسي في عدم قدرة السياسة على تحسين نفسها عند تشغيلها في بيئات عبر الإنترنت. للتغلب على ذلك، تم تقديم طرق الضبط الدقيق مثل (<span class="nodecor">Zheng2022-kr</span>)، التي تتيح تحسين السياسة بشكل مستمر. ومع ذلك، غالباً ما تعتمد هذه الطرق على التحسين التدريجي البطيء والمكلف حسابياً.</p>
      <p>من ناحية أخرى، يمكن للتعلّم في السياق، وهي خاصية بارزة في نماذج اللغة الكبيرة (<span class="nodecor">Large Language Models (LLMs)</span>)، التعامل مع المهام الجديدة من خلال توفير تفاصيل المهمة عبر تلميحات لغوية، مما يلغي الحاجة إلى الضبط الدقيق. يقترح (<span class="nodecor">laskin2023incontext</span>) خوارزمية تعلّم في السياق للتعلم المعزز، تستخدم نموذج تسلسلي لتقطير خوارزمية تعلم السياسات من مسارات تدريب التعلم المعزز. النموذج الناتج قادر على إجراء تعلم السياسات في وقت الاستدلال عبر عملية تكرارية لأخذ عينات من الإجراءات وزيادة التلميح. تتطلب هذه الطريقة تكاليف حسابية عالية، إذ تستلزم جمع مجموعات كبيرة من مسارات التدريب وتدريب نماذج محول ضخمة تحتاج إلى نمذجة جزء كبير من مسار التدريب. يرجع السبب الرئيسي لهذه التكاليف إلى طول مسارات التدريب الناتج عن عملية التجربة والخطأ البطيئة في خوارزميات تعلم السياسات التعزيزية.</p>
      <p>تهدف هذه الورقة إلى تحسين كفاءة تعلم السياسات في السياق من خلال القضاء على الحاجة إلى التعلم من مسارات التعلم. في سيناريو مثالي، يمكن تحقيق تعلم سياسة فعّال عبر عملية تجربة وخطأ فعّالة. بالنسبة لمشاكل التعلم المعزز المبسطة مثل الأذرع المتعددة (<span class="nodecor">Multi-Armed Bandits (MAB)</span>)، ثبت وجود عملية تجربة وخطأ فعّالة مثل عينة تومسون وحدود الثقة العليا. تُعرف هذه العملية بعملية الاستكشاف-الاستغلال (<span class="nodecor">Exploration-Exploitation (EE)</span>)، وتعتمد بشكل كبير على عدم اليقين المعرفي المستمد من الاعتقاد البايزي. ومع ذلك، من الصعب استنتاج عدم اليقين المعرفي الدقيق لمشاكل التعلم المعزز التسلسلي باستخدام الطرق البايزية التقليدية. بناءً على دراسات حديثة في تقدير عدم اليقين لنماذج اللغة الكبيرة (<span class="nodecor">yin-etal-2023-large</span>)، نفحص التوزيعات التنبؤية لهذه النماذج، ونجد أنه عبر التدريب الإشرافي البحت على بيانات غير متصلة، يستطيع نموذج التسلسل التقاط عدم اليقين المعرفي في التنبؤات التسلسلية. هذا يوحي بإمكانية تنفيذ الاستكشاف-الاستغلال في التعلم المعزز دون اتصال.</p>
      <p>استناداً إلى هذه الملاحظة، نطور خوارزمية الاستكشاف-الاستغلال في السياق (<span class="nodecor">ICEE</span>) لتعلم السياسات. تأخذ <span class="nodecor">ICEE</span> كمدخلات سلسلة من الحلقات المتعددة لنفس المهمة وتتنبأ بالإجراء المقابل في كل خطوة مشروطة ببعض المعلومات بأثرٍ رجعي. يشبه هذا التصميم المحول القراري (<span class="nodecor">Decision Transformer (DT)</span>) للتعلم المعزز دون اتصال، لكن <span class="nodecor">ICEE</span> يتعامل مع تعلم السياسات في السياق عبر نمذجة حلقات متعددة للمهمة بينما ينمذج <span class="nodecor">DT</span> حلقة واحدة فقط. علاوة على ذلك، لا تحتاج هذه الحلقات لأن تكون من مسار تدريبي، مما يتجنب التكاليف الحسابية العالية المرتبطة بتوليد واستهلاك مسارات التعلم. تتجه توزيعات الإجراءات المتعلمة في <span class="nodecor">DT</span> نحو سياسة جمع البيانات، التي قد لا تكون مثالية إذا كانت دون المستوى الأمثل. لمعالجة هذا التحيّز، نقدم هدفاً خالياً من التحيّز ونطوّر شكلاً خاصاً من المعلومات بالأثر الرجعي لتحقيق استكشاف-استغلال فعال عبر الحلقات.</p>
      <p>من خلال التجارب، نوضح أن سلوك الاستكشاف-الاستغلال يظهر في <span class="nodecor">ICEE</span> أثناء الاستدلال بفضل عدم اليقين المعرفي في التنبؤ بالإجراء. يتضح هذا بشكل خاص عند تطبيق <span class="nodecor">ICEE</span> على التحسين البايزي (<span class="nodecor">Bayesian Optimization (BO)</span>)، حيث يُضاهي أداء <span class="nodecor">ICEE</span> طرقاً تعتمد على العمليات الغاوسية في مهام <span class="nodecor">BO</span> المنفصلة. نوضح أيضاً أن <span class="nodecor">ICEE</span> يمكنها تحسين السياسة لمهمة جديدة بنجاح عبر التجربة والخطأ من الصفر لمشاكل التعلم المعزز التسلسلي. حسب علمنا، <span class="nodecor">ICEE</span> هي الطريقة الأولى التي تدمج بنجاح الاستكشاف-الاستغلال في السياق في التعلم المعزز من خلال النمذجة التسلسلية دون اتصال.</p>
    </section>

    <section id="الأعمال-ذات-الصلة">
      <h2>الأعمال ذات الصلة</h2>

      <h3>التعلم التعلمي</h3>
      <p>لقد زاد الاهتمام مؤخراً بخوارزميات <em>التعلم التعلمي</em> أو <em>تعلم التعلم</em>. بينما يكون المتعلم عبارة عن وكيل يتعلم حل مهمة باستخدام البيانات المرصودة، تتضمن خوارزمية التعلم التعلمي وجود <em>متعلم تعلمي</em> يحسّن باستمرار عملية تعلم المتعلم (<span class="nodecor">schmidhuber1996simple, thrun2012learning, hospedales2021meta, sutton2022history</span>). تم إجراء الكثير من الأعمال في هذا المجال. على سبيل المثال، اقترح (<span class="nodecor">finn2017model</span>) خوارزمية تعلم تعلمي شاملة لا تعتمد على النموذج تقوم بتدريب المعلمات الأولية للنموذج بحيث يحقق أداءً أمثل في مهمة جديدة بعد تحديث المعلمات عبر بضع خطوات تدريجية باستخدام كمية صغيرة من بيانات المهمة الجديدة. تشمل الأعمال الأخرى تحسين المحسنات (<span class="nodecor">andrychowicz2016learning, li2016learning, ravi2016optimization, wichrowska2017learned</span>)، التعلم بالقليل من الأمثلة (<span class="nodecor">mishra2017simple, duan2017one</span>)، تعلم الاستكشاف (<span class="nodecor">stadie2018some</span>)، والتعلم غير المراقب (<span class="nodecor">hsu2018unsupervised</span>).</p>

      <h3>تعلم التعزيز غير المتصل</h3>
      <p>بشكل عام، تم اقتراح تعلم التعزيز كنموذج أساسي عبر الإنترنت (<span class="nodecor">sutton1988learning, sutton1999policy, sutton2018reinforcement</span>). تأتي هذه الطبيعة التعليمية عبر الإنترنت مع بعض القيود مثل صعوبة تطبيقها في العديد من التطبيقات التي يستحيل فيها جمع البيانات عبر الإنترنت والتعلم في آن واحد—مثل القيادة الذاتية—وأحياناً عدم كفاءتها من حيث استخدام البيانات، حيث قد يجري تعلم من عينة ثم تجاهلها والانتقال إلى العينة التالية (<span class="nodecor">levine2020offline</span>). إحدى الأفكار لتعزيز الخبرة المجمعة هي استخدام مخازن إعادة التشغيل. عند استخدام هذه المخازن، يُحتفظ بجزء من العينات في الذاكرة ثم يُعاد استخدامها عدة مرات لتمكين الوكيل من التعلم بشكل أفضل (<span class="nodecor">lin1992self, mnih2015human</span>). يعرف <em>تعلم التعزيز غير المتصل</em> بأنه خوارزميات تعلم التعزيز التي تتعلم بالكامل دون اتصال من مجموعة ثابتة من البيانات التي جُمعت مسبقاً دون جمع بيانات جديدة أثناء التعلم (<span class="nodecor">ernst2005tree, riedmiller2005neural, lange2012batch, fujimoto2019off, siegel2020keep, gulcehre2020rl, nair2020awac</span>). تركز الأدبيات الحديثة على محولات القرار أيضاً على تعلم التعزيز غير المتصل (<span class="nodecor">NEURIPS2021_7f489f64</span>) لأنها تحتاج إلى حساب <em>العائد المتبقي</em> أثناء التدريب، مما يتطلب بيانات جُمعت مسبقاً.</p>

      <h3>التعلم في السياق</h3>
      <p>خوارزميات تعلم التعزيز في السياق هي تلك التي تحسّن سياستها بالكامل <em>في السياق</em> دون تحديث معلمات الشبكة أو إجراء أي ضبط دقيق للنموذج (<span class="nodecor">lu2021pretrained</span>). أُجري بعض الأعمال لدراسة ظاهرة التعلم في السياق ومحاولة شرح إمكانيته (<span class="nodecor">abernethy2023mechanism, min2022rethinking</span>). يعمل الوكيل "جاتو" الذي طوره (<span class="nodecor">reed2022generalist</span>) كوكيل عام متعدد النماذج والمهام والأجسام، حيث يمكن للوكيل نفسه المدرب أن يلعب أتاري، ويُعلق على الصور، ويدردش، ويكدس الكتل باستخدام ذراع روبوت حقيقي، مجرّدًا من سياقه. من خلال تدريب وكيل تعلّم التعزيز على نطاق واسع، أظهر (<span class="nodecor">team2023human</span>) أن وكيلاً في السياق يمكنه التكيف مع بيئات ثلاثية الأبعاد جديدة ومفتوحة النهايات. يحظى الاهتمام بشكل خاص بطريقة التقطير المعروفة باسم <em>AD</em>، وهي طريقة تعلم تعلّم بياني في السياق (<span class="nodecor">laskin2023incontext</span>). على وجه التحديد، تُعد AD طريقة تعلم تعلّم في السياق غير متصلة خالية من التدرّج—تتكيف مع المهام اللاحقة دون تحديث معلمات شبكتها.</p>
    </section>

    <section id="عدم-اليقين-المعرفي-في-تنبؤ-نموذج-التسلسل">
      <h2>عدم اليقين المعرفي في تنبؤ نموذج التسلسل</h2>
      <p>يعالج <span class="nodecor">DT</span>، المعروف أيضاً باسم RL المقلوب، مشكلة تعلم السياسات دون اتصال كمشكلة نمذجة تسلسلية. في هذا القسم، ننظر في نموذج تسلسلي عام ونحلل عدم اليقين التنبؤي له.</p>
      <!-- بقية النص كما هو -->
    </section>

  </div>
</body>
</html>