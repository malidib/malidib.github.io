<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Yeda Song, Dongwook Lee &amp; Gunhee Kim">
  <title>التحفظ التركيبي: نهج توصيلي في التعلم التعزيزي دون اتصال</title>

  <!-- Google Font for Arabic -->
  <link href="https://fonts.googleapis.com/css2?family=Amiri&display=swap" rel="stylesheet">

  <!-- MathJax -->
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

  <style>
    /* Reset & Base */
    * { box-sizing: border-box; margin: 0; padding: 0; }
    body {
      font-family: 'Amiri', serif;
      font-size: 22px;
      line-height: 1.6;
      color: #333;
      background: #fafafa;
      padding: 20px;
    }
    a { color: #0066cc; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code, .nodecor {
      background: #eee;
      padding: 0 4px;
      border-radius: 4px;
      font-family: Menlo, monospace;
    }

    /* Container */
    .container {
      max-width: 800px;
      margin: auto;
      background: #fff;
      padding: 30px;
      border-radius: 8px;
      box-shadow: 0 0 10px rgba(0,0,0,0.05);
    }

    /* Header */
    header {
      text-align: center;
      margin-bottom: 40px;
    }
    .title {
      font-size: 2.4em;
      margin-bottom: 0.2em;
    }
    .author {
      font-size: 1em;
      color: #666;
    }

    /* Sections */
    section {
      margin-bottom: 2em;
    }
    .section-title {
      font-size: 1.6em;
      margin-bottom: 0.5em;
      padding-bottom: 0.2em;
      border-bottom: 2px solid #ddd;
    }
    .subsection-title {
      font-size: 1.3em;
      margin: 1.2em 0 0.5em;
      color: #444;
    }

    /* Lists */
    ul {
      padding-inline-start: 1.2em;
      list-style: disc;
    }
    ul li {
      margin-bottom: 0.6em;
    }
  </style>
</head>

<body>
  <div class="container">
    <header>
      <h1 class="title">التحفظ التركيبي: نهج توصيلي في التعلم التعزيزي دون اتصال</h1>
      <p class="author">
        <span class="nodecor">Yeda Song</span> ،
        <span class="nodecor">Dongwook Lee</span> &amp;
        <span class="nodecor">Gunhee Kim</span>
      </p>
    </header>

    <article>
      <section id="ملخص">
        <h2 class="section-title">مُلَخَّص</h2>
        <p>
          التعلم التعزيزي دون اتصال إطار عمل جذاب يهدف إلى تعلم السياسات المثلى من البيانات المسبقة فقط، دون أي تفاعل إضافي مع البيئة. ومع ذلك، يعاني هذا الإطار بطبيعته من مشكلة التحوّلات التوزيعية؛ فقد تختلف الحالات والأفعال التي تواجهها السياسة الجديدة عن تلك المشمولة في بيانات التدريب. غالباً ما يُعالج ذلك بإضافة قيود إلى السياسة أو دالة القيمة للحد من المخاطر وعدم اليقين. في هذا العمل، نتبنّى وجهة نظر مختلفة لتحقيق نفس أهداف التحفّظ. نُقدّم ما نسميه «التحفّظ التركيبي مع البحث عن المرساة» (COCOA) للتعلم التعزيزي دون اتصال، وهو نهج يُطبّق فكرة إعادة المعايرة التوصيلية (<span class="nodecor">transd_aviv2023</span>) بطريقة تركيبية. في هذا الإطار، نحلّل مدخل الدالة (المتغير الداخلي) إلى مكوّنين: المرساة والفارق («الدلتا»). يفرض التحفّظ التركيبي على كل من المرساة والفوارق البقاء ضمن مجال التوزيع المعروف عبر استخدام نموذج ديناميكيات عكسي متعلم، مما يعزّز ثبات السياسة ودالة القيمة في فضاء المدخلات التركيبي. يظل هذا التحفّظ المستحدث مستقلاً عن التحفّظ السلوكي التقليدي في التعلم التعزيزي دون اتصال. قمنا بتطبيق COCOA على أربع خوارزميات حديثة للتعلم التعزيزي دون اتصال وقياس أدائها على معيار <span class="nodecor">D4RL</span>، وقد أظهرت نتائجنا تحسّناً عاماً في أداء كل منها. تتوفر الشيفرة على
          <a href="https://github.com/runamu/compositional-conservatism" class="uri">https://github.com/runamu/compositional-conservatism</a>.
        </p>
      </section>

      <section id="introduction">
        <h2 class="section-title">مُقَدِّمَة</h2>
        <p>
          حقق التعلم التعزيزي نجاحات ملحوظة في مجالات متعددة، من توجيه حركات الروبوتات (<span class="nodecor">dasari2020robonet</span>) وتحسين استراتيجيات الألعاب (<span class="nodecor">mnih2015human</span>) إلى التدريب الواعد لنماذج اللغة (<span class="nodecor">rajpurkar2016squad</span>). على الرغم من هذه الإنجازات، دفعت التحديات المتعلقة بالتفاعلات الزمنية الفعلية في البيئات المعقدة والحساسة إلى تطوير التعلم التعزيزي دون اتصال كمسار عملي. يتعلم التعلم التعزيزي دون اتصال (<span class="nodecor">wiering2012reinforcement, levine2020offline</span>) أو ما يُعرف بالتعلم الدفعي (<span class="nodecor">lange2012batch</span>) السياسات فقط من البيانات المتاحة مسبقاً، دون أي تفاعل مباشر مع البيئة. يتزايد هذا الاتجاه شعبية في تطبيقات مثل القيادة الذاتية (<span class="nodecor">yu2020bdd100k</span>) والرعاية الصحية (<span class="nodecor">gottesman2019guidelines</span>) حيث تتوفر كميات كبيرة من البيانات المسبقة.
        </p>
        <p>
          أساساً، يكون التعلم التعزيزي دون اتصال عرضة للتحوّلات التوزيعية. تنشأ هذه المشكلة عندما يختلف توزيع الحالات والأفعال التي تواجهها السياسة المستخرجة عن تلك الموجودة في مجموعة بيانات التدريب، مما يشكل تحدياً شائعاً في التعلم الآلي (<span class="nodecor">levine2020offline</span>). تناولت العديد من الخوارزميات هذا الأمر عبر نهج التحفّظ؛ إما بتقييد السياسة أو بتقييم مدى عدم اليقين للتخفيف من الانحرافات التوزيعية (<span class="nodecor">count_kim2023, prdc_ran2023, iql_kostrikov2022, cql_kumar2020, brac_wu2019, bear_kumar2019, bcq_fujimoto2019, mobile_sun2023, rambo_rigter2022, romi_wang2021, combo_yu2021, mopo_yu2020, morel_kidambi2020</span>). تهدف هذه الاستراتيجيات إلى إبقاء الوكيل ضمن التوزيعات المعروفة وتقليل مخاطر السلوكيات غير المتوقعة. في هذا العمل، نسعى أيضاً لتحقيق استقرار السياسة عبر مواءمة توزيع الاختبار مع البيانات المعروفة، لكن من منظور تركيبي مختلف.
        </p>
        <p>
          ندرك أولاً أن مشكلة التحوّل التوزيعي للحالة مرتبطة ارتباطاً وثيقاً بكيفية تعامل مقاربات الدالة مع نقاط الإدخال التي تقع خارج نطاق الدعم. بناءً على ذلك، نستكشف إمكانية تحويل مشكلة «التعلم خارج الدعم» إلى مشكلة «خارج التركيب» عبر حقن متحيّزات استقرائية في مقاربات الدالة الخاصة بالسياسة أو دالة القيمة Q. سبق وأن اقترح (<span class="nodecor">transd_aviv2023</span>) منهجاً توصيلياً يُسمى <em>التحويل الثنائي</em>، حيث تتم إعادة معايرة الدالة الهدف إلى صيغة ثنائية عن طريق تحليل المتغير الإدخالي إلى مكوّنين: المرساة والدلتا. هنا، تمثل المرساة نقطة مرجعية مأخوذة من بيانات التدريب، بينما تمثل الدلتا الفرق بين المتغير الإدخالي والمرساة. تحت افتراضات محددة على توزيعي التدريب والاختبار وخصائص الدالة الهدف، يمكن للتحويل الثنائي معالجة مشكلة «خارج التركيب» وبالتالي التخفيف من مسألة «خارج الدعم» للدالة الأصلية.
        </p>
        <p>
          في هذا السياق، نقدم إطار «الحفاظ التركيبي مع البحث عن المرساة» (<em>COCOA</em>) للتعلم التعزيزي دون اتصال. يستند COCOA إلى مبدأ إعادة المعايرة التوصيلية (<span class="nodecor">transd_aviv2023</span>) ويضيف طبقة تركيبية تعزّز الاستقرار. يقوم الإطار بتحويل مسألة التحوّل التوزيعي إلى مسألة «خارج التركيب» عن طريق نقل عناصر التعميم الرئيسية من البيانات إلى المكوّنات المحللة، ما يجعل اختيار المرساة والدلتا قريباً من توزيع بيانات التدريب.
        </p>
        <p>
          لتحقيق ذلك، نُقدّم «سياسة البحث عن المرساة» كطبقة إضافية تلزم الوكيل بتعيين مرساة ضمن المنطقة المعروفة من فضاء الحالة. يشجّع COCOA اختيار مراسي قريبة من أمثلة التدريب، مع تقييد الفارق («الدلتا») ضمن نطاق ضيق عبر اختيار مراسي متجاورة لحالات التدريب. يساعد هذا الإجراء في تقليل فضاء الإدخال وتهيئته نحو الجزء الذي استُكشف أثناء التدريب. باختصار، عبر تعلّم سياسة تبحث عن المرساة داخل التوزيع وتقدّر الفوارق باستخدام نموذج ديناميكيات عكسي متعلم، نحفّز الحفاظ على الاستقرار في فضاء الإدخال التركيبي لكل من دالة القيمة Q والسياسة. يظل هذا النهج مستقلاً وغير معتمد على التحفّظ السلوكي التقليدي في التعلم التعزيزي دون اتصال.
        </p>
        <p>
          من الناحية التجريبية، وجدنا أن COCOA يعزّز أداء أربع خوارزميات بارزة للتعلم التعزيزي دون اتصال—CQL (<span class="nodecor">cql_kumar2020</span>)، IQL (<span class="nodecor">iql_kostrikov2022</span>)، MOPO (<span class="nodecor">mopo_yu2020</span>)، وMOBILE (<span class="nodecor">mobile_sun2023</span>)—على معيار <span class="nodecor">D4RL</span> (<span class="nodecor">d4rl_fu2020</span>). بالإضافة إلى ذلك، تكشف دراسة الاستئصال عن أهمية سياسة البحث عن المرساة في رفع مستوى الأداء. تُلخّص مساهماتنا الرئيسية كما يلي:
        </p>

        <ul>
          <li>نسعى إلى الحفاظ على الاستقرار في <em>فضاء الإدخال التركيبي</em> لمقاربات الدالة لوظيفة القيمة Q والسياسة، بشكل مستقل وغير معتمد على التحفّظ السلوكي السائد في التعلم التعزيزي دون اتصال.</li>
          <li>نقدم الحفاظ التركيبي مع البحث عن المرساة (<em>COCOA</em>) الذي يجد المرساة والدلتا ضمن التوزيع باستخدام نموذج الديناميكيات العكسية المتعلم، وهو أمر حاسم للتعميم التركيبي.</li>
          <li>نظهر تجريبياً أن COCOA يحسن أداء أربع خوارزميات حديثة للتعلم التعزيزي دون اتصال على معيار D4RL. بالإضافة إلى ذلك، تظهر دراستنا الاستئصالية فعالية سياسة البحث عن المرساة مقارنة باختيار المرساة الاستدلالي.</li>
        </ul>
      </section>

      <section id="sec:prelim">
        <h2 class="section-title">المُقَدِّمات</h2>
        <h3 class="subsection-title" id="subsec:offlinerl">تعلم التعزيز دون اتصال</h3>
        <!-- بقية الوثيقة دون تغيير -->
      </section>
    </article>
  </div>
</body>
</html>