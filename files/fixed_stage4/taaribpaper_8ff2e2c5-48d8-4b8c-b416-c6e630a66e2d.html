<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</title>
  <!-- Google Font for Arabic -->
  <link href="https://fonts.googleapis.com/css2?family=Amiri&display=swap" rel="stylesheet">
  <style>
    /* Reset & Base */
    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
    html, body { height: 100%; }
    body {
      font-family: 'Amiri', serif;
      font-size: 1.1rem;
      line-height: 1.6;
      color: #333;
      background: #fdfdfd;
      padding: 1em;
    }
    a { color: #0066cc; text-decoration: none; }
    a:hover { text-decoration: underline; }

    /* Header */
    header {
      background: #fff;
      padding: 1.5em;
      border-bottom: 2px solid #ccc;
      margin-bottom: 2em;
    }
    header h1 {
      font-size: 2em;
      margin-bottom: 0.5em;
      color: #111;
      text-align: center;
    }
    .authors {
      text-align: center;
      font-size: 0.95em;
      color: #555;
      line-height: 1.4;
    }
    .authors span, .authors a, .authors strong { display: inline-block; margin: 0.15em 0; }
    .authors code {
      display: block;
      background: #eef;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-family: monospace;
    }

    /* Main content */
    main { max-width: 900px; margin: 0 auto; }
    section { margin-bottom: 2em; }
    section h2, section h1 {
      margin: 1em 0 0.5em;
      padding-bottom: 0.3em;
      border-bottom: 1px solid #ddd;
      color: #222;
    }

    /* Paragraphs */
    p { margin: 0.8em 0; }

    /* Lists */
    ul { margin: 1em 0 1em 1.2em; }
    ul li { margin: 0.5em 0; }

    /* Code & Math */
    code { font-family: monospace; background: #f5f5f5; padding: 0.1em 0.3em; border-radius: 2px; }
    .math { font-style: normal; }

    /* Nodecor highlight */
    .nodecor { font-weight: bold; color: #1a237e; }

    /* Figures placeholder */
    figure {
      margin: 1.5em 0;
      text-align: center;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
      margin-top: 0.5em;
    }
  </style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full"></script>
</head>

<body>
  <header>
    <h1>تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</h1>
    <div class="authors">
      <span>Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque</span><br>
      <span>University of Montreal &amp; Mila</span>
      <code>firstname.lastname@umontreal.ca</code><br>
      <strong>Shunichi Akatsuka</strong><br>
      <span>Hitachi, Ltd.</span>
      <code>shunichi.akatsuka.bo@hitachi.com</code><br>
      <strong>Aaron Courville</strong><br>
      <span>University of Montreal &amp; Mila</span>
      <code>aaron.courville@umontreal.ca</code>
    </div>
  </header>

  <main>
    <section id="ملخص">
      <h2>مُلَخَّص</h2>
      <p>نستكشف تحدي تعلم التعزيز العميق متعدد الوكلاء في بيئات تنافسية جزئية، حيث تواجه الأساليب التقليدية صعوبات في تعزيز التعاون القائم على المعاملة بالمثل. يتعلم وكلاء <span class="nodecor">LOLA</span> و<span class="nodecor">POLA</span> سياسات تعاونية قائمة على المعاملة بالمثل من خلال التفاضل على عدد محدود من خطوات تحديث الخصم المستقبلية. إلا أن لهذه التقنيات قيدًا أساسيًا: نظرًا لاعتمادها على عدد قليل من خطوات التحسين، قد يستغلها خصم قادر على اتخاذ خطوات إضافية لتعظيم عائده. استجابةً لذلك، نقدم نهجًا جديدًا يسمى تشكيل الاستجابة المثلى (<span class="nodecor">BRS</span>)، الذي يوظف خصمًا يحاكي الاستجابة المثلى، ويُطلق عليه "المحقِّق". لتكييف المحقِّق مع سياسة الوكيل في الألعاب المعقدة، نقترح آلية تكيف قابلة للتفاضل تعتمد على الحالة، ميسّرة عبر "الإجابة على الأسئلة" لاستخراج تمثيل للوكيل بناءً على سلوكه في مواقف بيئية محددة. للتحقق من صحة طريقتنا تجريبيًا، نعرض أداء نماذجنا المحسّن مقابل خصم <span class="nodecor">Monte Carlo Tree Search (MCTS)</span> الذي يعمل كتقريب للاستجابة المثلى في لعبة القطع النقدية. يوسّع هذا العمل نطاق تطبيق تعلم التعزيز متعدد الوكلاء في البيئات التنافسية الجزئية ويمهّد طريقًا جديدًا نحو تحقيق رفاهية اجتماعية أفضل في الألعاب ذات المنفعة الجماعية.</p>
    </section>

    <section id="sec:intro">
      <h2>مُقَدِّمَة</h2>
      <p>مكنت خوارزميات التعلم المعزز متعدد الوكلاء من تحقيق أداء متميز في ألعاب معقدة وعالية الأبعاد مثل لعبة الذهاب (<span class="nodecor">AlphaGo</span>) وستار كرافت (<span class="nodecor">AlphaStar</span>). الهدف الأسمى من التعلم المعزز هو تدريب وكلاء قادرين على مساعدة البشر في حل المشكلات الصعبة. لا محالة، سيحتاج هؤلاء الوكلاء إلى الاندماج في سيناريوهات الحياة الواقعية التي تتطلب التفاعل مع البشر ووكلاء تعلم آخرين. فعلى الرغم من تفوق التدريب متعدد الوكلاء في البيئات التعاونية أو التنافسية الكاملة، غالبًا ما يفشل في اكتشاف تعاون قائم على المعاملة بالمثل في البيئات التنافسية الجزئية. مثال بارز على ذلك غياب قدرة وكلاء MARL التقليديين على تعلم استراتيجيات كالرد بالمثل في معضلة السجين المتكررة.</p>
      <p>رغم الطابع التمثيلي لألعاب المنفعة الجماعية الشائعة مثل معضلة السجين، تتكرر مثل هذه المشكلات في المجتمع والطبيعة. تخيل سيناريو تحاول فيه دولتان (وكلاء) تعظيم إنتاجهما الصناعي، مع ضمان مناخ عمل مناسب يحدّ من الانبعاثات الكربونية. من ناحية، ترغب كل دولة في أن تفي الأخرى بالتزاماتها البيئية؛ ومن ناحية أخرى، قد يغريهما التزايد في الانبعاثات لتحقيق عوائد صناعية أكبر. تلزم المعاهدة الفعّالة كل دولة—من خلال تهديد بالعقوبات—بالالتزام بالحدود المتفق عليها للانبعاثات. وإذا أخفق الوكلاء في تطوير استراتيجيات كالمعاملة بالمثل، فمن المرجح أن ينتهي بهما المطاف بتصعيد متبادل مؤسف في استهلاك الطاقة والانبعاثات.</p>
      <p>...</p>
    </section>

    <section id="sec:background">
      <h2>الخَلْفِيَّة</h2>

      <h3 id="تعلم-التعزيز-المتعدد-العوامل">تَعَلُّم التَّعْزِيز المُتَعَدِّد العَوامِل</h3>
      <p>تُعَرَّف لعبة ماركوف متعددة العوامل بالرمز <span class="math inline">\(\bm{(} N, \mathcal{S},\{\mathcal{A}^i\}, \mathbb{P},\{r^i\}, \gamma \bm{)}\)</span>. هنا، <span class="math inline">\(N\)</span> تمثل عدد العوامل، <span class="math inline">\(\mathcal{S}\)</span> فضاء الحالات، و<span class="math inline">\(\mathcal{A}:=\bigotimes_i \mathcal{A}^i\)</span> مجموعة الأفعال. احتمالات الانتقال ممثلة بـ <span class="math inline">\(\mathbb{P}: \mathcal{S}\times\mathcal{A}\to\Delta(\mathcal{S})\)</span> والمكافأة بـ <span class="math inline">\(r^i:\mathcal{S}\times\mathcal{A}\to\mathbb{R}\)</span>. والعائد <span class="math inline">\(R^i=\sum_t \gamma^t r^i_t\)</span>. سياسة العامل <span class="math inline">\(i\)</span> هي <span class="math inline">\(\pi^i_{\theta_i}\)</span>، ويتم تدريبها عبر REINFORCE.</p>

      <h3 id="المعضلات-الاجتماعية-ومعضلة-السجين-المتكررة">المُعْضِلات الاِجْتِمَاعِيَّة ومُعْضِلَة السَّجِين المُتَكَرِّرَة</h3>
      <p>في الألعاب ذات المنفعة الجماعية تظهر معضلات اجتماعية عندما يسعى كل وكيل لتعظيم مكافأته الشخصية فيقوّض الناتج الجماعي أو الرفاهية الاجتماعية. المثال الكلاسيكي هو معضلة السجين حيث الاعتراف يؤدي إلى مكافأة فردية أكبر لكنه يقلل المنفعة الكلية.</p>
      <p>في <span class="nodecor">IPD</span> يصبح التعاون المستمر عبر استراتيجية <span class="nodecor">TFT</span> مثمرًا. ومع ذلك، تميل وكلاء MARL المصممة لتعظيم عوائد فردية إلى الانسحاب المطلق رغم القدرة على تعاون أفضل.</p>
    </section>

    <section id="sec:relatedworks">
      <h2>الأعمال ذات الصلة</h2>
      <p>تحاول <span class="nodecor">LOLA</span> تشكيل الخصم عبر التفاضل خطوة واحدة إلى الأمام من معاملات الخصم. وطرحت <span class="nodecor">POLA</span> تحديث سياسة الخصم القريب لتوسيع قدرة LOLA على ألعاب أكثر تعقيدًا. ...</p>
    </section>

    <!-- المزيد من الأقسام تباعًا -->
  </main>
</body>
</html>