<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Mukul Gagrani">
  <meta name="author" content="Raghavv Goel">
  <meta name="author" content="Wonseok Jeon">
  <meta name="author" content="Junyoung Park">
  <meta name="author" content="Mingu Lee">
  <meta name="author" content="Christopher Lott">
  <title>في الترميز التخميني لنماذج اللغة الكبيرة متعددة الوسائط</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    body {
      direction: rtl;
      font-family: 'Cairo', 'Noto Naskh Arabic', 'Amiri', 'Segoe UI', 'Tahoma', 'Arial', sans-serif;
      background: #f8f9fa;
      color: #222;
      font-size: 22px;
      margin: 0;
      padding: 0;
      line-height: 1.7;
    }
    header {
      background: linear-gradient(90deg, #e3e6f3 0%, #f8f9fa 100%);
      padding: 2.5rem 0 1.5rem 0;
      text-align: center;
      border-bottom: 1px solid #e0e0e0;
      margin-bottom: 2rem;
    }
    h1.title {
      font-size: 2.5rem;
      font-weight: 800;
      color: #2d3a4a;
      margin-bottom: 1rem;
      letter-spacing: 0.02em;
    }
    .author {
      display: inline-block;
      margin: 0 0.5rem;
      font-size: 1.1rem;
      color: #4b5d6b;
      font-weight: 500;
    }
    h1, h2, h3, h4 {
      color: #2d3a4a;
      margin-top: 2.2rem;
      margin-bottom: 1rem;
      font-weight: 700;
      letter-spacing: 0.01em;
    }
    h1 {
      font-size: 2rem;
      border-bottom: 2px solid #d1d5db;
      padding-bottom: 0.3rem;
    }
    h2 {
      font-size: 1.5rem;
      border-bottom: 1px solid #e0e0e0;
      padding-bottom: 0.2rem;
    }
    h3 {
      font-size: 1.2rem;
      color: #3b4c5a;
    }
    h4 {
      font-size: 1.1rem;
      color: #4b5d6b;
      margin-top: 1.5rem;
      margin-bottom: 0.7rem;
    }
    p {
      margin: 1.1em 0;
      text-align: justify;
    }
    ol, ul {
      margin: 1em 2em 1em 0;
      padding-right: 1.5em;
    }
    li {
      margin-bottom: 0.5em;
    }
    em {
      color: #5a6d7c;
      font-style: italic;
    }
    strong {
      color: #1a2a38;
      font-weight: 700;
    }
    code, pre {
      background: #f3f6fa;
      color: #b23c3c;
      border-radius: 4px;
      padding: 0.2em 0.4em;
      font-size: 0.95em;
      font-family: 'Fira Mono', 'Consolas', 'Monaco', monospace;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 2em 0;
      background: #fff;
      box-shadow: 0 1px 4px rgba(0,0,0,0.04);
    }
    table caption {
      caption-side: top;
      font-size: 1.1em;
      color: #4b5d6b;
      margin-bottom: 0.5em;
      font-weight: 600;
    }
    th, td {
      border: 1px solid #e0e0e0;
      padding: 0.7em 1em;
      text-align: right;
    }
    th {
      background: #f3f6fa;
      color: #2d3a4a;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f8f9fa;
    }
    tr:nth-child(odd) {
      background: #fff;
    }
    .math.inline, .math.display {
      direction: ltr;
      unicode-bidi: embed;
      font-family: 'Fira Mono', 'Consolas', 'Monaco', monospace;
      font-size: 1em;
      background: none;
      color: #1a2a38;
    }
    .nodecor {
      text-decoration: none !important;
      color: inherit;
    }
    blockquote {
      border-right: 4px solid #d1d5db;
      background: #f3f6fa;
      margin: 1.5em 0;
      padding: 1em 1.5em;
      color: #4b5d6b;
      font-size: 1.05em;
    }
    main {
      max-width: 900px;
      margin: 0 auto 3rem auto;
      padding: 0 1rem;
    }
    @media (max-width: 700px) {
      body {
        font-size: 18px;
        padding: 0 0.5em;
      }
      header {
        padding: 1.2rem 0 1rem 0;
      }
      h1.title {
        font-size: 1.5rem;
      }
      h1, h2 {
        font-size: 1.2rem;
      }
      table, th, td {
        font-size: 0.95em;
      }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">في الترميز التخميني لنماذج اللغة الكبيرة متعددة الوسائط</h1>
  <p class="author"><span class="nodecor">Mukul Gagrani</span></p>
  <p class="author"><span class="nodecor">Raghavv Goel</span></p>
  <p class="author"><span class="nodecor">Wonseok Jeon</span></p>
  <p class="author"><span class="nodecor">Junyoung Park</span></p>
  <p class="author"><span class="nodecor">Mingu Lee</span></p>
  <p class="author"><span class="nodecor">Christopher Lott</span></p>
</header>

<main>
<h1 id="ملخص">مُلخَّص</h1>
<p>الاستدلال باستخدام نماذج اللغة الكبيرة متعددة الوسائط (<span class="nodecor">MLLMs</span>) بطيء، إذ يعاني العمود الفقري اللغوي من عنق زجاجة في عرض النطاق الترددي للذاكرة ويولِّد الرموز بأسلوب ذَاتيّ الانحدار. في هذه الورقة، نستكشف تطبيق الترميز التخميني (<span class="nodecor">Speculative Decoding</span>، ويُختصر <span class="nodecor">SPD</span>) لتعزيز كفاءة الاستدلال في نماذج <span class="nodecor">MLLMs</span>، مع التركيز على نموذج <span class="nodecor">LLaVA-7B</span>. نُظهر أنّ نموذجًا لغويًا نصّيًا فقط يصلح كنموذج مسودّة فعّال للترميز التخميني مع <span class="nodecor">LLaVA-7B</span>، مستغنيًا عن رموز الصورة ومكوّنات معالجتها. تُبيّن تجاربنا عبر ثلاث مهام مختلفة أنّ الترميز التخميني يمكن أن يحقق تسريعًا محدودًا بالذاكرة يصل إلى <span class="nodecor">2.37<span class="math inline">\(\times\)</span></span> عند استخدام نموذج لغوي بعدد معاملات <span class="nodecor">115M</span> قمنا بتدريبه من الصفر. بالإضافة إلى ذلك، نقدّم نموذج مسودّة من نوع <span class="nodecor">LLaVA</span> مدمجًا يتضمّن مُسقِّط الصور، ويُظهر مكاسب طفيفة في توصيف الصور مع الحفاظ على نتائج مماثلة في المهام الأخرى.</p>

<h1 id="مقدمة">مقدمة</h1>
<p>أصبحت نماذج اللغة الكبيرة (Large Language Models) شائعة الاستخدام بفضل أدائها المميّز. ومع ذلك، تُقيَّد هذه النماذج غالبًا بمدخلات نصّية فحسب، في حين أنّ بيانات العالم الحقيقي متعدّدة الوسائط وتضمّ معلومات بصريّة. توفّر نماذج اللغة الكبيرة متعددة الوسائط (<span class="nodecor">MLLMs</span>) (<span class="nodecor">awadalla2023openflamingo</span>, <span class="nodecor">liu2024visual</span>, <span class="nodecor">tsimpoukelli2021multimodal</span>, <span class="nodecor">zhu2023minigpt</span>) قدرات لفهم الصور عبر دمج الرموز البصريّة والنصّية لتفاعل أكثر فائدة مع المستخدمين. تتكوّن هذه النماذج من مُشفِّر صور لمعالجة معلومات الصورة، ومُسقِّط يحوّل ترميزات الصورة إلى فضاء تضمين نموذج اللغة، إضافةً إلى العمود الفقري اللغوي الذي يرث منه التوليد الذاتيّ الانحدار وعنق الزجاجة في عرض النطاق الترددي للذاكرة، ممّا يُبطِّئ الاستدلال (<span class="nodecor">shazeer2019fast</span>).</p>
<p>اقتُرح الترميز التخميني (<span class="nodecor">speculative decoding</span>) (<span class="nodecor">leviathan2023fast</span>, <span class="nodecor">chen2023accelerating</span>, <span class="nodecor">sun2023spectr</span>, <span class="nodecor">miao2023specinfer</span>, <span class="nodecor">jeon2024recursive</span>) بوصفه وسيلة لتسريع الاستدلال في نماذج اللغة الكبيرة دون التضحية بالدقة؛ إذ يتنبّأ نموذجٌ مسودّة أصغر بعدّة رموز مستقبلية تُتحقَّق في استدعاءٍ واحد للنموذج الهدف الكبير. وبما أنّ نماذج اللغة الكبيرة متعددة الوسائط تعتمد على نموذج لغوي كبير كعمود فقري، يمكن تطبيق الترميز التخميني لجعل استدلالها أكثر كفاءة. تناولت أعمالٌ حديثة استخدام الترميز التخميني ومتغيّراته (<span class="nodecor">kim2023big</span>, <span class="nodecor">fu2023lookahead</span>, <span class="nodecor">medusa</span>, <span class="nodecor">santilli2023accelerating</span>, <span class="nodecor">sun2023spectr</span>, <span class="nodecor">jeon2024recursive</span>) لنماذج اللغة الكبيرة، لكن لا نعلم بدراساتٍ سابقة في سياق نماذج اللغة الكبيرة متعددة الوسائط.</p>
<p>في هذه الورقة، نُطبّق الترميز التخميني على نموذج <span class="nodecor">LLaVA-7B</span> (المعتمد على <span class="nodecor">LLaMA-7B</span> كعمود فقري لغوي) لتسريع الاستدلال. ونظرًا لغياب نماذج أصغر مُعلَنة من عائلتَي <span class="nodecor">LLaVA</span> و<span class="nodecor">LLaMA</span> دون <span class="nodecor">7B</span> مُعامِلًا، درّبنا نموذجًا لغويًا من الصفر بحجم <span class="nodecor">115M</span> لاستخدامه كنموذج مسودّة. نُظهر أنّ نموذجًا لغويًا لا يأخذ رموز الصورة في الحسبان (وبالتالي لا يحتاج إلى مُشفِّر الصور أو المُسقِّط) يمكن أن يكون نموذج مسودّة جيّدًا لـ<span class="nodecor">LLaVA-7B</span>. أجرينا تجارب على ثلاث مهام تشمل أسئلة وأجوبة على صور من مجموعة LLaVA Instruct 150K (<span class="nodecor">liu2024visual</span>)، وتوليد أوصاف لصور من مجموعة COCO (<span class="nodecor">lin2014microsoft</span>)، ومجموعة ScienceQA (<span class="nodecor">lu2022learn</span>)، باستخدام نماذج مسودّة خضعت لمستويات مختلفة من التدريب والضبط الدقيق. تُظهر نتائجنا إمكان تحقيق تسريعٍ محدود بالذاكرة يصل إلى <span class="nodecor">2.37<span class="math inline">\(\times\)</span></span> باستخدام نموذجٍ لغويّ فقط كنموذج مسودّة. كما أنشأنا نموذج مسودّة مدمجًا من نوع <span class="nodecor">LLaVA</span> يضمّ مُسقِّط صور إلى جانب النموذج اللغوي المدرَّب، وقد أظهر تحسّنًا طفيفًا في مهمّتَي توصيف COCO وScienceQA مع أداءٍ مماثل في بقيّة المهام.</p>

<h1 id="الطريقة">الطريقة</h1>
<h2 id="الخلفية">الخلفية</h2>
<h3 id="الترميز-التخميني">الترميز التخميني</h3>
<p>يتضمّن الترميز التخميني (<span class="nodecor">Speculative Decoding</span>) (<span class="nodecor">chen2023accelerating</span>, <span class="nodecor">leviathan2023fast</span>) استخدام نموذج مسودّة أصغر لتوليد عدّة رموز تُتحقَّق بالتوازي بواسطة النموذج اللغوي الكبير الهدف. بالاعتماد على سياق الإدخال <span class="math inline">\(X_{1:n}:=[X_{1}, \dots, X_{n}]\)</span>، يُولِّد نموذج المسودّة تسلسلًا من الرموز <span class="math inline">\(\hat{X}_{n+1:n+L}\)</span> بأسلوب ذَاتيّ الانحدار: <span class="math inline">\(\hat{X}_{n+j} \sim p(\cdot \mid X_{1:n}, \hat{X}_{n+1:n+j-1})\)</span>. ثم تُتحقَّق هذه الرموز في استدعاءٍ واحد للنموذج الهدف (<span class="math inline">\(q\)</span>) باستخدام آلية أخذ العينات بالرفض لضمان مطابقة التوزيع الأصلي. على وجه التحديد، يُقبَل الرمز <span class="math inline">\(\hat{X}_{n+j}\)</span> بالاحتمالية 
<span class="math display">\[
\min\left\{1, \frac{q(\hat{X}_{n+j}\mid X_{1:n}, \hat{X}_{n+1:n+j-1})}{p(\hat{X}_{n+j}\mid X_{1:n}, \hat{X}_{n+1:n+j-1})}\right\}.
\]</span>
إذا رُفِض رمزُ المسودّة <span class="math inline">\(\hat{X}_{n+j}\)</span>، تُؤخَذ عيّنة جديدة من التوزيع المتبقّي <span class="math inline">\(p_{\mathrm{res}}(x)=\max(0, q(x) - p(x))\)</span>.</p>
<h3 id="نماذج-اللغة-الكبيرة-متعددة-الوسائط">نماذج اللغة الكبيرة متعددة الوسائط</h3>
<p>يتكوّن نموذج اللغة الكبير متعدد الوسائط المعتمد على الصور من: 1) مُشفِّر رؤية لتضمين الصورة المدخلة، 2) مُسقِّط لتحويل ترميزات الصورة إلى تضمينات نموذج اللغة، و3) العمود الفقري لنموذج اللغة. نصف إطار عمل <span class="nodecor">LLaVA</span> بالتفصيل؛ فبالنظر إلى صورة مُدخلة <span class="math inline">\(I\)</span> واستعلام نصّي <span class="math inline">\(Q\)</span>، تُحوَّل الصورة إلى تسلسل من الترميزات <span class="math inline">\(H_1, H_2, \ldots, H_m\)</span>، ويُحوَّل الاستعلام النصّي إلى تسلسلٍ من تضمينات الرموز <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>. ثم يُحوِّل المُسقِّط <span class="math inline">\(g_\theta\)</span> هذا التسلسل إلى تضميناتٍ صوريّة <span class="math inline">\(V_i = g_\theta(H_i)\)</span> في فضاء نموذج اللغة. أخيرًا، يُولِّد نموذج اللغة الرموز التالية بالاعتماد على تضمينات الصورة والنص كما في: 
<span class="math display">\[
X_{n+1} \sim q(\cdot \mid V_{1:m}, X_{1:n})
\]</span>
</p>

<h1 id="تحليل-spd-لنماذج-mllm">تحليل الترميز التخميني (SPD) لنماذج MLLM</h1>
<p>لتحقيق مكاسب كبيرة بالترميز التخميني، نحتاج إلى نموذج مسودّة أصغر بكثير ومتوافق جيّدًا مع نموذجنا الهدف (<span class="nodecor">LLaVA-7B</span>). الخيار الشائع في الأدبيات هو استخدام نموذج مسودّة مدرَّب مسبقًا من العائلة نفسها، أو تدريب نموذج أصغر بالبنية عينها للهدف (<span class="nodecor">miao2023specinfer</span>). وبما أنّه لا يتوافر علنًا نموذجٌ أصغر من عائلة <span class="nodecor">LLaVA</span>، درّبنا نموذج مسودّة من الصفر. اخترنا بُنيتين بديلتين تُحاكيان هيكل <span class="nodecor">LLaVA</span>: 1) مسودّة مكوّنة من مُسقِّط صور أصغر مع نموذج اللغة للمسودّة، أو 2) مسودّة نصّية فقط تُولِّد الرموز اعتمادًا على النص وحده. بالاعتماد على تضمينات الصورة <span class="math inline">\(V_{1:m}\)</span> وتضمينات النص <span class="math inline">\(X_{1:n}\)</span>، يُولِّد نموذج المسودّة تسلسل الرموز <span class="math inline">\(\hat{X}_{n+1:n+L}\)</span> حيث 
<span class="math display">\[
\hat{X}_{n+j} \sim p(\cdot \mid X_{1:n}, \hat{X}_{n+1:n+j-1}).
\]</span>
يتحقّق نموذج الهدف <span class="nodecor">LLaVA</span> من هذه الرموز اعتمادًا على تضمينات الصورة والنص باحتمالية 
<span class="math display">\[
\min\left\{1, \frac{q(\hat{X}_{n+j}\mid V_{1:m}, X_{1:n}, \hat{X}_{n+1:n+j-1})}{p(\hat{X}_{n+j}\mid X_{1:n}, \hat{X}_{n+1:n+j-1})}\right\}.
\]</span>
تُعدّ المسودّة النصّية فقط أكثر كفاءةً لأنّها: 1) لا تحتاج إلى مُسقِّط إضافي لاستيعاب تضمينات الصورة، و2) لا تتطلّب تدريب ذلك المُسقِّط.</p>

<h1 id="التجارب">التجارب</h1>
<p>نجري التجارب على ثلاث مهام للتوجيه البصري باستخدام <span class="nodecor">SPD</span> مع نموذج <span class="nodecor">LLaVA-7B</span> (<span class="nodecor">liu2023improved</span>) الهدف، المعتمد على <span class="nodecor">LLaMA-7B</span> كنموذجٍ لغوي. جميع نماذج المسودّة تمتلك حجمًا ثابتًا لجزئها اللغوي قدره <span class="math inline">\(115\mathrm{M}\)</span>.</p>
<h4 id="نماذج-المسودّة-المرشّحة">نماذج المسودّة المرشّحة:</h4>
<p>درّبنا نموذج مسودّة بحجم <span class="math inline">\(115\mathrm{M}\)</span> وفق هيكلية <span class="nodecor">LLaMA-2</span> من الصفر، ثمّ أجرينا ضبطًا دقيقًا للمسودّة على بيانات التعليمات بخسارة <span class="nodecor">TVD++</span> (<span class="nodecor">goel2024direct</span>) وعلى مجموعة فرعية من <span class="nodecor">LLaVA Instruct 150K</span> (<span class="nodecor">liu2024visual</span>). ندرس المراحل التالية:</p>
<ol>
<li><em>LLaMA الأساسي</em>: نموذج <span class="nodecor">LLaMA</span> مُدرَّب مُسبقًا على <span class="math inline">\(600\mathrm{B}\)</span> رمز إنجليزي.</li>
<li><em>LLaMA للدردشة</em>: ضبطٌ دقيق موجَّه بالتعليمات انطلاقًا من نموذج <em>LLaMA الأساسي</em> (<span class="nodecor">goel2024direct</span>).</li>
<li><em>LLaVA المضبوط</em> (<span class="nodecor">ft-llava</span>): ضبطٌ دقيق كامل مع تهيئة مُسقِّط الصور من <span class="nodecor">LLaVA-7B</span> باستخدام تقنية تقسيم الأوزان (<span class="nodecor">samragh2023weight</span>)، وجزء لغوي مُهيَّأ من <em>LLaMA للدردشة</em>.</li>
<li><em>LLaVA (نصي فقط) مضبوط</em> (<span class="nodecor">ft-llava-text</span>): يستخدم الجزء اللغوي فقط من نموذج <em>ft-llava</em>.</li>
</ol>
<p>عندما تعتمدُ المسودّة على الصورة، نُشرك مُشفِّر الرؤية (<span class="nodecor">CLIP ViT-L/14</span>) مع النموذج الهدف لتجنّب إعادة حساب التضمينات. تفاصيل الإعدادات في الملحق [app:model_config].</p>
<h4 id="مهام-التقييم">مهام التقييم:</h4>
<p>نركّز على توليد نصّ مفتوح النهايات والإجابة متعدّدة الخيارات مع التفكير المتسلسل (<span class="nodecor">CoT</span>) لزيادة أطوال التوليد. نقوم بالتقييم على: 1) <span class="nodecor">LLaVA Instruct 150K</span> (<span class="nodecor">liu2024visual</span>)، 2) توصيف <span class="nodecor">COCO</span> (<span class="nodecor">lin2014microsoft</span>)، و3) <span class="nodecor">ScienceQA</span> مع <span class="nodecor">CoT</span> (<span class="nodecor">lu2022learn</span>). إعدادات المطالبات في الملحق [app:sys_prompts].</p>
<h4 id="المقاييس">المقاييس:</h4>
<p>نقيس فاعلية <span class="nodecor">SPD</span> عبر: 1) <strong>كفاءة الكُتلة</strong> (<span class="math inline">\(\tau\)</span>): متوسّط عدد الرموز المقبولة لكل استدعاء للنموذج الهدف عند طول مسودّة <span class="math inline">\(\gamma\)</span>. 2) <strong>التسريع المحدود بالذاكرة</strong> (<span class="nodecor">MBSU</span>): <span class="math inline">\(\mathrm{MBSU}(x)=\frac{c\,\tau(x)}{c\,\gamma+1}\)</span> حيث <span class="math inline">\(c\)</span> نسبة تكلفة معاملات نموذج المسودّة إلى النموذج الهدف. 3) <strong>معدّل الرموز</strong>: إجمالي عدد الرموز المُولَّدة مقسومًا على زمن التوليد (رمز/ثانية). نجري القياسات عند <span class="math inline">\(\gamma\in\{3,5\}\)</span>.</p>
<h4 id="فك-الترميز">فكّ الترميز:</h4>
<p>نستخدم فكّ الترميز الجشِع في جميع التجارب، بحيث يُطابق التوليدُ التوليدَ الذاتيّ الانحدار للنموذج الهدف. نترك استكشاف فكّ الترميز القائم على العيّنة (تغيير درجة الحرارة، <span class="nodecor">top-<span class="math inline">\(p\)</span></span>، <span class="nodecor">top-<span class="math inline">\(k\)</span></span>) عملًا مستقبليًا.</p>
<h4 id="النتائج">النتائج:</h4>
<p>تُظهر نتائجنا أنّ <span class="nodecor">SPD</span> مع نموذج الهدف <span class="nodecor">LLaVA-7B</span> يوفّر تسريعًا ملحوظًا في التوليد. وعند استخدام مسودّة نصّية فقط، يقدّم <span class="nodecor">SPD</span> تسريعًا تنافسيًا مقارنةً بمسودّةٍ تستفيد من معلومات الصورة.</p>
<p>من الشكل [fig:result] (الأعلى والوسط)، نرى أنّ <span class="nodecor">SPD</span> يحقّق مكاسب تتجاوز <span class="nodecor">2<span class="math inline">\(\times\)</span></span> من حيث كفاءة الكُتلة و<span class="nodecor">MBSU</span>. يميل الأداء إلى الارتفاع عند زيادة طول المسودّة من <span class="nodecor">3</span> إلى <span class="nodecor">5</span> في جميع المهام، باستثناء <span class="nodecor">SQA</span> حيث يتفوّق نموذج المسودّة <em>base-llama</em> عند <span class="math inline">\(\gamma=5\)</span>. في تقييم <span class="nodecor">LLaVA</span>، يتصدّر <em>ft-llava-text</em> ثم <em>ft-llava</em> لكلا الطولين. في <span class="nodecor">COCO</span>، يتصدّر <em>ft-llava</em> ثم <em>ft-llava-text</em>. في <span class="nodecor">SQA</span>، عند <span class="math inline">\(\gamma=3\)</span> يتفوّق <em>ft-llava</em> ثم <em>ft-llava-text</em>، وعند <span class="math inline">\(\gamma=5\)</span> يتفوّق <em>ft-llava</em> ثم <em>base-llama</em>. كما حسّنت جميع نماذج المسودّة معدّل الرموز مقارنةً بالتوليد الذاتيّ الانحدار الخالص، وكان <span class="math inline">\(\gamma=3\)</span> أفضل من <span class="math inline">\(\gamma=5\)</span> من حيث الرموز في الثانية.</p>
<p>نعرض أيضًا نتائج نوعيّة لتوليد تعليقات <span class="nodecor">COCO</span> باستخدام المسودّة <em>ft-llava-text</em> في الشكل [fig:qualitative_example]، حيث تُبرز الرموز المقبولة باللون الأزرق مع تسطير. نرى أنّ المسودّة تتنبّأ بكلمات شائعة وإكمالاتٍ دون معلوماتٍ بصريّة؛ مثلًا تتنبّأ بـ “tables” انطلاقًا من “vege”، وفي المثال الثاني من “app” تتنبّأ بـ “liances”. عمومًا، يحتوي التوليد المفتوح على العديد من الرموز الشائعة والإكمالات التي لا تتطلّب تضمينات بصريّة، لذا يُقدّم نموذج المسودّة النصّي أداءً تنافسيًا. ويمكنه أيضًا تكرار الرموز بعد توليدها — مثل “counter” و“bowls” في المثال الثاني. نترك ضبطًا أدقّ لنموذجٍ صغيرٍ متعدد الوسائط عملًا مستقبليًا.</p>
<p>استنادًا إلى نوع الرموز المقبولة، افترضنا أنّ نموذج <span class="nodecor">LLaVA</span> الأوّلي قد لا يستخدم معلومات الرؤية بالكامل (أي لا يُحسِّن كفاءة الكُتلة)، ربّما لأن مُسقِّط الصور الأوّلي لا يُرمِّز الرموز الصوريّة على نحوٍ كافٍ. لذلك جرّبنا <span class="nodecor">SPD</span> مع مسودّات أوليّة لا تستخدم رموز الصورة لرصد أثر المُسقِّط المدرَّب في تحسين كفاءة الكُتلة أو <span class="nodecor">MBSU</span>. ومن الشكل <span class="nodecor">fig:avg_token_all</span> و<span class="nodecor">fig:mbsu_all</span> نرى أن <em>LLaVA (نصي فقط) مضبوط</em> والنسخة الكاملة يقدّمان أداءً متقاربًا، ما يدعم فرضيّتنا تجريبيًا.</p>
<p>أضفنا أيضًا مسودّاتٍ نصّية أخرى لرصد التسريع باستخدام النص فقط. ولدهشتنا، أظهرت نماذج <span class="nodecor">LLaMA</span> الأساسية ونسخة الدردشة تسريعًا يزيد عن <span class="nodecor">2</span> مرّات في المتوسّط (<span class="nodecor">fig:mbsu_all</span>).</p>
<p>أداء <span class="nodecor">SPD</span> مع مسودّة نصّية قريب نسبيًا من مسودّة <span class="nodecor">LLaVA</span>، ويتفوّق أحيانًا في <span class="nodecor">ScienceQA</span>. وبناءً على ذلك، حلّلنا متوسّط قبول الرموز عبر النماذج المختلفة ودرجة الانتباه المُخصَّصة لرموز الصورة في النموذج الهدف <span class="nodecor">LLaVA</span> (انظر الملحق).</p>

<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>

<h1 id="الخلاصة">الخلاصة</h1>
<p>قدّمنا في هذه الورقة خطوةً أولى نحو استخدام الترميز التخميني لتسريع الاستدلال في نماذج اللغة الكبيرة متعددة الوسائط، مع التركيز على الصور والنصوص. وأظهرنا أنّ مسودّة نصّية فقط تُحقّق أداءً تنافسيًا مقارنةً بمسودّةٍ تستفيد من ميزات الصورة. أجرينا تجارب على مهام توليد نصّ مفتوح ومهام تفكيرٍ متسلسل باستخدام مسودّاتٍ نصّية ونص–صورة، وحقّقنا تسريعًا يصل إلى <span class="nodecor"><span class="math inline">\(2.37\times\)</span></span> للمسودّة النصّية وتسريعًا أعلى قليلًا للمسودّة النص–صورة، ما يُظهر تجريبيًا فعّالية الترميز التخميني في <span class="nodecor">MLLMs</span>.</p>
<p>يفتح عملُنا مساراتٍ مستقبليّة متعدّدة ضمن الإطار المُقدَّم. يمكن توسيعه ليشمل نماذج أخرى مثل (<span class="nodecor">li2023blip</span>)، (<span class="nodecor">zhu2023minigpt</span>)، (<span class="nodecor">awadalla2023openflamingo</span>)، وكذلك وسائط أخرى كالصوت (<span class="nodecor">chu2023qwen</span>) التي تعاني القيود نفسها للتوليد الذاتيّ الانحدار. علاوةً على ذلك، يمكن تبنّي أساليب ترميزٍ تخميني قائمة على الشجرة (<span class="nodecor">sun2023spectr</span>, <span class="nodecor">miao2023specinfer</span>, <span class="nodecor">medusa</span>, <span class="nodecor">jeon2024recursive</span>) لزيادة سرعة التوليد أكثر.</p>

<h1 id="الملحق">الملحق</h1>
<h2 id="app:model_config">تكوينات النموذج</h2>
<p>يستخدم نموذج <span class="nodecor">LLaVA-7B</span>: (i) مُشفِّر الرؤية، (ii) مُسقِّط/مُحوِّل الصور المبني على شبكة عصبية متعددة الطبقات، و(iii) نموذج اللغة <span class="nodecor">LLaMA-7B</span>. المُشفِّر البصري هو <span class="nodecor">CLIP ViT-L/14</span> مع تفاصيل في (<span class="nodecor">radford2021learning</span>)، ومُسقِّط الصور يحتوي على طبقتين خطّيتين بأبعاد <span class="math inline">\(1024\times4096\)</span> و<span class="math inline">\(4096\times4096\)</span>. أمّا في سيناريو المسودّات مع مُسقِّط صور، فتكون الأبعاد <span class="math inline">\(1024\times1024\)</span> و<span class="math inline">\(1024\times1024\)</span>.</p>
<p>تكوينات الجزء اللغوي في النموذج الهدف والمسودّة (كِلاهما من عائلة <span class="nodecor">LLaMA</span>) كما يلي:</p>
<table>
<caption>تكوينات النموذج: المسودّة والهدف</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">الهدف (7B)</th>
<th style="text-align: right;">المسودّة (115M)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">الطبقات</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">رؤوس الانتباه</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">البعد الوسيط</td>
<td style="text-align: right;"><span class="nodecor">11,008</span></td>
<td style="text-align: right;"><span class="nodecor">2,816</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">البعد الخفي</td>
<td style="text-align: right;"><span class="nodecor">2,048</span></td>
<td style="text-align: right;"><span class="nodecor">1,024</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">دالّة التنشيط</td>
<td style="text-align: right;">SiLU</td>
<td style="text-align: right;">SiLU</td>
</tr>
</tbody>
</table>
<p>[tab:model_config]</p>

<h2 id="app:sys_prompts">مطالبات النظام</h2>
<p>نستخدم المطالبات التالية لكلّ مهمّة. يُستخدم الرمز <em><span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span></em> لتمثيل بيانات الصورة.</p>
<p><strong>تقييم LLaVA.</strong> نتبع منهجيّة (<span class="nodecor">liu2024visual</span>)، حيث يعرض المساعد أسئلةً وإجاباتٍ متعدّدة.</p>
<p><em><span class="math inline">\(&lt;\)</span>s<span class="math inline">\(&gt;\)</span> دردشة بين مستخدمٍ فضولي ومساعد ذكاء اصطناعي. يقدّم المساعد إجاباتٍ مفصّلة ومهذّبة. المستخدم: <span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span>  \\ السؤال <span class="math inline">\(Q_{1}\)</span>  المساعد: <span class="math inline">\(R_{1}\)</span>. المستخدم: السؤال <span class="math inline">\(Q_{2}\)</span> …</em></p>
<p><strong>توصيف COCO.</strong> بما أنّ <span class="nodecor">COCO</span> لا يتضمّن أسئلة، استخدمنا مطلبًا شبيهًا بالسابق:</p>
<p><em><span class="math inline">\(&lt;\)</span>s<span class="math inline">\(&gt;\)</span> دردشة بين مستخدمٍ فضولي ومساعد ذكاء اصطناعي. يقدّم المساعد إجاباتٍ مفيدة ومفصّلة. المستخدم: <span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span>  \\ قدّم توصيفًا مفصّلًا للصورة  المساعد:</em></p>
<p><strong>أسئلة العلوم.</strong> نتبع (<span class="nodecor">lu2022learn</span>) مع مثالٍ واحد للسؤال والخيارات والإجابة والتعليل لتمكين التفكير المتسلسل. نستخدم عينات الاختبار المرتبطة بصورة.</p>
<p>
<span class="math display">\[
\begin{aligned}
    & \text{السؤال: } I_{i}^{ques} \\
    & \text{الخيارات: (0) } I_{i1}^{opt} \,(1)\, I_{i2}^{opt} \,(2)\, I_{i3}^{opt} \\
    & \text{السياق: } I_{i}^{cont} \\
    & \text{الإجابة: } I_{i}^{ans} \text{، لأن: } I_{i}^{lect} \text{. التفسير: } I_{i}^{exp} \\
    & \langle\text{image}\rangle \\
    & \text{السؤال: } I_{test}^{ques} \\
    & \text{الخيارات: (0) } I_{test,1}^{opt} \,(1)\, I_{test,2}^{opt} \,(2)\, I_{test,3}^{opt} \\
    & \text{السياق: } I_{test}^{cont} \\
    & \text{الإجابة:}
\end{aligned}
\]</span>
</p>
<p>يشير <span class="math inline">\(i\)</span> إلى العيّنة داخل السياق. في <span class="nodecor">SQA</span>، وُفِّر حقل <em>السياق</em> عبر تسمية صور مولَّدة تلقائيًا، لكنها كانت بسيطة؛ لذا استخدمنا حقل “التلميح” من البيانات. مثال السياق لا يتضمّن صورةً متعدّدة لتجنّب تعقيد الاستهداف. نترك <span class="nodecor">SPD</span> مع أكثر من مثالٍ في السياق عملًا مستقبليًا.</p>

<h2 id="درجة-الانتباه-لرموز-الصورة">درجة الانتباه لرموز الصورة</h2>
</main>
</body>
</html>