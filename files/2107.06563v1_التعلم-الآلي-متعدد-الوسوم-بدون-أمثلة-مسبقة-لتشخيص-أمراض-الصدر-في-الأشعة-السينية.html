<!DOCTYPE html>
<html lang="ar">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>التعلُّم الآلي مُتعدِّد الوسوم بدون أمثلة مُسبقة لتشخيص أمراض الصدر في الأشعّة السينيّة</title>
    <!-- MathJax for LaTeX math rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true,
                packages: {'[+]': ['ams', 'amssymb', 'amsmath', 'amsthm', 'newcommand', 'boldsymbol']}
            },
            options: {
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            font-family: 'Times New Roman', serif;
            line-height: 1.7;
            color: #333;
            background-color: #fafafa;
            direction: rtl;
        }
        .container {
            max-width: 900px;
            background-color: #fff;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            padding: 40px;
            margin: 20px auto;
            border-radius: 6px;
        }
        h1 {
            text-align: center;
            font-size: 2.2em;
            margin-bottom: 30px;
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 15px;
        }
        h2 {
            color: #34495e;
            font-size: 1.5em;
            margin-top: 35px;
            margin-bottom: 15px;
            border-right: 4px solid #3498db;
            padding-right: 10px;
        }
        h3 {
            color: #5d6d7e;
            font-size: 1.25em;
            margin-top: 25px;
            margin-bottom: 12px;
        }
        p {
            text-align: justify;
            margin-bottom: 15px;
        }
        .abstract {
            background-color: #f8f9fa;
            border-right: 4px solid #007bff;
            padding: 20px;
            margin: 30px 0;
            font-style: italic;
            border-radius: 6px;
        }
        .keywords {
            background-color: #e9ecef;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .equation {
            text-align: center;
            margin: 20px 0;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 5px;
        }
        figure.figure, .figure {
            text-align: center;
            margin: 30px 0;
            padding: 20px;
            background-color: #f8f9fa;
            border-radius: 5px;
            border: 1px solid #dee2e6;
        }
        .table {
            margin: 25px 0;
        }
        .reference {
            font-size: 0.95em;
            margin-bottom: 8px;
        }
        .meta-info {
            background-color: #e7f3ff;
            padding: 15px;
            margin-bottom: 30px;
            border-radius: 5px;
            font-size: 0.9em;
            direction: ltr;
            text-align: left;
        }
        code {
            background-color: #f8f9fa;
            padding: 2px 4px;
            border-radius: 3px;
        }
        pre {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        blockquote {
            border-right: 4px solid #6c757d;
            padding-right: 20px;
            font-style: italic;
            color: #6c757d;
        }
        .theorem, .lemma, .proposition, .corollary {
            background-color: #fff3cd;
            border: 1px solid #ffeaa7;
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
        }
        .proof {
            border-right: 2px solid #28a745;
            padding-right: 15px;
            margin: 15px 0;
        }
        nav#TOC ul { list-style: none; padding-right: 0; }
        nav#TOC li { margin: 6px 0; }
        nav#TOC a { text-decoration: none; color: #2c3e50; }
        nav#TOC a:hover { text-decoration: underline; }
        .author { text-align: center; color: #555; }
    </style>
</head>
<body>
    <div class="container">
        <div class="meta-info">
            <strong>ArXiv ID:</strong> 2107.06563v1<br>
            <strong>LaTeX الأصلي:</strong> <code>./nyuad_arxiv_papers/nyuad_papers_comprehensive/source_code/2107.06563v1_extracted/full-paper-template.tex</code><br>
            <strong>تمّ التحويل:</strong> 2025-06-06 13:11:45
        </div>
        <header id="title-block-header">
            <h1 class="title">التعلُّم الآلي مُتعدِّد الوسوم بدون أمثلة مُسبقة لتشخيص أمراض الصدر في الأشعّة السينيّة</h1>
            <p class="author">
                قسم الهندسة<br>
                جامعة نيويورك أبوظبي، أبوظبي، الإمارات العربيّة المتّحدة<br>
                قسم الهندسة<br>
                جامعة نيويورك أبوظبي، أبوظبي، الإمارات العربيّة المتّحدة<br>
                قسم الهندسة<br>
                جامعة نيويورك أبوظبي، أبوظبي، الإمارات العربيّة المتّحدة
            </p>
            <div class="abstract">
                <div class="abstract-title"><strong>الملخّص</strong></div>
                <p>على الرغم من نجاح الشبكات العصبيّة العميقة في تشخيص صور الأشعّة السينيّة للصدر (CXR)، فإن التعلُّم الخاضع للإشراف لا يتيح إلا توقّع فئات الأمراض التي شوهدت أثناء التدريب. عند الاستدلال، لا يمكن لهذه الشبكات التنبؤ بفئة مرض غير مرئيّة سابقًا. إن إدراج فئة جديدة يتطلّب جمع بيانات مُوسومة، وهو أمر غير يسير، لا سيّما للأمراض النادرة. ونتيجة لذلك، يصبح من غير الممكن بناء نموذج قادر على تشخيص جميع فئات الأمراض المحتملة. في هذا العمل، نقترح شبكة تعلُّم بدون أمثلة مُسبقة مُعمَّمة ومُتعدِّدة الوسوم (CXR-ML-GZSL) قادرة على التنبؤ المتزامن بعدّة أمراض مرئيّة وغير مرئيّة في صور الأشعّة السينيّة للصدر. عند إدخال صورة، تتعلّم الشبكة تمثيلًا بصريًّا مُوجَّهًا بدلالات مُستخرجة من نصوص طبّية غنيّة. لتحقيق هذا الهدف، نقترح إسقاط كلٍّ من التمثيلات البصريّة والدلاليّة إلى فضاء كامن مشترك باستخدام هدف تعلُّم مُبتكر. يضمن هذا الهدف أن: (1) تُرتَّب الوسوم الأكثر صِلة بالصورة أعلى من الوسوم غير ذات الصِّلة، (2) يتعلّم النموذج تمثيلًا بصريًّا مُتوافقًا مع دلالاته في الفضاء الكامن، و(3) تحافظ الدلالات المُسقَّطة على علاقاتها المتبادلة الأصليّة بين الفئات. الشبكة قابلة للتدريب من البداية إلى النهاية ولا تتطلّب تدريبًا مُسبقًا مستقلًّا للمُشفر البصري. أظهرت التجارب على مجموعة بيانات NIH للأشعّة السينيّة للصدر أنّ شبكتنا تتفوّق على نموذجين أساسيّين قويّين من حيث الاسترجاع والدقّة ودرجة F1 ومساحة تحت منحنى ROC. الشيفرة مُتاحة على: <a href="https://github.com/nyuad-cai/CXR-ML-GZSL.git" class="uri">https://github.com/nyuad-cai/CXR-ML-GZSL.git</a></p>
            </div>
        </header>

        <nav id="TOC" role="doc-toc" aria-label="جدول المحتويات">
            <ul>
                <li><a href="#introduction">المقدّمة</a></li>
                <li><a href="#generalizable-insights-about-ml-in-the-context-of-healthcare">رؤى عامة حول التعلُّم الآلي في سياق الرعاية الصحيّة</a></li>
                <li><a href="#sec:related-work">الأعمال ذات الصِّلة</a>
                    <ul>
                        <li><a href="#inductive-transductive-zsl">التعلُّم بدون أمثلة مُسبقة: الاستقرائي والانتقالي</a></li>
                        <li><a href="#multi-label-generalized-zero-shot-learning">التعلُّم بدون أمثلة مُسبقة مُعمَّم ومُتعدِّد الوسوم</a></li>
                        <li><a href="#deep-learning-for-chest-radiographs">التعلُّم العميق لصور الأشعّة السينيّة للصدر</a></li>
                    </ul>
                </li>
                <li><a href="#sec:methodology">المنهجيّة</a>
                    <ul>
                        <li><a href="#problem-formulation">صياغة المشكلة</a></li>
                        <li><a href="#network-architecture">معماريّة الشبكة</a></li>
                        <li><a href="#mathcall_rank-ranking-loss-of-relevance-scores">خسارة الترتيب لدرجات الصِّلة <span class="math inline">(\mathcal{L}_{rank})</span></a></li>
                        <li><a href="#mathcall_align-alignment-loss-of-visual-semantic-representations">خسارة المُحاذاة بين التمثيلات البصريّة والدلاليّة <span class="math inline">(\mathcal{L}_{align})</span></a></li>
                        <li><a href="#mathcall_con-semantics-inter-class-consistency-regularizer">مُنظِّم الاتّساق بين الفئات الدلاليّة <span class="math inline">(\mathcal{L}_{con})</span></a></li>
                        <li><a href="#inference">الاستدلال</a></li>
                    </ul>
                </li>
                <li><a href="#sec:experiments">الإعدادات التجريبية</a>
                    <ul>
                        <li><a href="#dataset">مجموعة البيانات</a></li>
                        <li><a href="#model-training-and-selection">تدريب النموذج واختياره</a></li>
                        <li><a href="#performance-metrics">مقاييس الأداء</a></li>
                    </ul>
                </li>
                <li><a href="#sec:results">النتائج</a>
                    <ul>
                        <li><a href="#comparison-to-baseline-models">المقارنة مع النماذج الأساسيّة</a></li>
                        <li><a href="#ablation-studies">دراسات الإلغاءِ الجزئي (Ablation)</a></li>
                    </ul>
                </li>
                <li><a href="#sec:discussion">المناقشة</a>
                    <ul>
                        <li><a href="#limitations">القيود</a></li>
                    </ul>
                </li>
                <li><a href="#conclusion">الخُلاصة</a></li>
            </ul>
        </nav>

        <section id="introduction" class="level1">
            <h1>المقدّمة</h1>
            <p>أتاح التعلُّم العميق تطوير أنظمة تشخيصيّة مُعتمدة على الحاسوب قادرة على تصنيف الأمراض في الصور الطبيّة بدقّة تقارب مستوى الإنسان <span class="citation" data-cites="Qin2018ComputeraidedDI chexnext chestemerg"></span>. من أبرز القيود حاجتها إلى كميّات كبيرة من البيانات المُوسومة للتدريب، وهو أمر يتطلّب جهدًا وخبرة عالية ويُعدّ مُكلفًا. تتزايد الصعوبة عند محاولة جمع بيانات كافية للأمراض النادرة أو للأوبئة الناشئة مثل كوفيد-19 <span class="citation" data-cites="covidzsl"></span>. لذا، يصبح من غير العملي توفير بيانات تدريبيّة مُوسومة لجميع الأمراض الممكنة لتدريب شبكة تعلُّم عميق، وبسبب هذا القيد لا تستطيع الشبكة تصنيف فئات الأمراض غير المرئيّة أثناء التدريب <span class="citation" data-cites="zsl"></span>. في المقابل، يستطيع أطبّاء الأشعّة التعرّف على أمراض جديدة بالاعتماد على معرفتهم بسمات الأمراض المُستقاة من الأدبيّات الطبيّة.</p>
            <p>يمتلك <em>التعلُّم بدون أمثلة مُسبقة</em> (ZSL) القدرة على محاكاة هذا السلوك عبر التعرّف على أمراض غير مرئيّة بالاستناد إلى مصادر معرفيّة أخرى <span class="citation" data-cites="zsl"></span>، وهو من أكثر صور التعلُّم تحت إشراف محدود تطلّبًا. باختصار، عند إعطاء صورة استعلام، تبحث طرق ZSL عن التوافق بين التمثيل البصري للصورة وتمثيلها الدلالي <span class="citation" data-cites="xianCVPR17"></span>، وقد حقّق ZSL نتائج مُبهرة في الصور الطبيعيّة <span class="citation" data-cites="deeptag"></span>. إلّا أنّ معظم الطرق المقترحة تُسنِد وسمًا واحدًا فقط لكل صورة، وغالبًا ما يكون الوسم من الفئات المرئيّة فقط <span class="citation" data-cites="zsl Long_2017_CVPR"></span>. في التصوير الطبي، مثل تصنيف الأمراض في صور الأشعّة السينيّة للصدر، قد تحتوي الصورة على أكثر من مرض واحد، وقد تكون الوسوم من الفئات المرئيّة أو غير المرئيّة <span class="citation" data-cites="wang2017chestxray"></span>، ما يحدّ من تطبيق طرق ZSL أحاديّة الوسم. من ناحية أخرى، يسمح <em>التعلُّم بدون أمثلة مُسبقة مُتعدِّد الوسوم</em> (ML-ZSL) بإسناد عدّة وسوم لكل صورة. أمّا في حالة <em>التعلُّم بدون أمثلة مُسبقة مُعمَّم ومُتعدِّد الوسوم</em> (ML-GZSL)، فالهدف هو إسناد عدّة وسوم للصورة قد تكون من الفئات المرئيّة أو غير المرئيّة <span class="citation" data-cites="gzsl"></span>. تعتمد الطرق الحاليّة لـ ML-GZSL في الصور الطبيعيّة على البحث عن الجار الأقرب في الفضاء الدلالي:
            <span class="math display">\[P^c_x = \langle f_x, \mathcal{W} \rangle,
  \label{eqn:scores}\]</span>
            حيث <span class="math inline">\(\langle \cdot,\cdot\rangle\)</span> هي دالّة تشابه جيب التمام، وتمثّل <span class="math inline">\(f_x\)</span> السمات البصريّة لصورة الاستعلام <span class="math inline">\(x\)</span>، و<span class="math inline">\(\mathcal{W}\)</span> هي التضمينات الدلاليّة لجميع الفئات. تُحسب هذه المعادلة درجات التشابه بين السمات البصريّة والتضمينات الدلاليّة لكل فئة محتملة، بما يعكس مدى ارتباط الوسم بالصورة. غالبًا ما يتم ذلك عبر إسقاط السمات البصريّة المُجمَّعة إلى الفضاء الدلالي <span class="citation" data-cites="deeptag Huynh_2020_CVPR"></span>. إلّا أنّ هذه الطرق تعتمد على تمثيل بصري ثابت مُستخرج من مُشفر بصري مُدرَّب مُسبقًا أو شبكة كشف، كما أنّ إسقاط هذه السمات إلى الفضاء الدلالي قد يُقلِّص تنوّع المعلومات البصريّة، ما يؤدّي إلى مشكلات جوهريّة مثل مشكلة المحوريّة (hubness) <span class="citation" data-cites="hubness"></span>.</p>
            <p>لتجاوز هذه التحدّيات، نقترح شبكة CXR-ML-GZSL لتصنيف الأمراض في صور الأشعّة السينيّة للصدر. تتكوّن شبكتنا من مُشفر بصري يتعلّم السمات البصريّة ووحدتَي إسقاط لكلٍّ من السمات البصريّة والدلاليّة، حيث يُسقَط كلٌّ منهما إلى فضاء كامن مشترك يمكن فيه تحقيق التوافق بينهما. قيّمنا الطريقة المقترحة على مجموعة بيانات NIH للأشعّة السينيّة للصدر <span class="citation" data-cites="wang2017chestxray"></span> وأظهرت النتائج تفوّق CXR-ML-GZSL على النماذج الأساسيّة. نقدّم في هذا العمل ثلاث مساهمات رئيسيّة:</p>
            <ul>
                <li>من الناحية التقنيّة، صمّمنا شبكة قابلة للتدريب من البداية للنهاية تتعلّم التمثيل البصري وتقوم بمُحاذاته مع التمثيل الدلالي دون الحاجة إلى تدريب مُسبق مستقلّ للمُشفر البصري.</li>
                <li>اقترحنا هدف تعلُّم جديدًا لشبكة CXR-ML-GZSL يحقّق توزيعًا تنبّؤيًا فعّالًا، ويضمن تمركز التمثيلات البصريّة حول دلالات الفئات في الفضاء الكامن، ويفرض قيدًا للحفاظ على التمثيل الأصلي لدلالات الفئات.</li>
                <li>من منظور التصوير الطبي، نعدّ - فيما نعلم - أوّل من يقترح إطار ML-GZSL لتصنيف الأمراض في صور الأشعّة السينيّة للصدر.</li>
            </ul>
            <p>نستعرض الأعمال ذات الصِّلة في القسم <a href="#sec:related-work" data-reference-type="ref" data-reference="sec:related-work">2</a>، والمنهجيّة في القسم <a href="#sec:methodology" data-reference-type="ref" data-reference="sec:methodology">3</a>، والتجارب في القسم <a href="#sec:experiments" data-reference-type="ref" data-reference="sec:experiments">4</a>، والنتائج في القسم <a href="#sec:results" data-reference-type="ref" data-reference="sec:results">5</a>، والمناقشة في القسم <a href="#sec:discussion" data-reference-type="ref" data-reference="sec:discussion">6</a>، وأخيرًا الخُلاصة في القسم <a href="#sec:conclusion" data-reference-type="ref" data-reference="sec:conclusion">7</a>.</p>
        </section>

        <section id="generalizable-insights-about-ml-in-the-context-of-healthcare" class="level1 unnumbered">
            <h1 class="unnumbered">رؤى عامة حول التعلُّم الآلي في سياق الرعاية الصحيّة</h1>
            <p>تعاني نماذج تصنيف صور الأشعّة السينيّة للصدر مُتعدِّدة الوسوم التقليديّة من محدوديّة توافر البيانات ووسمها. يتغلّب عملُنا على تحدّي جمع مجموعات بيانات مُوسومة واسعة النطاق من خلال الاستفادة من الأدبيّات الطبيّة الغنيّة، باعتبارها المصدر الرئيسي للمعرفة حول الأمراض التي اكتشفها المجتمع الطبي. يبرز ذلك أهميّة التعلُّم مُتعدِّد الوسائط في تطبيقات الرعاية الصحيّة. وعلى الرغم من تركيزنا على صور الأشعّة السينيّة للصدر، فإن تصميم الشبكة قابل للتعميم على أي مهمة تصوير طبي، إذ إنّ المُشفر الدلالي غير مرتبط بمهمة محدّدة. إن تحسين تشخيص الأمراض غير المرئيّة أثناء الاستدلال قد يُسهم في إنقاذ حياة المرضى.</p>
        </section>

        <section id="sec:related-work" class="level1">
            <h1>الأعمال ذات الصِّلة</h1>

            <section id="inductive-transductive-zsl" class="level2">
                <h2>التعلُّم بدون أمثلة مُسبقة: الاستقرائي والانتقالي</h2>
                <p>يُصنِّف التعلُّم بدون أمثلة مُسبقة (ZSL) الفئات غير المرئيّة أثناء التدريب عبر نقل المعرفة من الفئات المرئيّة، ويعتمد على دلالات الفئات لسدّ الفجوة بين الفئات المرئيّة وغير المرئيّة. تُستحصَل الدلالات إمّا من خلال سمات الفئات المُوسومة يدويًّا أو من أوصاف نصيّة مُضمّنة في فضاء عالِ الأبعاد <span class="citation" data-cites="att_zsl ZhangXG16a"></span>، أو عبر استخراج متجهات دلاليّة للوسوم باستخدام Word2vec أو GloVe <span class="citation" data-cites="mikolov2013distributed Elhoseiny"></span>. يمكن تصنيف طرائق تدريب ZSL إلى <em>استقرائي</em>، يُدرَّب فقط على بيانات الفئات المرئيّة <span class="citation" data-cites="xianCVPR17"></span>، و<em>انتقالي</em>، يفترض توافر أمثلة بصريّة غير مُوسومة للفئات غير المرئيّة أثناء التدريب <span class="citation" data-cites="transductive"></span>. غير أنّ التعلُّم الانتقالي يُنتهك فرضيّة عدم رؤية الفئات غير المرئيّة أثناء التدريب، وهو افتراض غير عملي. لذا فإن معالجة ZSL في الإعداد الاستقرائي أكثر واقعيّة وتقدِّم حلًّا عمليًّا لتصنيف صور الأشعّة السينيّة للصدر.</p>
            </section>

            <section id="multi-label-generalized-zero-shot-learning" class="level2">
                <h2>التعلُّم بدون أمثلة مُسبقة مُعمَّم ومُتعدِّد الوسوم</h2>
                <p>قليل من الأعمال تناولت التصنيف مُتعدِّد الوسوم في الصور الطبيعيّة. قام <span class="citation" data-cites="Lee_2018_CVPR"></span> ببناء رسوم بيانيّة معرفيّة باستغلال العلاقات الدلاليّة في WordNet <span class="citation" data-cites="wordnet"></span> لإسناد الوسوم إلى الفئات غير المرئيّة. في عمل حديث، deepTag <span class="citation" data-cites="deeptag"></span>، تم استخراج مجموعة من الرقع المحليّة باستخدام شبكة كشف مُدرَّبة مُسبقًا، ثم تجميع هذه المُقترحات وإسقاطها في الفضاء الدلالي لإيجاد التوافق بين الفئات المرئيّة وغير المرئيّة. غير أنّ هذا النهج يتطلّب مجموعة بيانات كبيرة مع وسوم صناديق الإحاطة لتدريب Faster R-CNN <span class="citation" data-cites="fasterrcnn zsd"></span>. اقترح <span class="citation" data-cites="Huynh_2020_CVPR"></span> نموذج انتباه مُشترك لتعلُّم سمات انتباه متعدِّدة غير مرتبطة بالفئة عبر استخراج سمات مناطق مقصوصة باستخدام شبكة CNN مُدرَّبة مُسبقًا. تُسقَط هذه السمات إلى الفضاء الدلالي لإيجاد صلة الوسوم. على الرغم من أنّ هذه الطرق تُظهر أداءً جيّدًا في الصور الطبيعيّة، إلّا أنّها غالبًا غير قابلة للتطبيق مباشرةً على تصنيف صور الأشعّة السينيّة للصدر أو تعتمد على مُشفرات بصريّة مُدرَّبة مُسبقًا على ImageNet <span class="citation" data-cites="ILSVRC15"></span>. كما أظهر <span class="citation" data-cites="raghu2019transfusion"></span> أنّ نقل المعرفة من ImageNet لا يقدِّم فائدة كبيرة لتشخيصات الأشعّة. لذلك، من المُهم تعلُّم تمثيل بصري فعّال لمُشفر الصور في CXR-ML-GZSL. نقترح هنا تعلُّم التمثيل البصري ومُحاذاته مع الدلالات في شبكة قابلة للتدريب من البداية للنهاية.</p>
            </section>

            <section id="deep-learning-for-chest-radiographs" class="level2">
                <h2>التعلُّم العميق لصور الأشعّة السينيّة للصدر</h2>
                <p>يفترض التصنيف مُتعدِّد الوسوم إمكانيّة ظهور عدّة وسوم في صورة واحدة، وهو أمر شائع في تصنيف صور الأشعّة السينيّة للصدر حيث قد تحتوي الصورة على عدّة أمراض. قام <span class="citation" data-cites="rajpurkar2017chexnet"></span> بتدريب DenseNet-121 <span class="citation" data-cites="densenet"></span> عبر صياغة مشكلة تصنيف مُتعدِّدة الوسوم تقليديّة، غير أنّ منهجهم لم يلتقط العلاقات بين الفئات المختلفة. استخدم <span class="citation" data-cites="Guan2018DiagnoseLA"></span> آليّة انتباه عبر قصّ مناطق الاهتمام المُستخرجة من خرائط انتباه صادرة عن مُشفر صورة عالمي، ثم دمج السمات من الفروع المحليّة والعالميّة لإسناد الدرجات التصنيفيّة. اعتمد <span class="citation" data-cites="yao2018learning"></span> على DenseNet لتمثيل الصورة عالميًّا، واستخدم شبكة LSTM لتعلُّم الارتباطات بين الفئات لتحسين التشخيص <span class="citation" data-cites="lstm"></span>. رغم الأداء الواعد، تعتمد هذه الطرق على كميّات كبيرة من البيانات المُوسومة، ولا يمكنها عند الاستدلال توقّع فئات لم تُرَ أثناء التدريب.</p>
                <p>هناك أعمال محدودة جدًا حول استخدام ZSL في تصوير الصدر. مؤخرًا، اقترح <span class="citation" data-cites="MVSE"></span> إطار GZSL باستخدام مُشفر تلقائي ثنائي الفروع يجمع المعرفة الخارجيّة من ثلاثة مصادر نصيّة: تقارير الأشعّة السينيّة، وتقارير الأشعّة المقطعيّة، وسمات بصريّة مُعرَّفة يدويًّا. إلّا أنّ تصميمهم يعاني من عدّة قيود: أوّلًا، يتنبّأ النموذج بوسم واحد فقط رغم أنّ البيانات مُتعدِّدة الوسوم؛ ثانيًا، يعتمد على توافر تقارير وصور غير مُوسومة للفئات غير المرئيّة أثناء التدريب، ما يُنتهك فرضيّة ZSL؛ ثالثًا، يُتعلَّم التمثيل الدلالي من تقارير لحالات مرئيّة فقط ويُسترشَد بسمات فئات مُوسومة يدويًّا، ما يقيّد التطبيق بمجموعة مُغلَقة من الفئات. على العكس من ذلك، يمكن لشبكتنا إسناد عدّة وسوم للصورة وتعميمها على مجموعة مفتوحة من الفئات غير المرئيّة دون الاعتماد على تقارير أو وسوم يدويّة.</p>

                <figure class="figure">
                    <embed src="figures/figure.pdf" id="fig:main_fig" />
                    <figcaption aria-hidden="true">
                        <strong>نظرة عامة على نموذج CXR-ML-GZSL المُقترَح لتعلُّم التمثيلات البصريّة لصور الأشعّة السينيّة للصدر.</strong>
                        يوضّح الشكل نظرة عامة على الشبكة المُقترَحة، والتي تتضمّن مُشفرًا بصريًّا قابلًا للتدريب وفضاءات بصريّة ودلاليّة بأبعاد <span class="math inline">\(v\)</span> و<span class="math inline">\(d\)</span> على التوالي. بالنسبة لصورة الإدخال <span class="math inline">\(x\)</span> ووسومها <span class="math inline">\(y\)</span>، تتعلّم الشبكة تمثيلًا بصريًّا مُوجَّهًا بدلالات مُستخرجة بواسطة BioBERT. تُدرَّب المكوّنات الثلاثة (المُشفر البصري ووحدة الإسقاط البصريّة ووحدة الإسقاط الدلاليّة) من البداية للنهاية كما هو موضّح بالخطّ المتقطّع الأسود.
                    </figcaption>
                </figure>
            </section>
        </section>

        <section id="sec:methodology" class="level1">
            <h1>المنهجيّة</h1>

            <section id="problem-formulation" class="level2">
                <h2>صياغة المشكلة</h2>
                <p>لنعتبر مجموعة <span class="math inline">\(\mathcal{X}^{s}\)</span> التي تحتوي على صور التدريب للفئات المرئيّة فقط. كل <span class="math inline">\(x \in \mathcal{X}^{s}\)</span> مرتبطة بمجمّعة وسوم <span class="math inline">\(\mathbf{y}_x\)</span>، حيث <span class="math inline">\(y_x^i \in \{0,1\},\, i=1,\dots,S\)</span> و"1" تعني وجود المرض رقم <span class="math inline">\(i\)</span> من بين <span class="math inline">\(S\)</span> فئة مرئيّة أثناء التدريب. نرمز للفئات المرئيّة وغير المرئيّة بـ <span class="math inline">\(\mathcal{Y}^{s} = \{1,\dots, S\}\)</span> و<span class="math inline">\(\mathcal{Y}^{u} = \{S+1,\dots, C\}\)</span> على التوالي، حيث <span class="math inline">\(C\)</span> هو العدد الكلّي للفئات. هاتان المجموعتان منفصلتان بحيث <span class="math inline">\(\mathcal{Y}^{s} \cap  \mathcal{Y}^{u} = \emptyset\)</span>. الهدف هو تعلُّم تمثيل بصري لـ <span class="math inline">\(x\)</span> مُوجَّه بدلالات وسومها <span class="math inline">\(\mathbf{y}_x\)</span>. عند الاستدلال، وبالنسبة لصورة اختبار <span class="math inline">\(x_{test}\)</span>، الهدف هو توقّع <span class="math inline">\(\mathbf{y}_{x_{test}}\)</span> حيث <span class="math inline">\(y_{x_{test}}^i \in [0,1],\, i=1,\dots,C\)</span>. في الأقسام التالية، نصف معماريّة الشبكة المُقترَحة وهدف التعلُّم.</p>
            </section>

            <section id="network-architecture" class="level2">
                <h2>معماريّة الشبكة</h2>
                <p>يوضّح الشكل <a href="#fig:main_fig" data-reference-type="ref" data-reference="fig:main_fig">1</a> نظرة عامة على معماريّة الشبكة. تتكوّن من مُشفر بصري قابل للتدريب، ومُشفر دلالي ثابت، ووحدات مُحاذاة. نوضّح فيما يلي تفاصيل كل مكوّن ودوالّ الخسارة.</p>
                <p><strong>المُشفر البصري:</strong> لتعلُّم التمثيل البصري، نُعرِّف مُشفرًا بصريًّا <span class="math inline">\(\boldsymbol{\rho}(x): \mathbb{R}^{w \times h \times c} \mapsto \mathbb{R}^{v}\)</span> يحسب <span class="math inline">\(f^v\)</span>، وهو تمثيل بصري بعدد أبعاد <span class="math inline">\(v\)</span> لصورة الإدخال <span class="math inline">\(x\)</span>. بعد ذلك، تُعالج وحدة الإسقاط البصريّة <span class="math inline">\(\boldsymbol{\psi}(f^v): \mathbb{R}^{v} \mapsto \mathbb{R}^{l}\)</span> هذا التمثيل وتُسقِطه إلى فضاء كامن <span class="math inline">\(l\)</span> مُتعلَّم مع المعلومات الدلاليّة.</p>
                <p><strong>الدلالات:</strong> لنفترض أن تضمينات الفئات المرئيّة هي <span class="math inline">\(\mathcal{W}^{s} = \{w_1, w_2, \cdots ,w_S\}\)</span>، حيث <span class="math inline">\(w_i\)</span> يمثّل تمثيلًا دلاليًّا بعدد أبعاد <span class="math inline">\(d\)</span> للفئة <span class="math inline">\(i \in \mathcal{Y}^{s}\)</span>، ويُستخرَج من الطبقة قبل الأخيرة في BioBERT <span class="citation" data-cites="Lee_2019"></span> لجميع الوسوم القابلة للتدريب. يُعدّ هذا العمل الأوّل الذي يستخدم BioBERT في مسائل ZSL في الرعاية الصحيّة، نظرًا لفاعليّته في تعلُّم تضمينات كلمات سياقيّة مُتخصّصة في النصوص الطبيّة الحيويّة.</p>
                <p>نُعرِّف <span class="math inline">\(\boldsymbol{\phi}(w^d): \mathbb{R}^{d} \mapsto \mathbb{R}^{l}\)</span> كوحدة إسقاط دلاليّة تتعلّم إسقاط التضمين الدلالي إلى الفضاء الكامن المشترك <span class="math inline">\(l\)</span>.</p>
                <p>بالنسبة للمعماريّة المُقترَحة، نُعيد تعريف المعادلة <a href="#eqn:scores" data-reference-type="ref" data-reference="eqn:scores">[eqn:scores]</a> كما يلي:
                <span class="math display">\[P_x^S = \langle \boldsymbol{\psi} ( \boldsymbol{\rho} (x)),\   \boldsymbol{\phi}(\mathcal{W}^{s}) \rangle,
  \label{eqn:scoresours}\]</span>
                حيث <span class="math inline">\(P_x^S\)</span> يمثّل درجات الصِّلة للوسوم التدريبيّة، أي <span class="math inline">\(P_x^S = \{p_1, p_2, \cdots ,p_S\}\)</span>. تعكس هذه الدرجات مدى التشابه بين الصورة وكل وسم محتمل.</p>
                <p><strong>هدف التدريب:</strong> نُصيغ هدف التدريب لتحسين معلمات الشبكة كما يلي:
                <span class="math display">\[\min_{\boldsymbol{\phi} ,\boldsymbol{\rho} ,\boldsymbol{\psi}} \mathcal{L} = \mathcal{L}_{rank} + \gamma_1 \mathcal{L}_{align} + \gamma_2 \mathcal{L}_{con},
    \label{eqn:full_loss}\]</span>
                حيث <span class="math inline">\(\gamma_1\)</span> و<span class="math inline">\(\gamma_2\)</span> معاملا تنظيم لخسارتي <span class="math inline">\(\mathcal{L}_{align}\)</span> و<span class="math inline">\(\mathcal{L}_{con}\)</span> على التوالي. في الأقسام التالية، نُعرِّف كل مكوّن من مكوّنات الخسارة.</p>
            </section>

            <section id="mathcall_rank-ranking-loss-of-relevance-scores" class="level2">
                <h2><span class="math inline">\(\mathcal{L}_{rank}\)</span> خسارة الترتيب لدرجات الصِّلة</h2>
                <p>أثناء التدريب، تُنتِج الشبكة درجات الصِّلة <span class="math inline">\(P_x^S = \{p_1, p_2, \cdots ,p_S\}\)</span> لكل الفئات المرئيّة <span class="math inline">\(S\)</span> لصورة الإدخال <span class="math inline">\(x\)</span>. بالاستناد إلى الوسوم الحقيقيّة <span class="math inline">\(\mathbf{y}_x\)</span>، حيث <span class="math inline">\(y_x^i \in \{0,1\},\, i=1,\dots,S\)</span>، نرمز إلى <span class="math inline">\(Y_p\)</span> كمجموعة الوسوم الإيجابيّة (الأمراض الموجودة في الصورة) و<span class="math inline">\(Y_n\)</span> كمجموعة الوسوم السلبيّة (الأمراض غير الموجودة). لنفترض أنّ <span class="math inline">\(p_{y_p}\)</span> و<span class="math inline">\(p_{y_n}\)</span> هما الدرجتان المحسوبتان للوسم الإيجابي والسلبي على التوالي. في التصنيف مُتعدِّد الوسوم، نرغب في شرطين: أن تكون <span class="math inline">\(p_{y_p}\)</span> أعلى من <span class="math inline">\(p_{y_n}\)</span>، وأن يكون الفارق بينهما على الأقل بقيمة هامشيّة <span class="math inline">\(\delta\)</span>. لذا نُصيغ خسارة ترتيب على مستوى الصورة كالتالي:
                <span class="math display">\[\mathcal{L}(P_x^S, \mathbf{y}_x) = \frac{1}{S}\sum_{y_p \in Y_p } \sum_{y_n \in Y_n} \max\!\big(\delta + (p_{y_n} - p_{y_p}) ,\ 0\big).
    \label{eq:rankingloss}\]</span>
                تكون الخسارة صفرًا إذا تحقّق الشرط بفارق لا يقلّ عن <span class="math inline">\(\delta\)</span>، وتفرض عقوبة لا تقلّ عن <span class="math inline">\(\delta\)</span> إذا لم يتحقّق. ويُؤخذ متوسّط الخسارة على جميع صور التدريب:</p>
                <p><span class="math display">\[\mathcal{L}_{rank} = \frac{1}{N} \sum_{\forall x \in \mathcal{X}^{s}} \mathcal{L} (P_x^S, \mathbf{y}_x),\]</span> حيث <span class="math inline">\(N\)</span> هو العدد الكلّي للصور.</p>
            </section>

            <section id="mathcall_align-alignment-loss-of-visual-semantic-representations" class="level2">
                <h2><span class="math inline">\(\mathcal{L}_{align}\)</span> خسارة المُحاذاة بين التمثيلات البصريّة والدلاليّة</h2>
                <p>لمُحاذاة التمثيلات البصريّة مع الدلاليّة أثناء التدريب، نُصيغ خسارة مُحاذاة بين النمطين كما يلي:
                <span class="math display">\[\mathcal{L}_{align} = \frac{1}{N} \sum_{\forall x \in \mathcal{X}^s} \Big(1 - \langle \boldsymbol{\psi} (\boldsymbol{\rho} (x)),\  \boldsymbol{\phi}(w_x) \rangle\Big),\]</span>
                حيث <span class="math inline">\(w_x\)</span> هو التضمين الدلالي المقابل لصورة الإدخال <span class="math inline">\(x\)</span> و<span class="math inline">\(\langle \cdot,\cdot \rangle\)</span> دالّة تشابه جيب التمام. في حال وجود عدّة وسوم للصورة، يُحسَب متوسّط التضمينات الدلاليّة في <span class="math inline">\(w_x\)</span>، بما يسمح بمُحاذاة التمثيل البصري مع دلالاته في حالة تعدّد الوسوم.</p>
            </section>

            <section id="mathcall_con-semantics-inter-class-consistency-regularizer" class="level2">
                <h2><span class="math inline">\(\mathcal{L}_{con}\)</span> مُنظِّم الاتّساق بين الفئات الدلاليّة</h2>
                <p>تنشأ التمثيلات الدلاليّة والبصريّة من نمطين مختلفين: تُتعلّم الدلالات من الأدبيّات الطبيّة النصيّة، بينما تُتعلّم التمثيلات البصريّة من الصور. لجسر الفجوة بين النمطين، نتعلّم دالّتَي إسقاط إلى فضاء مشترك. وبينما تُحسَّن التمثيلات البصريّة أثناء التدريب، تظلّ الدلالات ثابتة بعد استخراجها من المُشفر اللغوي. قد يؤدّي إسقاط الدلالات إلى الفضاء الكامن إلى فقدان العلاقات بين الفئات. لذا، نهدف إلى الحفاظ على اتّساق الفضاء الدلالي عبر تنظيم يعتمد على العلاقة بين الفئات في الفضاء الأصلي والمُسقَّط، وذلك عبر تنظيم <span class="math inline">\(L_1\)</span> كما يلي:
                <span class="math display">\[\mathcal{L}_{con} = \sum_{w_i \in \mathcal{W}} \sum_{\substack{w_j \in \mathcal{W} \\ j\neq i}} \big| \langle w_i,\  w_j \rangle - \langle \boldsymbol{\phi}(w_i),\  \boldsymbol{\phi}(w_j) \rangle \big|,\]</span>
                حيث <span class="math inline">\(w_i\)</span> و<span class="math inline">\(w_j\)</span> هما التمثيلان الدلاليّان الأصليّان لفئتَين، و<span class="math inline">\(\boldsymbol{\phi}(\cdot)\)</span> تمثّل الإسقاط. من المثالي أن يكون تشابه جيب التمام بين الفئتَين في الفضاء الأصلي مساويًا له في الفضاء المُسقَّط، وبالتالي تكون الخسارة صفرًا.</p>
            </section>

            <section id="inference" class="level2">
                <h2>الاستدلال</h2>
                <p>بعد تدريب الشبكة، نحصل على مُشفر الصور المُحسَّن <span class="math inline">\(\boldsymbol{\rho}(x)\)</span> ووحدتَي الإسقاط <span class="math inline">\(\boldsymbol{\psi}(f^v)\)</span> و<span class="math inline">\(\boldsymbol{\phi}(w^d)\)</span> اللتين تُسقطان كلًّا من السمات البصريّة والتضمينات الدلاليّة إلى الفضاء الكامن المشترك. عند الاستدلال، وبالنسبة لصورة اختبار <span class="math inline">\(x \in \mathcal{X}^{C}\)</span>، نقوم بتحديث <span class="math inline">\(\mathcal{W}\)</span> في المعادلة <a href="#eqn:scoresours" data-reference-type="ref" data-reference="eqn:scoresours">[eqn:scoresours]</a> لتشمل التضمينات الدلاليّة للفئات المرئيّة وغير المرئيّة. بخلاف التصنيف التقليدي مُتعدِّد الوسوم، يمكننا بعد تعديل <span class="math inline">\(\mathcal{W}\)</span> الحصول على درجات التنبّؤ <span class="math inline">\(P_x^C = \{p_1, p_2, \cdots ,p_C\}\)</span> لمجموعة من <span class="math inline">\(C\)</span> فئات تشمل المرئيّة وغير المرئيّة، كما يلي:
                <span class="math display">\[P^C_x = \langle \boldsymbol{\psi} (\boldsymbol{\rho} (x)),\  \boldsymbol{\phi} (\mathcal{W}^{C}) \rangle.
  \label{eqn:infer}\]</span>
                حيث <span class="math inline">\(\mathcal{W}^{C}\)</span> تمثّل التضمينات الدلاليّة المُحدّثة لكافّة الفئات.</p>
            </section>
        </section>

        <section id="sec:experiments" class="level1">
            <h1>الإعدادات التجريبية</h1>

            <section id="dataset" class="level2">
                <h2>مجموعة البيانات</h2>
                <p>لتقييم الشبكة المُقترَحة CXR-ML-GZSL، أجرينا التجارب على مجموعة بيانات NIH للأشعّة السينيّة للصدر <span class="citation" data-cites="wang2017chestxray"></span>. تحتوي المجموعة على 112,120 صورة أماميّة لـ 30,805 مريضًا. قُسِّمت البيانات عشوائيًا إلى تدريب (70%)، وتحقّق (10%)، واختبار (20%). كل صورة مرتبطة بـ 14 فئة محتملة. شملنا جميع الفئات الـ 14 وقسّمناها عشوائيًا إلى فئات مرئيّة (10) وغير مرئيّة (4). الفئات المرئيّة: Atelectasis, Effusion, Infiltration, Mass, Nodule, Pneumothorax, Consolidation, Cardiomegaly, Pleural Thickening, Hernia. الفئات غير المرئيّة: Edema, Pneumonia, Emphysema, Fibrosis. استُبعدت جميع الصور المرتبطة بأي فئة غير مرئيّة من مجموعة التدريب، وفقًا للإعداد الاستقرائي. بلغ عدد صور التدريب النهائي 30,758، والتحقّق 4,474، والاختبار 10,510.</p>
            </section>

            <section id="model-training-and-selection" class="level2">
                <h2>تدريب النموذج واختياره</h2>
                <p>نوضح هنا إعدادات التجربة لـ CXR-ML-GZSL. لتشفير المعلومات البصريّة، صمّمنا طريقتنا للعمل مع أي شبكة عصبيّة التفافيّة متقدّمة. أجرينا جميع التجارب باستخدام DenseNet-121 نظرًا لأدائه الممتاز في تصنيف صور الأشعّة السينيّة للصدر <span class="citation" data-cites="rajpurkar2017chexnet"></span>. أزلنا الطبقة التصنيفيّة النهائيّة واستخدمنا الشبكة الناتجة كمُشفر بصري <span class="math inline">\(\boldsymbol{\rho}(x)\)</span> لإنتاج تمثيل بصري <span class="math inline">\(f^v \in \mathbb{R}^{1024}\)</span>.</p>
                <p>تمّت برمجة وحدة الإسقاط البصريّة كشبكة عصبيّة أماميّة من ثلاث طبقات:
                <span class="math inline">\(\boldsymbol{\psi}: f^v \xrightarrow{} \texttt{fc1} \xrightarrow{} \texttt{ReLU} \xrightarrow{} \texttt{fc2} \xrightarrow{} \texttt{ReLU} \xrightarrow{} \texttt{fc3} \xrightarrow{} f^l\)</span>، حيث <span class="math inline">\(\texttt{fc1}\)</span> طبقة كاملة التوصيل بوزن <span class="math inline">\(\mathbf{W}_{\texttt{fc1}} \in \mathbb{R}^{1024 \times 512}\)</span> وانحياز <span class="math inline">\(\mathbf{b}_{\texttt{fc1}} \in \mathbb{R}^{512}\)</span>. الطبقة التالية بوزن <span class="math inline">\(\mathbf{W}_{\texttt{fc2}} \in \mathbb{R}^{512 \times 256}\)</span> وانحياز <span class="math inline">\(\mathbf{b}_{\texttt{fc2}} \in \mathbb{R}^{256}\)</span>. الطبقة الأخيرة بوزن <span class="math inline">\(\mathbf{W}_{\texttt{fc3}} \in \mathbb{R}^{256 \times 128}\)</span> وانحياز <span class="math inline">\(\mathbf{b}_{\texttt{fc3}} \in \mathbb{R}^{128}\)</span>، ليتم الإسقاط النهائي إلى الفضاء الكامن المشترك مع التضمينات الدلاليّة. تتبع وحدة الإسقاط الدلاليّة البنية نفسها. يتيح هذا التصميم التعامل مع تضمينات مُستخرجة من معماريّات مختلفة وإسقاطها إلى فضاء مشترك بغضّ النظر عن أبعادها الأصليّة.</p>
                <p>دُرِّبت الشبكة باستخدام خوارزميّة Adam <span class="citation" data-cites="kingma2017adam"></span> لمدة 100 حقبة، مع تقليل معدّل التعلُّم <span class="math inline">\(lr\)</span> بمقدار 0.01 عند ثبات خسارة التحقّق لعشر حِقب. استغرق تدريب النموذج الواحد نحو 8 ساعات على بطاقة NVIDIA Quadro RTX 6000. تمّ تعيين <span class="math inline">\(\delta=0.5\)</span> في المعادلة <a href="#eq:rankingloss" data-reference-type="ref" data-reference="eq:rankingloss">[eq:rankingloss]</a>. لاختيار أفضل القيم لـ <span class="math inline">\(\gamma_1\)</span> و<span class="math inline">\(\gamma_2\)</span> ومعدّل التعلُّم، أجرينا عدّة تجارب باختيار عشوائي من <span class="math inline">\(\gamma \in \{0.1, 0.01, 0.05\}\)</span> و<span class="math inline">\(lr \in \{ 0.0001, 0.00005, 0.00001\}\)</span>، ثم اخترنا النموذج الأفضل على مجموعة التحقّق بناءً على المتوسّط التوافقي لـ AUROC. طُوِّرت الشيفرة باستخدام مكتبة PyTorch <span class="citation" data-cites="pytorch"></span>.</p>
            </section>

            <section id="performance-metrics" class="level2">
                <h2>مقاييس الأداء</h2>
                <p>استخدمنا مقاييس تقييم شائعة في طرق ML-GZSL <span class="citation" data-cites="Lee_2018_CVPR Huynh_2020_CVPR"></span>. حسبنا الدقّة والاسترجاع ودرجة F1 لأفضل <span class="math inline">\(k\)</span> تنبّؤات حيث <span class="math inline">\(k \in \{2, 3\}\)</span> في إعداد GZSL. اخترنا قيمة صغيرة لـ <span class="math inline">\(k\)</span> نظرًا لقلة عدد الفئات مقارنةً ببيانات الصور الطبيعيّة <span class="citation" data-cites="nus_wide_civr09 openimages"></span>. كما أبلغنا عن متوسّط مساحة تحت منحنى ROC (AUROC) للفئات المرئيّة وغير المرئيّة ومتوسّطهما التوافقي، إذ قد لا يعكس الاسترجاع لأفضل <span class="math inline">\(k\)</span> الأداء على مستوى كل فئة. ومن المهم الإشارة إلى أنّ المتوسّط التوافقي يقيس التحيّز الكامن في طرائق GZSL تجاه الفئات المرئيّة.</p>
                <p><span id="tab:recall" label="tab:recall"></span></p>
                <p><span id="tab:class_wise" label="tab:class_wise"></span></p>
            </section>
        </section>

        <section id="sec:results" class="level1">
            <h1>النتائج</h1>

            <section id="comparison-to-baseline-models" class="level2">
                <h2>المقارنة مع النماذج الأساسيّة</h2>
                <p>قارَنّا أداء النهج المُقترَح (<span class="math inline">\(OUR_{e2e}\)</span>) مع طريقتين متقدّمتين في ML-GZSL: LESA <span class="citation" data-cites="Huynh_2020_CVPR"></span> وMLZSL <span class="citation" data-cites="Lee_2018_CVPR"></span>. يُلخّص الجدول <a href="#tab:recall" data-reference-type="ref" data-reference="tab:recall">[tab:recall]</a> النتائج على مجموعة الاختبار. أظهرت النتائج أنّ طريقتنا تتفوّق في جميع المقاييس، حيث بلغت AUROC للفئات غير المرئيّة 0.66، والمتوسّط التوافقي 0.72 عبر جميع الفئات. حققت LESA الأداء الأضعف، بينما جاءت MLZSL في المرتبة الثانية. تفوّقت طريقتنا على MLZSL بفارق كبير، على سبيل المثال بنسبة 73.68% في precision@2.</p>
                <p>يقارن الجدول <a href="#tab:class_wise" data-reference-type="ref" data-reference="tab:class_wise">[tab:class_wise]</a> قيم AUROC لكل فئة مع الطرق المتقدمة. حقّقت طريقتنا أفضل أداء في جميع الفئات المرئيّة مقارنةً بالنماذج الأساسيّة، باستثناء Hernia حيث كان الأداء مُقارِبًا لـ MLZSL (0.90 AUROC). أمّا في الفئات غير المرئيّة، فقد حقّقت أفضل أداء AUROC مقارنةً بكلا النموذجين.</p>
                <p>يوضح الشكل <a href="#fig:qual" data-reference-type="ref" data-reference="fig:qual">[fig:qual]</a> أمثلة لتنبّؤات الشبكة على 9 صور اختبار. تم اختيار أفضل ثلاثة تنبّؤات لكل صورة. نلاحظ أنّ طريقتنا قادرة على التنبّؤ بالفئات غير المرئيّة حتى عند وجود عدد كبير من الوسوم الحقيقيّة، ما يبرز فاعليّة الطريقة في التنبّؤ المتزامن بعدّة فئات مرئيّة وغير مرئيّة.</p>
                <p class="text-center">
                    <img src="visuals_updated/2.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;">
                    <img src="visuals_updated/5.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;">
                    <img src="visuals_updated/3.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;"><br>
                    <img src="visuals_updated/4.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;">
                    <img src="visuals_updated/9.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;">
                    <img src="visuals_updated/13.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;"><br>
                    <img src="visuals_updated/14.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;">
                    <img src="visuals_updated/19.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;">
                    <img src="visuals_updated/12.png" alt="عينات تنبّؤات" style="max-width: 30%; margin: 4px;">
                </p>
            </section>

            <section id="ablation-studies" class="level2">
                <h2>دراسات الإلغاءِ الجزئي (Ablation)</h2>
                <p>أجرينا دراستين إلغائيتين باستخدام مجموعة التحقّق. في جميع الدراسات، جرى تعيين معدّل التعلُّم الابتدائي إلى 0.0001، و<span class="math inline">\(\gamma_{1}=0.01\)</span>، و<span class="math inline">\(\gamma_{2}=0.01\)</span>. يوضّح الجدول <a href="#tab:ablation" data-reference-type="ref" data-reference="tab:ablation">[tab:ablation]</a> قيم AUROC للفئات المرئيّة وغير المرئيّة ومتوسّطها التوافقي مع صِيَغ مختلفة لدالّة الهدف. أجرينا تجارب لتقييم مساهمة كلٍّ من خسارة المُحاذاة ومُنظِّم الاتّساق الدلالي. وبينما بقي أداء الفئات المرئيّة ثابتًا (0.783–0.791 AUROC)، لوحِظ تحسّن في AUROC للفئات غير المرئيّة عند إضافة خسارة المُحاذاة <span class="math inline">\(\mathcal{L}_{align}\)</span>، التي تضمن تمركز التمثيلات البصريّة حول دلالات الفئات. كما ساهم الحفاظ على اتّساق الفضاء الدلالي باستخدام <span class="math inline">\(\mathcal{L}_{con}\)</span> في تحسين الأداء للفئات غير المرئيّة. بناءً عليه، استخدمنا جميع مكوّنات الخسارة في تدريب النموذج النهائي.</p>
                <p>كما درسنا أثر استخدام مُشفر بصري غير قابل للتدريب مقارنةً بالنهج المُقترَح القابل للتدريب من البداية للنهاية. أجرينا تجارب بتجميد المُشفر البصري المُدرَّب مُسبقًا على ImageNet لاستخراج السمات البصريّة، ثم درّبناه بشكل منفصل على مجموعة بيانات NIH للفئات المرئيّة فقط واستخدمناه لاستخراج سمات ثابتة. يوضّح الجدول <a href="#tab:training_approaches" data-reference-type="ref" data-reference="tab:training_approaches">[tab:training_approaches]</a> أداء هذه النُهج مقارنةً بالنهج القابل للتدريب من البداية للنهاية. ومن اللافت أنّ النهج الأخير أظهر أداءً أفضل، ما يؤكّد أهميّة تعلُّم تمثيل بصري مُتوافق مع التضمينات الدلاليّة، لا سيّما للفئات غير المرئيّة حيث تحسّن AUROC بشكل ملحوظ.</p>
                <p><span id="tab:ablation" label="tab:ablation"></span></p>
                <p><span id="tab:training_approaches" label="tab:training_approaches"></span></p>
            </section>
        </section>

        <section id="sec:discussion" class="level1">
            <h1>المناقشة</h1>
            <p>تعتمد التطوّرات الحديثة في مجال التعلُّم العميق للتصوير الطبي اعتمادًا كبيرًا على توافر مجموعات بيانات واسعة النطاق. في هذا البحث، نقدّم نهجًا واعدًا لتطوير شبكة تشخيصيّة مُتعدِّدة الوسوم قادرة على تصنيف الفئات غير المرئيّة باستخدام التعلُّم بدون أمثلة مُسبقة مُعمَّم. تستفيد شبكة CXR-ML-GZSL من الدلالات السياقيّة المُستقاة من الأدبيّات الطبيّة الغنيّة وتتعلم تمثيلات بصريّة مُوجَّهة بهذه الدلالات عبر هدف تعلُّم فريد. درّبنا النموذج على مجموعة من الفئات المرئيّة ثم اختبرناه على فئات مرئيّة وغير مرئيّة باستخدام مجموعة بيانات NIH. الفرضيّة أنّ الفئات غير المرئيّة لم تُعرَض على النموذج أثناء التدريب لمحاكاة سيناريو واقعي لتصنيف الأمراض النادرة. أظهرت النتائج أنّ الشبكة تُعمِّم جيّدًا على الفئات المرئيّة وغير المرئيّة وتُحقّق مكاسب ملحوظة مقارنةً بالطرائق السابقة. نوصي في التطبيق السريري بعرض قائمة الأمراض مُرتّبة من الأكثر إلى الأقل احتمالًا بناءً على درجات التنبّؤ، ويمكن للأطبّاء تحديد عتبة لتحويل الدرجات إلى نتائج ثنائيّة باستخدام تحليل الحساسيّة والنوعيّة.</p>
            <p>اقترح <span class="citation" data-cites="MVSE"></span> مؤخرًا طريقة GZSL لتصوير الصدر، شملت 9 فئات من أصل 14 بناءً على توافر تقارير الأشعّة المقطعيّة، واختاروا 6 فئات كمرئيّة و3 كغير مرئيّة. إلّا أنّ افتراضهم بتوافر بيانات الفئات غير المرئيّة أثناء التدريب (بما في ذلك التقارير والصور) يُنتهك فرضيّة ZSL، ما قد لا يُعطي تقييمًا دقيقًا للأداء على الفئات غير المرئيّة. لذا، ولضمان تقييم متين، حرصنا على عدم استخدام أي بيانات مُساعِدة للفئات غير المرئيّة أثناء التدريب. وبسبب اختلاف الافتراضات، لا يمكن مقارنة النتائج مباشرةً مع عمل <span class="citation" data-cites="MVSE"></span>.</p>

            <section id="limitations" class="level2 unnumbered">
                <h2 class="unnumbered">القيود</h2>
                <p>اقتصر تقييم الشبكة المُقترَحة على مجموعة بيانات مُتاحة، ما يفرض عدّة قيود. أوّلًا، تحتوي المجموعة على عدد فئات أقل من مجموعات الصور الطبيعيّة. لتقييم مُتانة الطريقة والأعمال المُستقبليّة، نؤكّد الحاجة إلى إنشاء مجموعة بيانات معياريّة أكثر تحدّيًا بعدد فئات أكبر. كما اخترنا الفئات المرئيّة وغير المرئيّة عشوائيًا، ويجب تقييم الطريقة على تقسيمات أخرى، وربما على مهام طبيّة أخرى وأنواع تصوير مختلفة وبيانات من مؤسّسات أخرى لاختبار قابليّة التعميم، وكذلك على مجموعات بيانات الرؤية الحاسوبيّة القياسيّة. بالإضافة إلى ذلك، اقتصرنا في ضبط المعاملات على معدّل التعلُّم ومعاملات <span class="math inline">\(\gamma\)</span> فقط، ونتوقّع تحسّن النتائج عند ضبط معاملات أخرى مثل حجم الدفعة وأبعاد وحدات الإسقاط.</p>
            </section>
        </section>

        <section id="conclusion" class="level1">
            <h1>الخُلاصة</h1>
            <p>في هذا العمل، نقترح شبكة تعلُّم بدون أمثلة مُسبقة مُعمَّم ومُتعدِّد الوسوم (CXR-ML-GZSL) لتصنيف صور الأشعّة السينيّة للصدر. أظهرنا من خلال التجارب أنّ الشبكة قادرة عند الاستدلال على إسناد عدّة وسوم من الفئات المرئيّة وغير المرئيّة آن معًا. ونظرًا إلى أنّ التدريب يقتصر على الفئات المرئيّة فقط دون أي معلومات مُساعِدة من صور أو تقارير سريريّة للفئات غير المرئيّة، نعتقد أنّ CXR-ML-GZSL لديها إمكانات كبيرة لتشخيص الفئات غير المرئيّة عند الاستدلال، خصوصًا للأمراض النادرة أو الناشئة التي تعاني من نقص البيانات المُوسومة. <span id="sec:conclusion" label="sec:conclusion"></span></p>
        </section>
    </div>

    <hr style="margin: 40px 0;">
    <div class="text-muted text-center">
        <small>
            تمّ تحويل هذا الإصدار HTML تلقائيًا من LaTeX.<br>
            تمّ عرض المعادلات الرياضيّة باستخدام MathJax.
        </small>
    </div>
    <!-- Bootstrap JS -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>