<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Leona Hennig, Tanja Tornede, Marius Lindauer">
  <title>نحو استثمار AutoML للتعلُّم العميق المُستدام: نهج تحسين فعّاليّة مُتعدِّد الأهداف على شبكات الإزاحة العميقة</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    html, body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 22px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.7;
    }
    header {
      background: linear-gradient(90deg, #3a8dde 0%, #6dd5ed 100%);
      color: #fff;
      padding: 2.5rem 1.5rem 1.5rem 1.5rem;
      text-align: center;
      border-bottom-left-radius: 40px;
      border-bottom-right-radius: 40px;
      box-shadow: 0 4px 16px rgba(58, 141, 222, 0.08);
      margin-bottom: 2.5rem;
    }
    h1.title {
      font-size: 2.3rem;
      font-weight: 700;
      margin-bottom: 1.2rem;
      letter-spacing: 0.5px;
      line-height: 1.3;
    }
    .author {
      font-size: 1.1rem;
      font-weight: 400;
      margin-top: 0.5rem;
      color: #e3f2fd;
    }
    main.container {
      max-width: 1100px;
      margin: 0 auto;
      padding: 0 1rem 3rem;
    }
    h1, h2, h3 {
      color: #2b5d8c;
      font-weight: 700;
      margin-top: 2.2rem;
      margin-bottom: 1.1rem;
      line-height: 1.2;
    }
    h1 {
      font-size: 2rem;
      border-right: 6px solid #3a8dde;
      padding-right: 0.7rem;
      background: #e3f2fd;
      border-radius: 0 20px 20px 0;
      display: inline-block;
      margin-bottom: 1.5rem;
    }
    h2 {
      font-size: 1.4rem;
      border-right: 4px solid #6dd5ed;
      padding-right: 0.5rem;
      background: #f1f8fe;
      border-radius: 0 14px 14px 0;
      display: inline-block;
      margin-bottom: 1rem;
    }
    h3 {
      font-size: 1.1rem;
      color: #388e3c;
      margin-top: 2rem;
      margin-bottom: 0.7rem;
      border-right: 3px solid #a5d6a7;
      padding-right: 0.4rem;
      background: #e8f5e9;
      border-radius: 0 10px 10px 0;
      display: inline-block;
    }
    p {
      margin: 1.1rem 0;
      text-align: justify;
    }
    ol, ul {
      margin: 1.2rem 2.5rem 1.2rem 0;
      padding-right: 1.5rem;
    }
    li {
      margin-bottom: 0.7rem;
    }
    code, pre {
      font-family: 'Fira Mono', 'Consolas', 'Courier New', monospace;
      background: #f3f6fa;
      color: #1a237e;
      border-radius: 6px;
      padding: 0.2em 0.5em;
      font-size: 0.95em;
    }
    pre {
      display: block;
      padding: 1em;
      overflow-x: auto;
      background: #f3f6fa;
      border: 1px solid #e3eaf2;
      margin: 1.5em 0;
      border-radius: 10px;
    }
    .math.display, .math.inline {
      direction: ltr;
      unicode-bidi: embed;
      font-size: 1.05em;
      background: #f1f8fe;
      padding: 0.2em 0.5em;
      border-radius: 6px;
      margin: 0.5em 0;
      display: inline-block;
    }
    .nodecor {
      text-decoration: none !important;
      color: #1976d2 !important;
      font-weight: 600;
      font-family: inherit;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 2em 0;
      background: #fff;
      border-radius: 10px;
      overflow: hidden;
      box-shadow: 0 2px 8px rgba(58, 141, 222, 0.05);
    }
    th, td {
      border: 1px solid #e3eaf2;
      padding: 0.7em 1em;
      text-align: center;
    }
    th {
      background: #e3f2fd;
      color: #2b5d8c;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f8f9fa;
    }
    a {
      color: #1976d2;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover {
      color: #0d47a1;
    }
    @media (max-width: 800px) {
      html, body {
        font-size: 18px;
      }
      header {
        padding: 1.5rem 0.5rem 1rem 0.5rem;
      }
      h1.title {
        font-size: 1.3rem;
      }
      h1 {
        font-size: 1.1rem;
        padding-right: 0.4rem;
      }
      h2 {
        font-size: 1rem;
        padding-right: 0.3rem;
      }
      h3 {
        font-size: 0.9rem;
        padding-right: 0.2rem;
      }
      table, th, td {
        font-size: 0.95em;
      }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">نحو استثمار <span class="nodecor">AutoML</span> للتعلُّم العميق المُستدام: نهج تحسين فعّاليّة مُتعدِّد الأهداف على شبكات الإزاحة العميقة</h1>
  <p class="author"><span class="nodecor">Leona Hennig</span>, <span class="nodecor">Tanja Tornede</span>, <span class="nodecor">Marius Lindauer</span></p>
</header>
<main class="container">
<h1 id="ملخص">مُلخَّص</h1>
<p>أسهم التعلُّم العميق في تقدُّم العديد من المجالات بفضل قدرته على استخلاص الأنماط المعقّدة من مجموعات بيانات ضخمة. ومع ذلك، فإن المتطلّبات الحاسوبيّة الكبيرة لنماذجه تُثير تحدّيات بيئيّة واقتصاديّة متعلّقة بالموارد. تُقدِّم شبكات الإزاحة العميقة (<span class="nodecor">Deep Shift Neural Networks, DSNN</span>) حَلًّا عبر استغلال عمليات الإزاحة—وخاصةً إزاحة البِتّ—لتخفيض التعقيد الحاسوبي أثناء الاستدلال. واستنادًا إلى رؤى من الشبكات العصبيّة التقليديّة، نركّز على استثمار الإمكانات الكاملة لهذه الشبكات عبر تقنيات <span class="nodecor">AutoML</span>. نَدْرس تأثير تحسين المعاملات الفائقة لتعظيم أداء شبكات الإزاحة مع تقليل استهلاك الموارد. ونظرًا لأن هذا يوازن بين الدقّة واستهلاك الطاقة كهدفين متنافِسَيْن، نقترح الجمع بين تحسين المعاملات الفائقة مُتعدِّد الأوجه والتحسين مُتعدِّد الأهداف. تُظهر النتائج التجريبيّة فعاليّة نهجنا، إذ تقود إلى نماذج تبلغ دقّتها أكثر من <span class="nodecor">80%</span> مع كلفة حاسوبيّة منخفضة. إجمالًا، تُسرِّع طريقتنا تطوير نماذج فعّالة وتمكّن تطبيقات ذكاء اصطناعي مُستدامة.</p>

<h1 id="مقدمة">مقدّمة</h1>
<p>يُعَدّ التعلُّم العميق من أكثر النُهُج وُعودًا لاستخراج المعلومات من مجموعات بيانات كبيرة ذات بُنى معقّدة، بما يشمل إجراء الحسابات في بيئات إنترنت الأشياء وعلى أجهزة الحافّة (<span class="nodecor">DBLP:journals/network/LiOD18</span>, <span class="nodecor">DBLP:journals/pieee/ZhouCLZLZ19</span>). ومع الزيادة المستمرّة في حجم هذه النماذج وأدائها نتيجة التقدّم العلمي والصناعي، ترتفع كلفتها الحاسوبيّة (<span class="nodecor">DBLP:journals/pieee/SzeCYE17</span>). إن تقليل هذه الكلفة يؤثّر مباشرةً في الأثر البيئي للنموذج (<span class="nodecor">DBLP:journals/cacm/SchwartzDSE20</span>)، كما يحرّر موارد لاستخدامها في مهام أخرى، مثل التطبيقات محدودة الموارد (<span class="nodecor">DBLP:journals/corr/HowardZCKWWAA17</span>). يُسهم نهجُنا في توسيع استخدام التعلُّم العميق في هذه البيئات المحدودة الموارد.</p>
<p>تُقدِّم شبكات الإزاحة العميقة إمكانات كبيرة لخفض استهلاك الطاقة مقارنةً بنماذج التعلُّم العميق التقليديّة (<span class="nodecor">DBLP:conf/cvpr/ElhoushiCSTL21</span>). فبدلًا من العمليات العائمة المكثّفة، تستفيد هذه الشبكات من عمليات الإزاحة البِتّية وعكس الإشارة بوصفها وحدةً حسابيّة، ما يُعزّز الكفاءة عبر استبدال عمليات الضرب المكلفة في الشبكات الالتفافيّة. نرى أن تكوين هذه الشبكات يؤثّر بوضوح في الأداء والكفاءة الحاسوبيّة معًا.</p>
<p>من التحدّيات الرئيسة في شبكات الإزاحة العميقة تحديد مستوى الدقّة المناسب لعمليات الإزاحة للحدّ من أخطاء التكميم دون زيادة العبء الحاسوبي زيادةً مُفرِطة. في هذه الدراسة، نستثمر شبكات الإزاحة إلى جانب التعلُّم الآلي المؤتمت لاكتشاف التكوين الأمثل ضمن إطار التعلُّم الآلي الأخضر (<span class="nodecor">DBLP:journals/jair/TornedeTHMWH23</span>). نعتمد على تحسين المعاملات الفائقة باستخدام إطار <span class="nodecor">SMAC3</span> (<span class="nodecor">DBLP:journals/jmlr/LindauerEFBDBRS22</span>)، الذي يؤتمت البحث عن التكوينات المثلى. ويُسهِّل دمج تقنيات التحسين مُتعدِّد الدقّات ومُتعدِّد الأهداف (<span class="nodecor">Belakaria2020-re</span>) استكشاف فضاء المعاملات الفائقة مع إيلاء الأولويّة للأداء واستهلاك الطاقة في آنٍ معًا (<span class="nodecor">Deb2014</span>). ويُتيح تنفيذ <span class="nodecor">SMAC</span> للتحسين مُتعدِّد الأهداف موازنةً فعّالة بين تحقيق دقّة تنبُّئيّة عالية وتقليل استهلاك الطاقة. كما يسمح الجانب مُتعدِّد الدقّات باستخدام الموارد الحاسوبيّة بكفاءة عبر تقييم التكوينات على مستويات متفاوتة من الدقّة. ويوفّر استخدام أدوات مثل <span class="nodecor">CodeCarbon</span> (<span class="nodecor">DBLP:journals/corr/abs-1910-09700</span>, <span class="nodecor">DBLP:journals/corr/abs-1911-08354</span>) خلال مرحلتي التدريب والتقييم رُؤى فوريّة عن استهلاك الطاقة وانبعاثات الكربون لكل تكوين.</p>
<p>بوجهٍ عام، نُقدِّم المساهمات الآتية:</p>
<ol>
  <li><p>فضاء تكوين مُفصَّل لشبكات الإزاحة العميقة،</p></li>
  <li><p>نهج تعلُّم آليّ أخضر لبناء نماذج مُوجَّهة نحو الكفاءة،</p></li>
  <li><p>رُؤى حول قرارات التصميم لتحقيق توازن مثاليّ بين الدقّة وكفاءة الطاقة، و</p></li>
  <li><p>دمج تقنيات التحسين مُتعدِّد الدقّات ومُتعدِّد الأهداف في <span class="nodecor">SMAC</span> لتعزيز فعاليّة التحسين واستخدام الموارد الحاسوبيّة.</p></li>
</ol>

<h1 id="background">الخلفيّة</h1>
<p>يقدّم هذا القسم المفاهيم الأساسيّة لمنهجيّتنا.</p>

<h2 id="شبكات-الإزاحة-العميقة">شبكات الإزاحة العميقة</h2>
<p>شبكات الإزاحة العميقة تُعدّ نهجًا حديثًا لتقليل المتطلّبات الحاسوبيّة والطاقيّة للتعلُّم العميق (<span class="nodecor">DBLP:conf/cvpr/ElhoushiCSTL21</span>). تُحقِّق هذه الشبكات خفضًا كبيرًا في زمن الانتظار عبر تبسيط معماريّة الشبكة باستبدال عمليات الضرب التقليديّة بعمليات الإزاحة البِتّية وعكس الإشارة، ما يجعلها مناسبة لأجهزة الحوسبة محدودة الموارد. وهناك طريقتان شائعتان لتدريب هذه الشبكات: تقنية التكميم (Quantization) وتقنية «قوى 2 والإشارة» (Powers of Two and Sign). تتضمّن الأولى تكميم الأوزان إلى أقرب قوّة للعدد 2 خلال المرورَيْن الأمامي والخلفي، بينما تتيح الثانية تدريب كلٍّ من الأُسّ (قوّة 2) وإشارة الوزن بوصفهما معاملات قابلة للتعلُّم.</p>
<p>في نهج التكميم، نستخرج مصفوفة الإشارة <span class="math inline">\(S\)</span> من مصفوفة الأوزان المدربة <span class="math inline">\(W\)</span>: <span class="math inline">\( S = \mathit{sign}(W) \)</span>. وتكون مصفوفة الأُسّ <span class="math inline">\(P\)</span> هي لوغاريتم الأساس الثنائي للقيم المطلقة لـ<span class="math inline">\(W\)</span>، أي <span class="math inline">\( P = \log_{2}(|W|) \)</span>. بعد تقريب <span class="math inline">\(P\)</span> إلى أقرب عدد صحيح (<span class="math inline">\( P_{\mathit{r}} = \mathit{round}(P) \)</span>)، تُحسب الأوزان المُكمَّمة <span class="math inline">\(\tilde{W}_q\)</span> بتطبيق الإشارة: <span class="math inline">\(\tilde{W}_q = \mathit{flip}(2^{P_{\mathit{r}}}, S)\)</span>. أمّا في تقنية «قوى 2 والإشارة»، فتُعدَّل قِيَم الأُسّ (<span class="math inline">\( \tilde{P} \)</span>) وقِيَم الإشارة (<span class="math inline">\( \tilde{S} \)</span>) مباشرةً، حيث <span class="math inline">\( \tilde{P} = \mathit{round}(P) \)</span> و<span class="math inline">\( \tilde{S} = \mathit{sign}(\mathit{round}(S)) \)</span>، ثم تُحسب الأوزان <span class="math inline">\(\tilde{W}_{\mathit{ps}} = \mathit{flip}(2^{\tilde{P}}, \tilde{S})\)</span>.</p>

<h2 id="hpo">تحسين المعاملات الفائقة</h2>
<p>تزيد تعقيدات خوارزميّات التعلُّم العميق الحاجةَ إلى التحسين المؤتمت للمعاملات الفائقة (<span class="nodecor">HPO</span>) لتعزيز أداء النماذج (<span class="nodecor">DBLP:journals/widm/BischlBLPRCTUBBDL23</span>). لنفترض أنّ لدينا مجموعة بيانات <span class="math inline">\( \mathcal{D} = \{(x_i,y_i)\}_{i=1}^{N} \in \mathbb{D} \subset \mathcal{X} \times \mathcal{Y} \)</span>، حيث <span class="math inline">\( \mathcal{X} \)</span> فضاء المُدخلات و<span class="math inline">\( \mathcal{Y} \)</span> فضاء المُخرجات، ولدينا فضاء تكوين المعاملات الفائقة <span class="math inline">\( \Lambda = \{\lambda_1, \ldots, \lambda_L\} \)</span>. في دراستنا، يمثِّل <span class="math inline">\( \mathcal{M} \)</span> فضاء النماذج الممكنة لـ<span class="nodecor">DSNN</span>. وتقوم الخوارزميّة <span class="math inline">\( \mathcal{A} : \mathbb{D} \times \Lambda \rightarrow \mathcal{M} \)</span> بتدريب نموذج <span class="math inline">\( M \)</span> استنادًا إلى تكوين يحوي <span class="math inline">\(L\)</span> معلمة فائقة مأخوذة من <span class="math inline">\( \Lambda \)</span> على مجموعة التدريب <span class="math inline">\( \mathcal{D}_{\textit{train}} \)</span>. تُقسَّم البيانات إلى مجموعات التدريب والتحقّق والاختبار <span class="math inline">\( \mathcal{D}_{\textit{train}}, \mathcal{D}_{\textit{val}}, \mathcal{D}_{\textit{test}} \)</span>. ويُقيَّم أداء النموذج بدالّة خسارة <span class="math inline">\( \mathcal{L} : \mathcal{M} \times \mathbb{D} \rightarrow \mathbb{R} \)</span>. هدف الـ<span class="nodecor">HPO</span> هو إيجاد التكوين <span class="math inline">\( \lambda^* \in \Lambda \)</span> الذي يُقلِّل هذه الخسارة:</p>
<div class="math display">\[
\lambda^* \in \arg\min_{\lambda \in \Lambda} \mathcal{L}\big(\mathcal{A}(\mathcal{D}_{\textit{train}},\lambda), \mathcal{D}_{\textit{val}}\big).
\]</div>
<p>وبعد اختيار التكوين الأفضل بناءً على أداء مجموعة التحقّق، يُعاد تدريب النموذج النهائي ويُختبَر على <span class="math inline">\(\mathcal{D}_{\textit{test}}\)</span>.</p>

<h2 id="bo">التحسين البايزيّ</h2>
<p>التحسين البايزيّ استراتيجية عالميّة لتحسين دوالّ خسارة صندوقٍ أسود <span class="math inline">\(\mathcal{L}:\mathcal{M}\times\mathbb{D}\to\mathbb{R}\)</span> مُكلفة التقييم (<span class="nodecor">DBLP:journals/jgo/JonesSW98</span>). يُوظِّف نموذجًا بَديلًا احتماليًّا <span class="math inline">\(\mathcal{S}\)</span>—عادةً عملية غاوسيّة أو غابةً عشوائيّة—لتمثيل دالّة الخسارة (<span class="nodecor">DBLP:books/lib/RasmussenW06</span>, <span class="nodecor">DBLP:conf/lion/HutterHL11</span>). وتوجّه دالّة الاقتناء <span class="math inline">\(\mathcal{C}:\Lambda\to\mathbb{R}\)</span> البحثَ عن النقاط التالية، مُوازِنةً بين الاستكشاف والاستغلال استنادًا إلى القيَم المُلاحظة سابقًا <span class="math inline">\(\{(\lambda_i,\mathcal{L}_i)\}_{i=1}^{m-1}\)</span>، ثم يُحدَّث النموذج البَديل بعد تقييم <span class="math inline">\(\mathcal{L}\)</span> عند تلك النقاط.</p>

<h2 id="multi-fidelity">التحسين مُتعدِّد الدقّات</h2>
<p>لتجنّب كُلفة تدريب جميع التكوينات حتى النهاية، نستخدم نهجًا مُتعدِّد الدقّات (<span class="nodecor">MF</span>) (<span class="nodecor">DBLP:journals/jmlr/LiJDRT17</span>) يوازن بين الأداء وخطأ التقريب (<span class="nodecor">DBLP:books/sp/HKV2019</span>). يُخصِّص هذا النهج عددًا قليلًا من الحِقَب لعدد كبير من التكوينات أولًا، ثم يوجّه الميزانيّة تدريجيًّا إلى الأفضل منها. صوريًّا، نُحدِّد مجموعة مستويات الدقّة <span class="math inline">\(\mathcal{F}\)</span> ونهدف إلى تقليل الدالّة عالية الدقّة <span class="math inline">\(F\in\mathcal{F}\)</span>:</p>
<div class="math display">\[
\min_{\lambda\in\Lambda} F(\lambda)\, .
\]</div>
<p>ثم نُقرِّب <span class="math inline">\(F\)</span> بسلسلة من التقريبات الأقل دقّة والأقل كُلفة <span class="math inline">\(\{f_1(\lambda),\ldots,f_j(\lambda)\}\)</span> عبر مستويات متعددة <span class="math inline">\(j\)</span>، حيث تُخصَّص لكل تقييم ميزانيّة تناسب مستواه.</p>

<h2 id="التحسين-متعدد-الأهداف">التحسين مُتعدِّد الأهداف</h2>
<p>يعالج التحسين مُتعدِّد الأهداف مسائل تتضمّن أهدافًا متعارِضة، مثل رفع الدقّة وتقليل استهلاك الطاقة في شبكات الإزاحة العميقة. يهدف إلى إيجاد حلول باريتو المثلى (<span class="nodecor">Deb2014</span>) عبر إضافة نقاط جديدة استنادًا إلى المُشاهدات الحاليّة <span class="math inline">\( \mathcal{D}_\mathit{obs} = \{(\lambda_i,\mathcal{L}(\lambda_i))\}_{i=1}^{n}\)</span>. تُشكِّل هذه النقاط السطح غير المُهيمَن عليه <span class="math inline">\( D^\star_n \)</span>، بحيث لا يمكن تحسين أيّ هدف دون الإضرار بآخر.</p>

<h1 id="approach">المنهج</h1>
<p>لتعزيز كفاءة شبكات الإزاحة العميقة حاسوبيًّا عبر التعلُّم الآلي المؤتمت، نعتمد التحسين مُتعدِّد الدقّات. نقدّم مستويات دقّة مختلفة لعملية التدريب بزيادة عدد طبقات الإزاحة في النموذج تدريجيًّا. تبدأ العملية بنماذج تحتوي على عددٍ محدود من طبقات الإزاحة، ثم نسمح بزيادتها خلال التحسين. نفترض أنّ ذلك يوجّه البحث نحو التكوينات الأعلى أداءً، إذ يُوازِن عددٌ أقل من الطبقات بين دقّة التمثيل وخطأ التقريب. نُحقِّق ذلك عبر توسيع خوارزمية <span class="nodecor">HyperBand</span> (<span class="nodecor">DBLP:journals/jmlr/LiJDRT17</span>)—المعتمدة على «التنصيف التعاقبي» (<span class="nodecor">jamieson-aistats16a</span>)—ضمن إطار مُتعدِّد الأهداف.</p>
<p>كما شرحنا في الخلفيّة، يُدرَّب في «التنصيف التعاقبي» <span class="math inline">\(n_c\)</span> تكوينًا على ميزانيّة أوليّة <span class="math inline">\(b_I\)</span>، ثم يُنتقَى أفضل <span class="math inline">\(\nu/(\nu+1)\)</span> منها لميزانيّة جديدة مقدارها <span class="math inline">\(\nu b_I/(\nu+1)\)</span>، وهكذا حتى نصل إلى أفضل تكوين. وتجمع <span class="nodecor">HyperBand</span> عدّة أقواس من هذه العملية مع توزيع الميزانيّة الإجماليّة عبرها.</p>
<p>نوسِّع ذلك ليشمل التحسين مُتعدِّد الأهداف. نعالج هدفين معًا: دقّة النموذج واستهلاك الطاقة. نُعرِّف دالّة هدف ثنائيّة البُعد:</p>
<div class="math display">\[
f_{MO}:\Lambda\to\mathbb{R}^2,\quad f_{MO}(\lambda) = \big(f_{\text{loss}}(\lambda), f_{\text{emission}}(\lambda)\big),
\]</div>
<p>حيث <span class="math inline">\(f_{\text{loss}}(\lambda)\)</span> تُقلِّل الخسارة لرفع الدقّة، و<span class="math inline">\(f_{\text{emission}}(\lambda)\)</span> تُقلِّل استهلاك الطاقة أثناء التدريب والاستدلال. نرغب في حلّ:</p>
<div class="math display">\[
\arg\min_{\lambda\in\Lambda} f_{MO}(\lambda)\, .
\]</div>
<p>يستخدم <span class="nodecor">SMAC3</span> استراتيجية <span class="nodecor">ParEGO</span> (<span class="nodecor">DBLP:journals/tec/Knowles06</span>) لدمج الأهداف عبر أوزانٍ متغيّرة في كل تكرار ضمن <span class="nodecor">HyperBand</span>، ما يُحوِّل المشكلة مُتعدِّدة الأهداف إلى سلسلة من مسائل أحاديّة الهدف ويُحسِّن تقريب واجهة باريتو.</p>

<h1 id="experiments">التجارب</h1>
<p>نعرض في ما يأتي الإعداد والمنهجيّة المُستخدمة لتقييم المنهج المُقدَّم في قسم المنهج، مع التركيز على تحسين شبكات الإزاحة العميقة عبر التحسين مُتعدِّد الدقّات ومُتعدِّد الأهداف. ونناقش كيف وازنّا بين أداء النموذج وأثره البيئي.</p>

<h2 id="eval-setup">إعداد التقييم</h2>
<p>قمنا بتدريب النماذج وتقييمها على مجموعة بيانات <span class="nodecor">CIFAR-10</span> (<span class="nodecor">krizhevsky2009learning</span>) باستخدام وحدات معالجة الرسوميات <span class="nodecor">NVIDIA A100</span>. ولتحسين المعاملات الفائقة، اعتمدنا على <span class="nodecor">SMAC3</span> (<span class="nodecor">DBLP:journals/jmlr/LindauerEFBDBRS22</span>). ولإدخال الأثر البيئي في الحسبان، استخدمنا متتبّع الانبعاثات <span class="nodecor">CodeCarbon</span> (<span class="nodecor">DBLP:journals/corr/abs-1910-09700</span>, <span class="nodecor">DBLP:journals/corr/abs-1911-08354</span>) لرصد استهلاك الطاقة وانبعاثات الكربون بوحدة <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>. واخترنا شبكة <span class="nodecor">ResNet-20</span> مُدرَّبة مُسبقًا (<span class="nodecor">DBLP:conf/cvpr/HeZRS16</span>).</p>

<h2 id="results">النتائج</h2>
<p>يُوضّح الشكل [fig:results] أعلى دقّة اختبار لـ<span class="nodecor">DSNN</span> بالتكوين الافتراضي (الجدول [table:model_config]) مقارنةً بدقّة <span class="nodecor">DSNN</span> المُكوَّنة عبر <span class="nodecor">SMAC3</span> مع <span class="nodecor">MF</span> باستخدام <span class="nodecor">HyperBand</span> وطبقات إزاحة متفاوتة. وقد جرى تقييم حلّ واجهة باريتو الأمثل رقم <span class="nodecor">1</span> من الجدول [table:model_config] من بين خمسين تكوينًا. وعلى الرغم من بعض التقلّبات، يتجاوز أداء النموذج المُحسَّن دقّة النموذج الافتراضي بما لا يقل عن ثلاثة بالمئة، ما يؤكّد نجاح نهج <span class="nodecor">MF</span>.</p>
<p>ويدعم ذلك افتراضَنا بأن النماذج المُقيَّمة تحت مستويات دقّة مبسّطة تُحافظ على تحسّن الأداء عند إضافة المزيد من طبقات الإزاحة.</p>
<p>يعرض الجدول [table:model_config] حلَّيْن أمثلَيْن من واجهة باريتو من بين <span class="nodecor">33</span> تكوينًا. يحقّق الحل رقم <span class="nodecor">1</span> دقّة اختبار قدرها <span class="nodecor">83.50%</span> مع <span class="nodecor">0.1661</span> <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>، بينما يحقّق الحل رقم <span class="nodecor">2</span> دقّة <span class="nodecor">84.67%</span> مع <span class="nodecor">0.1673</span> <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>—وقد دُرِّبا لعددٍ أقل من الحِقَب بسبب قيودٍ حاسوبيّة. ويُوازِن كلا التكوينَيْن بين أداءٍ عالٍ وانبعاثاتٍ منخفضة.</p>
<p>تشمل المعاملات الرئيسة عدد بِتّات التفعيل، ودقّة الفاصلة العائمة، وعمق طبقات الإزاحة. في الحل رقم <span class="nodecor">1</span> يوجد عدد أقل من طبقات الإزاحة مع عدد محدود من البِتّات لتمثيل الأوزان، ما يُخفِّض الطلب الحاسوبي مع الحفاظ على انبعاثاتٍ مماثلة للحل رقم <span class="nodecor">2</span> الذي يستخدم طبقاتٍ أكثر وبِتّاتٍ أكثر. ويُظهِر هذا التوازن أنّ التحكُّم في عدد طبقات الإزاحة وعدد البِتّات يُفضي إلى تكوينات متوازنة عالية الأداء وفعّالة طاقيًّا، بما يدعم جدوى نهج <span class="nodecor">MFMO</span>.</p>

<h1 id="conclusion">الخلاصة</h1>
<p>قدّمنا في هذا العمل نهجًا أخضر للتعلُّم الآلي المؤتمت يستهدف تحسينًا مُستدامًا لشبكات الإزاحة العميقة عبر إطار تحسين مُتعدِّد الدقّات ومُتعدِّد الأهداف، سعيًا إلى تحقيق توازنٍ دقيق بين قدرات التعلُّم العميق والاستدامة البيئيّة. وبإدراج أثر الطاقة كهدف، نجحنا في توجيه تحسين المعاملات الفائقة نحو دقّةٍ عالية مع انخفاض استهلاك الموارد.</p>
<p>أظهرت نتائجنا إمكانات النهج في تحسين شبكة إزاحة عميقة لتحقيق دقّة مرتفعة وتقليل الانبعاثات. وقد قدّمنا فضاء تكوين شاملًا، ونهجًا أخضر لتطوير النموذج، ورُؤى بشأن قرارات التصميم. مستقبلًا، سنوسّع التطبيق ليشمل مقاييس إضافيّة وبُنى متنوّعة للتحقّق من عموميّة المنهج، وضبط التفاعل بين <span class="nodecor">ParEGO</span> و<span class="nodecor">HyperBand</span> بطرائق أكثر فعاليّة، فضلًا عن دراسة مستويات دقّة مختلفة وخوارزميّات مُتعدِّدة الأهداف لتحقيق مزيدٍ من خفض الانبعاثات.</p>

<h3 id="الشكر-والتقدير" class="unnumbered">الشكر والتقدير</h3>
<p>دعم هذا العملَ وزارةُ البيئة الاتحاديّة الألمانيّة لحماية الطبيعة والسلامة النوويّة وحماية المستهلك (مشروع <span class="nodecor">GreenAutoML4FAS</span> رقم <span class="nodecor">67KI32007A</span>).</p>
</main>
</body>
</html>