<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Leona Hennig, Tanja Tornede, Marius Lindauer">
  <title>نحو استغلال AutoML للتعلم العميق المستدام: نهج تحسين الفعالية متعددة الأهداف على شبكات العصبونات العميقة المتغيرة</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">نحو استغلال <span class="nodecor">AutoML</span> للتعلم العميق المستدام: نهج تحسين الفعالية متعددة الأهداف على شبكات العصبونات العميقة المتغيرة</h1>
<p class="author"><span class="nodecor">Leona Hennig</span>, <span class="nodecor">Tanja Tornede</span>, <span class="nodecor">Marius Lindauer</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">مُلَخَّص</h1>
<p>ساهم التعلم العميق في تقدم العديد من المجالات من خلال استخراج الأنماط المعقدة من مجموعات البيانات الكبيرة. ومع ذلك، فإن المتطلبات الحسابية لنماذج التعلم العميق تطرح تحديات بيئية ومواردية. تقدم شبكات العصبونات العميقة المتغيرة حلاً من خلال استغلال عمليات الإزاحة لتقليل التعقيد الحسابي أثناء الاستدلال. استناداً إلى الرؤى المستخلصة من الشبكات العصبونية العميقة التقليدية، نحن مهتمون باستغلال الإمكانات الكاملة لشبكات العصبونات العميقة المتغيرة عبر تقنيات <span class="nodecor">AutoML</span>. ندرس تأثير تحسين المعلمات الفائقة لتعظيم أداء شبكات العصبونات العميقة المتغيرة مع تقليل استهلاك الموارد. ونظراً لأن هذا يجمع بين التحسين متعدد الأهداف مع الدقة واستهلاك الطاقة كأهداف متكاملة محتملة، نقترح دمج تحسين المعلمات الفائقة متعدد الأوجه مع التحسين متعدد الأهداف. تظهر النتائج التجريبية فعالية نهجنا، مما يؤدي إلى نماذج بدقة تتجاوز <span class="nodecor">80%</span> وتكلفة حسابية منخفضة. بشكل عام، تسرّع طريقتنا تطوير النماذج الفعالة مع تمكين تطبيقات الذكاء الاصطناعي المستدامة.</p>
<h1 id="مُقَدِّمَةِ">مقدمة</h1>
<p>يُعَد التعلم العميق من أكثر النهج الواعدة لاستخراج المعلومات من مجموعات البيانات الكبيرة ذات الهياكل المعقدة. ويشمل ذلك إجراء الحسابات في بيئات إنترنت الأشياء وعلى أجهزة الحافة (<span class="nodecor">DBLP:journals/network/LiOD18</span>, <span class="nodecor">DBLP:journals/pieee/ZhouCLZLZ19</span>). مع الزيادة المستمرة في حجم وأداء هذه النماذج نتيجة التقدم في العلوم والصناعة، يرتبط بها تكلفة حسابية (<span class="nodecor">DBLP:journals/pieee/SzeCYE17</span>). تقليل هذه التكلفة يؤثر مباشرة على الأثر البيئي للنموذج (<span class="nodecor">DBLP:journals/cacm/SchwartzDSE20</span>). وبالتالي، يتم تحرير الموارد ويمكن استخدامها لمهام أخرى، مثل تلك التي تعاني من قيود الموارد (<span class="nodecor">DBLP:journals/corr/HowardZCKWWAA17</span>). من خلال نهجنا، نساهم في التعلم العميق في هذه البيئات محدودة الموارد.</p>
<p>تقدم شبكات العصبونات العميقة المتغيرة إمكانات كبيرة في تقليل استهلاك الطاقة مقارنة بنماذج التعلم العميق التقليدية (<span class="nodecor">DBLP:conf/cvpr/ElhoushiCSTL21</span>). بدلاً من الحسابات العائمة، تستفيد من عمليات الإزاحة - وتحديداً، إزاحة البت - كوحدة حسابية، مما يعزز الكفاءة من خلال استبدال عمليات الضرب المكلفة في الشبكات التلافيفية. نشك في أن تكوين شبكات العصبونات العميقة المتغيرة له تأثير كبير على كل من الأداء والكفاءة الحسابية.</p>
<p>أحد التحديات الرئيسية مع شبكات العصبونات العميقة المتغيرة هو تحديد مستوى الدقة المناسب لعمليات الإزاحة لتقليل أخطاء الكمية دون زيادة العبء الحسابي بشكل مفرط. في هذه الدراسة، نستفيد من شبكات العصبونات العميقة المتغيرة بالاشتراك مع التعلم الآلي الآلي للعثور على التكوين المثالي لشبكات العصبونات العميقة المتغيرة في إطار التعلم الآلي الأخضر (<span class="nodecor">DBLP:journals/jair/TornedeTHMWH23</span>). يتم تحقيق ذلك من خلال تحسين المعلمات الفائقة باستخدام إطار العمل <span class="nodecor">SMAC3</span> الذي اقترحه (<span class="nodecor">DBLP:journals/jmlr/LindauerEFBDBRS22</span>)، والذي يؤتمت البحث عن التكوينات النموذجية المثلى. يسهّل دمج تقنيات التحسين متعددة الأمانة ومتعددة الأهداف (<span class="nodecor">Belakaria2020-re</span>) استكشافاً مثالياً لمساحة المعلمات الفائقة التي تعطي الأولوية للأداء واستهلاك الطاقة معاً (<span class="nodecor">Deb2014</span>). تنفيذ <span class="nodecor">SMAC</span> للتحسين متعدد الأهداف يوازن بفعالية بين تحقيق دقة تنبؤية عالية وتقليل استهلاك الطاقة. يسمح الجانب متعدد الأمانة باستخدام الموارد الحسابية بكفاءة من خلال تقييم التكوينات على مستويات متفاوتة من التفصيل. استخدام أدوات مثل <span class="nodecor">CodeCarbon</span> (<span class="nodecor">DBLP:journals/corr/abs-1910-09700</span>, <span class="nodecor">DBLP:journals/corr/abs-1911-08354</span>) خلال مراحل التدريب والتقييم يوفر رؤى في الوقت الفعلي حول استهلاك الطاقة وانبعاثات الكربون المرتبطة بكل تكوين نموذج.</p>
<p>بشكل عام، نقدم المساهمات التالية:</p>
<ol>
<li><p>مساحة تكوين محددة لشبكات العصبونات العميقة المتغيرة،</p></li>
<li><p>نهج التعلم الآلي الأخضر لبناء النماذج الموجهة نحو الكفاءة،</p></li>
<li><p>رؤى حول قرارات التصميم للحصول على توازنات مثالية بين الدقة وكفاءة الطاقة، و</p></li>
<li><p>دمج تقنيات التحسين متعددة الأهداف ومتعددة الأمانة في <span class="nodecor">SMAC</span> لتحسين أداء التحسين واستخدام الموارد الحسابية.</p></li>
</ol>
<h1 id="background">الخلفية</h1>
<p>يقدم هذا الفصل المفاهيم الأساسية لمنهجيتنا.</p>
<h2 id="شبكات-العصبونات-العميقة-المتغيرة">شبكات العصبونات العميقة المتغيرة</h2>
<p>شبكات العصبونات العميقة المتغيرة هي نهج جديد لتقليل المتطلبات الحسابية والطاقية للتعلم العميق (<span class="nodecor">DBLP:conf/cvpr/ElhoushiCSTL21</span>). تحقق هذه الشبكات تخفيضاً كبيراً في زمن الانتظار من خلال تبسيط هندسة الشبكة بحيث تستبدل عمليات الضرب التقليدية في الشبكات العصبية بعمليات التحويل البتّي وقلب الإشارة، مما يجعل شبكات العصبونات العميقة المتغيرة مناسبة لأجهزة الحوسبة ذات الموارد المحدودة. هناك طريقتان لتدريب شبكات العصبونات العميقة المتغيرة (<span class="nodecor">DBLP:conf/cvpr/ElhoushiCSTL21</span>): تقنية التكميم (Quantization) وتقنية القوى الثنائية والإشارة (Powers of two and Sign). تقنية التكميم تتضمن تدريب الأوزان العادية المقيدة إلى قوى الاثنين من خلال تكميم الأوزان إلى أقرب قوة اثنين خلال المرورين الأمامي والخلفي. تقنية القوى الثنائية والإشارة تتضمن مباشرة قيم التحويل وقلب الإشارة كمعاملات قابلة للتدريب.</p>
<p>يتم الحصول في نهج تقنية التكميم على مصفوفة الإشارة <span class="math inline">\(S\)</span> من مصفوفة الوزن المدربة <span class="math inline">\(W\)</span> كما يلي: <span class="math inline">\( S = \mathit{sign}(W) \)</span>. مصفوفة القوة <span class="math inline">\(P\)</span> هي لوغاريتم الأساس الثنائي للقيم المطلقة لـ<span class="math inline">\(W\)</span>، أي <span class="math inline">\( P = \log_{2}(|W|) \)</span>. بعد تقريب <span class="math inline">\(P\)</span> إلى أقرب قوة اثنين، <span class="math inline">\( P_{\mathit{r}} = \mathit{round}(P) \)</span>، يتم حساب الأوزان المكممة <span class="math inline">\(\tilde{W}_q\)</span> بتطبيق الإشارة من <span class="math inline">\(S\)</span>، كما هو موضح <span class="math inline">\(\tilde{W}_q = \mathit{flip}(2^{P_{\mathit{r}}}, S)\)</span>. يعمل نهج تقنية القوى الثنائية والإشارة على تحسين أوزان الشبكة العصبية من خلال تكييف قيم التحويل (<span class="math inline">\( \tilde{P} \)</span>) وقلب الإشارة (<span class="math inline">\( \tilde{S} \)</span>) مباشرة. يتم الحصول على مصفوفة التحويل <span class="math inline">\( \tilde{P} \)</span> بتقريب لوغاريتم الأساس الثنائي لقيم الوزن، <span class="math inline">\( \tilde{P} = \mathit{round}(P) \)</span>، ويتم حساب قلب الإشارة <span class="math inline">\( \tilde{S} \)</span> كـ <span class="math inline">\( \tilde{S} = \mathit{sign}(\mathit{round}(S)) \)</span>. يتم حساب الأوزان كـ <span class="math inline">\(\tilde{W}_{\mathit{ps}} = \mathit{flip}(2^{\tilde{P}}, \tilde{S})\)</span>، حيث تعين عملية قلب الإشارة <span class="math inline">\( \tilde{S} \)</span> القيم <span class="math inline">\(-1\)</span>، <span class="math inline">\(0\)</span>، أو <span class="math inline">\(+1\)</span> بناءً على <span class="math inline">\( s \)</span>.</p>
<h2 id="hpo">تحسين المعلمات الفائقة</h2>
<p>تزيد تعقيدات خوارزميات التعلم العميق من الحاجة إلى تحسين المعلمات الفائقة المؤتمت (<span class="nodecor">HPO</span>) لزيادة أداء النموذج (<span class="nodecor">DBLP:journals/widm/BischlBLPRCTUBBDL23</span>). لنفترض لدينا مجموعة بيانات <span class="math inline">\( \mathcal{D} = \{(x_i,y_i)\}_{i=1}^{\textnormal{N}} \in \mathbb{D} \subset \mathcal{X} \times \mathcal{Y} \)</span>، حيث <span class="math inline">\( \mathcal{X} \)</span> هو فضاء الحالات و<span class="math inline">\( \mathcal{Y} \)</span> هو فضاء الهدف، وفضاء تكوين المعلمات الفائقة <span class="math inline">\( \Lambda = \{\lambda_1, \ldots, \lambda_L\} \)</span>، <span class="math inline">\( L \in \mathbb{N} \)</span>. في دراستنا، <span class="math inline">\( \mathcal{M} \)</span> يدل على فضاء النماذج الممكنة لـ <span class="nodecor">DSNN</span>. خوارزمية <span class="math inline">\( \mathcal{A} : \mathbb{D} \times \Lambda \rightarrow \mathcal{M} \)</span> تدرب نموذج <span class="math inline">\( M \in \mathcal{M} \)</span>، مستندة إلى تكوين من <span class="math inline">\( L \)</span> معلمات فائقة مأخوذة من <span class="math inline">\( \Lambda \)</span>، على مجموعة فرعية للتدريب من <span class="math inline">\( \mathcal{D} \)</span>. يتم تقسيم مجموعة البيانات <span class="math inline">\( \mathcal{D} \)</span> إلى مجموعات التدريب، التحقق، والاختبار: <span class="math inline">\( \mathcal{D}_{\textit{train}}, \mathcal{D}_{\textit{val}}, \)</span> و <span class="math inline">\( \mathcal{D}_{\textit{test}} \)</span> على التوالي. يتم تقييم أداء الخوارزمية من خلال دالة خسارة مخصصة للتقييم <span class="math inline">\( \mathcal{L} : \mathcal{M} \times \mathbb{D} \rightarrow \mathbb{R} \)</span>. هدف التحسين لـ <span class="nodecor">HPO</span> هو إيجاد التكوين <span class="math inline">\( \lambda^* \in \Lambda \)</span> بأقل خسارة تحقق <span class="math inline">\( \mathcal{L} \)</span>، بحيث: <span class="math display">\[\lambda^* \in \argmin_{\lambda \in \Lambda} \mathcal{L}\big(\mathcal{A}(\mathcal{D}_{\textit{train}},\lambda), \mathcal{D}_{\textit{val}}\big).\]</span> هذه العملية تعدل المعلمات الفائقة بناءً على أداء مجموعة التحقق. يتم تدريب النماذج على <span class="math inline">\(\mathcal{D}_{\textit{train}}\)</span> ويتم تحسينها باستخدام <span class="math inline">\(\mathcal{D}_{\textit{val}}\)</span>. وأخيراً، يتم تقييم أداء النموذج على <span class="math inline">\(\mathcal{D}_{\textit{test}}\)</span>.</p>
<h2 id="bo">التحسين البايزي</h2>
<p>التحسين البايزي هو استراتيجية للتحسين العالمي لدوال الخسارة الصندوق الأسود <span class="math inline">\(\mathcal{L}:\mathcal{M}\times\mathbb{D}\xrightarrow{}\mathbb{R}\)</span> التي تكون مكلفة في التقييم (<span class="nodecor">DBLP:journals/jgo/JonesSW98</span>). يستخدم التحسين البايزي نموذجاً بديلاً <span class="math inline">\(\mathcal{S}\)</span>، وهو نموذج احتمالي لتقريب دالة الخسارة، والذي يُعطى عادة بواسطة عملية غاوسية أو غابة عشوائية (<span class="nodecor">DBLP:books/lib/RasmussenW06</span>, <span class="nodecor">DBLP:conf/lion/HutterHL11</span>). توجه دالة الاستحواذ <span class="math inline">\(\mathcal{C}:\Lambda\xrightarrow{}\mathbb{R}\)</span> البحث عن نقاط التقييم المثلى التالية من خلال موازنة الاستكشاف والاستغلال، استناداً إلى مجموعة النقاط التي تم استعلامها سابقاً <span class="math inline">\(\{(\lambda_1,\mathcal{L}_1),...,(\lambda_{m-1},\mathcal{L}_{m-1})\}\)</span> في الوقت <span class="math inline">\(m\)</span>. يتم تقييم <span class="math inline">\(\mathcal{L}\)</span> فقط في نقاط معينة، ويتم بها تحديث النموذج البديل.</p>
<h2 id="multi-fidelity">تحسين متعدد الدقة</h2>
<p>نظراً لأنه لا يمكن تدريب تكوينات متعددة من الشبكات العصبية العميقة المتقدمة بشكل كامل للمقارنة بسبب الكلفة الحسابية، فإننا نستخدم نهجاً متعدد الدقة (<span class="nodecor">MF</span>) (<span class="nodecor">DBLP:journals/jmlr/LiJDRT17</span>)، وهو استراتيجية شائعة في التعلم الآلي الآلي للتنقل بين التوازن بين الأداء وخطأ التقريب (<span class="nodecor">DBLP:books/sp/HKV2019</span>). تدرب النهج متعددة الدقة نماذج بديلة سهلة التقييم للوظائف الصندوق الأسود تتبع مبادئ توجيهية مختلفة، مثل تخصيص عدد قليل من العصور للعديد من التكوينات في البداية وتدريب الأفضل أداءً على عدد متزايد من العصور. رسمياً، نحدد مجالاً للدقة <span class="math inline">\(\mathcal{F}\)</span> ونهدف إلى تقليل وظيفة عالية الدقة <span class="math inline">\(F\in\mathcal{F}\)</span> (<span class="nodecor">DBLP:journals/jair/KandasamyDOSP19</span>): <span class="math display">\[\min_{\lambda\in\Lambda} F(\lambda)\, .\]</span> نهدف إلى تقريب <span class="math inline">\(F\in\mathcal{F}\)</span>، باستخدام سلسلة من التقريبات أقل دقة وأقل تكلفة <span class="math inline">\(\{f(\lambda)_1,\ldots,f(\lambda)_j\}\)</span>، حيث يشير <span class="math inline">\(j\)</span> إلى العدد الإجمالي لمستويات الدقة. يُشار إلى الموارد المخصصة لتقييم أداء نموذج في دقات مختلفة بميزانيتها. يُفترض <span class="nodecor">MF</span> أن أعلى الدقات تقرب الوظيفة الصندوق الأسود بشكل أفضل.</p>
<h2 id="التحسين-متعدد-الأهداف">التحسين متعدد الأهداف</h2>
<p>يتناول التحسين متعدد الأهداف مشكلات تشمل أهدافاً متعددة، غالباً ما تكون متنافسة. يُستخدم هذا النهج في السيناريوهات التي يجب فيها التنقل بين تنازلات بين هدفين أو أكثر متعارضين، مثل، في سياق شبكات الدماغ العصبية العميقة المتقدمة، تحسين الدقة جنباً إلى جنب مع تقليل استهلاك الطاقة. يهدف التحسين متعدد الأهداف إلى تحديد الحلول المثلى باريتو (<span class="nodecor">Deb2014</span>). لضمان تقريب فعال لجبهة باريتو، تتم إضافة نقاط جديدة استناداً إلى مجموعة البيانات الحالية للملاحظات <span class="math inline">\( \mathcal{D}_\mathit{obs} = \{(\lambda_1,\mathcal{L}(\lambda_1)), \ldots, (\lambda_n,\mathcal{L}(\lambda_n))\} \)</span> في الوقت <span class="nodecor">n</span>. تعزز هذه النقاط السطح الذي تشكله مجموعة الحلول غير المهيمنة <span class="math inline">\( D^\star_n \)</span>، والتي تلبي الشرط لمتغيرات الهدف <span class="nodecor">d</span> ودالة الخسارة <span class="math inline">\(\mathcal{L} = (\mathcal{L}_1,\ldots,\mathcal{L}_d)\)</span> (<span class="nodecor">DBLP:journals/corr/abs-1905-02370</span>): لجميع <span class="math inline">\(\lambda, (\lambda,\mathcal{L}(\lambda)) \in \mathcal{D}^\star_n \subset \mathcal{D}_n\)</span> و <span class="math inline">\(\lambda&#39;, (\lambda&#39;,\mathcal{L}(\lambda&#39;)) \in \mathcal{D}_n\)</span> يوجد <span class="math inline">\(k \in \{1,\ldots,d\}\)</span>، بحيث <span class="math inline">\(\mathcal{L}_k(\lambda) \leq \mathcal{L}_k(\lambda&#39;)\)</span>.</p>
<h1 id="approach">المنهج</h1>
<p>لتعزيز الشبكات العصبية المتغيرة العميقة (Deep Shift Neural Networks) حسابياً عبر التعلم الآلي الآلي، نستخدم تحسين الأمانة المتعددة. نقدم مستويات أمانة مختلفة لعملية التدريب من خلال زيادة عدد طبقات التحويل في النموذج. ستشمل النماذج المدربة في البداية عدداً أقل من طبقات التحويل، والتي ستزداد خلال عملية التحسين. نفترض أن هذا سيوجه البحث نحو الشبكات المتغيرة الأعلى أداءً، حيث أن عدداً قليلاً من طبقات التحويل يقدم أقل عدم دقة رقمية وشكوك رياضية للنموذج. نهدف إلى التحقيق فيما إذا كان اختيار النماذج الأعلى أداءً تحت انخفاض الشكوك الذاتية للنموذج يستمر في إظهار تحسن في الأداء عند دمجها مع المزيد من طبقات التحويل.</p>
<p>في القسم [background]، شرحنا المنهج متعدد الأمانة. إحدى الخوارزميات التي تصيغ قواعدها هي الإنقاص التدريجي (<span class="nodecor">jamieson-aistats16a</span>)، حيث يتم تدريب <span class="math inline">\(n_c\)</span> تكوينات على ميزانية صغيرة أولاً <span class="math inline">\(b_I\)</span>. بعد ذلك، يتم تدريب أفضل <span class="math inline">\(\nu/(\nu+1)\)</span> تكوينات أداءً على ميزانية <span class="math inline">\(\nu b_I/(\nu +1)\)</span> حتى يتم تحديد أفضل تكوين. نتناول التوازن بين <span class="math inline">\(b_I\)</span> و <span class="math inline">\(n_c\)</span>، أو بين خطأ التقريب والاستكشاف المتأصل في الإنقاص التدريجي، باستخدام خوارزمية HyperBand للتحسين متعدد الأمانة. تقوم HyperBand (<span class="nodecor">DBLP:journals/jmlr/LiJDRT17</span>) بتشغيل الإنقاص التدريجي في أقواس متعددة، حيث يوفر كل قوس مجموعة من <span class="math inline">\(n_c\)</span> وجزء من الميزانية الإجمالية لكل تكوين بحيث تتجمع إلى الميزانية الكلية.</p>
<p>نوسع هذا إلى تحسين متعدد الأمانة ومتعدد الأهداف. نعالج دقة النموذج وكذلك استهلاكه للطاقة في آن واحد. الهدف هو تحسين أداء الشبكات العصبية المتغيرة العميقة، مع ضمان تحقيقها لدقة عالية ومتانة في قدراتها التنبؤية؛ وثانياً، لتقليل استهلاك الطاقة، وهو عامل حاسم بالنظر إلى الآثار البيئية للكفاءة الحسابية. لهذا الغرض، ننفذ دالة هدف ثنائية الأبعاد <span class="math inline">\(f_{MO}:\Lambda\xrightarrow{}\mathbb{R}^2,\quad f_{M0}(\lambda) = \big(f_{loss}(\lambda), f_{emission}(\lambda)\big)\)</span>، حيث، بالنظر إلى تكوين <span class="math inline">\(\lambda\in\Lambda\)</span>، <span class="math inline">\(f_{loss}(\lambda)\)</span> تهدف إلى تقليل الخسارة، مما يعزز دقة النموذج، و<span class="math inline">\(f_{emission}(\lambda)\)</span> تسعى إلى تقليل استهلاك الطاقة أثناء التدريب والاستدلال، مما يعزز الاستدامة البيئية. نهدف إلى حل مشكلة التحسين التالية:</p>
<p><span class="math display">\[\argmin_{\lambda\in\Lambda} f_{M0}(\lambda)\, .\]</span></p>
<p>يستخدم SMAC3 استراتيجية تجميع المتوسط الحسابي بحساب المتوسط الحسابي للهدف لتجميع أهداف متعددة في قيمة عددية واحدة للتحسين متعدد الأمانة. نستخدم خوارزمية ParEGO (<span class="nodecor">DBLP:journals/tec/Knowles06</span>) لحساب الأهداف المثلى باريتو. تُحوَّل المشكلة متعددة الأهداف إلى سلسلة من المشكلات أحادية الهدف من خلال إدخال معاملات وزن متغيرة للأهداف في كل تكرار من HyperBand. وبالتالي تحسين تقريب مختلف للواجهة الأمامية باريتو في كل تقييم. يمكن الآن تقييم دالة التحسين أحادية الهدف الناتجة في إعداد التحسين متعدد الأمانة.</p>
<h1 id="experiments">التجارب</h1>
<p>في القسم التالي، نوضح الإعداد والمنهجية المستخدمة لتقييم منهجنا المقدم في القسم [approach]، مع التركيز على تحسين شبكات العصبونات العميقة المتغيرة من خلال التحسين متعدد الأمانة ومتعدد الأهداف بالإضافة إلى التحسين متعدد الأمانة. نناقش كيف نجح منهجنا في الموازنة بين أداء النموذج والتأثير البيئي.</p>
<h2 id="eval setup">إعداد التقييم</h2>
<p>نقوم بتدريب وتقييم نماذجنا على مجموعة بيانات Cifar10 (<span class="nodecor">krizhevsky2009learning</span>)، باستخدام وحدات معالجة الرسومات NVIDIA A100. لتحسين المعلمات الفائقة، نعتمد على استخدام SMAC3 (<span class="nodecor">DBLP:journals/jmlr/LindauerEFBDBRS22</span>). لدمج الأثر البيئي في سير عمل تحسين المعلمات الفائقة لدينا، نستخدم متتبع انبعاثات CodeCarbon (<span class="nodecor">DBLP:journals/corr/abs-1910-09700</span>, <span class="nodecor">DBLP:journals/corr/abs-1911-08354</span>) لتتبع انبعاثات الكربون من العمليات الحسابية من خلال مراقبة استهلاك الطاقة ومزيج الطاقة الإقليمي بوحدة <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>، غرامات من مكافئ <span class="math inline">\(\mathit{CO}_2\)</span>. اخترنا شبكة ResNet20 المدربة مسبقاً (<span class="nodecor">DBLP:conf/cvpr/HeZRS16</span>).</p>
<h2 id="results">النتائج</h2>
<p>يعرض الشكل [fig:results] أعلى دقة اختبار لـ DSNN المدربة باستخدام التكوين الافتراضي من الجدول [table:model_config] كما هو محدد بواسطة (<span class="nodecor">DBLP:conf/cvpr/ElhoushiCSTL21</span>)، بالإضافة إلى دقة الاختبار من DSNN المكونة وفقاً لـ SMAC3 مع MF باستخدام HyperBand، مع عدد طبقات الانتقال التي تحدد الدقة. يتم استخدام تكوين الحل الأمثل باريتو رقم <span class="nodecor">1</span> من الجدول [table:model_config]. تحدد خوارزمية MF، المدربة على نفس البذرة كـ MFMO، أنه التكوين الأمثل للنموذج من بين خمسين تكويناً تم تقييمها. على الرغم من بعض التقلبات، فإن الرسم البياني يظهر بوضوح أن أداء النموذج مع التكوين الأمثل يتجاوز أداء النموذج الأصلي بالتكوين الافتراضي بما لا يقل عن ثلاثة في المئة. هذا يؤكد نهجنا بأن MF يمكن تنفيذها بنجاح مع ميزانيات DSNN المحددة مثل عمق الانتقال.</p>
<p>علاوة على ذلك، يؤكد ذلك افتراضنا من القسم [approach] بأن النماذج يمكن تقييمها تحت انخفاض الشكوك الذاتية للنموذج وتستمر في الأداء بشكل جيد عند تنفيذها مع المزيد من طبقات الانتقال.</p>
<p>يظهر الجدول [table:model_config] حلين أمثلين باريتو من <span class="nodecor">33</span> تكويناً تم تقييمها باستخدام نهجنا MFMO. يؤدي تقييم الحل رقم <span class="nodecor">1</span> إلى دقة أعلى <span class="nodecor">1</span> بنسبة <span class="nodecor">83.50</span>% و <span class="nodecor">0.1661</span> <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>. يحقق الحل رقم <span class="nodecor">2</span> دقة أعلى <span class="nodecor">1</span> بنسبة <span class="nodecor">84.67</span>%، مما ينتج عنه <span class="nodecor">0.1673</span> <span class="math inline">\(g\mathit{CO}_{2}\mathit{eq}\)</span>. لاحظ أنه تم تدريبهما لفترات أقل بسبب القيود الحسابية. كلا التكوينين يحققان نتائج أداء جيدة مع الحفاظ على انخفاض الانبعاثات.</p>
<p>تشمل المعلمات النوعية لنموذج DSNN بتات التنشيط الصحيحة، بتات كسر التنشيط، بتات الوزن، وعمق الانتقال. يشتمل الحل رقم <span class="nodecor">1</span> على عدد أقل من طبقات الانتقال مع عدد أقل من البتات التي تمثل قيم وظيفة تنشيط الأوزان. يبدو أن بتات التنشيط وبتات الوزن تؤثر على الانبعاثات وأداء النموذج بشكل مماثل. تزيد طبقات الانتقال الأقل من الطلب الحسابي للنموذج. ومع ذلك، يبدو أن العدد المنخفض من البتات المستخدمة لاستبدال العمليات العائمة (FLOPs) يعوض عن ذلك لأن الحل رقم <span class="nodecor">1</span> ينتج كمية تقريباً مماثلة من الانبعاثات كما في الحل رقم <span class="nodecor">2</span>. هذا الأخير يحتوي على أكثر من ثلاثة أضعاف عدد طبقات الانتقال، باستخدام أربع طبقات تلافيفية فقط مع FLOPs، ولكنه يستخدم عدداً كبيراً من بتات التمثيل. يبدو أن هناك توازناً بين عدد طبقات الانتقال وعدد بتات التمثيل التي، عند إدارتها بكفاءة، تؤدي إلى تكوينات نموذج ذات أداء جيد واستهلاك منخفض للطاقة. هذا يدعم ادعاءنا بأن نهج MFMO لدينا واعد لبناء DSNNs عالية الأداء وفعالة من حيث الطاقة.</p>
<h1 id="conclusion">الخلاصة</h1>
<p>في هذا العمل، نقدم نهجنا الأخضر لتعلم الآلة الآلي نحو تحسين مستدام لشبكات العصبونات العميقة المتسلسلة من خلال إطار عمل تحسين هدف متعدد الأوجه ومتعدد الدقة. نتناول التقاطع الحرج بين تقدم قدرات التعلم العميق والاستدامة البيئية. من خلال استخدام أدوات تعلم الآلة الآلي ودمج الأثر البيئي كهدف، ننتقل عبر التوازن بين أداء النموذج واستخدام الموارد بكفاءة.</p>
<p>تسلط نتائج تجاربنا الضوء على إمكانات نهجنا. لقد نجحنا في تحسين شبكة عصبونات عميقة متسلسلة لتحقيق دقة عالية مع تقليل استهلاك الطاقة. لقد قدمنا مساحة تكوين شاملة لشبكات العصبونات العميقة المتسلسلة، وأدخلنا نهج تعلم الآلة الآلي الأخضر لتطوير النموذج الموجه نحو الكفاءة، وقدمنا رؤى قيمة حول قرارات التصميم. تشمل الأعمال المستقبلية توسيع نهجنا إلى معايير متعددة وهياكل شبكة عصبونية للتحقق بشكل كافٍ من نهجنا على طيف واسع من تصاميم النماذج والتطبيقات. بهذه الطريقة، نأمل في الحصول على مزيد من الرؤى حول قرارات التصميم الحاسمة لشبكات العصبونات العميقة المتسلسلة والنماذج الأخرى المصممة للحوسبة الموفرة للطاقة. إعادة النظر في تنفيذنا للهدف متعدد الأوجه ومتعدد الدقة لإيجاد طريقة أكثر كفاءة لتداخل ParEGO وHyperband، على سبيل المثال بإيجاد طريقة أكثر فعالية لتعيين الميزانيات والأوزان للتكوينات، يخضع لمزيد من العمل لتخفيف العبء الحسابي عند حساب جبهات باريتو للهدف متعدد الأوجه ومتعدد الدقة. علاوة على ذلك، سنحقق في أنواع الدقة المحددة لشبكات العصبونات العميقة المتسلسلة وخوارزميات الهدف المتعدد لتحقيق مزيد من التخفيضات في انبعاثات النموذج.</p>
<h3 id="الشكر-والتقدير" class="unnumbered">الشكر والتقدير</h3>
<p>لقد دعم هذا العمل وزارة البيئة الاتحادية الألمانية لحماية الطبيعة والسلامة النووية وحماية المستهلك (مشروع GreenAutoML4FAS رقم <span class="nodecor">67KI32007A</span>).</p>
</body>
</html>
