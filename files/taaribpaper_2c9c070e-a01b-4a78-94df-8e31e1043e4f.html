<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Cevat V. Karadağ و Nezih Topaloğlu">
  <title>تدريب الشبكات العصبية المُقسَّمة باستخدام التسميات الوسيطة الاصطناعية</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 22px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.8;
    }
    header {
      background: linear-gradient(90deg, #3a6073 0%, #16222a 100%);
      color: #fff;
      padding: 40px 0 30px 0;
      text-align: center;
      margin-bottom: 40px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.07);
    }
    h1.title {
      font-size: 2.3em;
      font-weight: 700;
      margin: 0 0 10px 0;
      letter-spacing: 0.5px;
      line-height: 1.3;
    }
    .author {
      font-size: 1.05em;
      margin-top: 10px;
      color: #e0e0e0;
      letter-spacing: 0.5px;
    }
    h1, h2 {
      color: #2c3e50;
      font-weight: 700;
      margin-top: 40px;
      margin-bottom: 20px;
      border-right: 5px solid #3a6073;
      padding-right: 15px;
      background: linear-gradient(90deg, #e0eafc 0%, #cfdef3 100%);
      border-radius: 0 20px 20px 0;
      display: inline-block;
      box-shadow: 0 1px 4px rgba(58,96,115,0.07);
    }
    h2 {
      font-size: 1.2em;
      margin-top: 30px;
      border-right: 3px solid #3a6073;
      padding-right: 10px;
    }
    main.container {
      max-width: 1000px;
      margin: 0 auto 60px auto;
      padding: 0 18px;
    }
    p {
      margin: 0 0 20px 0;
      padding: 0 20px 0 0;
      text-align: justify;
      background: #fff;
      border-radius: 12px;
      box-shadow: 0 1px 4px rgba(44,62,80,0.04);
      padding: 18px 28px 18px 10px;
    }
    ul, ol {
      margin: 0 0 20px 40px;
      padding: 0 20px 0 0;
      background: #fff;
      border-radius: 12px;
      box-shadow: 0 1px 4px rgba(44,62,80,0.04);
      padding: 18px 28px 18px 10px;
    }
    code, pre {
      font-family: 'Fira Mono', 'Consolas', 'Courier New', monospace;
      background: #f4f4f4;
      color: #c7254e;
      border-radius: 6px;
      padding: 2px 6px;
      font-size: 0.95em;
    }
    .math.inline, .math.display {
      direction: ltr;
      unicode-bidi: embed;
      font-size: 1em;
      background: #f4f4f4;
      border-radius: 6px;
      padding: 2px 6px;
      color: #1a237e;
      margin: 0 2px;
      font-family: 'Fira Mono', 'Consolas', 'Courier New', monospace;
    }
    .math.display {
      display: block;
      margin: 18px 0;
      text-align: center;
      font-size: 1.1em;
    }
    .nodecor {
      text-decoration: none;
      color: inherit;
      font-weight: 600;
    }
    a {
      color: #3a6073;
      text-decoration: underline;
      transition: color 0.2s;
    }
    a:hover {
      color: #16222a;
    }
    @media (max-width: 900px) {
      body {
        font-size: 19px;
      }
      header {
        padding: 25px 0 18px 0;
      }
      h1.title {
        font-size: 1.5em;
      }
      p, ul, ol {
        padding: 12px 10px 12px 5px;
      }
    }
    @media (max-width: 600px) {
      h1, h2 {
        font-size: 1.05em;
        padding-right: 7px;
      }
      p, ul, ol {
        font-size: 0.98em;
        padding: 8px 4px 8px 2px;
      }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">تدريب الشبكات العصبية المُقسَّمة باستخدام التسميات الوسيطة الاصطناعية</h1>
  <p class="author"><span class="nodecor">Cevat V. Karadağ</span> و <span class="nodecor">Nezih Topaloğlu</span></p>
</header>
<main class="container">
<h1 id="ملخص">مُلَخَّص</h1>
<p>يُعَدُّ الانتشار الواسع لبُنى الشبكات العصبية، ولا سيّما نماذج التعلُّم العميق، تحدّياً من حيث التدريب كثيف الموارد. فقد غدت قيود ذاكرة وحدات معالجة الرسوميّات عائقاً رئيساً أمام تدريب هذه النماذج الكبيرة. تُقدِّم الاستراتيجيات الراهنة، ومن بينها توازي البيانات، وتوازي النموذج، وتوازي خطّ الأنابيب، وتوازي البيانات المُجزَّأة بالكامل، حلولاً جزئية. يتيح توازي النموذج، على وجه الخصوص، توزيع النموذج كلّه عبر وحدات معالجة رسوميّات عدّة، غير أنّ الاتصالات بين هذه الأجزاء تُبطِئ عملية التدريب. إضافةً إلى ذلك، يُثقل كاهل الذاكرة على كل وحدة معالجة رسوميّات بسبب تخزين حالات المُحسِّن وسائر المعاملات المساعدة. بدلاً من استخدام النموذج كاملاً أثناء التدريب، تقترح هذه الدراسة تقسيم النموذج عبر الوحدات وتوليد تسميات وسيطة اصطناعية لتدريب الأجزاء منفردة. تساعد هذه التسميات، المُنشأة بعملية عشوائية، في تخفيف عبء الذاكرة والحِمل الحسابي. يفضي هذا النهج إلى عملية تدريب أكفأ تقلّ فيها الاتصالات مع الحفاظ على دقّة النموذج. وللتحقُّق من الفكرة، تُقسم شبكة عصبية مُتصلة بالكامل مكوّنة من <span class="nodecor">6</span> طبقات إلى قسمين ويُقوَّم أداؤها على مجموعة بيانات <span class="nodecor">MNIST</span> الموسّعة. تُشير النتائج التجريبية إلى أنّ النهج المقترح يحقق دقّة اختبار مماثلة لطرق التدريب التقليدية، مع خفضٍ كبير في متطلبات الذاكرة والحساب. تُسهم هذه الأعمال في تخفيف كثافة الموارد اللازمة لتدريب الشبكات العصبية الكبيرة، وتُمَهِّد الطريق لنماذج تعلُّم عميق أكفأ.</p>

<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>في السنوات الأخيرة برزت الشبكات العصبية، ولا سيّما نماذج التعلُّم العميق، كأدوات قويّة لحلّ مهام معقّدة في مجالات شتّى. وقد نمت هذه الشبكات كثيراً حجماً وتعقيداً، ما أتاح إنجازات في مجالات كالتعرُّف على الصور، ومعالجة اللغات الطبيعية، وتوليد الكلام. لكن هذا النموّ ولّد تحدِّياً كبيراً للمستخدمين: تدريب الشبكات العميقة والكبيرة جدّاً. على سبيل المثال، تتطلّب نماذج اللغة مثل GPT-3، بما تحويه من مليارات المعاملات، موارد حسابية هائلة وخبرة متخصصة لتدريبها بفاعلية. وبصورة خاصّة، تُشكِّل متطلبات ذاكرة وحدة معالجة الرسوميّات عقبةً أمام تدريب الشبكات الكبيرة.</p>
<p>وإدراكاً لأهمية الذاكرة الشحيحة في التعلُّم العميق، يستكشف الباحثون والممارسون طرائق مختلفة لجعل هذه التكنولوجيا أيسر منالاً (<span class="nodecor">Sutton09,Shiram19,Fu2021,Yao2018,Meng2017TrainingDM</span>). ومن الوسائل البسيطة والفعّالة خفضُ دقة معاملات النموذج: فبدلاً من استخدام نوع بيانات النقطة العائمة ب<span class="nodecor">32</span>-بت، يمكن استخدام <span class="nodecor">BFLOAT16</span> الذي يستهلك <span class="nodecor">16</span> بت (<span class="nodecor">8</span> للأسّ و<span class="nodecor">7</span> للكَسْر) (<span class="nodecor">URL_BFLOAT16</span>). ورغم أنّ هذا النهج يُنصِّف استهلاك الذاكرة، إلّا أنّه يأتي على حساب دقّة معاملات النموذج. كما أن العائد محدود: فالانتقال إلى أنواع بيانات ب<span class="nodecor">8</span>-بت، على سبيل المثال، يؤدي عادةً إلى تدهورٍ ملحوظ في الدقة.</p>
<p>ومن الطرائق الواعدة لتقليل استهلاك ذاكرة وحدة المعالجة الرسوميّة اعتمادُ أشكالٍ من التقسيم الموازي: توازي البيانات، وتوازي النموذج، وتوازي خطّ الأنابيب (<span class="nodecor">jia2018data</span>). في توازي البيانات (<span class="nodecor">DP</span>) تُقسَّم دفعات التدريب إلى مجموعات فرعية وتُوزّع عبر أجهزة متعددة، فيُجرى الأمام والخلف بالتوازي ثم تُجمع التدرُّجات. أمّا توازي النموذج (<span class="nodecor">MP</span>) فيقسِّم النموذج عمودياً أو داخلياً إلى مكونات أصغر تُعالَج على أجهزة مختلفة ثم تُدمَج. ويقسِّم توازي خطّ الأنابيب (<span class="nodecor">PP</span>) الشبكة عبر العمق إلى مراحل متتابعة تعمل على دفعات صُغرى. وقد حظيت هذه الأشكال من التوازي باهتمام واسع لِما تعد به من تقليل متطلبات الحوسبة والذاكرة عند تدريب نماذج التعلُّم العميق الكبيرة.</p>
<p>في توازي البيانات يمكن أن يتّسع النموذج داخل وحدة معالجة رسوميّات واحدة؛ إذ يُنسَخ النموذج ومعاملات التدريب إلى وحدات عدّة وتُقسَّم البيانات بينها. ويُجرى المروران الأمامي والخلفي على كل وحدة على حدة، ثم تُزامَن التدرجات لتحديث معاملات النموذج على جميع الوحدات. ورغم فاعلية هذه الطريقة حسابياً، إلّا أنّها لا تُقلِّل متطلبات الذاكرة لكل جهاز. ويُستخدم توازي البيانات على نحوٍ واسع في PyTorch تحت اسم التوازي الموزّع للبيانات (<span class="nodecor">DDP</span>) (<span class="nodecor">URL_DDP</span>)؛ ويمكن العثور على تحقيق شامل له في (<span class="nodecor">Li20</span>). ومع أنّ <span class="nodecor">DDP</span> حلٌّ مرن طالما اتّسع النموذج في ذاكرة جهاز واحد، فإنّه لا يكفي وحده حين يفوق حجم النموذج ذاكرة وحدة المعالجة الرسوميّة.</p>
<p>وقد يكون توازي خطّ الأنابيب مفيداً عندما يمكن تقسيم الشبكة على امتداد عمقها إلى مراحل، كما في تطبيقات الرؤية الحاسوبية. ومن الأمثلة المعروفة G-Pipe الذي اقترحه Huang <span class="nodecor">et al.</span> (<span class="nodecor">huang2019gpipe</span>)، حيث تُقسَّم الدفعة الكبيرة إلى دفعات صُغرى متساوية وتُجمع التدرُّجات في نهاية كل دفعة صغرى لتحديث المعاملات. ومن عيوب هذا الأسلوب فقاعةُ الأنبوب الناجمة عن عدم توازن العمل أو التبعيات بين المراحل، كما تتطلب تسوية الإحصاءات في طبقات التطبيع الدفعي بين المراحل معالجةً خاصة.</p>
<p>أمّا توازي النموذج فهو أساسي حين يفوق حجم النموذج ومعاملات التدريب (مثل التدرجات، وحالات المُحسِّن، والمتغيرات المؤقتة) ذاكرةَ وحدة المعالجة الرسوميّة (<span class="nodecor">Castello19,Wanwu2022</span>). وقد اقترح Rajbhandari <span class="nodecor">et al.</span> أسلوباً لتقسيم النموذج ومكوّنات التدريب مع تجنُّب التكرار غير الضروري عبر الأجهزة (<span class="nodecor">rajbhandari2020zero</span>). وتطوّر هذا لاحقاً إلى توازي البيانات المُجزَّأة بالكامل (<span class="nodecor">FSDP</span>) مع تنفيذ في PyTorch (<span class="nodecor">zhao2023pytorch</span>): إذ تُقسّم البيانات بين الأجهزة، وكذلك تُقسّم معاملات النموذج وحالات المُحسِّن من دون تداخل. وبعد المرورين الأمامي والخلفي بصورة متوازية، تُزامَن التدرجات مركزياً ثم يُجرى التحديث. ومؤخراً، اقترح Mlodozeniec <span class="nodecor">et al.</span> الجمعَ بين توازي النموذج وتوازي البيانات حيث تُحسَّن كلُّ شريحة من النموذج على قِطع بيانات مخصصة (<span class="nodecor">mlodozeniec2023</span>). كما اقترح Akintoye <span class="nodecor">et al.</span> تقسيم الطبقات (<span class="nodecor">akintoye2022</span>) لتقليل كلفة الاتصالات بين الأجهزة وبصمة الذاكرة أثناء التدريب. ويشيع التقسيم النموذجي أيضاً في الشبكات العصبية البيانية (<span class="nodecor">liao2018graph</span>) وفي الأنظمة الحافّة كشبكات الأشياء (<span class="nodecor">IoT</span>) (<span class="nodecor">Na22,Oliveira19,parthasarathy2023</span>).</p>
<p>وعلاوةً على أشكال التوازي السابقة، طُرحت طرائق أخرى لخفض الطلب على الذاكرة أثناء التدريب. فاقترح Jain <span class="nodecor">et al.</span> ترميز خرائط الميزات لتوفير الذاكرة (<span class="nodecor">Jain18</span>)، واقترح Wang <span class="nodecor">et al.</span> عُصبونات فائقة مع تخصيص ديناميكي لمساحات العمل التحويلية (<span class="nodecor">Wang18</span>). كما أنّ طرائق التحسين التكيفية تُضيف عبئاً ذاكرّياً كبيراً، وقد خفّضت دراساتٌ هذا العبء بتبسيط نماذج المُحسِّن (<span class="nodecor">anil19memoryefficient,shazeer2018adafactor</span>)، مع مخاطرة بالتأثير في تقارب التدريب.</p>
<p>في هذه الدراسة نطوّر منهجية تدريب جديدة للشبكات العصبية نُسميها تدريب الشبكات العصبية المُقسَّمة (<span class="nodecor">PNN</span>). نُقسِّم الشبكة إلى شبكتين فرعيتين أو أكثر. وبدلاً من تدريب النموذج كاملاً، نُدرّب هذه الأقسام على نحوٍ مستقل باستخدام بياناتٍ أصلية وتسمياتٍ وسيطة اصطناعية. وبما أنّ كل قسم يُدرَّب منفصلاً، تنتفي الحاجة إلى تبادل مخرجات المرور الأمامي أو التدرجات مع الأقسام الأخرى أو عبر المُضيف، فتتقلّص كلفة الاتصالات مقارنةً بتوازي النموذج التقليدي. كذلك يتيح الفصل ضبط فرط المعاملات لكل قسم على حدة، بما يقلّل الطلب الحسابي الإجمالي مع الحفاظ على الدقّة.</p>

<h1 id="التدريب-المنفصل-لأقسام-النموذج">التدريب المنفصل لأقسام النموذج</h1>
<p>يساعد التدريب المنفصل لأقسام النموذج أيضاً في التخفيف من مشكلة تلاشي التدرُّجات، وهي أشيع ملاحظةً في الشبكات العصبية العميقة (<span class="nodecor">Kolbusz2017vanishing</span>).</p>

<h1 id="الطريقة-المقترحة">الطريقة المقترحة</h1>
<p>تعتمد الفكرة على ملاحظة أنّ أوزان الطبقات الوسطى في الشبكات العصبية تُظهِر قدراً من العشوائية بطبيعتها؛ إذ تنبع من التهيئة العشوائية الأولى للأوزان ومن طبيعة عملية التدريب التكرارية (<span class="nodecor">Maennel20</span>). وبناءً عليه تختلف قيم الأوزان في الطبقات المتوسطة بحسب التهيئة الأولية العشوائية (<span class="nodecor">franchi2021tradi</span>). لذا نفترض أنّه عند تقسيم الشبكة يمكن تدريب القطاعات منفردة باستخدام تسميات وسيطة اصطناعية أو خرائط ميزات تُنشَأ بعملية عشوائية.</p>
<p>ولإظهار الطابع العشوائي لمعاملات الشبكة بعد التدريب، نُدرِّب شبكة متصلة بالكامل مراراً على مجموعة بيانات المعهد الوطني للمعايير والتكنولوجيا (<span class="nodecor">deng2012mnist</span>). تتألف الشبكة من ثلاث طبقات بعدد عصبونات <span class="nodecor">100</span> و<span class="nodecor">50</span> و<span class="nodecor">10</span>. في كل تكرار نُعيد تهيئة الأوزان عشوائياً بإعادة إنشاء الشبكة. التهيئة افتراضية في PyTorch (<span class="nodecor">URL_torch_init_linear</span>). نُدرِّب لِـ<span class="nodecor">15</span> دورة بحجم دفعة <span class="nodecor">256</span> ونحفظ النموذج بعد كل دورة. وبعد تكرار التدريب <span class="nodecor">300</span> مرة نرسم قيماً لثلاث إحصاءات من أوزان الطبقة الوسطى: الحد الأقصى والحد الأدنى والفارق بينهما. تُظهِر الرسوم أنّ الأوزان تبقى عشوائيةً إلى حدٍّ ما حتى بعد التدريب، نتيجة التهيئة العشوائية. وبالتبعية ستُظهِر خرائط التنشيط الوسطيّة قدراً كبيراً من العشوائية، ما يفتح الباب لتقسيم الشبكة وتدريبها باستعمال تسميات وسيطة اصطناعية مُولَّدة عشوائياً.</p>
<p>تبدأ الطريقة بتقسيم النموذج إلى شبكتين فرعيتين: قسم أيسر وقسم أيمن. ورغم إمكان التقسيم إلى أكثر من شبكتين، نعرض الفكرة بافتراض قسمين لِلبساطة. نُدرِّب أولاً القسم الأيسر من دون استخدام القسم الأيمن. ولتحقيق ذلك نُنشئ تسمياتٍ اصطناعية للقسم الأيسر تُسمّى التسميات الوسيطة الاصطناعية (<span class="nodecor">SIL</span>). بافتراض أنّ عدد عصبونات الطبقة النهائية في القسم الأيسر <span class="math inline">\(N_P\)</span> وعدد الفئات <span class="math inline">\(M\)</span>، نُنشئ <span class="math inline">\(M\)</span> متجهات بحجم <span class="math inline">\([N_P\times1]\)</span>. يمكن تمثيل التسميات الوسيطة الاصطناعية بمصفوفة <span class="math inline">\(SIL \in \mathbb{R}^{N_P\times M}\)</span> حيث يمثّل كل عمود فئة. تُنشأ عناصر المصفوفة عشوائياً من التوزيع الموحد <span class="math inline">\((0,1)\)</span> ثم تُضبَط بمعامل قياس <span class="math inline">\(\kappa\)</span> وفق:
<span class="math display">\[
    SIL_{i,j} \sim \kappa \, U(0,1)
\]</span>
حيث <span class="math inline">\(U(0,1)\)</span> هو التوزيع المنتظم على <span class="math inline">\((0,1)\)</span> و<span class="math inline">\(i \in \{1,2,\ldots,N_P\}\)</span> و<span class="math inline">\(j \in \{1,2,\ldots,M\}\)</span> مؤشّرا الصف والعمود.</p>
<p>باستخدام مُدخلات مجموعة التدريب الأصلية وSIL نُدرِّب القسم الأيسر على مدى <span class="math inline">\(N_L\)</span> دورات تدريبية من دون خلط المُدخلات. في هذه المرحلة لا يُستخدم القسم الأيمن إطلاقاً. بعد <span class="math inline">\(N_L\)</span> دورة نُنهي تدريب القسم الأيسر ونحفظ مخرجه النهائي (استجابة الدورة الأخيرة).</p>
<p>في المرحلة الثانية نُدرِّب القسم الأيمن على مُدخل هو المخرج النهائي المحفوظ للقسم الأيسر، مع استخدام تسميات مجموعة البيانات الأصلية. يجري التدريب على مدى <span class="math inline">\(N_R\)</span> دورات، وبعدها تُدمَج الأقسام لاستخدام الشبكة كاملةً.</p>
<p>يمكن توسيع الطريقة بسهولة عندما يُقسَّم النموذج إلى أكثر من شبكتين فرعيتين. عندها يلزم وجود تسمية وسيطة اصطناعية مميّزة لكل طبقة وسيطة، وتبقى مزايا النهج المقترح قائمة.</p>
<p>يمكن كذلك التخلي عن الطابع التسلسلي وتدريب كل شبكة فرعية على نحوٍ متزامن باستخدام تسميات وسيطة اصطناعية كمدخلات وتسميات. تتضمّن هذه البنية أقساماً وسطى تُدرَّب بمدخلات وتسميات مُنشأة عشوائياً. وقد أُثبت سابقاً إمكانُ الوصول إلى خسارة تدريب صفرية بشبكة تُدرَّب على بيانات عشوائية إذا كثرَت الدورات والمعاملات بما يكفي (<span class="nodecor">zhang2017understanding</span>). غير أنّ ذلك يتطلّب عدداً كبيراً من الدورات لكل قسم لتحقيق دقة مقبولة، ما يرفع الكلفة الحسابية ويجعلُه غير عملي.</p>

<h2 id="المزايا-مقارنة-بالتوازي-النموذجي-القياسي">المزايا مقارنةً بتوازي النموذج القياسي</h2>
<p>يتطلّب توازي النموذج التقليدي قدراً كبيراً من الاتصالات بين الأجهزة. فعلى عقدة تضم وحدات معالجة رسوميّات مترابطة تكون الاتصالات من الجهاز إلى المُضيف وبالعكس ضرورية لنقل الاستجابات والتدرّجات وتحديثات المعاملات (<span class="nodecor">Jain20</span>, <span class="nodecor">zhuang2022optimizing</span>). ومع ازدياد عدد الوحدات يتعاظم الحمل الزائد للاتصالات، ما يضع سقفاً للأداء الكلي (<span class="nodecor">rajbhandari2020zero</span>).</p>
<p>يُيسِّر النهج المقترح تدريب كل قسم داخل وحدة معالجته الرسوميّة الخاصة. ويقتصر الحمل الزائد للاتصالات أثناء التدريب على نقل مخرجات القسم السابق بوصفها مُدخلات للقسم الجاري تدريبه. تُسهم هذه الاستراتيجية في خفض الاتصالات، مُقدِّمةً نهجاً فعّالاً ودقيقاً للتدريب.</p>
<p>وتنطبق ملاحظة مشابهة عند استخدام معالجات بمساحات كبيرة من ذاكرة المُخبّأ من المستوى الثالث (L3)، مثل AMD EPYC-9684X بسعة <span class="nodecor">1152</span> ميغابايت (<span class="nodecor">URL_epyc</span>). وعلى خلاف توازي النموذج، فإن الطريقة المقترحة تسلسلية ويمكن تطبيقها حتى عند توافر جهاز واحد فقط بذاكرةٍ أصغر من حجم النموذج؛ بينما يكون توازي النموذج القياسي بطيئاً في مثل هذه الظروف أو غير قابلٍ للتطبيق.</p>
<p>يسمح التدريب المنفصل لكل قسم بتخصيص فرط المعاملات، مثل حجم الدفعة وعدد الدورات ومعدّل التعلُّم. على سبيل المثال، بدلاً من إخضاع الشبكة كاملةً لِـ<span class="nodecor">40</span> دورة، يمكن تخصيص <span class="nodecor">5</span> دورات للقسم الأيسر و<span class="nodecor">80</span> دورة للقسم الأيمن. وسنرى في قسم النتائج والمناقشة أنّ دقّة القسم الأيسر تتقارب خلال عددٍ محدود من الدورات، ما يُبرز ميزة هذه الاستراتيجية.</p>

<h1 id="تنفيذ-على-الشبكات-المتصلة-بالكامل">تنفيذ على الشبكات المتصلة بالكامل</h1>
<p>نُطبّق الخوارزمية على شبكة تصنيف متصلة بالكامل. تُستخدم مجموعة بيانات الأحرف الموسّعة المتوازنة (<span class="nodecor">EMNIST</span>) (<span class="nodecor">cohen2017emnist</span>) التي تتضمن <span class="nodecor">47</span> فئة تشمل الأرقام والحروف الكبيرة والصغيرة. المُدخلات صور رمادية بحجم <span class="math inline">\(28\times28\)</span> تُسطَّح إلى متجهات بحجم <span class="math inline">\(784\times1\)</span>.</p>
<p>الشبكة الأساسية (غير المُقسّمة) شبكة متصلة بالكامل ذات ست طبقات مع انحياز: تبدأ بطبقة مُدخلات بحجم <span class="nodecor">784</span>، ثم طبقات بعدد عصبونات <span class="nodecor">80</span>، <span class="nodecor">60</span>، <span class="nodecor">60</span>، <span class="nodecor">60</span>، وأخيراً طبقة مخرجات <span class="nodecor">47</span>. يجري التقسيم عند الطبقة الثالثة؛ وعليه يضم القسم الأيسر الطبقات حتى طبقة <span class="nodecor">60</span> الثالثة (بعدد إجمالي <span class="nodecor">140</span> عصبوناً مخفيّاً)، ويضم القسم الأيمن الطبقات اللاحقة (<span class="nodecor">167</span> عصبوناً مخفيّاً ومخرَجاً). ونظراً إلى أنّ حجم المُدخلات <span class="nodecor">784</span>، فإن عدد معاملات القسم الأيسر أكبر بكثير: <span class="math inline">\((784+1)\times80 + (80+1)\times60 = 67660\)</span> معلمة، بينما في القسم الأيمن <span class="math inline">\((60+1)\times60 + (60+1)\times60 + (60+1)\times47 = 10187\)</span> معلمة. ويتناسب عدد عمليات الضرب-والجمع (MACs) مع عدد المعاملات في شبكة متصلة بالكامل. وباستخدام مكتبة عدّاد MACs (<span class="nodecor">ptflops</span>)، تُقدَّر MACs للقسمين الأيسر والأيمن على التوالي بـ<span class="nodecor">67800</span> و<span class="nodecor">10307</span>، ما يجعل تدريبَ القسم الأيسر أثقل حسابياً.</p>
<p>تُولَّد التسميات الوسيطة الاصطناعية وفق المعادلة أعلاه مع <span class="math inline">\(\kappa=10\)</span>. أبعاد مصفوفة التسميات <span class="math inline">\(60\times47\)</span> حيث <span class="nodecor">47</span> عدد الفئات و<span class="nodecor">60</span> عدد عصبونات طبقة التقسيم. يبلغ عدد صور التدريب <span class="nodecor">112800</span>. التسمية لكل صورة مُدخلة متجه بحجم <span class="math inline">\(60\times1\)</span> مأخوذ من مصفوفة SIL. وبما أنّ المُدخلات لا تُخلط أثناء التدريب، تُرتَّب التسميات اللازمة لتدريب القسم الأيسر (<span class="nodecor">112800</span> تسمية بحجم <span class="math inline">\(60\times1\)</span>) وتُحمَّل إلى وحدة المعالجة الرسوميّة على دفعات. نستخدم طريقة التدرّج العشوائي مع معدّل تعلُّم <span class="nodecor">0.01</span> وزَخَم <span class="nodecor">0.9</span> للتحسين. دالّة التنشيط في كل طبقة هي ReLU، باستثناء الطبقة النهائية حيث تُستخدم دالّة الهوية. حجم الدفعة <span class="nodecor">1410</span>. نُفِّذت العمليات على بطاقة رسوميات AMD Radeon RX 7600.</p>

<h1 id="sec:results">النتائج والمناقشة</h1>
<p>نُقارن في هذا القسم دقّة النهج المقترح بوصفها دالةً للحِمل الحسابي، مقابل التدريب الأساسي. في خط الأساس نستخدم مجموعة البيانات نفسها لتدريب الشبكة ذات الطبقات الست غير المُقسّمة وبفرط المعاملات وحجم الدفعة نفسيهما.</p>
<p>نحسب الحِمل الحسابي بعدد عمليات MACs لكلٍّ من القسم الأيسر والقسم الأيمن والنموذج غير المُقسّم. يُضيف النهج المقترح كلفةَ اتصال لنقل مصفوفة SIL إلى وحدة المعالجة الرسوميّة، وكذلك نقل مخرجات القسم الأيسر لاستخدامها مُدخلاتٍ للقسم الأيمن؛ وكلا النقلين يحدث مرّة واحدة فقط، ولذلك استُثنيا من التحليل.</p>
<p>لِحالة محدّدة (من دون إرفاق شكل)، عددُ الدورات هو <span class="math inline">\(N_L = 5\)</span> للأيسر و<span class="math inline">\(N_R = 160\)</span> للأيمن و<span class="math inline">\(N_B = 40\)</span> لخط الأساس، مع <span class="math inline">\(\kappa=10\)</span>. أُجري التدريب لكلٍّ من خط الأساس والشبكة المُقسَّمة <span class="nodecor">10</span> مرّات؛ عُرض المتوسط بعلامات دائرية ونطاق الانحراف المعياري (<span class="math inline">\(68\%\)</span>) بأشرطة خطأ.</p>
<p>تُظهر النتائج بوضوح أنّ النهج المقترح يحقق دقّة اختبار مماثلة للتدريب التقليدي (<span class="nodecor">71.5%</span> للشبكة المُقسَّمة و<span class="nodecor">76.2%</span> لخط الأساس) مع استهلاكٍ حسابي أقلّ بكثير. وعلى الرغم من تدريب القسم الأيمن لِـ<span class="nodecor">160</span> دورة، فإن دقته تتقارب خلال عددٍ قليل من الدورات. ويُلاحظ أيضاً أنّ القسم الأيسر، الذي دُرِّب لِـ<span class="nodecor">5</span> دورات فقط، يُسهم كثيراً في خفض الحِمل الحسابي الإجمالي.</p>
<p>تُشير النتائج إلى إمكان ضبط معلمات مثل عدد الدورات ومعامل القياس <span class="math inline">\(\kappa\)</span> لتحسين كفاءة التدريب ودقّته. يُبيّن تأثير <span class="math inline">\(N_L\)</span> (من دون إرفاق شكل) أنّ نحو <span class="nodecor">5</span> دورات للقسم الأيسر تكفي لنتائج متقاربة، وأن تأثير <span class="math inline">\(N_L\)</span> يزداد بزيادة <span class="math inline">\(\kappa\)</span>. ويمثّل التدريب غير المتناظر للقسمين (<span class="math inline">\(N_L = 5\)</span> و<span class="math inline">\(N_R = 160\)</span>) سمةً فارقة لِـPNN؛ فمن خلال تحسين عدد الدورات وسائر الفرط لكل قسم يمكن خفض الزمن الحسابي الإجمالي إلى حدٍّ كبير، وهو ما يتعذّر تحقيقه في توازي النموذج التقليدي.</p>

<h1 id="تعزيز-الدقة-من-خلال-مراحل-التعافي">تعزيز الدقة من خلال مراحل التعافي</h1>
<p>على الرغم من مزايا النهج المقترح، تُظهِر النتائج أنّ دقّة الاختبار أدنى بقليل من خط الأساس. ويمكن التخفيف من ذلك بمتابعة التدريب بعد اكتمال الطريقة المقترحة: إذ يُدرَّب القسم الأيسر، الذي تلقّى عدداً قليلاً من الدورات، لِعدّة دورات إضافية مع تجميد أوزان القسم الأيمن. يُشبه هذا الضبطَ الدقيق (أو تعلُّماً نقليّاً جزئياً) ويُحسِّن الدقة.</p>
<p>عند تطبيق مرحلة تعافٍ من <span class="nodecor">10</span> دورات إضافية بالتكوين نفسه (من دون إرفاق شكل)، بلغت دقّة الاختبار المتوسّطة <span class="nodecor">72%</span> عند نهاية تدريب القسم الأيمن، وارتفعت إلى <span class="nodecor">77.5%</span> بعد التعافي. وبذلك لا يوفّر النهج مكاسبَ كفاءة فحسب، بل يقدّم أيضاً مساراً لتحسين الدقة عبر تدريبٍ ممتد.</p>

<h1 id="الاستنتاجات-والآفاق-المستقبلية">الاستنتاجات والآفاق المستقبلية</h1>
<p>ختاماً، يقدّم هذا البحث منهجية جديدة لمواجهة تحدّيات تدريب الشبكات العصبية الضخمة. فبدمج توازي النموذج مع التسميات الوسيطة الاصطناعية، طوّرنا نهجاً يعزّز كفاءة التدريب بصورةٍ كبيرة من دون التضحية بدقّة النموذج.</p>
<p>وتؤكّد التجارب على <span class="nodecor">EMNIST</span> فاعلية النهج المقترح: إذ إنّ تقسيم شبكةٍ ذات <span class="nodecor">6</span> طبقات إلى أقسام، مقروناً بتسميات وسيطة اصطناعية، يحافظ على دقّة اختبار قريبة من الطرق التقليدية مع خفضٍ واضح في الذاكرة والمتطلبات الحسابية. يحمل ذلك آفاقاً واسعة بوصفه حلاً عملياً لقيود الموارد في التعلُّم العميق الحديث، ويمهّد الطريق إلى نماذج أيسر وصولاً وأعلى كفاءة.</p>
<p>وعلاوةً على ذلك، يُنتظَر أن تستكشف الأبحاثُ المقبلة تعميم المنهجية إلى ما وراء الشبكات المتصلة بالكامل، لتشمل الشبكات الالتفافية (<span class="nodecor">CNNs</span>) والمتكرّرة (<span class="nodecor">RNNs</span>) وبُنى المُحوِّلات. وبمواءمة الطريقة مع هذه العائلات المتنوعة يمكن التحقّق من تعميمها وفاعليتها على طيفٍ أوسع من نماذج التعلُّم العميق، دافعاً بالمجال نحو نماذج أكثر سلاسةً ويُسراً في الوصول.</p>
</main>
</body>
</html>