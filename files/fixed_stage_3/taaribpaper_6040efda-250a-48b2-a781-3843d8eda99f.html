<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Osvaldo Luamba Quinjica David Ifeoluwa Adelani">
  <title>AngOFA: استغلال تهيئة التضمين OFA والبيانات الاصطناعية لنموذج اللغة الأنغولية</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">AngOFA: استغلال تهيئة التضمين OFA والبيانات الاصطناعية لنموذج اللغة الأنغولية</h1>
<p class="author"><span class="nodecor">Osvaldo Luamba Quinjica</span><br />
<span class="nodecor">David Ifeoluwa Adelani</span></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>في السنوات الأخيرة، شهد تطويرُ نماذجِ اللغةِ المدرَّبةِ مسبقًا (PLMs) زخمًا متزايدًا بفضل قدرتها على تجاوز الحواجز اللغوية وتيسير نقل المعرفة عبر لغات متنوعة. ومع ذلك، ركَّز معظم هذا التقدم على اللغات ذات الموارد العالية، فاتسعت فجوةٌ واضحة في المشهد متعدد اللغات. يسعى هذا البحث إلى سد هذه الفجوة من خلال تقديم أربعة نماذج PLMs مصممةٍ خصيصًا وخضعت لتكييفٍ دقيقٍ للغات الأنغولية باستخدام نهج التكييف الدقيق متعدد اللغات (MAFT). نُسلط الضوء في هذا العمل على دور التهيئة المستنيرة للتضمين والبيانات الاصطناعية في تعزيز أداء نماذج MAFT في المهام اللاحقة. وتمكّنا من تحقيق تحسينات بلغت <span class="nodecor">12.3</span> نقطة عند الاعتماد على AfroXLMR-base (مكيف بواسطة MAFT)، و<span class="nodecor">3.8</span> نقطة باستخدام OFA (تهيئة التضمين الفعالة).</p>
<h1 id="تقديم-الأوراق-لورشة-عمل-africanlp-في-iclr2023">تقديم الأوراق البحثية في ورشة عمل AfricaNLP ضمن مؤتمر ICLR<span class="nodecor">2023</span></h1>
<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>لقد شهدت نماذجُ اللغة ومجموعاتُ التقييم اللغوي تقدّمًا ملحوظًا عبر لغات العالم (<span class="nodecor">devlin-etal-2019-bert</span>, <span class="nodecor">conneau-etal-2020-unsupervised</span>, <span class="nodecor">workshop2023bloom</span>, <span class="nodecor">xue-etal-2021-mt5</span>). ومع ذلك، غالبًا ما غُفِلَت العديد من اللغات الأفريقية، فأفضى ذلك إلى فجوةٍ واضحة. وكثيرًا ما لم تضم معظم النماذج الموجّهة لأفريقيا اللغات الأنغولية ضمن مقاربتها (<span class="nodecor">dossou-etal-2022-afrolm</span>, <span class="nodecor">alabi-etal-2022-adapting</span>, <span class="nodecor">ogueji-etal-2021-small</span>). وتجلى نشاطُ مجتمع أبحاث معالجة اللغات الطبيعية في أفريقيا مؤخرًا في توسيع مجموعات التقييم (<span class="nodecor">adelani-etal-2021-masakhaner</span>, <span class="nodecor">adelani-etal-2022-masakhaner</span>, <span class="nodecor">muhammad-etal-2023-semeval</span>, <span class="nodecor">ma2023taxi1500</span>). ورغم هذه المبادرات، لا تزال اللغات الأنغولية تعاني نقصًا في التمثيل الملائم.</p>
<p>ينطوي النهج الأول على بناء نموذج من الصفر وتدريبه مباشرة على لغات متعددة باستخدام أهداف التعلم الذاتي مثل نمذجة اللغة المقنّعة (<span class="nodecor">devlin-etal-2019-bert</span>). أما النهج الثاني، التكييف الدقيق متعدد اللغات (<span class="nodecor">MAFT</span>)، فيتضمن تكييف نموذج متعدد اللغات مدرَّبٍ مسبقًا بإضافة مجموعة جديدة من اللغات (<span class="nodecor">alabi-etal-2022-adapting</span>, <span class="nodecor">wang-etal-2022-expanding</span>, <span class="nodecor">imanigooghari-etal-2023-glot500</span>). يتميز MAFT بالكفاءة في استغلال الموارد، لا سيما في ظل ارتفاع التكاليف الحاسوبية وتنامي حجم النماذج (<span class="nodecor">tay2022scale</span>, <span class="nodecor">gupta2023continual</span>). ويمكن أيضًا تعزيز أداء MAFT بإضافة رموز مفردات جديدة واستخدام تهيئة تضمين غير غاوسية (<span class="nodecor">minixhofer-etal-2022-wechsel</span>, <span class="nodecor">dobler-de-melo-2023-focus</span>, <span class="nodecor">liu2023ofa</span>).</p>
<p>في هذا البحث، نعرض المجموعة الأولى من نماذج PLM متعددة اللغات المصممة خصيصًا لخمس لغات أنغولية باستخدام نهج MAFT. نقارن النماذج المطوّرة عبر MAFT—والمسماة <span class="nodecor">angofa</span> و<span class="nodecor">angbert</span>—بأقرانها التي لم تستفد من التهيئة المستنيرة. من خلال تطبيق نهج OFA لتهيئة التضمين قبل MAFT، تكشف نتائجنا أن <span class="nodecor">angofa</span> يتفوق بشكل ملحوظ على <span class="nodecor">angbert</span> وOFA، مما يبرز التحسينات الجوهرية الناتجة عن دمج التهيئة المستنيرة والبيانات الاصطناعية.</p>
<h1 id="مفاجأة">نتيجة مفاجئة</h1>
<p>أظهرت نتائجنا أن نموذج <span class="nodecor">OFA</span> المطور على أكثر من <span class="nodecor">500</span> لغة يحقق أداءً يُقارِب أداء <span class="nodecor">AngOFA</span>، مما يؤكد قدرة <span class="nodecor">OFA</span> على التوسع ليشمل لغات إضافية.</p>
<h1 id="اللغات-الأنغولية">اللغات الأنغولية</h1>
<p>يشهد المشهد اللغوي في أنغولا تنوعًا يضم أكثر من <span class="nodecor">40</span> لغة، مع تعداد سكاني يقارب <span class="nodecor">32</span> مليون نسمة. تضم هذه اللغات البرتغالية وبعض لغات الخويسان، وغالبيتها تنتمي إلى عائلة النيجر–الكونغو البانتو. ورغم ذلك، هناك نقص واضح في الأدب والمحتوى الإذاعي والتلفزيوني باللغات الأنغولية الأصلية. تُكتب جميع لغات أنغولا بالأبجدية اللاتينية، ويشترك كثير منها في ديغرافات محددة. وبالنظر إلى ندرة الموارد، سيركز هذا البحث على خمس لغات أنغولية هي الأوسع انتشارًا: أومبوندو، كيمبوندو، كيكونغو، تشوكوي، ولوبا-كاساي. انظر الجدول [table-angola-languages] لمزيد من التفاصيل.</p>
<h1 id="النهج-لتحسين-maft">النهج لتحسين <span class="nodecor">MAFT</span></h1>
<h2 id="vocab-expansion">توسيع المفردات</h2>
<p>تميل نماذج اللغة المدربة مسبقًا إلى مواجهة رموز خارجية غير مدرجة ضمن مفرداتها عندما تتعامل مع لغات أو نصوص لم تُغطَ أثناء التدريب المسبق (<span class="nodecor">adelani-etal-2021-masakhaner</span>, <span class="nodecor">pfeiffer-etal-2021-unks</span>). وأحد أكثر الطرق فعالية للتعامل مع ذلك هو توسيع مفردات النموذج لتغطية الرموز الجديدة (<span class="nodecor">wang-etal-2019-improving</span>). فمثلاً، تم إنشاء Glot-500 (<span class="nodecor">imanigooghari-etal-2023-glot500</span>) عن طريق توسيع مفردات XLM-R من <span class="nodecor">250K</span> إلى <span class="nodecor">400K</span> قبل MAFT، ولكن الرموز الجديدة تمت تهيئتها بشكل عشوائي.</p>
<h2 id="عامل-التضمين-ofa">عامل التضمين OFA</h2>
<p>يعالج OFA مشكلتين رئيسيتين في تكييف النماذج المدربة مسبقًا مع لغات جديدة: (١) التهيئة العشوائية لتضمينات الكلمات الفرعية الجديدة التي لا تستفيد من المعرفة المشفرة في النموذج المصدر، و(٢) إدخال معاملات إضافية يعرقل بعضها تدريب النموذج المعدَّل (<span class="nodecor">liu2023ofa</span>). يحل OFA هذه المشكلات بالاستفادة من التضمينات متعددة اللغات الخارجية والتضمينات في النموذج المصدر لتهيئة التضمينات الجديدة. في هذا النهج، يحلل OFA مصفوفة التضمين الخاصة بالنموذج المصدر إلى مصفوفتين أصغر، ثم يُمثل تضمينات الكلمات الفرعية الجديدة غير المتداخلة كمجموعات من تضمينات الكلمات الفرعية في النموذج المصدر داخل فضاء أقل بعدًا. وتُوزن هذه المجموعات بالتشابهات المستمدة من التضمينات متعددة اللغات الخارجية المحاذاة جيدًا، مثل ColexNet+ (<span class="nodecor">liu2023crosslingual</span>) الذي يغطي أكثر من ألف لغة. أما تضمينات الكلمات الفرعية المشتركة بين النموذج المصدر والمفردات الموسعة فتُنسخ مباشرة، مما يضمن تكاملها واستمرارية التمثيل. لاستكمال العملية، يكرر OFA جميع المعاملات غير الخاصة بالتضمين من النموذج المصدر، ويستبدل محلل الأشكال (tokenizer) الأصلي بالخاص بالنموذج الهدف بعد توسيع المفردات.</p>
<h1 id="النماذج-الأساسية">النماذج الأساسية</h1>
<h2 id="synthetic-data">البيانات الاصطناعية لنمذجة اللغة</h2>
<p>بالنسبة للغات التي تفتقر إلى بيانات كافية قبل التدريب، يمكن توليد بيانات اصطناعية من خلال توسيع القاموس (<span class="nodecor">reid-etal-2021-afromt</span>) أو عبر نموذج الترجمة الآلية (MT)—وهو نهج شائع في بحوث الترجمة الآلية يعرف بالترجمة العكسية، طريقٌ فعّالة لتحسين النماذج للغات منخفضة الموارد (<span class="nodecor">sugiyama-yoshinaga-2019-data</span>, <span class="nodecor">xia-etal-2019-generalized</span>). في هذه الورقة، نستخدم البيانات الاصطناعية المُولَّدة بالترجمة الآلية كما وُصفت في (<span class="nodecor">adelani2023sib200</span>). قام المؤلفون بتوليد بيانات مترجمة آليًا لـ <span class="nodecor">34</span> لغة أفريقية (بما في ذلك اللغات الأنغولية) بأقل من <span class="nodecor">10MB</span> من البيانات، باستخدام مجموعة تعليقات الأخبار الإنجليزية (<span class="nodecor">kocmi-etal-2022-findings</span>) التي تضم أكثر من <span class="nodecor">600K</span> جملة.</p>
<h1 id="البيانات">البيانات</h1>
<h2 id="train_data">بيانات التدريب</h2>
<p>اعتمدنا على مجموعة بيانات NLLB (<span class="nodecor">nllb2022</span>)، مستثنين الترجمات الإنجليزية، وركزنا فقط على لغات كيمبوندو، أومبوندو، كيكونغو، تشوكوي، ولوبا-كاساي. تم دمج هذه اللغات في ملفٍ واحدٍ كمجموعة بيانات أولية للتدريب، وأضفنا إليها البيانات الاصطناعية المُولَّدة بواسطة NLLB. تُعرضُ التفاصيل بيانات أحادية اللغة.</p>
<h2 id="eval_data">بيانات التقييم</h2>
<p>قمنا بالتقييم على مجموعة تصنيف النصوص SIB-<span class="nodecor">200</span> (<span class="nodecor">adelani2023sib200</span>)، التي توفر مجموعات تدريب/تطوير/اختبار وتضم <span class="nodecor">7</span> فئات في أكثر من <span class="nodecor">200</span> لغة ولهجة أفريقية. توزيع الفئات: العلوم/التكنولوجيا (<span class="nodecor">252</span>)، السفر (<span class="nodecor">198</span>)، السياسة (<span class="nodecor">146</span>)، الرياضة (<span class="nodecor">122</span>)، الصحة (<span class="nodecor">110</span>)، الترفيه (<span class="nodecor">93</span>)، الجغرافيا (<span class="nodecor">83</span>). تُعدّ SIB-<span class="nodecor">200</span> المجموعة الوحيدة التي تغطي اللغات الأنغولية، وقيمنا الأداء فقط على هذه اللغات في عملنا.</p>
<h1 id="الإعداد-التجريبي">الإعداد التجريبي</h1>
<p>استفدنا من قدرات XLM-R متعدد اللغات في مرحلة التدريب، فأنشأنا نموذجين جديدين مبنيين على XLM-R: <span class="nodecor">AngBERT</span> و<span class="nodecor">AngOFA</span>. خضع كل منهما لعمليات تهيئة دقيقة مختلفة. خضع <span class="nodecor">AngBERT</span> لتهيئة باستخدام MAFT كما هو موضح في (<span class="nodecor">alabi-etal-2022-adapting</span>)، بنوعين من البيانات: أحدهما يعتمد على البيانات أحادية اللغة فقط (<span class="nodecor">281.6</span> MB)، والآخر يجمع بين البيانات الأحادية والبيانات الاصطناعية (<span class="nodecor">808.7</span> MB).</p>
<p>وبالمثل، خضع <span class="nodecor">AngOFA</span> لنوعي تهيئة باستخدام نفس مجموعات البيانات، لكن مع اتباع التكوينات الخاصة بـ <code>ofa-multi-768</code> كما وصفها (<span class="nodecor">liu2023ofa</span>). اخترنا الحفاظ على البُعد الكامن <span class="nodecor">768</span> في تجاربنا استنادًا إلى النتائج الأولية والرؤى من (<span class="nodecor">imanigooghari-etal-2023-glot500</span>, <span class="nodecor">liu2023ofa</span>). كشفت هذه التجارب عن دلائل لفقدان المعلومات عند استخدام أبعاد أقل، وهو ما ظهر جليًا في مهام مثل تصنيف النصوص. هدف هذا التقسيم في البيانات كان استكشاف تأثير أسلوبي MAFT وOFA، مع البيانات الاصطناعية أو بدونها، على أداء النموذج.</p>
<p>قمنا بمقارنة نماذجنا مع النماذج الأساسية التالية:</p>
<ol>
<li><p>XLM-R (<span class="nodecor">conneau-etal-2020-unsupervised</span>): نموذج مشفر فقط، درِّب مسبقًا على <span class="nodecor">100</span> لغة عبر هدف نمذجة اللغة المقنّعة، ولا يغطي أي لغة تم تقييمها في هذا العمل.</p></li>
<li><p>Serengeti (<span class="nodecor">adebara-etal-2023-serengeti</span>): درِّب على <span class="nodecor">500</span> لغة أفريقية، بما فيها <span class="nodecor">10</span> لغات غنية بالموارد، من بينها كيمبوندو، أومبوندو، وتشوكوي.</p></li>
<li><p>Glot-500 (<span class="nodecor">imanigooghari-etal-2023-glot500</span>): مشتق من XLM-R، تم توسيعه ليغطي <span class="nodecor">500</span> لغة برفع المفردات من <span class="nodecor">250K</span> إلى <span class="nodecor">400K</span>، لاستيعاب رموز <span class="nodecor">400</span> لغة لم تكن مغطاة سابقًا. يغطي جميع اللغات الأنغولية في تقييمنا.</p></li>
<li><p>AfroXLMR-base (<span class="nodecor">alabi-etal-2022-adapting</span>): مطوَّر عبر MAFT، يغطي <span class="nodecor">20</span> لغة بحجم بيانات أحادية لا يقل عن <span class="nodecor">50MB</span>. اللغات الأنغولية غير مشمولة.</p></li>
<li><p>AfroXLMR-base-76L (<span class="nodecor">adelani2023sib200</span>): مطوَّر عبر MAFT، يغطي لغات على الويب لا تقل بياناتها عن <span class="nodecor">10MB</span>. وسّع التغطية لتشمل مزيدًا من اللغات في NLLB-200، وأُنشئت بيانات اصطناعية لحوالي <span class="nodecor">30</span> لغة منخفضة الموارد، منها جميع اللغات الأنغولية الخمس. يغطي النموذج <span class="nodecor">76</span> لغة.</p></li>
<li><p>OFA (<span class="nodecor">liu2023ofa</span>): يدمج تهيئة التضمين OFA مع MAFT باستخدام Glot500-c (<span class="nodecor">imanigooghari-etal-2023-glot500</span>)، ليشمل جميع اللغات المعالجة في هذا العمل.</p></li>
</ol>
<h1 id="مهمة-التقييم">مُهِمَّة التقييم</h1>
<h1 id="النتائج-والمناقشة">النتائج والمناقشة</h1>
<p><strong>نتائج مقارنة المعايير</strong>: مقارنة فعالية <span class="nodecor">OFA</span> مع التهيئة العشوائية قبل MAFT</p>
<p>تُظهر الجدول [table-1] أداءَ النماذج الأساسية باستخدام <strong>مقياس F1 الموزون</strong>. نلخص أهم النتائج فيما يلي:</p>
<h4 id="نماذج-اللغة-المحددة-بالمنطقة-أفضل-من-تلك-المدربة-مسبقا-من-الصفر-بالعديد-من-اللغات">النماذج الإقليمية تتفوق على النماذج المدربة من الصفر على عدة لغات</h4>
<p>أظهرت نتائجنا أن <span class="nodecor">AngBERT</span>، المطوَّر عبر MAFT، حقق أداءً أفضل من <span class="nodecor">XLM-R</span> و<span class="nodecor">AfroXLMR</span> و<span class="nodecor">Serengeti</span> و<span class="nodecor">Glot-500</span> بفروق قدرها <span class="math inline">+5.5</span> و<span class="math inline">+1.2</span> و<span class="math inline">+3.6</span> و<span class="math inline">+6.6</span> نقطة على التوالي. ورغم أن Serengeti وGlot-500 تدربا مسبقًا على أكثر من 500 لغة مع تمثيل محدود للغات الأنغولية، فإن أدائهما كان أقل من أداء AfroXLMR (مكيف لـ20 لغة عبر MAFT) وAngBERT (مكيف لخمس لغات أنغولية). يوضح ذلك أن النماذج الإقليمية التي تغطي لغات متصلة ضمن نفس العائلة قد تكون أكثر فعالية.</p>
<h4 id="يمكن-تعزيز-نتائج-maft-من-خلال-الاستفادة-من-البيانات-أحادية-اللغة-الاصطناعية">تعزيز MAFT باستخدام البيانات الأحادية الاصطناعية</h4>
<p>بدمج البيانات الاصطناعية الإضافية، تحسن أداء <span class="nodecor">AngBERT</span> (+SYN data) بفارق <span class="math inline">+5.5</span> عن AngBERT بدون بيانات اصطناعية. ومع ذلك، لم يتجاوز أداء AfroXLMR-base-76L المدرب على 76 لغة أفريقية (بما في ذلك جميع اللغات الأنغولية عدا لوبا-كاساي) على عينة بيانات أكبر. وقد أظهرت تجربتنا أن نموذج الـ76 لغة تفوق على Serengeti المدرب مسبقًا على 500 لغة، مما يبرهن على إمكانية بناء نماذج لغوية فعالة أكثر لتغطية لغات إضافية عبر التكييف دون الحاجة للتدريب من الصفر المكلف.</p>
<h4 id="تهيئة-التضمين-ofa-مع-بيانات-أكبر-أكثر-فعالية">فعالية تهيئة التضمين عبر OFA مع بيانات أكبر</h4>
<p>قدّمت النماذج المهيأة بواسطة <span class="nodecor">OFA</span> تحسينًا مستمرًا مقارنةً بالنماذج الأخرى، مما يدل على تفوق OFA الذي يستفيد صراحة من المعلومات المشفرة في النموذج المصدر والتضمينات متعددة اللغات الخارجية. ولاحظنا أن <span class="nodecor">AngOFA</span> ارتقى بأدائه فوق <span class="nodecor">OFA</span> بعد الوصول إلى مجموعة بيانات أكبر للغات المعنية باستخدام البيانات الاصطناعية. من دون هذه البيانات، كان أداء <span class="nodecor">AngOFA</span> أقل من نموذج OFA المدرب على 500 لغة بانخفاض <span class="math inline">−3.2</span>. ولكن عند التدريب على البيانات الاصطناعية، حقق <span class="nodecor">AngOFA</span> أفضل أداء شامل بفروق <span class="math inline">+16.6</span> على <span class="nodecor">XLM-R</span>، و<span class="math inline">+12.3</span> على <span class="nodecor">AfroXLMR</span>، و<span class="math inline">+5.6</span> على <span class="nodecor">AngBERT</span> (مع البيانات الاصطناعية).</p>
<h1 id="الخلاصة-والأعمال-المستقبلية">الخلاصة والأعمال المستقبلية</h1>
<p>يقدم هذا البحث أربعة نماذج لغوية متعددة اللغات مصممة خصيصًا للغات الأنغولية. توضح نتائج تجاربنا أن التهيئة المستنيرة للتضمين تعزّز بشكل كبير أداء MAFT في المهام اللاحقة. وتُظهر النماذج التي خضعت لتهيئة <span class="nodecor">OFA</span> نتائج متفوقة مقارنةً بأقرانها؛ حتى عندما تدربت <span class="nodecor">AngBERT</span> على مجموعة بيانات أكبر، ظل أداؤها أقل من <span class="nodecor">OFA</span> المدربة على مجموعة أصغر. ومع ذلك، تُثير العوامل التي أدّت إلى تفوق AngBERT على OFA، لاسيما في سياق لوبا-كاساي، أسئلةً حول المحددات الأساسية لأداء النماذج في المهام اللاحقة، بما في ذلك الاعتبارات المرتبطة بحجم البيانات مقابل التهيئة المستنيرة. نترك هذه الأسئلة للبحث المستقبلي، كما نخطط لتوسيع تطبيق <span class="nodecor">OFA</span> على مزيد من اللغات الأفريقية لاستكشاف آفاق أوسع.</p>
<h3 id="الشكر-والتقدير" class="unnumbered">الشكر والتقدير</h3>
<p>تم دعم هذا العمل جزئيًا بواسطة اعتمادات وموارد <span class="nodecor">Oracle Cloud</span> المقدمة من <span class="nodecor">Oracle</span>. ويعترف <span class="nodecor">David Adelani</span> بدعم برنامج <span class="nodecor">DeepMind Academic Fellowship</span>.</p>
<h1 id="الملحق">المُلْحَق</h1>
</body>
</html>