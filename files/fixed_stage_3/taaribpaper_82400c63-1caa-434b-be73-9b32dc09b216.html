<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Aayush Dhakal, Subash Khanal, Srikumar Sastry, Adeel Ahmad, Nathan Jacobs">
  <title>GeoBind: ربط النص والصورة والصوت عبر صور الأقمار الصناعية</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">GeoBind: ربط النص والصورة والصوت عبر صور الأقمار الصناعية</h1>
<p class="author"><span class="nodecor">Aayush Dhakal</span>, <span class="nodecor">Subash Khanal</span>, <span class="nodecor">Srikumar Sastry</span>, <span class="nodecor">Adeel Ahmad</span>, <span class="nodecor">Nathan Jacobs</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">ملخّص</h1>
<p>في مجال الاستشعار عن بُعد، نهتم بنمذجة الوسائط المتعددة لموقع جغرافي معيَّن. ركَّزت العديد من الدراسات على تعلّم العلاقة بين الموقع وأنواع المناظر الطبيعية وصلاحية السكن والخصائص الصوتية والأوصاف النصية وغيرها. مؤخرًا، أصبح النهج الشائع لمعالجة هذه المشكلات هو تدريب نموذج تعلم عميق يستند إلى صور الأقمار الصناعية لاستنتاج خصائص فريدة للموقع. في هذا البحث، نقدم نموذج تعلم عميق يُدعى <span class="nodecor">GeoBind</span>، قادرًا على الاستدلال على وسائط متعددة—تحديدًا النصّ والصورة والصوت—انطلاقًا من صورة قمر صناعي لموقعٍ ما. للقيام بذلك، نعتمد على صور الأقمار الصناعية كعنصر ربط، وننسّق تباينياً بين وسائطنا المختلفة وتضمينات صور الأقمار الصناعية. ينتج عن تدريبنا فضاء تضمين مشترك يضم بيانات صور الأقمار الصناعية وصور مستوى سطح الأرض والتسجيلات الصوتية والنصوص. علاوةً على ذلك، لا يتطلب نهجنا وجود مجموعة بيانات موحدة كاملة تحتوي على جميع هذه الوسائط، بل يكتفي ببيانات مترابطة مع صور الأقمار الصناعية. على الرغم من أننا نقوم بمحاذاة ثلاث وسائط فقط في هذه الورقة، إلا أننا نقدم إطارًا عامًا يمكن تطبيقه لإنشاء فضاء تضمين لأي عدد من الوسائط باستخدام صور الأقمار الصناعية كعنصر ربط. تظهر نتائجنا أن نموذج <span class="nodecor">GeoBind</span> متعدد الاستخدامات ويمكنه التعامل مع وسائط متعددة عند إدخال صورة قمر صناعي.</p>
<h1 id="sec:intro">مقدمة</h1>
<p>يُعدّ استنتاج الخصائص المختلفة المتعلقة بمواقع جغرافية محددة مهمةً هامة في مجال الاستشعار عن بُعد. تركزت الجهود البحثية السابقة بشكلٍ رئيسي على بناء علاقات بين الموقع ووسيط واحد مثل استخدام الأراضي، أو مقاييس صلاحية السكن، أو الخصائص الصوتية، أو المناظر الأرضية، أو الأوصاف النصية (<span class="nodecor">yurui2020towards</span>, <span class="nodecor">zhu2022land</span>, <span class="nodecor">khanal2023learning</span>, <span class="nodecor">basu2021investigating</span>, <span class="nodecor">dhakal2023sat2cap</span>). أدّى ذلك إلى تطوير نماذج تعلم عميق تستنتج خصائص فريدة بناءً على صورة قمر صناعي معينة (<span class="nodecor">greenwell2018goes</span>, <span class="nodecor">zang2021land</span>, <span class="nodecor">9323706</span>, <span class="nodecor">sastry2024birdsat</span>, <span class="nodecor">klemmer2023satclip</span>).</p>
<p>تهدف هذه الدراسة إلى توسيع هذا النهج من خلال إنشاء فضاء تضمين مشترك يربط الوسائط المتعددة بالموقع الجغرافي بسلاسة. تتمثل المساهمة الرئيسية في إنشاء فضاء واحد يمكن استخدامه لاستنتاج خصائص مختلفة لموقع معين اعتمادًا على صور الأقمار الصناعية فقط. ومع ذلك، يواجه تدريب مثل هذا النموذج تحديًا في جمع بيانات عالية الأبعاد تغطي جميع الوسائط. فعلى سبيل المثال، لإنشاء فضاء يربط صور الأقمار الصناعية بالنصوص والتسجيلات الصوتية وصور مستوى الأرض، ستحتاج إلى مجموعة بيانات رباعية كاملة، وهو ما يصعب جمعه مع تزايد عدد الوسائط.</p>
<p>تناولت أعمال حديثة (مثل <span class="nodecor">girdhar2023imagebind</span>) هذه المشكلة بإثبات إمكانية تعلم فضاء تضمين مشترك للوسائط المتعددة باستخدام الصور كعنصر ربط. يستخدم ImageBind مجموعات بيانات متعددة مترابطة بالصور، ثم يقوم بمحاذاة تضمينات كل وسيط مع تضمينات الصور، مما ينتج فضاء تمثيل مشترك يغطي جميع الوسائط. مستوحين من ذلك، نقترح إطار عمل يعتمد على بيانات متعددة مرتبطة بصور الأقمار الصناعية لتعلّم فضاء تضمين مشترك يربط الوسائط المتعددة عبر الجغرافيا. سيكون هذا الفضاء مفيدًا لمجموعة واسعة من المهام المكانية. في هذا الإطار، نعتمد على نوعين من البيانات: تسجيلات صوتية مرفقة بصور الأقمار الصناعية، وصور مستوى الأرض المرفقة بصور جوية. نعتبر صور الأقمار الصناعية النقطة المشتركة لربط هذه الوسائط المختلفة. يتكون التدريب من مرحلتين: الأولى لمحاذاة تضمينات الصور الفضائية مع تضمينات الصور الأرضية (والتي تمثل أيضًا الوصف النصي) وفقًا لطريقة Sat2Cap (<span class="nodecor">dhakal2023sat2cap</span>)، والثانية لمحاذاة تضمينات الصوت مع تضمينات الصور الفضائية الناتجة عن المرحلة الأولى.</p>
<h1 id="sec:method">الطريقة</h1>
<h2 id="مجموعة-البيانات">مجموعة البيانات</h2>
<p>نستخدم في هذا العمل مجموعتين من البيانات مترابطة بصور الأقمار الصناعية. أولًا، مجموعة <span class="nodecor">dhakal2023sat2cap</span> التي تضم 6.1 مليون زوج من صور الأقمار الصناعية وصور مستوى سطح الأرض. تبلغ دقة صور الأقمار الصناعية 0.6 متر لكل بكسل، وهي مأخوذة من خرائط Bing بحجم 800×800 بكسل لكل صورة. ثانيًا، بيانات <span class="nodecor">SoundingEarth</span> (<span class="nodecor">wu2023large</span>) التي تحتوي على 50 ألف تسجيل صوتي معنّون جغرافيًا، مرفق بصور أقمار صناعية متمركزة بدقة 0.6 متر لكل بكسل وبالحجم نفسه مأخوذة أيضًا من خرائط Bing.</p>
<h2 id="المنهج">المنهج</h2>
<p>يتألف منهجنا من خطوتين أساسيتين. الأولى لمحاذاة تضمينات الصور الفضائية مع تضمينات الصور الأرضية في فضاء <span class="nodecor">CLIP</span>، والثانية لمحاذاة تضمينات الصوت مع تضمينات الصور الفضائية الناتجة.</p>
<p>في الخطوة الأولى، نتبع إجراء Sat2Cap (<span class="nodecor">dhakal2023sat2cap</span>) لمحاذاة دفعة من الصور الفضائية \(S_i\) وتضميناتها \(O_i\) مع تضمينات CLIP للصور الأرضية \(C_i\). نستخدم دالة خسارة InfoNCE:</p>
<p><span class="math display">\[L = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{\exp(o_i \cdot c_i / \tau)}{\sum_{j=0}^{k} \exp(o_i \cdot c_j / \tau) }\]</span></p>
<p>حيث \(\tau\) معامل الحرارة و k حجم الدفعة. ونظرًا لأن فضاء CLIP يوازن دلاليًا بين الصور الطبيعية والنصوص، فإن محاذاة الصور الفضائية مع تضمينات CLIP للصور الأرضية تنعكس تلقائيًا على الأوصاف النصية للمشاهد الأرضية.</p>
<p>في الخطوة الثانية، نستخدم بيانات SoundingEarth لتدريب مشفر الصوت. لدفعة من الصور الفضائية \(S_i\)، نستخرج أولًا تضميناتها \(O_i\) باستخدام المشفر الفضائي المدرب. ثم ندرب مشفر الصوت على دفعة من التسجيلات \(H_i\) لإخراج تضمينات \(A_i\) مع تجميد المشفر الفضائي، باستخدام خسائـر تباينية ثنائية الاتجاه:</p>
<p><span class="math display">\[L_1 = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{\exp(o_i \cdot a_i / \tau)}{\sum_{j=0}^{k} \exp(o_i \cdot a_j / \tau) }\]</span></p>
<p><span class="math display">\[L_2 = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{\exp(a_i \cdot o_i / \tau)}{\sum_{j=0}^{k} \exp(a_i \cdot o_j / \tau) }\]</span></p>
<p><span class="math display">\[L = \frac{L_1 + L_2}{2}\]</span></p>
<p>مع اقتراب تضمينات الصوت من تضمينات الصور الفضائية المقابلة، فإنها تتماشى أيضًا مع الصور الأرضية والنصوص ذات الصلة دلاليًا، فتتكوّن لدينا مساحة تضمين موحدة تتيح للوسائط المختلفة التفاعل ضمنها.</p>
<h1 id="sec:exp">التجارب والنتائج</h1>
<h2 id="تفاصيل-التنفيذ">تفاصيل التنفيذ</h2>
<p>نستخدم نموذج CLIP ViT-32B المدرب مسبقًا لتوليد تضمينات CLIP. كما نستخدم ViT-32B كمشفّر للصور الفضائية، مهيأً بمعاملات نموذج CLIP، ومشفّر الصوت <span class="nodecor">CLAP</span> من Hugging Face. نعتمد على <span class="nodecor">RandAugment</span> (<span class="nodecor">cubuk2020randaugment</span>) مع ثلاث عمليات لزيادة تنويع صور الأقمار الصناعية أثناء التدريب. نستخدم محسّن AdamW (<span class="nodecor">loshchilov2017decoupled</span>) بمعدل تعلم 5e-05، \(\beta_1=0.99\)، \(\beta_2=0.98\)، ومع جدولة CosineAnnealing مع إعادة تشغيل دافئة (<span class="nodecor">loshchilov2016sgdr</span>). معامل الحرارة \(\tau\) قابل للتعلّم وبدأ بقيمة 0.07.</p>
<h2 id="استرجاع-متعدد-الوسائط">استرجاع متعدد الوسائط</h2>
<p>لإثبات أن فضاء التضمين المشترك يجمع البيانات ذات الصلة دلاليًا، نجري تجارب استرجاع على مجموعة اختبار محجوزة مكوّنة من 10000 عينة. أولًا، نثبت أن الفضاء يربط الصور الفضائية بالصور الأرضية من نفس المواقع. نحسب تضمينات الصور الفضائية باستخدام مشفّرنا وتضمينات الصور الأرضية باستخدام مشفّر <span class="nodecor">CLIP</span>، ثم نحسب تشابه الجيب التمامي لجميع الأزواج ونستخلص مقاييس أعلى-k. نظرًا لأن التدريب الأولي مطابق لـ Sat2Cap (<span class="nodecor">dhakal2023sat2cap</span>)، نرى في (<span class="nodecor">table:image_retrieval</span>) أداء استرجاع مماثلًا، حيث تقع الصورة الحقيقية ضمن أعلى-10 في حوالي 56% من الحالات. وهذا يدل على توافق الصور الفضائية والأرضية دلاليًا في فضاء التضمين المشترك.</p>
<p>ثانيًا، نقيّم استرجاع الصوت بناءً على الصور الفضائية. نحسب تضمينات الصور الفضائية باستخدام مشفّرنا وتضمينات الصوت باستخدام مشفّر الصوت، ثم نستخرج مقاييس أعلى-k كما في (<span class="nodecor">table:sound_retrieval</span>). نلاحظ أن النتائج أقل من استرجاع الصور الأرضية، وهو أمر متوقع لكون المهمة أصعب بطبيعتها (<span class="nodecor">heidler2023self</span>). مع ذلك، فإن نتائجنا قريبة من النماذج الحالية لهذه المهمة. يشير ذلك إلى أن إطار عمل <span class="nodecor">GeoBind</span> يخلق فضاء تضمين مشترك يربط الصور الفضائية بالأرضية والصوت (وبالتالي النص عبر محاذاة CLIP)، مما يتيح استخدام مشفّرات متعددة لوضع وسائط مختلفة في فضاء واحد دون الحاجة لنموذج منفصل لكل مهمة.</p>
<h1 id="المناقشة-والخلاصة">المناقشة والخلاصة</h1>
<p>في هذا العمل، قدمنا إطار عمل يمكّن الصور الفضائية من التفاعل مع أنواع متعددة من البيانات. بربط الوسائط المتعددة بصور الأقمار الصناعية، أنشأنا فضاء تضمين مشترك يجمع النصوص الدلالية والصور الأرضية والتسجيلات الصوتية والصور الفضائية. يمكن توجيه هذه الوسائط إلى فضاء واحد لحل مشكلات متعددة دون الحاجة إلى نماذج مخصّصة لكل سيناريو.</p>
<p>يهدف هذا الإطار إلى تشجيع تطوير نماذج تعلم عميق عامة ومتعددة الاستخدامات للبيانات الفضائية. رغم اعتمادنا على تدريب ثنائي المرحلة، يمكن إضافة مراحل جديدة لأي عدد من الوسائط عبر محاذاتها بصور الأقمار الصناعية. يفتح عملنا الطريق لنماذج أكثر تكاملاً وكفاءة بدلاً من الطرازات أحادية الوضع الضيقة النطاق. في الأعمال المستقبلية، نعتزم استكشاف إضافة وسائط جديدة ودراسة الخصائص الناشئة في فضاء التضمين المشترك.</p>
</body>
</html>