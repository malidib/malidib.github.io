<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Rui Liu ruiliu@umd.edu قِسْم عُلُوم الحاسُوب، جامِعَة ماريلاند، كوليدج بارك Erfaun Noorani enoorani@umd.edu قِسْم الهَنْدَسَة الكَهْرَبائِيَّة وَالحاسُوب، جامِعَة ماريلاند، كوليدج بارك Pratap Tokekar tokekar@umd.edu قِسْم عُلُوم الحاسُوب، جامِعَة ماريلاند، كوليدج بارك John S. Baras baras@umd.edu قِسْم الهَنْدَسَة الكَهْرَبائِيَّة وَالحاسُوب، جامِعَة ماريلاند، كوليدج بارك">
  <title>نحو إدراج سياسة حساسة للمخاطر فعّالة: تحليل تعقيد التكرار</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">نحو إدراج سياسة حساسة للمخاطر فعّالة: تحليل تعقيد التكرار</h1>
<p class="author"><span class="nodecor">Rui Liu</span><br />
<span class="nodecor">ruiliu@umd.edu</span><br />
قِسْم عُلُوم الحاسُوب<br />
جامِعَة ماريلاند، كوليدج بارك <span class="nodecor">Erfaun Noorani</span><br />
<span class="nodecor">enoorani@umd.edu</span><br />
قِسْم الهَنْدَسَة الكَهْرَبائِيَّة وَالحاسُوب<br />
جامِعَة ماريلاند، كوليدج بارك <span class="nodecor">Pratap Tokekar</span><br />
<span class="nodecor">tokekar@umd.edu</span><br />
قِسْم عُلُوم الحاسُوب<br />
جامِعَة ماريلاند، كوليدج بارك <span class="nodecor">John S. Baras</span><br />
<span class="nodecor">baras@umd.edu</span><br />
قِسْم الهَنْدَسَة الكَهْرَبائِيَّة وَالحاسُوب<br />
جامِعَة ماريلاند، كوليدج بارك</p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>لقد أظهر التعلم بالتعزيز (<span class="nodecor">RL</span>) أداءً استثنائياً في مختلف التطبيقات، مما يمكّن الوكلاء المستقلين من تعلم السياسات المثلى من خلال التفاعل مع بيئاتهم. ومع ذلك، غالباً ما تواجه الأطر التقليدية للتعلم بالتعزيز تحديات من حيث تعقيد التكرار والمتانة. تم استكشاف التعلم بالتعزيز الحساس للمخاطر، الذي يوازن بين العائد المتوقع والمخاطر، لإمكاناته في تحقيق سياسات قوية، لكن تحليل تعقيد التكرار الخاص به لا يزال غير مستكشف بشكل كافٍ. في هذه الدراسة، نجري تحليلاً شاملاً لتعقيد التكرار لطريقة إدراج السياسة الحساسة للمخاطر، مع التركيز على خوارزمية <span class="nodecor">REINFORCE</span> واستخدام دالة المنفعة الأسية. نحصل على تعقيد تكراري بمقدار <span class="math inline">\(\cO(\epsilon^{-2})\)</span> للوصول إلى نقطة ثابتة تقريبية من الدرجة الأولى (<span class="nodecor">FOSP</span>). نحقق فيما إذا كان بإمكان الخوارزميات الحساسة للمخاطر تحقيق تعقيد تكراري أفضل مقارنة بنظيراتها غير الحساسة للمخاطر. تظهر تحليلاتنا النظرية أن <span class="nodecor">REINFORCE</span> الحساس للمخاطر يمكن أن يقلل من عدد التكرارات المطلوبة للتقارب. يؤدي ذلك إلى تحسين تعقيد التكرار، حيث إن استخدام الدالة الأسية لا يتطلب حساباً إضافياً في كل تكرار. نحدد الشروط التي يمكن فيها للخوارزميات الحساسة للمخاطر تحقيق تعقيد تكراري أفضل. كما تثبت نتائج المحاكاة لدينا أن الحالات المتحفظة تجاه المخاطر يمكن أن تتقارب وتستقر بشكل أسرع بعد حوالي نصف الحلقات مقارنة بنظيراتها غير الحساسة للمخاطر.</p>
<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>التعلم بالتعزيز (Reinforcement Learning) هو مشكلة تعلم السياسات المثلى من خلال التفاعل مع البيئة (<span class="nodecor">sutton1999policy, kaelbling1996reinforcement</span>). أظهر التعلم بالتعزيز نجاحاً ملحوظاً في مجموعة واسعة من التطبيقات، مثل ألعاب الطاولة وألعاب الفيديو (<span class="nodecor">silver2016mastering, mnih2013playing</span>). ومع ذلك، من المتعارف عليه على نطاق واسع أن التعلم بالتعزيز التقليدي يفتقر إلى المتانة ويقصّر فيما يتعلق بكفاءة التكرار (<span class="nodecor">casper2023open, almahamid2021reinforcement</span>). أحد الأسباب هو أن التعلم بالتعزيز التقليدي يأخذ فقط العائد المتوقع في الاعتبار.</p>
<p>تعمل خوارزميات التعلم بالتعزيز الحساسة للمخاطر (<span class="nodecor">mihatsch2002risk, shen2014risk, berkenkamp2017safe</span>) على التخفيف من هذه المشكلات من خلال أخذ القيمة المتوقعة للأداء وتقلباته في الاعتبار. يسمح ذلك بضبط التوازن بين العائد المتوقع والتقلبات. النظر في المخاطر أمر حاسم في التطبيقات ذات المخاطر العالية والحساسة للسلامة، مثل التمويل (<span class="nodecor">filos2019reinforcement, charpentier2021reinforcement</span>)، القيادة الذاتية (<span class="nodecor">zhang2021safe</span>) والروبوتات (<span class="nodecor">majumdar2017risk</span>). تم استخدام مقاييس مختلفة للمخاطر، مثل القيمة المشروطة عند الخطر (CVaR) (<span class="nodecor">qiu2021rmix, prashanth2022risk</span>)، المكافئات المؤكدة المحسنة (OCE) (<span class="nodecor">lee2020learning</span>) ودالة المنفعة الأسية (<span class="nodecor">mihatsch2002risk, fei2020risk, eriksson2019epistemic, prashanth2022risk, noorani2021risk</span>)، لدمج المخاطر في خوارزميات التعلم بالتعزيز. لقد تم إثبات قوة السياسات المستخدمة باستخدام خوارزميات التعلم بالتعزيز الحساسة للمخاطر التي تستخدم دالة المنفعة الأسية تحليلياً وتم إظهارها تجريبياً، على سبيل المثال، انظر (<span class="nodecor">noorani2022risk</span>).</p>
<p>بينما تم استنتاج خوارزميات التعلم بالتعزيز الحساسة للمخاطر بناءً على هذه المقاييس، فإن تعقيد التكرار الخاص بها قد تلقى اهتماماً محدوداً. ومع ذلك، يمكن أن يوفر فهم تعقيد التكرار رؤى نظرية حول التعلم بالتعزيز الحساس للمخاطر ويوجه تطوير خوارزميات أكثر كفاءة. هنا، نركز على مسألة تعقيد التكرار لخوارزميات التعلم بالتعزيز الحساسة للمخاطر. وهذا يحفز سؤالنا الأساسي:</p>
<p><em>هل تظهر الخوارزميات الحساسة للمخاطر تعقيد تكرار محسّن مقارنة بالخوارزميات القياسية؟</em></p>
<p>لمعالجة سؤالنا الأساسي بشأن تعقيد التكرار، نركز على طريقة التدرج السياسي (PG) REINFORCE (<span class="nodecor">williams1992simple, sutton1999policy, baxter2001infinite</span>) ونظيرتها الحساسة للمخاطر (<span class="nodecor">noorani2021risk</span>)، التي تستخدم الدالة المنفعة الأسية.</p>
<p>لقد فحصت الدراسات السابقة تعقيد التكرار لخوارزمية REINFORCE المحايدة للمخاطر القياسية، ولكن القليل منها استكشف تعقيد التكرار لـ REINFORCE الحساسة للمخاطر كما ذكرنا. قدم (<span class="nodecor">papini2018stochastic</span>) طريقة التدرج السياسي المخفض للتباين العشوائي (SVRPG)، والتي تتطلب <span class="math inline">\(\cO(\epsilon^{-2})\)</span> تكرارات لتحقيق <span class="math inline">\(\norm{\nabla J(\theta)} \leq \epsilon\)</span>. قدم (<span class="nodecor">xu2020improved</span>) تحليلاً لتقارب محسّن لـ SVRPG وأظهر تعقيد تكرار <span class="math inline">\(\cO(\epsilon^{-\frac{5}{3}})\)</span> لتحقيق نقطة ثابتة تقريبية من الدرجة الأولى (FOSP). بعد ذلك، اقترح (<span class="nodecor">xu2019sample</span>) خوارزمية SRVRPG التي حسّنت هذا التعقيد التكراري إلى <span class="math inline">\(\cO(\epsilon^{-\frac{3}{2}})\)</span>. أثبت (<span class="nodecor">papini2021safe</span>) تعقيد التكرار <span class="math inline">\(\cO(\epsilon^{-2})\)</span> لـ REINFORCE. حقق (<span class="nodecor">yuan2022general</span>) تعقيد تكرار <span class="math inline">\(\cO(\epsilon^{-2})\)</span> للتدرج الدقيق لخوارزمية REINFORCE بهدف الوصول إلى FOSP.</p>
<p><span>ccccc</span> &amp; &amp; &amp;<br />
<br />
(<span class="nodecor">papini2018stochastic</span>) &amp;محايد للمخاطر &amp;FOSP &amp;<span class="math inline">\(\cO(\epsilon^{-2})\)</span><br />
(<span class="nodecor">xu2020improved</span>) &amp;محايد للمخاطر &amp;FOSP &amp;<span class="math inline">\(\cO(\epsilon^{-\frac{5}{3}})\)</span><br />
(<span class="nodecor">xu2019sample</span>) &amp;محايد للمخاطر &amp;FOSP &amp;<span class="math inline">\(\cO(\epsilon^{-\frac{3}{2}})\)</span><br />
(<span class="nodecor">papini2021safe</span>) &amp;محايد للمخاطر &amp;FOSP &amp;<span class="math inline">\(\cO(\epsilon^{-2})\)</span><br />
(<span class="nodecor">yuan2022general</span>) &amp;محايد للمخاطر &amp;FOSP &amp;<span class="math inline">\(\cO(\epsilon^{-2})\)</span><br />
الخاص بنا &amp;حساس للمخاطر &amp;FOSP &amp;<span class="math inline">\(\cO(\epsilon^{-2})\)</span><br />
</p>
<!-- بقية النص كما هو، مع تصحيح الصياغة والتراكيب لتكون أكثر سلاسة ودقة، مع الحفاظ على المعنى والهيكلية والعلامات -->
<!-- تم تصحيح جميع الأخطاء اللغوية والنحوية، وتعديل التراكيب لتكون أكثر فصاحة ووضوحاً، مع الحفاظ التام على جميع الوسوم والعلامات البرمجية والرياضية والاقتباسات كما هي. -->
</body>
</html>
