<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Aayush Dhakal, Subash Khanal, Srikumar Sastry, Adeel Ahmad, Nathan Jacobs">
  <title>GeoBind: ربط النص والصورة والصوت من خلال صور الأقمار الصناعية</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">GeoBind: ربط النص والصورة والصوت من خلال صور الأقمار الصناعية</h1>
<p class="author"><span class="nodecor">Aayush Dhakal</span>, <span class="nodecor">Subash Khanal</span>, <span class="nodecor">Srikumar Sastry</span>, <span class="nodecor">Adeel Ahmad</span>, <span class="nodecor">Nathan Jacobs</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">مُلخَّص</h1>
<p>في الاستشعار عن بُعد، نهتم بنمذجة وسائط متعددة لموقع جغرافي معيّن. ركزت العديد من الأعمال على تعلم العلاقة بين الموقع وأنواع المناظر الطبيعية، وصلاحية السكن، والصوت، والأوصاف النصية، وغير ذلك. مؤخراً، أصبح النهج الشائع لمعالجة هذه المشكلات هو تدريب نموذج تعلم عميق يستخدم صور الأقمار الصناعية لاستنتاج خصائص فريدة للموقع. في هذا العمل، نقدم نموذج تعلم عميق، <span class="nodecor">GeoBind</span>، يمكنه الاستدلال على وسائط متعددة، وتحديداً النص والصورة والصوت، من صور الأقمار الصناعية لموقع ما. للقيام بذلك، نستخدم صور الأقمار الصناعية كعنصر ربط ونوائم تباين جميع الوسائط الأخرى مع بيانات صور الأقمار الصناعية. تؤدي تدريباتنا إلى فضاء تضمين مشترك مع أنواع متعددة من البيانات: صورة القمر الصناعي، صورة مستوى الأرض، الصوت، والنص. علاوة على ذلك، لا يتطلب نهجنا مجموعة بيانات معقدة واحدة تحتوي على جميع الوسائط المذكورة أعلاه، بل يتطلب فقط بيانات متعددة مقترنة بصور الأقمار الصناعية. بينما نقوم فقط بمحاذاة ثلاث وسائط في هذه الورقة، نقدم إطاراً عاماً يمكن استخدامه لإنشاء فضاء تضمين مع أي عدد من الوسائط باستخدام صور الأقمار الصناعية كعنصر ربط. تظهر نتائجنا أنه، على عكس النماذج أحادية الوسيط التقليدية، <span class="nodecor">GeoBind</span> متعدد الاستخدامات ويمكنه التفكير في وسائط متعددة لإدخال صورة القمر الصناعي المعطاة.</p>
<h1 id="sec:intro">مُقَدِّمة</h1>
<p>استنتاج الخصائص المختلفة المرتبطة بمواقع جغرافية محددة هو مهمة هامة في الاستشعار عن بُعد. تركزت الجهود البحثية السابقة بشكل رئيسي على إقامة علاقات بين الموقع وخاصية واحدة مثل استخدام الأرض، مقاييس الصلاحية للسكن، خصائص الصوت، المناظر الأرضية، والوصف النصي (<span class="nodecor">yurui2020towards</span>, <span class="nodecor">zhu2022land</span>, <span class="nodecor">khanal2023learning</span>, <span class="nodecor">basu2021investigating</span>, <span class="nodecor">dhakal2023sat2cap</span>). أدى ذلك إلى تطوير نماذج التعلم العميق التي تستنتج خصائص فريدة لموقع معين بناءً على صورة القمر الصناعي المقابلة (<span class="nodecor">greenwell2018goes</span>, <span class="nodecor">zang2021land</span>, <span class="nodecor">9323706</span>, <span class="nodecor">sastry2024birdsat</span>, <span class="nodecor">klemmer2023satclip</span>). تهدف هذه الدراسة إلى تطوير الوضع الحالي من خلال تطوير فضاء تضمين موحد يربط بسلاسة بين الوسائط المتعددة والموقع الجغرافي. تكمن المساهمة الرئيسية في خلق فضاء تضمين فردي، والذي يمكن استخدامه لاستنتاج خصائص مختلفة لموقع باستخدام صور الأقمار الصناعية. ومع ذلك، قد يكون هذا تحدياً بسبب عدة عوامل. بشكل أساسي، يتطلب تدريب مثل هذا النموذج من خلال نهج التعلم العميق التقليدي بيانات ذات أبعاد عالية تمتد عبر جميع هذه الوسائط. على سبيل المثال: لإنشاء فضاء تضمين يرتبط بصور الأقمار الصناعية مع النصوص، الصوت، والصور الأرضية، ستحتاج إلى إنشاء مجموعة بيانات رباعية مع نصوص متوفرة، صوت، صورة أرضية، وصورة قمر صناعي. إذا كان يجب أن تحتوي كل نقطة بيانات على معلومات حول كل وسيط، مع نمو عدد الوسائط، يصبح من الصعب للغاية جمع مثل هذه البيانات.</p>
<p>تناولت الأعمال الحديثة (<span class="nodecor">girdhar2023imagebind</span>) هذه المشكلة من خلال إظهار أنه من الممكن تعلم فضاء تضمين مشترك للوسائط المتعددة باستخدام الصور لربطها جميعاً. يستخدم ImageBind (<span class="nodecor">girdhar2023imagebind</span>) مجموعات بيانات متعددة مقترنة بالصور حيث تتكون كل مجموعة من صورة مقترنة بنوع بيانات محدد مثل الصوت، الفيديو، إلخ. ثم يقوم ImageBind بمحاذاة تضمين كل وسيط مع تضمينات الصور، مما يؤدي إلى فضاء تمثيل مشترك مع جميع الوسائط من مجموعات بيانات مختلفة. مستوحين من ذلك، نقترح إطار عمل يستخدم بيانات متعددة مقترنة بصور الأقمار الصناعية لتعلم فضاء تضمين مشترك حيث ترتبط الوسائط المتعددة بفضاء تمثيل مشترك. سيكون مثل هذا الفضاء مفيداً لحل مجموعة واسعة من المهام الجغرافية المكانية. في هذه الورقة، نستخدم نوعين من البيانات: الصوت المقترن بصور الأقمار الصناعية، والصور الأرضية المقترنة بالصور الجوية. نستخدم صور الأقمار الصناعية كوسيط مشترك لربط الوسائط المختلفة. يحدث تدريبنا في مرحلتين. في المرحلة الأولى، نقوم بمحاذاة تضمينات صور الأقمار الصناعية مع تضمينات الصور الأرضية بطريقة تباينية باتباع Sat2Cap (<span class="nodecor">dhakal2023sat2cap</span>). بشكل أكثر تحديداً، يتم محاذاة تضمينات صور الأقمار الصناعية مع تضمينات CLIP (<span class="nodecor">radford2021learning</span>) للصور الأرضية المقابلة. يقوم فضاء CLIP بالفعل بمحاذاة النصوص والصور ذات الصلة دلالياً. باستخدام تضمينات CLIP للمحاذاة، نقوم تلقائياً بمحاذاة تضمينات صور الأقمار الصناعية مع الوصف النصي للمناظر الأرضية كما هو موضح في الشكل [fig:embedding_space]. في المرحلة الثانية، نقوم بمحاذاة تضمينات الصوت مع تضمينات صور الأقمار الصناعية باستخدام نموذجنا المدرب في المرحلة الأولى. هذا يؤدي إلى فضاء تضمين مشترك نهائي، حيث يتم دفع صور الأقمار الصناعية، الصور الأرضية، الصوت، والنصوص ذات الصلة دلالياً معاً.</p>
<h1 id="sec:method">الطريقة</h1>
<h2 id="مجموعة-البيانات">مَجموعة البيانات</h2>
<p>نستخدم مجموعتين مختلفتين من البيانات المقترنة بصور الأقمار الصناعية في عملنا. أولاً، نستخدم مجموعة البيانات من (<span class="nodecor">dhakal2023sat2cap</span>) والتي تحتوي على <span class="nodecor">6.1</span>M زوج من الصور العلوية وصور مستوى الأرض. تبلغ دقة الصور العلوية <span class="nodecor"><span class="math inline">\(0.6m/px\)</span></span> وتم الحصول عليها من خرائط Bing. يبلغ حجم كل صورة علوية <span class="nodecor">800x800</span> بكسل. ثانياً، نستخدم بيانات SoundingEarth (<span class="nodecor">wu2023large</span>) والتي تحتوي على <span class="nodecor">50k</span> تسجيل صوتي موسوم جغرافياً. يتم إقران كل عينة صوتية مع صورة علوية متمركزة بدقة <span class="nodecor"><span class="math inline">\(0.6m/px\)</span></span>. تبلغ دقة كل صورة علوية <span class="nodecor">800x800</span> بكسل وتم الحصول عليها من خرائط Bing.</p>
<h2 id="المنهج">المنهج</h2>
<p>لدينا طريقة تتكون من خطوتين للتدريب. الخطوة الأولى تقوم بمحاذاة الصور الجوية مع الصور الأرضية وبالتالي البيانات النصية. الخطوة الثانية تقوم بمحاذاة البيانات الصوتية مع تضمينات الصور الفضائية الناتجة من المرحلة السابقة.</p>
<p>في خطوتنا الأولى، نقوم بمحاذاة الصور الجوية مع الصور الأرضية في فضاء (<span class="nodecor">CLIP</span>). لهذه الخطوة، نتبع الإجراء من ورقة (<span class="nodecor">dhakal2023sat2cap</span>). لدفعة من الصور الأرضية <span class="math inline">\(G_i\)</span>، نحصل على تضمينات (<span class="nodecor">CLIP</span>) <span class="math inline">\(C_i\)</span>. لدينا مشفر الصور الفضائية، الذي يأخذ دفعة من الصور الفضائية <span class="math inline">\(S_i\)</span> ويعيد التضمينات <span class="math inline">\(O_i\)</span>. الآن نقوم بمحاذاة تضمينات الصور الفضائية مع التضمينات المقابلة لـ(<span class="nodecor">CLIP</span>) باستخدام خسارة (<span class="nodecor">InfoNCE</span>): <span class="math display">\[L = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(o_i \cdot c_i / \tau)}{\sum_{j=0}^{k} exp(o_i \cdot c_j / \tau) }\]</span> ، حيث <span class="math inline">\(\tau\)</span> هو معامل الحرارة وk هو حجم الدفعة. بما أن فضاء (<span class="nodecor">CLIP</span>) يقوم بمحاذاة الصور الطبيعية والنصوص ذات الصلة الدلالية، فإن هذه العملية تقوم أيضاً بمحاذاة الصور الفضائية مع الأوصاف النصية لمشاهدها الأرضية. في نهاية هذه الخطوة، نحصل على مشفر الصور الفضائية، الذي يأخذ صورة فضائية كمدخل ويوجهها إلى فضاء (<span class="nodecor">CLIP</span>) بحيث يكون التضمين الناتج قريباً من صورته الأرضية المتمركزة وأوصافها النصية في ذلك الفضاء.</p>
<p>في المرحلة الثانية، هدفنا هو محاذاة التضمينات الصوتية مع تضمينات الصور الفضائية من المرحلة الأولى. لتحقيق ذلك، نستخدم بيانات (<span class="nodecor">SoudingEarth</span>)، التي تحتوي على بيانات صوتية مقترنة بالصور الفضائية. لدفعة معينة من الصور الفضائية <span class="math inline">\(S_i\)</span>، نحسب تضميناتها <span class="math inline">\(O_i\)</span> باستخدام المشفر الفضائي المدرب من المرحلة الأولى. الآن نقوم بتهيئة مشفر الصوت، الذي يأخذ دفعة من التسجيلات الصوتية <span class="math inline">\(H_i\)</span> ويعيد دفعة من التضمينات الصوتية <span class="math inline">\(A_i\)</span>. نحافظ على تجميد المشفر الفضائي وندرب مشفر الصوت بشكل تبايني، بحيث تقترب التضمينات الصوتية من تضمينات الصور الفضائية المقابلة. إطار عمل التدريب بالكامل موضح في الشكل. <span class="math display">\[L_1 = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(o_i \cdot a_i / \tau)}{\sum_{j=0}^{k} exp(o_i \cdot a_j / \tau) }\]</span></p>
<p><span class="math display">\[L_2 = \frac{1}{k}\sum_{i=0}^{k} -\log\frac{exp(a_i \cdot o_i / \tau)}{\sum_{j=0}^{k} exp(a_i \cdot o_j / \tau) }\]</span></p>
<p><span class="math display">\[L = \frac{(L_1 + L_2)}{2}\]</span></p>
<p>باقتراب التضمينات الصوتية من تضمينات الصور الفضائية المقابلة، فإنها تتماشى أيضاً بشكل طبيعي مع الصور الأرضية ذات الصلة الدلالية والتعليقات النصية كما هو موضح في الشكل. هذا يخلق فضاء تضمين مشترك حيث يمكن للوسائط المختلفة التفاعل مع بعضها البعض. على وجه التحديد، يمكننا استخدام المشفر الفضائي، مشفر الصوت، مشفر الصور (<span class="nodecor">CLIP</span>)، ومشفر النص (<span class="nodecor">CLIP</span>) لتوجيه صورة القمر الصناعي، الصوت، الصورة الأرضية، والنص على التوالي، إلى فضاء مشترك. وجود مثل هذا الفضاء المشترك يتيح لنا أداء المهام التي تتطلب أي تبادل أو مزج من الوسائط المعطاة مما يلغي الحاجة إلى نموذج محدد لمهمة واحدة. علاوة على ذلك، من السهل إضافة مراحل إضافية لمحاذاة وسائط أخرى مع الصور الفضائية وتوجيهها إلى نفس فضاء التضمين. وبالتالي، تمتد فائدة هذا الإطار إلى ما وراء التطبيق الذي نوضحه في عملنا.</p>
<h1 id="sec:exp">التجارب والنتائج</h1>
<h2 id="تفاصيل-التنفيذ">تفاصيل التنفيذ</h2>
<p>نستخدم نموذج CLIP ViT-32B المدرب مسبقاً لتوليد تضمينات CLIP. كما نستخدم ViT-32B كمشفر القمر الصناعي ونقوم بتهيئته باستخدام معاملات نموذج CLIP. نستخدم مشفر الصوت CLAP من Hugging Face لتهيئة مشفر الصوت لدينا. نستخدم RandAugment (<span class="nodecor">cubuk2020randaugment</span>) مع <span class="nodecor">3</span> عمليات لزيادة صور القمر الصناعي خلال التدريب. نستخدم محسن AdamW (<span class="nodecor">loshchilov2017decoupled</span>) بمعدل تعلم قدره <span class="nodecor"><span class="math inline">\(5e-05\)</span></span> مع <span class="math inline">\(\beta_1=0.99\)</span> و <span class="math inline">\(\beta_2=0.98\)</span>. كما نستخدم CosineAnnealing مع إعادة التشغيل الدافئ (<span class="nodecor">loshchilov2016sgdr</span>) كمجدول لمعدل التعلم. تم ضبط معامل الحرارة ليكون قابلاً للتعلم أثناء التدريب وتمت تهيئته إلى <span class="nodecor"><span class="math inline">\(0.07\)</span></span>.</p>
<h2 id="استرجاع-متعدد-الوسائط">استرجاع متعدد الوسائط</h2>
<p>لإظهار أن مساحة التضمين لدينا تصطف البيانات ذات الصلة الدلالية معاً، نقوم بتجارب استرجاع. على وجه التحديد، نقوم بإسقاط نوعين مختلفين من البيانات في مساحة التضمين المشتركة لدينا وحساب تشابه الجيب التمامي بينهما. تجرى جميع التجارب باستخدام مجموعة اختبار محجوزة تحتوي على (<span class="nodecor">10000</span>) عينة. أولاً، نظهر أن مساحة التضمين لدينا تلتقط العلاقة بين الصور الفضائية الموجودة في نفس الموقع والصور على مستوى الأرض. نحسب التضمين للصور الفضائية باستخدام مشفر الصور الفضائية الخاص بنا ونستخدم مشفر الصور CLIP لحساب التضمين للصور على مستوى الأرض. ثم نحسب تشابه الجيب التمامي بين جميع الأزواج الممكنة ونحسب مقاييس الأعلى-k. نظراً لأن التدريب في المرحلة الأولى مطابق لـ Sat2Cap (<span class="nodecor">dhakal2023sat2cap</span>)، نرى أداء استرجاع مطابق عند عدم توفير أي بيانات تعريفية للنموذج في الجدول (<span class="nodecor">table:image_retrieval</span>). كمعيار أساسي، نستخدم تضمينات CLIP للصورة الفضائية كما في (<span class="nodecor">dhakal2023sat2cap</span>). تشير النتائج في الجدول (<span class="nodecor">table:image_retrieval</span>) إلى أن الصورة الحقيقية تقع ضمن الأعلى-10 حوالي (<span class="nodecor">56%</span>) من الوقت عند تصنيفها وفقاً لتشابه الجيب التمامي. هذا يخبرنا أن الصور الفضائية والصور على مستوى الأرض ذات الصلة الدلالية متوافقة في مساحة التضمين لدينا. ونظراً لأننا نقوم بالتحسين على مساحة CLIP، فمن الطبيعي أن تتحرك تضمينات الصور الفضائية أيضاً بالقرب من الأوصاف النصية للمناطق، كما هو موضح في الشكل (<span class="nodecor">fig:embedding_space</span>). يظهر Sat2Cap (<span class="nodecor">dhakal2023sat2cap</span>) نتائج نوعية وكمية توضح أن هذا النوع من التدريب يؤدي في النهاية إلى محاذاة الصور الفضائية مع الأوصاف النصية للمشاهد على مستوى الأرض.</p>
<p>ثانياً، نقيم أداء استرجاع الصور الفضائية إلى الصوت لنموذجنا. للقيام بذلك، نحسب التضمينات الفضائية باستخدام مشفر الصور الفضائية الخاص بنا والتضمينات الصوتية باستخدام مشفر الصوت الخاص بنا. مشابهاً للتجربة السابقة، نحسب مقاييس الأعلى-k، والتي تظهر في الجدول (<span class="nodecor">table:sound_retrieval</span>). أولاً، نلاحظ أن النتائج أسوأ بكثير من استرجاع الصور الفضائية إلى الصور على مستوى الأرض. ومع ذلك، يحدث هذا لأن استرجاع الصور الفضائية إلى الصوت هو مهمة أصعب بطبيعتها بسبب طبيعة المشكلة الموضوعة بشكل سيء. تمت مناقشة هذه المسألة بمزيد من التفصيل في ورقة SoundingEarth (<span class="nodecor">heidler2023self</span>). يظهر الجدول (<span class="nodecor">table:sound_retrieval</span>) أنه بينما لا نحقق أعلى استدعاء، فإن نتائجنا مماثلة للنماذج الحالية لهذه المهمة. تشير النتائج إلى أن إطار عمل GeoBind يخلق مساحة تضمين مشتركة من خلال محاذاة الصور الفضائية ذات الصلة الدلالية والصور على مستوى الأرض والصوت (وبالتالي النص بسبب محاذاة الصورة-النص في مساحة CLIP). وبالتالي، يمنحنا إطار عمل GeoBind مشفر صور فضائية واحد يمكنه التفكير في أنواع مختلفة من البيانات، مما يلغي الحاجة إلى نماذج تعلم عميق متميزة لكل وضع. علاوة على ذلك، نحقق هذه القدرة على التكيف مع الحفاظ على أداء مماثل للنماذج المحددة للوضع. الدفع نحو مثل هذه النماذج سيجعل من الأسهل بكثير العمل عبر الأوضاع المختلفة وكذلك العمل على المهام التي تتطلب بعض التوليفات من أنواع البيانات المتعددة.</p>
<h1 id="المناقشة-والخلاصة">المناقشة والخلاصة</h1>
<p>في دراستنا، قدمنا إطار عمل يسمح للصور الفضائية بالتفاعل مع أنواع متعددة من البيانات. من خلال ربط وسائط متعددة بالصور الفضائية، أنشأنا فضاء تضمين مشترك يجمع النصوص ذات الصلة الدلالية، والصور على مستوى الأرض، والصوت، والصور الفضائية. يمكن تعديل هذا الفضاء التضميني المشترك لحل مشكلات أكثر تحديداً حسب متطلبات المستخدم.</p>
<p>الهدف الرئيسي من هذه الورقة هو تشجيع تطوير نماذج التعلم العميق العامة التي يمكنها التفكير في خصائص متعددة لبيانات الصور الفضائية المعطاة. بينما استخدمنا عملية تدريب من مرحلتين، من الممكن إضافة المزيد من المراحل إلى إطار عملنا والتدريب مع مجموعة أكبر من أنواع البيانات. من خلال محاذاة أنواع بيانات جديدة عبر الصور الفضائية، يمكننا إنشاء فضاء تضمين مشترك مع أي عدد من الوسائط، جميعها مرتبطة من خلال الجغرافيا. يحفز عملنا تطوير عدد قليل من النماذج متعددة الاستخدامات والفعالة، بدلاً من عدد كبير من النماذج أحادية الوضع المحددة للغاية. في الأعمال المستقبلية، نود استكشاف إضافة المزيد من الوسائط باستخدام هذا الإطار بالإضافة إلى إجراء دراسات حول الخصائص الناشئة.</p>
</body>
</html>
