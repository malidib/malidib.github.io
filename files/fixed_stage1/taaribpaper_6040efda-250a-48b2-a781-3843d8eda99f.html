<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Osvaldo Luamba Quinjica David Ifeoluwa Adelani">
  <title>AngOFA: استغلال تهيئة التضمين OFA والبيانات الاصطناعية لنموذج اللغة الأنغولية</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">AngOFA: استغلال تهيئة التضمين OFA والبيانات الاصطناعية لنموذج اللغة الأنغولية</h1>
<p class="author"><span class="nodecor">Osvaldo Luamba Quinjica</span><br />
<span class="nodecor">David Ifeoluwa Adelani</span></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>في السنوات الأخيرة، شهد تطوير نماذج اللغة المدربة مسبقاً (PLMs) زخماً متزايداً، حيث أظهرت قدرتها على تجاوز الحواجز اللغوية وتسهيل نقل المعرفة عبر لغات متنوعة. ومع ذلك، فقد شمل هذا التقدم بشكل رئيسي اللغات ذات الموارد العالية، مما أدى إلى خلق فجوة ملحوظة في المشهد متعدد اللغات. يتناول هذا البحث هذه الفجوة من خلال تقديم أربعة نماذج PLMs مصممة خصيصاً ومعدلة بدقة للغات الأنغولية، باستخدام نهج التكييف الدقيق متعدد اللغات (MAFT). في هذا البحث، نستعرض دور تهيئة التضمين المستنير والبيانات الاصطناعية في تعزيز أداء نماذج MAFT في المهام اللاحقة. نحن نحسن الأداء بالاعتماد على AfroXLMR-base (المطورة من خلال MAFT) وOFA (تهيئة التضمين الفعالة) بمقدار <span class="nodecor">12.3</span> و <span class="nodecor">3.8</span> نقاط على التوالي.</p>
<h1 id="تقديم-الأوراق-لورشة-عمل-africanlp-في-iclr2023">تقديم الأوراق لورشة عمل AfricaNLP في ICLR<span class="nodecor">2023</span></h1>
<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>لقد شهدت نماذج اللغة ومجموعات تقييم اللغات تقدماً ملحوظاً عبر العديد من لغات العالم (<span class="nodecor">devlin-etal-2019-bert</span>, <span class="nodecor">conneau-etal-2020-unsupervised</span>, <span class="nodecor">workshop2023bloom</span>, <span class="nodecor">xue-etal-2021-mt5</span>). ومع ذلك، غالباً ما تم تجاهل العديد من اللغات الأفريقية، مما أدى إلى خلق فجوة كبيرة. في الوقت نفسه، تجاهلت معظم نماذج اللغة المركزة على أفريقيا تضمين اللغات الأنغولية (<span class="nodecor">dossou-etal-2022-afrolm</span>, <span class="nodecor">alabi-etal-2022-adapting</span>, <span class="nodecor">ogueji-etal-2021-small</span>). لقد كانت جهود مجتمع أفريقيا في معالجة اللغات الطبيعية واضحة في توسيع مجموعات تقييم اللغات النهائية (<span class="nodecor">adelani-etal-2021-masakhaner</span>, <span class="nodecor">adelani-etal-2022-masakhaner</span>, <span class="nodecor">muhammad-etal-2023-semeval</span>, <span class="nodecor">ma2023taxi1500</span>). ومع ذلك، على الرغم من هذه المبادرات، لا تزال اللغات الأنغولية تفتقر إلى التمثيل الكافي.</p>
<p>في سعينا لتطوير نموذج لغة متعدد اللغات مدرب مسبقاً، هناك نهجان رئيسيان. الأول يتضمن بناء نموذج من الصفر، وتدريبه مباشرة على لغات متعددة، باستخدام تعلم ذاتي محدد مثل نمذجة اللغة المقنعة (<span class="nodecor">devlin-etal-2019-bert</span>). النهج البديل هو التكييف الدقيق متعدد اللغات (MAFT) والذي يتضمن تكييف نموذج لغة متعدد اللغات مدرب مسبقاً موجود مع مجموعة جديدة من اللغات (<span class="nodecor">alabi-etal-2022-adapting</span>, <span class="nodecor">wang-etal-2022-expanding</span>, <span class="nodecor">imanigooghari-etal-2023-glot500</span>). يكتسب MAFT تفضيلاً لكفاءته في استخدام الموارد، خاصة في السيناريوهات التي تفرض فيها الميزانيات الحاسوبية قيوداً وسط تصاعد أحجام النماذج (<span class="nodecor">tay2022scale</span>, <span class="nodecor">gupta2023continual</span>). يمكن تعزيز أداء MAFT من خلال إدخال رموز مفردات جديدة للغات الإضافية واستخدام تهيئة تضمين غير غاوسية (<span class="nodecor">minixhofer-etal-2022-wechsel</span>, <span class="nodecor">dobler-de-melo-2023-focus</span>, <span class="nodecor">liu2023ofa</span>).</p>
<p>في هذه الورقة، نقدم أول مجموعة من نماذج PLM متعددة اللغات مصممة لخمس لغات أنغولية باستخدام نهج MAFT. نقارن PLMs المطورة من خلال MAFT مع وبدون تهيئة التضمين المستنير، المشار إليهما باسم <span class="nodecor">angofa</span> و <span class="nodecor">angbert</span>، على التوالي. من خلال الاستفادة من نهج OFA لأداء تهيئة التضمين قبل تنفيذ MAFT، تكشف نتائجنا أن <span class="nodecor">angofa</span> يتفوق بشكل كبير على <span class="nodecor">angbert</span> وOFA، مما يبرز التحسينات الكبيرة في الأداء التي يمكن تحقيقها من خلال دمج تهيئة التضمين المستنير والبيانات الاصطناعية.</p>
<h1 id="مفاجأة">مُفاجَأَة</h1>
<p>وجدنا أن <span class="nodecor">OFA</span> المطور على أكثر من <span class="nodecor">500</span> لغة يمتلك أداءً مشابهاً لأداء <span class="nodecor">AngOFA</span>، مما يؤكد على قابلية <span class="nodecor">OFA</span> للتوسع عبر لغات متعددة.</p>
<h1 id="اللغات-الأنغولية">اللُغات الأنغولية</h1>
<p>تتميز أنغولا بمشهد لغوي غني يضم أكثر من <span class="nodecor">40</span> لغة وعدد سكان يبلغ <span class="nodecor">32</span> مليون نسمة. تشمل اللغات الأنغولية البرتغالية، وبعض لغات الخويسان، ومعظمها من لغات البانتو التابعة لعائلة النيجر-الكونغو. على الرغم من هذا التنوع اللغوي، هناك نقص ملحوظ في الأدب والبرامج الإذاعية والتلفزيونية باللغات الأنغولية الأصلية. تكتب جميع اللغات في أنغولا بالأبجدية اللاتينية، وتشترك العديد منها في ديغرافات مشتركة. نظراً لندرة البيانات، سيركز اهتمامنا بشكل أساسي حول اللغات الأنغولية الخمس الأكثر تحدثاً: أومبوندو، كيمبوندو، كيكونغو، تشوكوي، ولوَبَأ-كاساي. انظر الجدول [table-angola-languages] لمزيد من التفاصيل.</p>
<h1 id="النهج-لتحسين-maft">النهج لتحسين <span class="nodecor">MAFT</span></h1>
<h2 id="vocab-expansion">توسيع المفردات</h2>
<p>تميل نماذج اللغة المبرمجة إلى مواجهة رموز خارج المفردات للغات أو النصوص التي لم تُغطَّ أثناء التدريب المسبق. يظهر هذا بشكل أوضح في النصوص غير المرئية (<span class="nodecor">adelani-etal-2021-masakhaner</span>, <span class="nodecor">pfeiffer-etal-2021-unks</span>)، وأحد أكثر الطرق فعالية للتعامل مع ذلك هو توسيع مفردات نموذج اللغة المبرمجة لتغطية الرموز الجديدة (<span class="nodecor">wang-etal-2019-improving</span>). تم إنشاء Glot-500 (<span class="nodecor">imanigooghari-etal-2023-glot500</span>) عن طريق توسيع مفردات XLM-R من <span class="nodecor">250K</span> إلى <span class="nodecor">400K</span> قبل MAFT. ومع ذلك، تم تهيئة الرموز الجديدة المضافة بشكل عشوائي.</p>
<h2 id="عامل-التضمين-ofa">عامل التضمين OFA</h2>
<p>يعالج OFA مشكلتين في تكييف نماذج اللغة المبرمجة مسبقاً مع لغات جديدة: (١) البدء العشوائي لتضمينات الكلمات الفرعية الجديدة لا يستفيد من المعرفة اللغوية المشفرة في النموذج المصدر، (٢) إدخال معاملات إضافية يشكل عقبات محتملة أمام التدريب الفعال للنموذج المعدل (<span class="nodecor">liu2023ofa</span>). يحل OFA هذه المشكلات من خلال الاستفادة من التضمينات متعددة اللغات الخارجية والتضمينات في نموذج اللغة المبرمجة المصدر لتهيئة تضمينات الكلمات الفرعية الجديدة. في هذا النهج، يقوم OFA بتحليل مصفوفة التضمينات لنموذج اللغة المبرمجة المصدر إلى مصفوفتين أصغر كبدائل. في فضاء ذو أبعاد أقل، يتم التعبير عن تضمينات الكلمات الفرعية الجديدة غير المتداخلة كمجموعات من تضمينات الكلمات الفرعية لنموذج اللغة المبرمجة المصدر. توزن هذه المجموعات بواسطة التشابهات المستمدة من التضمينات متعددة اللغات الخارجية المحاذاة جيداً، أي ColexNet+ (<span class="nodecor">liu2023crosslingual</span>)، التي تغطي أكثر من ألف لغة. تُنسخ تضمينات الكلمات الفرعية المتداخلة مباشرة. يضمن هذا النهج أن تضمينات الكلمات الفرعية المشتركة بين نموذج اللغة المبرمجة المصدر والمفردات الموسعة متكاملة، محافظة على الاستمرارية في التمثيل. لإكمال العملية، يقوم OFA بتكرار جميع المعاملات غير التضمينية من نموذج اللغة المبرمجة المصدر، ويستبدل المحلل اللغوي المصدر بالمحلل اللغوي الهدف بعد توسيع المفردات.</p>
<h1 id="النماذج-الأساسية">النماذج الأساسية</h1>
<h2 id="synthetic-data">البيانات الاصطناعية لنمذجة اللغة</h2>
<p>بالنسبة للغات التي تفتقر إلى بيانات كافية قبل التدريب، يمكن توليد بيانات اصطناعية من خلال توسيع القاموس (<span class="nodecor">reid-etal-2021-afromt</span>) أو نموذج الترجمة الآلية (MT) - وهو نهج شائع جداً في بحوث الترجمة الآلية يعرف باسم الترجمة العكسية، وهو طريقة فعالة لتحسين نموذج الترجمة الآلية للغات ذات الموارد المنخفضة (<span class="nodecor">sugiyama-yoshinaga-2019-data</span>, <span class="nodecor">xia-etal-2019-generalized</span>). في هذه الورقة، نستخدم البيانات الاصطناعية التي تم الحصول عليها من خلال الترجمة الآلية كما وصف في (<span class="nodecor">adelani2023sib200</span>). لقد قام المؤلفون بتوليد بيانات مترجمة آلياً لـ <span class="nodecor">34</span> لغة أفريقية (بما في ذلك اللغات الأنغولية) بأقل من <span class="nodecor">10MB</span> من البيانات، باستخدام مجموعة بيانات تعليقات الأخبار الإنجليزية (<span class="nodecor">kocmi-etal-2022-findings</span>)، والتي تحتوي على أكثر من <span class="nodecor">600K</span> جملة.</p>
<h1 id="البيانات">البيانات</h1>
<h2 id="train_data">بيانات التدريب</h2>
<p>استفدنا من مجموعة بيانات NLLB (<span class="nodecor">nllb2022</span>)، مستثنين الترجمات الإنجليزية، وركزنا فقط على لغات كيمبوندو، أومبوندو، كيكونغو، تشوكوي، ولوَبَأ-كاساي. تم دمج هذه اللغات في ملف واحد كمجموعة بيانات أولية للتدريب. بالإضافة إلى ذلك، أضفنا بيانات اصطناعية تم توليدها من خلال NLLB. تعرض التفاصيل بيانات أحادية اللغة.</p>
<h2 id="eval_data">بيانات التقييم</h2>
<p>في عملنا، قمنا بالتقييم على مجموعة بيانات تصنيف النصوص SIB-<span class="nodecor">200</span> (<span class="nodecor">adelani2023sib200</span>)، والتي توفر مجموعات تدريب/تطوير/اختبار مع <span class="nodecor">7</span> فئات في أكثر من <span class="nodecor">200</span> لغة ولهجة أفريقية. توزيع الفئات هو: العلوم/التكنولوجيا (<span class="nodecor">252</span>)، السفر (<span class="nodecor">198</span>)، السياسة (<span class="nodecor">146</span>)، الرياضة (<span class="nodecor">122</span>)، الصحة (<span class="nodecor">110</span>)، الترفيه (<span class="nodecor">93</span>)، الجغرافيا (<span class="nodecor">83</span>). SIB-<span class="nodecor">200</span> هي المجموعة الوحيدة التي تغطي اللغات الأنغولية. لقد قمنا بالتقييم فقط على مجموعة اللغات الأنغولية المغطاة في هذا العمل.</p>
<h1 id="الإعداد-التجريبي">الإعداد التجريبي</h1>
<p>لقد استفدنا من القدرات اللغوية المتعددة لـ XLM-R (<span class="nodecor">conneau-etal-2020-unsupervised</span>) للتدريب، مما أدى إلى إنشاء مجموعة جديدة من نماذج اللغة المبرمجة: <span class="nodecor">AngBERT</span> و <span class="nodecor">AngOFA</span>. هذه النماذج خضعت لعمليات تهيئة دقيقة مختلفة. على وجه التحديد، خضع <span class="nodecor">AngBERT</span> لعملية التهيئة باستخدام طريقة MAFT كما هو موضح في (<span class="nodecor">alabi-etal-2022-adapting</span>)، بنوعين - أحدهما تم تدريبه فقط على البيانات أحادية اللغة (<span class="nodecor">281.6</span> MB)، والآخر يشمل كلاً من البيانات أحادية اللغة والبيانات الاصطناعية (<span class="nodecor">808.7</span> MB).</p>
<p>وبالمثل، خضع <span class="nodecor">AngOFA</span> أيضاً لنوعين من التهيئة، باستخدام مجموعات البيانات بنفس الطريقة كما في <span class="nodecor">AngBERT</span>. ومع ذلك، اتبع <span class="nodecor">AngOFA</span> التكوينات الموضحة لـ <code>ofa-multi-768</code>، كما هو موصوف في (<span class="nodecor">liu2023ofa</span>). اخترنا الحفاظ على <span class="nodecor">768</span> كبعد كامن وحيد في تجاربنا استناداً إلى الرؤى من (<span class="nodecor">imanigooghari-etal-2023-glot500</span>, <span class="nodecor">liu2023ofa</span>) والتي تدعمها أيضاً النتائج الأولية من تجاربنا الخاصة. كشفت هذه النتائج عن دلائل على فقدان المعلومات في الأبعاد الأدنى، وهو ما كان ملحوظاً بشكل خاص في مهام مثل تصنيف النصوص. كان الهدف من هذا النهج في تقسيم البيانات هو استكشاف تأثيرات طرق MAFT وOFA، سواء مع البيانات الاصطناعية أو بدونها، على أداء النموذج.</p>
<p>قمنا بمقارنة نماذجنا الجديدة مع النماذج الأساسية التالية:</p>
<ol>
<li><p>XLM-R (<span class="nodecor">conneau-etal-2020-unsupervised</span>): نموذج يعتمد فقط على المشفر والذي خضع للتدريب المسبق على <span class="nodecor">100</span> لغة من خلال هدف نمذجة اللغة المقنعة. XLM-R لا يغطي أي لغة تم تقييمها في هذا العمل.</p></li>
<li><p>Serengeti (<span class="nodecor">adebara-etal-2023-serengeti</span>): تم تدريبه على <span class="nodecor">500</span> لغة أفريقية، بما في ذلك <span class="nodecor">10</span> لغات ذات موارد عالية. يشمل Kimbundu، Umbundu، و Chokwe.</p></li>
<li><p>Glot-500 (<span class="nodecor">imanigooghari-etal-2023-glot500</span>): مشتق من XLM-R، تم توسيعه ليغطي <span class="nodecor">500</span> لغة من خلال توسيع مفرداته من <span class="nodecor">250K</span> إلى <span class="nodecor">400K</span>، وبالتالي استيعاب رموز جديدة تمثل <span class="nodecor">400</span> لغة غير موجودة سابقاً في XLM-R. Glot-500 يغطي جميع اللغات الأنغولية المستخدمة في تقييمنا.</p></li>
<li><p>AfroXLMR-base (<span class="nodecor">alabi-etal-2022-adapting</span>): تم تطويره باستخدام طريقة MAFT، يغطي <span class="nodecor">20</span> لغة مع مجموعة أحادية اللغة لا تقل عن <span class="nodecor">50MB</span>. اللغات الأنغولية غير مشمولة.</p></li>
<li><p>AfroXLMR-base-76L (<span class="nodecor">adelani2023sib200</span>): تم تطويره باستخدام طريقة MAFT، يغطي اللغات التي لديها بيانات على الويب لا تقل عن <span class="nodecor">10MB</span>. يوسع التغطية لتشمل المزيد من اللغات، ولا سيما تلك المدرجة في نموذج NLLB-200 MT. تم إنشاء بيانات اصطناعية أيضاً لحوالي <span class="nodecor">30</span> لغة ذات بيانات محدودة، بما في ذلك جميع اللغات الأنغولية الخمس. في المجمل، يغطي <span class="nodecor">76</span> لغة.</p></li>
<li><p>OFA (<span class="nodecor">liu2023ofa</span>): يدمج تهيئة التضمين OFA جنباً إلى جنب مع MAFT باستخدام Glot500-c (<span class="nodecor">imanigooghari-etal-2023-glot500</span>)، وبالتالي يشمل جميع اللغات المعالجة في هذا العمل.</p></li>
</ol>
<h1 id="مهمة-التقييم">مُهِمَّة التقييم</h1>
<h1 id="النتائج-والمناقشة">النتائج والمناقشة</h1>
<p><strong>نتائج المعيار</strong>: مقارنة فعالية (<span class="nodecor">OFA</span>) مع التهيئة العشوائية قبل التكييف الدقيق متعدد اللغات (<span class="nodecor">MAFT</span>)</p>
<p>Table[table-1] تُظهر أداء نماذجنا الأساسية باستخدام <strong>مقياس F1 الموزون</strong>. نناقش أهم النتائج أدناه:</p>
<h4 id="نماذج-اللغة-المحددة-بالمنطقة-أفضل-من-تلك-المدربة-مسبقا-من-الصفر-بالعديد-من-اللغات">نماذج اللغة المحددة بالمنطقة أفضل من تلك المدربة مسبقاً من الصفر بعدة لغات</h4>
<p>أظهرت نتائجنا أن (<span class="nodecor">AngBERT</span>) المنشأ باستخدام (<span class="nodecor">MAFT</span>) أدى أداءً أفضل من (<span class="nodecor">XLM-R</span>)، (<span class="nodecor">AfroXLMR</span>)، (<span class="nodecor">Serengeti</span>) و(<span class="nodecor">Glot-500</span>) بـ <span class="math inline">\(+5.5\)</span>، <span class="math inline">\(+1.2\)</span>، <span class="math inline">\(+3.6\)</span>، <span class="math inline">\(+6.6\)</span> نقاط على التوالي. لقد تم تدريب آخر نموذجين مسبقاً على أكثر من 500 لغة مع عدد قليل من اللغات الأنغولية ولكن أداؤهما كان أسوأ من (<span class="nodecor">AfroXLMR</span>) (المكيف من خلال (<span class="nodecor">MAFT</span>) إلى 20 لغة)، و(<span class="nodecor">AngBERT</span>) (المكيف إلى خمس لغات أنغولية). هذا يظهر أن نماذج اللغة المحددة بالمنطقة التي تغطي اللغات المتصلة ضمن نفس العائلة اللغوية يمكن أن تكون أكثر فعالية.</p>
<h4 id="يمكن-تعزيز-نتائج-maft-من-خلال-الاستفادة-من-البيانات-أحادية-اللغة-الاصطناعية">يمكن تعزيز نتائج (<span class="nodecor">MAFT</span>) من خلال الاستفادة من البيانات أحادية اللغة الاصطناعية</h4>
<p>من خلال دمج بيانات اصطناعية إضافية، تحسن أداء (<span class="nodecor">AngBERT</span>) (+<span class="nodecor">SYN</span> data) بـ <span class="math inline">\(+5.5\)</span> عن (<span class="nodecor">AngBERT</span>) بدون بيانات اصطناعية. ومع ذلك، فشل في تجاوز أداء (<span class="nodecor">AfroXLMR-base-76L</span>) الذي تم تدريبه على 76 لغة أفريقية بما في ذلك جميع اللغات الأنغولية باستثناء لوَبَأ-كاساي مع بيانات أكبر. أظهرت تجربتنا أن النموذج المكيف لـ 76 لغة أدى أداءً أفضل من (<span class="nodecor">Serengeti</span>) المدرب مسبقاً على 500 لغة، مما يظهر أنه يمكننا إنشاء نماذج لغة أفضل لتغطية المزيد من اللغات من خلال التكييف دون العملية المكلفة للتدريب من الصفر.</p>
<h4 id="تهيئة-التضمين-ofa-مع-بيانات-أكبر-أكثر-فعالية">تهيئة التضمين (<span class="nodecor">OFA</span>) مع بيانات أكبر أكثر فعالية</h4>
<p>أظهرت النماذج المهيأة مع (<span class="nodecor">OFA</span>) تحسناً مستمراً مقارنة بالنماذج الأساسية الأخرى. هذا يشير إلى أن (<span class="nodecor">OFA</span>)، الذي يستفيد صراحة من المعلومات المشفرة في تضمينات النموذج المصدر والتضمينات متعددة اللغات الخارجية، أفضل من التهيئة العشوائية. بشكل ملحوظ، تم تعزيز ميزة (<span class="nodecor">AngOFA</span>) على (<span class="nodecor">OFA</span>) من خلال وصوله إلى مجموعة بيانات أكبر بكثير للغات المعنية من خلال استخدام البيانات الاصطناعية. بدون البيانات الاصطناعية الإضافية، أدى (<span class="nodecor">AngOFA</span>) أداءً أسوأ من (<span class="nodecor">OFA</span>) المدرب مسبقاً على 500 لغة بانخفاض قدره <span class="math inline">\(-3.2\)</span>. ومع ذلك، عندما تم التدريب على البيانات الاصطناعية، حقق (<span class="nodecor">AngOFA</span>) أفضل أداء شامل بـ <span class="math inline">\(+16.6\)</span> على (<span class="nodecor">XLM-R</span>)، <span class="math inline">\(+12.3\)</span> على (<span class="nodecor">AfroXLMR</span>)، و <span class="math inline">\(+5.6\)</span> على (<span class="nodecor">AngBERT</span>) (مع بيانات اصطناعية).</p>
<h1 id="الخلاصة-والأعمال-المستقبلية">الخلاصة والأعمال المستقبلية</h1>
<p>هذا البحث يقدم أربعة من النماذج اللغوية متعددة اللغات مصممة خصيصاً للغات أنغولا. توضح نتائج تجاربنا أن استخدام تهيئة التضمين المستنيرة يعزز بشكل كبير أداء نموذج <span class="nodecor">MAFT</span> في المهام اللاحقة. بينما النماذج التي تم تهيئتها باستخدام <span class="nodecor">OFA</span> تظهر نتائج متفوقة مقارنة بنظيراتها، حتى في الحالة التي يتم فيها تدريب <span class="nodecor">AngBert</span> على مجموعة بيانات أكبر للغات المعنية ولكنه يؤدي بشكل ضعيف مقارنة بـ <span class="nodecor">OFA</span> المدرب على مجموعة بيانات أصغر. ومع ذلك، فإن العوامل المحددة التي تسهم في تفوق <span class="nodecor">AngBert</span> على <span class="nodecor">OFA</span>، خاصة في سياق لوَبَأ-كاساي، تثير أسئلة مثيرة للاهتمام حول العوامل الأساسية التي تؤثر على أداء النماذج في المهام اللاحقة، بما في ذلك اعتبارات مثل حجم مجموعة البيانات مقابل تهيئة التضمين المستنيرة. هذه الأسئلة متروكة للتحقيق في المستقبل. علاوة على ذلك، نهدف إلى توسيع تطبيق <span class="nodecor">OFA</span> لمزيد من اللغات الأفريقية لاستكشاف أوسع.</p>
<h3 id="الشكر-والتقدير" class="unnumbered">الشكر والتقدير</h3>
<p>تم دعم هذا العمل جزئياً بواسطة اعتمادات وموارد <span class="nodecor">Oracle Cloud</span> المقدمة من <span class="nodecor">Oracle</span>. يعترف <span class="nodecor">David Adelani</span> بدعم برنامج <span class="nodecor">DeepMind Academic Fellowship</span>.</p>
<h1 id="الملحق">المُلْحَق</h1>
</body>
</html>
