<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Feilong Tang">
  <meta name="author" content="Zhongxing Xu">
  <meta name="author" content="Zhaojun Qu">
  <meta name="author" content="Wei Feng">
  <meta name="author" content="Xingjian Jiang">
  <meta name="author" content="Zongyuan Ge">
  <title>استراتيجيات التعلم المدرك للنموذج النمطي: تعلم مدرك للسياق للتجزئة الدلالية ضعيفة الإشراف</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">استراتيجيات التعلم المدرك للنموذج النمطي: تعلم مدرك للسياق للتجزئة الدلالية ضعيفة الإشراف</h1>
<p class="author"><span class="nodecor">Feilong Tang</span></p>
<p class="author"><span class="nodecor">Zhongxing Xu</span></p>
<p class="author"><span class="nodecor">Zhaojun Qu</span></p>
<p class="author"><span class="nodecor">Wei Feng</span></p>
<p class="author"><span class="nodecor">Xingjian Jiang</span></p>
<p class="author"><span class="nodecor">Zongyuan Ge</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">مُلَخَّص</h1>
<p>تسعى الطرق الحديثة للتجزئة الدلالية ضعيفة الإشراف (<span class="nodecor">WSSS</span>) إلى دمج المعرفة السياقية لتحسين اكتمال خرائط تنشيط الفئة (<span class="nodecor">CAM</span>). في هذا العمل، نؤكد أن التحيز المعرفي بين الحالات والسياقات يؤثر على قدرة النموذج النمطي على فهم دلالات الحالة بشكل كافٍ. مستوحين من نظرية تعلم النموذج النمطي، نقترح استخدام الوعي بالنموذج النمطي لالتقاط السمات المتنوعة والدقيقة للحالات. تفترض الفرضية أن النماذج النمطية السياقية قد تنشط بشكل خاطئ فئات الأشياء المتشابهة والمتكررة بسبب هذا التحيز المعرفي. لذلك، نقترح تعزيز قدرة تمثيل النموذج النمطي من خلال التخفيف من التحيز لالتقاط التغطية المكانية بشكل أفضل في مناطق الأشياء الدلالية. لتحقيق هذا الهدف، نقدم استراتيجية التعلم المدرك للنموذج النمطي السياقي (<span class="nodecor">CPAL</span>)، والتي تستفيد من السياق الدلالي لإثراء فهم الحالة. جوهر هذه الطريقة هو التقاط التباينات داخل الفئة في ميزات الأشياء بدقة من خلال النماذج النمطية المدركة للسياق، مما يسهل التكيف مع السمات الدلالية لمختلف الحالات. نقوم بتصميم محاذاة توزيع الميزات لتحسين الوعي بالنموذج النمطي، عبر محاذاة توزيعات ميزات الحالة مع الميزات الكثيفة. بالإضافة إلى ذلك، يُقترح إطار تدريب موحد يدمج الإشراف التصنيفي الموجه بالتسميات والإشراف الذاتي الموجه بالنماذج النمطية. تظهر النتائج التجريبية على <span class="nodecor">PASCAL VOC 2012</span> و <span class="nodecor">MS COCO 2014</span> أن <span class="nodecor">CPAL</span> يحسن بشكل كبير الطرق المتاحة ويحقق أداءً رائداً. المشروع متاح على <a href="https://github.com/Barrett-python/CPAL">https://github.com/Barrett-python/CPAL.</a></p>
<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>تُعَد التجزئة الدلالية مهمة أساسية في مجال الرؤية الحاسوبية. أصبحت التجزئة الدلالية ضعيفة الإشراف (WSSS) نهجاً شائعاً في المجتمع، حيث تتعلم من تسميات ضعيفة مثل تسميات مستوى الصورة (<span class="nodecor">kolesnikov2016seed, lee2021anti</span>)، أو الخربشات (<span class="nodecor">lin2016scribblesup,vernaza2017learning</span>)، أو مربعات الحدود (<span class="nodecor">dai2015boxsup,lee2021bbam,song2019box</span>)، بدلاً من التعليقات التوضيحية على مستوى البكسل. تستخدم معظم مناهج WSSS خرائط تنشيط الفئة (<span class="nodecor">zhou2016learning</span>) لتوفير إشارات تحديد المواقع للأهداف، وبالتالي ترسم المفاهيم البصرية إلى مناطق البكسل.</p>
<p>المفتاح في WSSS هو توليد خرائط تنشيط الفئة بتغطية أفضل للكائن بالكامل. تهدف الدراسات الحديثة (<span class="nodecor">chang2020weakly,sun2020mining,zhang2020inter,wang2023hunting</span>) بشكل أساسي إلى تحسين دقة تجزئة النموذج واستقراره من خلال دمج المعرفة السياقية. مستوحاة من تقدم تعلم التمثيل (<span class="nodecor">fan2020learning,wu2021embedded</span>)، قدمت بعض الدراسات (<span class="nodecor">li2021group,su2021context,zhang2020causal,zhang2022multi</span>) المعرفة السياقية والمعرفة التمثيلية لنمذجة السياق على نطاق عالمي لتحليل الخصائص الدلالية للعينات بدقة أكبر. لكنها تتجاهل تحدي التباين الكبير داخل الفئة، أي أن المناطق التي تنتمي إلى نفس الفئة قد تظهر مظهراً مختلفاً جداً حتى في نفس الصورة. يجعل التحيز بين المعرفة السياقية (الميزات العالمية داخل الفئة) والمعرفة المحددة للعينات (الميزات الفريدة) نقل التسميات من مستوى الصورة إلى مستوى البكسل أمراً صعباً. في هذا العمل، نؤكد أن تخفيف التحيز المعرفي بين العينات والسياقات يمكن أن يلتقط مناطق أكثر دقة واكتمالاً. علاوة على ذلك، ندمج إشارات إشرافية إضافية لتسريع تخفيف التحيزات المعرفية.</p>
<p>تمثيل النموذج الأولي للفئة، من خلال تقليل التحيز، أظهر إمكانياته في الكشف عن أنماط الميزات في خوارزميات التعلم القليل الأمثلة مثل شبكة النقاط البيانية ثنائية الاتجاه (<span class="nodecor">liu2020prototype</span>). تنص نظرية تعلم النماذج الأولية (<span class="nodecor">zhou2022rethinking,wang2019panet</span>) على أن النماذج الأولية يمكن أن تمثل الميزات المحلية أو العالمية أو السمات المحددة للكائن. استناداً إلى التباين داخل الفئة في ميزات الكائن، يمكن للنموذج الأولي للعينة (<span class="nodecor">chen2022self</span>) أن يميز بشكل ديناميكي الميزات التمييزية للصورة المحددة. علاوة على ذلك، فإن النماذج الأولية التي تدمج المعرفة السياقية (<span class="nodecor">zhou2022regional</span>) لديها القدرة على التقاط أنماط دلالية فئوية أكثر تحديداً ودقة. تمكنها من التقاط منطقة الكائن بشكل أكثر اكتمالاً مقارنة بنموذج أولي لعينة واحدة. على الرغم من أن إدخال المعرفة السياقية يعزز قدرة النماذج الأولية على معالجة المعلومات الدلالية، فإن التحيز المعرفي بين العينات والسياقات يؤدي إلى تنشيط النماذج الأولية لفئات متشابهة أو متزامنة بشكل كبير (<em>مثلاً،</em> <span class="nodecor">cat</span> و <span class="nodecor">dog</span>).</p>
<p>في هذا العمل، نقترح استراتيجية تعلم مدرك للنموذج الأولي السياقي (CPAL) لاستخراج سمات فعالة من هيكل العنقود السياقي. على وجه التحديد، نستكشف عينات أخرى ذات صلة بالصورة المحددة لبناء نماذج أولية سياقية كجيران مرشحين. ثم يتم البحث عن السمات داخل الفئة في مجموعة الجيران المرشحين، مع تحديد نموذج العينة الحالية كمِرساة. في الوقت نفسه، نصمم درجة إيجابية زوجية تدل على الارتباط بين السمات، بهدف تحديد النماذج الأولية السياقية (<em>أي،</em> الجيران الناعمين) المرتبطة ارتباطاً وثيقاً بالميزة الحالية. بعد تطبيق درجة الإيجابية المعنية، يتم تعديل مساهمات هذه النماذج في العينة المِرساة بشكل ديناميكي، مما يخفف بشكل صريح التحيزات المرتبطة بالتنوع داخل الفئة وسمات العينة.</p>
<p>جوهر طريقتنا هو الوعي بالنموذج الأولي. نقيس بلطف المسافة بين النموذج الأولي للعينة والنموذج الأولي السياقي لإدراك سمات العينة. لتحقيق تقدير قوي، يُقترح بنوك دعم فئوية للتغلب على القيود الناتجة عن الدُفعات الصغيرة، بحيث يمكن ملاحظة تنوع الميزات داخل الفئة بطريقة من الميزة إلى البنك حيث يمكن تقريب توزيع الفئة عالمياً. ومع ذلك، بسبب الكمية المحدودة من ميزات العينة، هناك تحيز نسبي لتوزيع الميزات السياقية، مما يؤثر على الوعي الدقيق بالعينة. لذلك، نقترح محاذاة توزيع الميزات من خلال إدخال مصطلح تحويل <span class="math inline">\(\delta\)</span> إلى ميزات العينة النادرة، دافعاً إياها نحو توزيع الميزات الكثيفة لبنك الدعم الفئوي.</p>
<p>في مجموعات بيانات PASCAL VOC 2012 (<span class="nodecor">everingham2010pascal</span>) و MS COCO 2014 (<span class="nodecor">lin2014microsoft</span>)، نقيم طريقتنا في إعدادات WSSS المختلفة، حيث يحقق نهجنا أداءً رائداً. تتلخص المساهمات على النحو التالي:</p>
<ul>
<li><p>نقترح استراتيجية تعلم مدركة للنموذج الأولي السياقي تولد خرائط تحديد مواقع أكثر دقة واكتمالاً من خلال تخفيف التحيز المعرفي بين العينات والسياقات.</p></li>
<li><p>نقترح وحدة محاذاة الميزات مع بنوك دعم ديناميكية لإدراك سمات الكائن بدقة.</p></li>
<li><p>نقترح إطار تعلم موحد يتكون من التعلم الذاتي الإشرافي والتعلم المدرك للنموذج الأولي السياقي، حيث يكمل النظامان بعضهما البعض. تظهر التجارب أن طريقتنا تحقق تحسيناً كبيراً وتحقق أداءً رائداً.</p></li>
</ul>
<h1 id="sec:Related_Work">الأعمال ذات الصلة</h1>
<p><strong>التجزئة الدلالية ضعيفة الإشراف</strong> باستخدام تسميات على مستوى الصورة تُنتج عادةً خرائط الفعالية الدلالية كبذرة لتوليد تسميات زائفة على مستوى البكسل. العيب النموذجي لخرائط الفعالية الدلالية هو فعاليتها غير الكاملة وغير الدقيقة. لمعالجة هذا العيب، اقترحت الأعمال الحديثة مخططات تدريب متنوعة، مثل المحو العدائي (<span class="nodecor">kweon2021unlocking,yoon2022adversarial,sun2021ecs,kweon2023weakly</span>)، ونمو المنطقة (<span class="nodecor">huang2018weakly,wei2018revisiting</span>)، واستكشاف قيود الحدود (<span class="nodecor">rong2023boundary,chen2020weakly,lee2021railroad</span>). يركز نموذج التعلم والاستدلال للصورة الفردية (<span class="nodecor">araslanov2020single,lee2021railroad</span>) على فهم أعمق للميزات داخل صورة فردية لتوليد خرائط فعالية دلالية أكثر اكتمالاً. يقوم SIPE (<span class="nodecor">chen2022self</span>) باستخراج النماذج الأولية المخصصة لميزات متعددة الأحجام لتوسيع خرائط تحديد مواقع الكائنات الخشنة للحصول على مدى كامل لمناطق الكائنات.</p>
<p>بينما اعتبرت الجهود السابقة كل صورة على حدة، تركز الأعمال الحديثة على الحصول على سياق دلالي غني بين الصور المختلفة في مجموعة البيانات. تتناول الأعمال الحديثة (<span class="nodecor">sun2020mining,fan2020cian</span>) التنقيب الدلالي بين الصور من خلال التركيز على التقاط العلاقات الزوجية بين الصور. وتقوم (<span class="nodecor">li2021group,zhang2022multi,du2022weakly</span>) بأداء التنقيب الدلالي عالي الترتيب للعلاقات الأكثر تعقيداً داخل مجموعة من الصور. في الوقت نفسه، من أجل تعزيز علاقة التمثيل للفضاء المميز (استكشاف أنماط الكائنات على مجموعة البيانات بالكامل)، قدم RCA (<span class="nodecor">zhou2022regional</span>) بنك ذاكرة لتخزين ميزات الفئة عالية الجودة وأداء نمذجة السياق. اقترح CPSPAN (<span class="nodecor">jin2023deep</span>) محاذاة تمثيل الميزات للحالات المزدوجة تحت وجهات نظر مختلفة، وتم أيضاً إدخال هذه المحاذاة في توزيع البيانات تحت سياقات مختلفة (<span class="nodecor">zhao2023dual</span>). على عكس الأعمال السابقة حول تطبيق المعرفة السياقية، يمكن لطريقتنا أن تدرك بشكل تكيفي السمات الدلالية والاختلافات داخل الفئة، مما يؤدي إلى مناطق تنشيط أكثر اكتمالاً لخرائط الفعالية الدلالية.<br />
<br />
<strong>التعلم القائم على النماذج الأولية</strong> تمت دراسته جيداً في التعلم بعدد قليل من الأمثلة (<span class="nodecor">snell2017prototypical,snell2017prototypical</span>)، والتعلم بدون أمثلة (<span class="nodecor">he2019dynamic</span>) والتعلم غير الخاضع للإشراف (<span class="nodecor">xu2020attribute</span>). من الجدير بالذكر أن العديد من نماذج التجزئة يمكن اعتبارها شبكات تعلم مبنية على النماذج الأولية (<span class="nodecor">wang2019panet, liu2020part, xu2022semi, zhou2022rethinking, ge2023soft</span>)، مما يكشف عن إمكانية التطبيق في تجزئة الصور. اقترح (<span class="nodecor">du2022weakly</span>) طريقة تعلم ثنائية المقياس مبنية على النماذج الأولية تفرض الاتساق على مستوى الميزات في المقابلات وتنظيم داخلي وخارجي. يستخدم LPCAM (<span class="nodecor">chen2023extracting</span>) التعلم المبني على النماذج الأولية لاستخراج ميزات غنية للكائنات أيضاً. في عملنا، نتعلم سمات الميزات الفعالة ضمن هيكل التجميع للسياق لنمذجة ميزات الكائنات المتنوعة على مستوى دقيق.</p>
<h1 id="sec:Method">المنهجية</h1>
<p>يقوم نظام التعلم الضمني للتصنيف بتدريب شبكة التصنيف أولاً لتحديد منطقة الكائن المقابلة لكل فئة، ثم يتم تنقيحها لتوليد تسميات زائفة كمشرفين على شبكة التجزئة الدلالية. يُبنى الإطار على أساس شبكة التصنيف، كما هو موضح في القسم <span class="nodecor">[3.1]</span>. يتكون من إشارتين إشرافيتين: خسارة التصنيف والخسارة الذاتية الإشرافية. يشجع نهجنا على الاتساق بين الخريطة الفئوية المتوقعة من خلال التعلم الواعي للنموذج والمصنف، مما يحفز النموذج بشكل ضمني على تعلم ميزات أكثر تميزاً. نحن نمثل النموذج الأولي للحالة كمِرساة ونستخرج نماذج أولية سياقية من بنك الدعم كمجموعة مرشحة للجيران، والتي يتم وصفها في القسم <span class="nodecor">[3.2]</span>. جوهر منهجنا هو الوعي بالنموذج الأولي لالتقاط التباينات داخل الفئة، كما هو مفصل في القسم <span class="nodecor">[3.3]</span>. نقيس بلطف إيجابية كل جارٍ مرشح على الحالة الحالية، نقوم بتصفية الجيران انتقائياً ونعدل مساهماتهم. في الوقت نفسه، توجه محاذاة توزيع الميزات ميزات الحالة الحالية نحو مركز العنقود للميزات الكثيفة في البنك.</p>
<h2 id="نموذج-التحسين-الذاتي-المشرف">نموذج التحسين الذاتي المشرف</h2>
<p><strong>تحسين الشبكة.</strong> يُبنى إطار عملنا على شبكة تصنيف، مستخدمين هذه الشبكة <span class="math inline">\(\theta\)</span> لاستخراج إشراف فعال من تسميات الصور، ملتقطين مناطق الكائن لكل فئة (<span class="math inline">\(i.e.,\)</span> خرائط التنشيط الفئوي). نقترح تعلم النموذج الأولي السياقي لتوليد خريطة التنشيط الفئوي الأولي الأكثر اكتمالاً (PACAM)، موفرين إشارات إشرافية إضافية لخريطة التنشيط الأولية وتشكيل نموذج ذاتي الإشراف. العنصر الأساسي لهذا النموذج هو تنظيم الاتساق، مما يقلل بشكل ضمني المسافة المميزة بين البكسلات التمييزية والمفقودة، مشجعاً النموذج على تعلم ميزات أكثر اتساقاً وتميزاً. هذا التعديل البسيط يؤدي إلى تحسينات كبيرة. دالة الخسارة الموحدة لتحسين النموذج: <span class="math display">\[\label{coefficients}
\mathcal{L}=\lambda_{BCE}\mathcal{L}^{{BCE}}+\lambda_{Self}\mathcal{L}^{ {Self}}\]</span> حيث <span class="math inline">\(\lambda_{BCE}\)</span> و <span class="math inline">\(\lambda_{Self}\)</span> هما معاملات، <span class="math inline">\(\mathcal{L}^{{BCE}}\)</span> هي خسارة التصنيف، و <span class="math inline">\(\mathcal{L}^{ {Self}}\)</span> هي الخسارة الذاتية الإشرافية. الخسائر موصوفة بالتفصيل في الأقسام التالية.<br />
<br />
<strong>خسارة التصنيف وخرائط التنشيط الفئوي.</strong> كل صورة تدريب <span class="math inline">\(I \in \mathbb{R}^{w \times h \times 3}\)</span> في مجموعة البيانات <span class="math inline">\(\mathcal{I}\)</span> مرتبطة فقط بمتجه تسمية على مستوى الصورة <span class="math inline">\(\boldsymbol{y}=\{y_n\}^N_{n=1} \in \{0,1\}^N\)</span> حيث <span class="math inline">\(N\)</span> هي الفئات المحددة مسبقاً. يقترح CAM لتحديد مواقع الكائنات الأمامية من خلال تدريب شبكة تصنيف. يأخذ CAM صورة دفعة صغيرة <span class="math inline">\(I\)</span> كمدخل لاستخراج خرائط الميزات <span class="math inline">\(f\in \mathbb{R}^{W \times H \times D}\)</span>، ب <span class="math inline">\(D\)</span> قنوات وحجم مكاني <span class="math inline">\(H \times W\)</span>. لتقريب الفجوة بين مهمة التصنيف ومهمة التجزئة، يتم استخدام وزن المصنف <span class="math inline">\(\mathbf{w}_n\)</span> وطبقة التجميع المتوسط العالمي (GAP) لإنتاج تنبؤ اللوجيت <span class="math inline">\(\hat{y}_i \in \mathbb{R}^N\)</span>. أثناء التدريب، يستخدم خسارة التبادل الثنائية كما يلي: <span class="math display">\[\mathcal{L}^{BCE}=\frac{1}{N} \sum_{i=1}^N y_i \log \sigma\left(\hat{y}_i\right)+\left(1-y_i\right) \log \left(1-\sigma\left(\hat{y}_i\right)\right),\]</span> حيث <span class="math inline">\(\sigma(\cdot)\)</span> هي الدالة السيجمودية. للحصول على معلومات تقريبية عن الموقع للخلفية والأمام. يمكن تمثيل خريطة التنشيط الفئوي <span class="math inline">\({M}_{\boldsymbol{f}}=\left\{{M}_n\right\}_{n=1}^N\)</span> على <span class="math inline">\(N\)</span> فئات أمامية كما يلي: <span class="math display">\[{M}_{n}=\frac{\operatorname{ReLU}\left(\boldsymbol{\mathbf{w}_n^{\top} f}\right)}{\max \left(\operatorname{ReLU}\left(\boldsymbol{\mathbf{w}_n^{\top} f}\right)\right)}, \quad \forall n \in N.\]</span> مع الأخذ في الاعتبار أهمية الخلفية في مهمة التجزئة، نتبع (<span class="nodecor">wang2020self</span>) لتقدير خريطة تنشيط الخلفية <span class="math inline">\({M}_{b}=1-\max_{1 \leq n \leq N} M_n\)</span> استناداً إلى <span class="math inline">\(M_f\)</span>. نجمع خريطة تنشيط الخلفية المعالجة مع خريطة تنشيط الأمام ككل، <em>i.e.</em> <span class="math inline">\({M} = M_f \cup M_b\)</span>، لمساعدة النموذج على فهم المعرفة الخلفية.</p>
<h2 id="3.2">نمذجة النموذج الأولي</h2>
<p>مستوحاة من التعلم القائم على النماذج الأولية، تهدف استراتيجيتنا للوعي بالنماذج الأولية إلى استكشاف الخصائص بشكل فعال ضمن مجموعة الجيران المرشحين. نقترح إجراء بحث عن النموذج الأولي ضمن مجموعة النماذج الأولية السياقية لكل فئة، واضعين النموذج الأولي للحالة الحالية كمِرساة لتعزيز فهم خصائص الحالة.</p>
<p><strong>نمذجة النموذج الأولي للحالة كمِرساة.</strong> لكل صورة <span class="math inline">\(I\)</span>، يتم تعيين خرائط الميزات إلى فضاء الإسقاط <span class="math inline">\(z=v(f)\)</span> بواسطة رأس الإسقاط <span class="math inline">\(v\)</span> لنمذجة النموذج الأولي للحالة. يمثل كل نموذج أولي للحالة الدلالات الإقليمية للفئات الملحوظة في <span class="math inline">\(I\)</span> استناداً إلى <span class="math inline">\(M\)</span>. على وجه التحديد، بالنسبة للفئة <span class="math inline">\(n\)</span>-th التي تظهر في <span class="math inline">\(I\)</span> (<span class="math inline">\(i.e.,\)</span> <span class="math inline">\(y_c=1\)</span>)، يتم تلخيص ميزاتها المعروضة إلى متجه <span class="math inline">\(\mathcal{P}^{I}_n \in \mathbb{R}^D\)</span> بواسطة التجميع المتوسط المقنع (MAP) (<span class="nodecor">siam2019amp</span>): <span class="math display">\[\mathcal{P}^{I}_n=\frac{\sum_{x=1, y=1}^{W, H} \textbf{P}_n(x,y) * z(x,y)}{\sum_{x=1, y=1}^{W, H} \textbf{P}(x,y)}, 
\label{tau}\]</span> حيث <span class="math inline">\(\textbf{P}_n= \mathbbm{1}\left({M}_n&gt;\tau\right) \in \{0,1\}^{W \times H}\)</span> هو قناع ثنائي، يؤكد فقط على البكسلات المنشطة بقوة للفئة <span class="math inline">\(n\)</span> في خريطة التنشيط. <span class="math inline">\(\mathbbm{1}(\cdot)\)</span> هي دالة مؤشر، والعتبة <span class="math inline">\(\tau\)</span> هي معلمة فائقة وتدل على عتبة درجة الثقة. هنا، <span class="math inline">\(\mathcal{P}^{I}_n\)</span> مضغوط وخفيف، مما يسمح بالاستكشاف القابل للتطبيق لعلاقاته مع العديد من العينات الأخرى وتموضعه كمِرساة.</p>
<p><strong>نمذجة النماذج الأولية السياقية كجيران مرشحين.</strong> نفترض أن الميزات الفئوية داخل الصور أو الدُفعات توفر فقط نظرة محدودة للفئة. لذلك، نستخدم بنك الدعم كمجموعة مرشحة <span class="math inline">\(\mathcal{C}\)</span>، حيث يكون كل عنصر هو النموذج الأولي السياقي لفئات مختلفة. عند استخدام دفعات العينات لتدريب الشبكة، نخزن نماذجها الأولية <span class="math inline">\(\mathcal{P}^{I}_n\)</span> في <span class="math inline">\(\mathcal{C}\)</span> ونستخدم استراتيجية الأول داخل/الأول خارج لتحديث مجموعة المرشحين. تحافظ هذه المجموعة على طول نسبي كبير لكل فئة نموذج أولي لتوفير نماذج أولية سياقية محتملة بشكل كافٍ. استناداً إلى هذه المجموعة، يتم تطبيق تجميع <span class="math inline">\(\mathrm{k}\)</span>-means عبر الإنترنت لتنقيح كل فئة إلى مجموعات نموذج أولي مجمعة <span class="math inline">\(\mathcal{G}=\left\{G_i\right\}_{i=1}^{N_p}\)</span> لكشف الخصائص العميقة لكل فئة. نقوم بعمليات التوسيط على كل مجموعة نموذج أولي مجمعة من <span class="math inline">\(\mathcal{G}\)</span> لتوليد <span class="math inline">\(N_p\)</span> جيران مرشحين <span class="math inline">\(\mathbf{p}_i\)</span> كما يلي: <span class="math display">\[\mathbf{p}_i=\frac{1}{\left|G_i\right|} \sum_{\mathbf{r}_j \in G_i} \mathbf{r}_j,\]</span> حيث <span class="math inline">\(\mathbf{r}_j\)</span> يشير إلى النموذج الأولي <span class="math inline">\(j\)</span>-th الذي ينتمي إلى مجموعة العنقود <span class="math inline">\(i\)</span>-th <span class="math inline">\(G_i\)</span>. <span class="math inline">\(\mathbf{p}_i\)</span> يمثل النموذج الأولي السياقي <span class="math inline">\(i\)</span>-th لمجموعة الجيران المرشحين <span class="math inline">\(\mathcal{P}_n^c=\left\{\mathbf{p}_i\right\}_{i=1}^{N_p}\)</span>.</p>
<h2 id="3.3">التعلم المدرك لنموذج السياق</h2>
<p>مع نماذج الربط الأساسية ومجموعة الجيران المرشحين من القسم [3.2]، تدرك مجموعة الجيران المرشحين أو تدعم ميزة الربط. يمكن للتعلم المدرك لنموذج السياق قياس وضبط مدى هذا الدعم.<br />
<br />
<strong>تحديد الجار الإيجابي الناعم.</strong> اختيار النموذج أمر حاسم في نهجنا المقترح حيث يحدد إلى حد كبير جودة الإشراف. يمكن لنماذج الحالات تمثيل الصفات الفئوية للصورة الحالية بشكل خاص، بينما تظهر نماذج السياق أنماطاً فئوية أكثر شمولاً وتنوعاً. تستخدم استراتيجيتنا درجات الإيجابية <span class="math inline">\(w_i\)</span> لقياس صلة الجيران المرشحين في الفئة بصفات الحالة الحالية. نقترح اختيار أعلى <span class="math inline">\(K\)</span> جيران معدلّين بدرجات الإيجابية، الموجودين بالقرب من الربط. يمكن صياغة الجار الإيجابي الناعم كما يلي: <span class="math display">\[\tilde{\mathcal{P}}_n^{\text {c}}=\left\{w_i \mathbf{p}_{\mathbf{i}}: i \in \underset{i \in N_p}{ \arg \max }\left(d\left(w_i \mathbf{p}_{\mathbf{i}}, \mathcal{P}_n^I\right), \text { top } =K\right)\right\}
\label{value_K}\]</span> حيث <span class="math inline">\(d()\)</span> تدل على التشابه الجيبي التمامي كمقياس محسوب، و<span class="math inline">\(\tilde{\mathcal{P}}^{c}_n\)</span> يمثل أعلى <span class="math inline">\(K\)</span> نماذج واعية بالسياق مصممة للحالة الحالية.<br />
<br />
<strong>توقعات الإيجابية.</strong> لقد صممنا درجات إيجابية زوجية لقياس (بشكل غير ثنائي) الصلة بين نموذج الحالة والجيران المرشحين في نفس الفئة. بالنسبة لزوج النموذج (<span class="math inline">\(\mathbf{p}_i\)</span> , <span class="math inline">\(\mathcal{P}^I_{n}\)</span>)، يمكن حساب درجة الإيجابية <span class="math inline">\(w_{i}\)</span> كما يلي: <span class="math display">\[w_i=\frac{1}{\gamma_i} \texttt{softmax}\left[l_1\left(\mathbf {\mathcal{P}}^{I}_n\right) \times l_2\left(\mathbf {p}_i\right)^{\top}\right], \quad {\mathbf{p}}_{i} \in {\mathcal{P}}^{c}_n, 
\label{eq7}\]</span> حيث <span class="math inline">\(l_1(\cdot)\)</span> و<span class="math inline">\(l_2(\cdot)\)</span> هما طبقات تحويل ميزات خالية من المعاملات. <span class="math inline">\(\gamma_i\)</span> هو عامل تحجيم لضبط درجة الإيجابية <span class="math inline">\(w_i\)</span>. تم استكشاف هياكل مختلفة للدرجة <span class="math inline">\(w_{i}\)</span> في القسم [Ablation].<br />
<br />
<strong>الادعاء <span class="nodecor">1</span>.</strong> <em>نفترض أننا ندرب نموذج <span class="math inline">\(\theta\)</span> باستخدام طريقة التحسين المقترحة، <span class="math inline">\(\mathcal{P}_n^I\)</span> و<span class="math inline">\(\tilde{\mathcal{P}}_n^c\)</span> هما نموذج الحالة الحالية للفئة النوعية ونماذج السياق على التوالي. يمكن التعبير عن القيمة المثلى لمقياس التشابه <span class="math inline">\(s_i^*\)</span> كـ <span class="math inline">\(\frac{w_{i}}{\sum_{k=1}^{K} w_{k}}\)</span>، حيث <span class="math inline">\(w_{i}\)</span> هي درجة الإيجابية المقابلة لزوج النموذج (<span class="math inline">\(\mathcal{P}_n^I, \quad {\mathbf{p}}_{i} \in {\tilde{\mathcal{P}}}_n^{c}\)</span>) في المعادلة [eq7].</em><br />
<br />
يمكن العثور على البرهان في الملحق A. يشير الادعاء <span class="nodecor">1</span> إلى أننا نحسن النموذج لتعظيم التشابه بين نموذج السياق والحالة الحالية من نفس الفئة بنسبة مباشرة إلى درجة الإيجابية المقابلة. نحن ننقل المعرفة بفعالية من الفرع الذاتي الإشراف إلى النموذج، فضلاً عن أداء النموذج العام وقدراته على التعميم.<br />
<br />
<strong>محاذاة توزيع الميزات.</strong> تشكل الميزات المتفرقة (<span class="nodecor">hoefler2021sparsity</span>) وتنوع الفئة الداخلي تحديات لتمثيل الميزات المحددة للفئة بدقة، مما يعيق التمييز بين الفئات. وبالتالي، نفترض وجود تحيز بين ميزات الحالة وميزات الفئة الداخلية. لمعالجة ذلك، نوجه الميزات لمحاذاة ميزاتها المحددة للفئة المتجمعة بكثافة لتعزيز كثافة الميزة الداخلية للفئة. بالنظر إلى أن تطبيع الدُفعات الصغيرة (<span class="nodecor">ioffe2015batch</span>) أو تطبيع الحالة (<span class="nodecor">ulyanov2016instance</span>) يتبع اتجاه التعلم بالدُفعات، يتم محاذاة ميزات الدُفعات الصغيرة من خلال إدخال مصطلحات الانتقال <span class="math inline">\(\delta_n\)</span> لدفعها نحو مراكز العنقود. يتم استنتاج ذلك كما يلي.</p>
<p>نحدد مقياس التقييم للتشابه الجيبي التمامي الأمثل (OCSEM) لتقييم التشابه الجيبي التمامي بين العينة الحالية والعينات الأخرى، بهدف تعزيز دقة النموذج من خلال تعظيم هذا المقياس. يعرف الهدف الأمثل كما يلي: <span class="math display">\[\begin{split}
\text{OCSEM} = \frac{1}{{N_p}{Q_n}} \sum^{{N_p}}_{i=1} \sum^{{Q_n}}_{q=1} &amp; \cos({\mathbf {p}}_{i},\mathcal P^I_{n,q}) &gt; \\ &amp;  \max_{h \neq i}\{\cos({\mathbf {p}}_{h},\mathcal P^I_{n,q})\}, 
\end{split}\]</span> حيث <span class="math inline">\({\mathbf {p}}_{i}\)</span> هو نموذج السياق في مجموعة الجيران المرشحين <span class="math inline">\(\mathcal{P}_n^c=\left\{\mathbf{p}_i\right\}_{i=1}^{N_p}\)</span> للفئة النوعية، و<span class="math inline">\(\mathcal P^I_{n,q}\)</span> هو نموذج الحالة المقابل في المجموعة <span class="math inline">\(\mathcal{P}_n^b=\left\{\mathcal P^I_{n,q}\right\}_{q=1}^{Q_n}\)</span> في الدُفعة الصغيرة. <span class="math inline">\(Q_n\)</span> يدل على عدد النماذج للفئة النوعية في الدُفعة الصغيرة. نفترض أن التحيز يمكن تقليله بإضافة مصطلح الانتقال <span class="math inline">\(\delta_n\)</span> إلى ميزة الحالة. يجب أن يتبع المصطلح <span class="math inline">\(\delta_n\)</span> الهدف: <span class="math display">\[\underset{\delta_n}{\arg \max } \frac{1}{{N_p}{Q_n}} \sum_{i=1}^{N_p} \sum_{q=1}^{Q_n} \cos \left({\mathbf {p}}_{i}, \mathcal P^I_{n,q}+\delta_n\right).
\label{9}\]</span> نفترض أن كل ميزات النموذج <span class="math inline">\(\mathcal P^I_{n,q}\)</span> يمكن تمثيلها كـ <span class="math inline">\({\mathbf {p}}_{i} + \epsilon_{i,q}\)</span>. يمكن صياغة المعادلة [9] بشكل أكثر تفصيلاً كما يلي: <span class="math display">\[\underset{\delta_n}{\arg \max } \frac{1}{{N_p}{Q_n}} \sum_{i=1}^{N_p} \sum_{q=1}^{Q_n} \cos \left({\mathbf {p}}_{i}, {\mathbf {p}}_{i}+\delta_n+\epsilon_{i, q}\right).\]</span> لتعظيم التشابه الجيبي التمامي، يجب تقليل الهدف التالي: <span class="math display">\[\min \frac{1}{{N_p}{Q_n}} \sum^{{N_p}}_{i=1} \sum^{{Q_n}}_{q=1} (\epsilon_{i,q}+\delta_n).\]</span> يتم حساب المصطلح <span class="math inline">\(\delta_n\)</span> على النحو التالي: <span class="math display">\[\delta_n=-\mathbb{E}\left[\epsilon_{i,q}\right]=\frac{1}{{N_p}{Q_n}} \sum_{i=1}^{N_p} \sum_{q=1}^{Q_n} \left({\mathbf {p}}_{i}-\mathcal{P}_{n, q}^I\right).
\label{shift}\]</span></p>
<h2 id="الوعي-بالنموذج-في-cam-والخسارة-الذاتية-التوجيهية">الوعي بالنموذج في CAM والخسارة الذاتية التوجيه</h2>
<p><strong>الوعي بالنموذج في CAM.</strong> مع وضوح معنى النماذج، يمكن فهم إجراء CAM المتوقع بشكل حدسي كاسترجاع النماذج الأكثر تشابهاً. لكل نموذج <span class="math inline">\(\tilde{\mathcal{P}}^{c}_n\)</span> في المعادلة [value_K]، نحسب تشابه الجيب التمامي بين الميزات في كل موضع ونموذج الفئة المقابل. ثم يتم تجميع خرائط التشابه كما يلي: <span class="math display">\[{\tilde{M}}_n(j) = \ ReLU \left(\frac{1}{K} 
\sum_{{\mathcal{\mathbf p}}_i \in {\tilde{\mathcal{P}}}^{c}_n}
\frac{{{f}}{(j)} \cdot {\mathcal{\mathbf p}}_i}{\left\|{{f}}(j)\right\| \cdot\left\|{\mathcal{\mathbf p}}_i\right\|}\right),\]</span> حيث يشير <span class="math inline">\(\|\cdot\|\)</span> إلى القاعدة L2 للمتجه. يمثل <span class="math inline">\(\tilde{M}_n(j)\)</span> PACAM للفئة <span class="math inline">\(n\)</span>-th في البكسل <span class="math inline">\(j\)</span>.</p>
<p><strong>الخسارة الذاتية التوجيه.</strong> للاستفادة أكثر من المعرفة السياقية، نقدم نموذجاً للتعلم الذاتي التوجيهي يشجع على الاتساق بين النتائج من التنبؤات المدركة للنموذج ومصنف مشرف. هذا يعزز من قدرة النموذج على التعرف على الميزات التمييزية بشكل أكبر ويدمج المعرفة المدركة للنموذج في تمثيل الميزة، مما يعزز التحسين التعاوني طوال دورات التدريب. تعريف التنظيم الاتساقي بتطبيع L1 لاثنين من CAMs: <span class="math display">\[\mathcal{L}^{self}=\frac{1}{N+1}\|{M}- {\tilde{M}}\|_1,
\label{self}\]</span> حيث <span class="math inline">\(M\)</span> و <span class="math inline">\(\tilde{M}\)</span> تمثلان CAM الأصلي و PACAM على التوالي.</p>
<h1 id="sec:Experiments">التجارب</h1>
<h2 id="مجموعات-البيانات-وتفاصيل-التنفيذ">مجموعات البيانات وتفاصيل التنفيذ</h2>
<p><strong>مجموعة البيانات ومقياس التقييم.</strong> تُجرى التجارب على معيارين: PASCAL VOC 2012 (<span class="nodecor">everingham2010pascal</span>) بـ 21 فئة و MS COCO 2014 (<span class="nodecor">lin2014microsoft</span>) بـ 81 فئة. بالنسبة لـ PASCAL VOC 2012، وفقاً لـ (<span class="nodecor">wang2020self, lee2021anti, chen2022self, li2022expansion</span>)، نستخدم SBD المعزز (<span class="nodecor">hariharan2011semantic</span>) بـ 10,582 صورة موسومة. نقيم CPAL من حيث i) جودة توليد تسميات التجزئة الزائفة على VOC 2012 <code>train</code>، و ii) التجزئة الدلالية على VOC 2012 <code>val/test</code> و COCO 2014 <code>val</code>. يستخدم متوسط التقاطع على الاتحاد (mIoU) (<span class="nodecor">long2015fully</span>) كمقياس في كلتا الحالتين. تُحصل النتائج على اختبار VOC 2012 <code>test</code> من الخادم التقييمي الرسمي.</p>
<p><strong>تفاصيل التنفيذ.</strong> في تجاربنا، يتم اعتماد ResNet50 (<span class="nodecor">he2016deep</span>) المدرب مسبقاً على ImageNet (<span class="nodecor">deng2009imagenet</span>) كالعَمُود الفقري بخطوة إخراج قدرها 16، حيث يحل مصنف محل الطبقة المتصلة بالكامل بقنوات إخراج تبلغ 20. استراتيجية التعزيز هي نفسها كما في (<span class="nodecor">chen2022self, ahn2019weakly, chen2023extracting</span>)، بما في ذلك القلب العشوائي، التحجيم، والقص. يتم تدريب النموذج بحجم دفعة 16 على 8 وحدات معالجة رسومات Nvidia 4090. يتم اعتماد محسن SGD لتدريب نموذجنا لمدة 5 دورات، بزخم قدره 0.9 وتآكل الوزن 1e-4. تحدد معدلات التعلم للعمود الفقري والطبقات المضافة حديثاً على 0.1 و1، على التوالي. نستخدم جدول تعلم بولي متضائل بقوة 0.9 لمعدل التعلم.</p>
<p>تحدد معاملات الخسارة <span class="math inline">\(\lambda_{BCE}\)</span> و <span class="math inline">\(\lambda_{Self}\)</span> كـ 1 في المعادلة [coefficients]. بالنسبة لـ VOC 2012، يحدد العتبة <span class="math inline">\(\tau\)</span> في المعادلة [tau] على 0.1. حجم البنك الداعم لكل فئة لتخزين التضمينات الإقليمية، مع تحديد الحجم على 1000 لتجنب استهلاك دعم كبير. يتم إجراء تجميع النماذج الأولية <span class="math inline">\(k\)</span>-means في القسم [3.2] مرة واحدة فقط في بداية كل دورة، ويحدد عدد النماذج الأولية لكل فئة <span class="math inline">\(N_p\)</span> على 50، ويحدد عدد الجيران المرشحين الأعلى <span class="math inline">\(K\)</span> على 20 في المعادلة [value_K]. بالنسبة لشبكة التجزئة، أجرينا تجارب مع DeepLab-v2 (<span class="nodecor">chen2017deeplab</span>) مع العمود الفقري ResNet101 و ResNet38. <em>المزيد من التفاصيل (بما في ذلك COCO) موجودة في الملحق.</em></p>
<h2 id="Ablation">دراسة الاستئصال</h2>
<p>لدراسة مساهمات كل مكون من مكونات طريقتنا، أجرينا دراسات استئصال على مجموعة بيانات VOC 2012. جميع التجارب استخدمت Resnet-50 كالعَمُود الفقري. <strong>فعالية كل مكون.</strong> في الجدول [abl]، نجري دراسات استئصال لإظهار فعالية نهجنا. نستخدم نموذجاً تم تدريبه فقط بإشراف التصنيف (التجربة الأولى) كخط أساس. ثم يتم تقديم استراتيجية تعلم النموذج الأولي للسياق بسيطة في التجربة الثانية والتي تحقق مكاسب محدودة في mIoU على مجموعة <code>train</code>. تظهر التجربة الثالثة أن تقديم تعلم النموذج الأولي للسياق المدرك (مجموعة المرشحين الأعلى-<span class="math inline">\(K\)</span> وتنبؤ الإيجابية) لتوليد PACAM يعزز الأداء بشكل كبير بنسبة +3.3%. في التجربة الرابعة، عند تقديم وحدة محاذاة الميزات، يزداد الأداء بمقدار +2.3%. في التجربة الخامسة، يتحسن الأداء بمقدار +5.7% عند تقديمه للتدريب الذاتي كإشراف تكميلي، مما يدل على أهميته في إطار عملنا. خسارة الاتساق تجبر النموذج على التركيز على التفاصيل الدقيقة للدلالات، مما يعزز إدراكه للبنية الجوهرية والميزات الدلالية.<br />
<strong>فعالية الجيران المرشحين والإيجابية.</strong> نحلل أهمية الجيران المرشحين والإيجابية، كما هو موضح في الجدول [neighbor]. إزالة الإيجابية واستخدام جميع الجيران للتنبؤ، تقل دقة Miou في CAM من 62.5% إلى 60.3%. يشير ذلك إلى أن الإيجابية ليست مجرد زخرفة بسيطة بل توفر آلية فعالة للنموذج. تمكن النموذج من التركيز بشكل تكيفي وانتقائي على الجيران الذين يساهمون بشكل كبير في المهمة أثناء عملية التعلم مع تجاهل الجيران غير المعلوماتيين للتنبؤات. في الكتلة الثالثة من الجدول [neighbor]، نجري أيضاً تجارب لتحليل تأثير عدد الجيران. من ناحية، يعزز وجود عدد كافٍ من الجيران تنوع الميزات. من ناحية أخرى، قد يؤدي تضمين النماذج الأولية ذات الارتباط الضعيف إلى إدخال الكثير من الضوضاء أثناء عملية التدريب ويقلل من قدرة النموذج على إدراك الميزات التمييزية. القياس الناعم المقترح يقدم إيجابية زوجية لضبط مساهمة النماذج الأولية المختلفة في الحالة المِرساة في المعادلة [coefficients]. نطبق مقاييس تشابه مختلفة لحساب درجة الإيجابية. كما هو موضح في الجدول [function]، تم استكشاف أربع خيارات: المسافة المانهاتنية (<span class="math inline">\(L_1\)</span>)، المسافة الإقليدية (<span class="math inline">\(L_2\)</span>)، التشابه الجيبي التمامي، والضرب النقطي. يظهر الضرب النقطي أداءً متفوقاً بشكل كبير مقارنة بالاستراتيجيات الأخرى ويستخدم كطريقتنا لقياس الإيجابية.</p>
<table>
<caption>تحليل الإيجابية وعدد الجيران المرشحين <span class="math inline">\(K\)</span>. تم تقييم قيم mIoU على مجموعة PASCAL VOC 2012 <code>train</code>.</caption>
<thead>
<tr class="header">
<th style="text-align: center;">الجار</th>
<th style="text-align: center;">الإيجابية</th>
<th style="text-align: center;"><span class="math inline">\(K\)</span></th>
<th style="text-align: center;">mIou(%)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">62.5</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">59.2</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">60.3</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">61.3</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">62.5</td>
</tr>
<tr class="even">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">60.1</td>
</tr>
<tr class="odd">
<td style="text-align: center;">[neighbor]</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<table>
<caption>مقارنة كمية لاستراتيجيات قياس المسافة المختلفة في <span class="math inline">\(الإيجابية\)</span> على مجموعة PASCAL VOC <code>train</code>. النتائج الأفضل معروضة بخط عريض.</caption>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;"><span class="math inline">\(L_1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(L_2\)</span></th>
<th style="text-align: center;">Cosine</th>
<th style="text-align: center;">Dot</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">mIou (%)</td>
<td style="text-align: center;">59.6</td>
<td style="text-align: center;">58.7</td>
<td style="text-align: center;">61.9</td>
<td style="text-align: center;"><strong>62.5</strong></td>
</tr>
</tbody>
</table>
<p>[function]</p>
<p><strong>فعالية محاذاة الميزات.</strong> في الجدول [abl]، نقدم نتائج تحسين الأداء التي تم تحقيقها من خلال تقليل التحيز في التوزيع. بالإضافة إلى ذلك، أجرينا مقارنة بصرية باستخدام t-SNE (<span class="nodecor">van2008visualizing</span>) في الشكل [tsnet]. تشير النتائج إلى أنه بعد محاذاة توزيعات الميزات، يمكن للنموذج أن يولد مجموعات أكثر تماسكاً مع قابلية فصل أعلى بين المجموعات. تعديل المتغير الديناميكي للإزاحة يساعد في تخفيف الاختلافات بين ميزات الحالات من نفس الفئة، مما يجعل الحالات التي تنتمي إلى نفس الفئة أكثر تشابهاً. هذا بدوره، يسهل على النموذج التمييز بين الحالات من فئات مختلفة بدقة أكبر.<br />
<strong>تحليل العوامل الفائقة.</strong> نجري تحليلاً لحساسية العوامل الفائقة، بتغيير قيم مثل (أ) العتبة <span class="math inline">\(\tau\)</span> لتوليد قناع البذور 0-1. الشكل [hyperparameter] (أ) يشير إلى أن القيمة المثلى لـ <span class="math inline">\(\tau\)</span> هي 0.1. بالإضافة إلى ذلك، نفحص (ب) طول مجموعة الدعم، حيث نجد أن مجموعة أكبر تعزز أداء النموذج. الشكل [hyperparameter] (ب) يوضح هذه النتائج.</p>
<h1 id="تحليل-نوعي">تحليل نوعي</h1>
<p>نقوم بتصور مناطق الاستجابة ونتائج التنبؤ للوعي بالنماذج في الشكل [fig31] (أ). يوضح ذلك بوضوح أن النماذج مرتبطة بسمات معينة للحالات. على وجه التحديد، على سبيل المثال، بالنظر إلى الصور (<em>مثلاً،</em> <code>horse</code> و <code>cat</code>)، يتوافق كل نموذج مع أجزاء مختلفة من الحالة، مما يتيح نمذجة أفضل للتباينات داخل الفئة في الأجسام الدلالية. في الشكل [fig31] (ب)، نقوم بتصور دراسات استئصال على مكونات مختلفة من طريقتنا. عند إزالة الوعي بالنموذج (الإيجابية والجيران الأعلى-<span class="math inline">\(K\)</span>)، ينشط النموذج مناطق بشكل خاطئ تتزامن بقوة (<em>مثلاً،</em> <code>train</code> و <code>railroad</code>) أو تظهر مظاهر متشابهة (<em>مثلاً،</em> <code>cat</code> و <code>dog</code>)، مما يشير إلى نقص في التعلم الدقيق والقدرات التمييزية للميزات المحددة للحالة. بدون خسارة الإشراف الذاتي <span class="math inline">\(\mathcal{L}^{Self}\)</span>، يظهر CAM تنشيطاً ناقصاً، مما يشير إلى عدم كفاية تعلم ميزات الفئة. تشير هذه النتائج إلى أن طريقتنا، مع إدخال هذه المكونات، يمكن أن تدرك وتميز سمات الفئة المختلفة بدقة أكبر.</p>
<table>
<caption>مقارنات بين طريقتنا وطرق التعلم شبه المشرف الأخرى. نقيم mIoU (%) على مجموعة <code>train</code> من PASCAL VOC 2012 على المستويات: CAM، مع CRF، والقناع الزائف.</caption>
<thead>
<tr class="header">
<th style="text-align: left;">الطريقة</th>
<th style="text-align: center;">البذرة</th>
<th style="text-align: center;">مع CRF</th>
<th style="text-align: center;">القناع</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">SEAM (wang2020self)</td>
<td style="text-align: center;"><span class="nodecor">55.4</span></td>
<td style="text-align: center;"><span class="nodecor">56.8</span></td>
<td style="text-align: center;"><span class="nodecor">63.6</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">AdvCAM (lee2021anti)</td>
<td style="text-align: center;"><span class="nodecor">55.6</span></td>
<td style="text-align: center;"><span class="nodecor">62.1</span></td>
<td style="text-align: center;"><span class="nodecor">68.0</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">CLIMS (xie2022clims)</td>
<td style="text-align: center;"><span class="nodecor">56.6</span></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="nodecor">70.5</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">SIPE (chen2022self)</td>
<td style="text-align: center;"><span class="nodecor">58.6</span></td>
<td style="text-align: center;"><span class="nodecor">64.7</span></td>
<td style="text-align: center;"><span class="nodecor">68.0</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ESOL (li2022expansion)</td>
<td style="text-align: center;"><span class="nodecor">53.6</span></td>
<td style="text-align: center;"><span class="nodecor">61.4</span></td>
<td style="text-align: center;"><span class="nodecor">68.7</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">AEFT (yoon2022adversarial)</td>
<td style="text-align: center;"><span class="nodecor">56.0</span></td>
<td style="text-align: center;"><span class="nodecor">63.5</span></td>
<td style="text-align: center;"><span class="nodecor">71.0</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">PPC (du2022weakly)</td>
<td style="text-align: center;"><span class="nodecor">61.5</span></td>
<td style="text-align: center;"><span class="nodecor">64.0</span></td>
<td style="text-align: center;"><span class="nodecor">64.0</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">ReCAM (chen2022class)</td>
<td style="text-align: center;"><span class="nodecor">54.8</span></td>
<td style="text-align: center;"><span class="nodecor">60.4</span></td>
<td style="text-align: center;"><span class="nodecor">69.7</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Mat-Label (wang2023treating)</td>
<td style="text-align: center;"><span class="nodecor">62.3</span></td>
<td style="text-align: center;"><span class="nodecor">65.8</span></td>
<td style="text-align: center;"><span class="nodecor">72.9</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">FPR (chen2023fpr)</td>
<td style="text-align: center;"><span class="nodecor">63.8</span></td>
<td style="text-align: center;"><span class="nodecor">66.4</span></td>
<td style="text-align: center;"><span class="nodecor">68.5</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">LPCAM (chen2023extracting)</td>
<td style="text-align: center;"><span class="nodecor">62.1</span></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="nodecor">72.2</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">ACR (kweon2023weakly)</td>
<td style="text-align: center;"><span class="nodecor">60.3</span></td>
<td style="text-align: center;"><span class="nodecor">65.9</span></td>
<td style="text-align: center;"><span class="nodecor">72.3</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">SFC (zhao2024sfc)</td>
<td style="text-align: center;"><span class="nodecor">64.7</span></td>
<td style="text-align: center;"><span class="nodecor">69.4</span></td>
<td style="text-align: center;"><span class="nodecor">73.7</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">IRN (ahn2019weakly)</td>
<td style="text-align: center;"><span class="nodecor">48.8</span></td>
<td style="text-align: center;"><span class="nodecor">53.7</span></td>
<td style="text-align: center;"><span class="nodecor">66.5</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">+CPAL (لنا)</td>
<td style="text-align: center;"><strong><span class="nodecor">62.5</span></strong></td>
<td style="text-align: center;"><strong><span class="nodecor">66.2</span></strong></td>
<td style="text-align: center;"><strong><span class="nodecor">72.7</span></strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">AMN (lee2022threshold)</td>
<td style="text-align: center;"><span class="nodecor">62.1</span></td>
<td style="text-align: center;"><span class="nodecor">66.1</span></td>
<td style="text-align: center;"><span class="nodecor">72.2</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">+CPAL (لنا)</td>
<td style="text-align: center;"><strong><span class="nodecor">65.7</span></strong></td>
<td style="text-align: center;"><strong><span class="nodecor">68.2</span></strong></td>
<td style="text-align: center;"><strong><span class="nodecor">74.1</span></strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">MCTformer (xu2022multi)</td>
<td style="text-align: center;"><span class="nodecor">61.7</span></td>
<td style="text-align: center;"><span class="nodecor">64.5</span></td>
<td style="text-align: center;"><span class="nodecor">69.1</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">+CPAL (لنا)</td>
<td style="text-align: center;"><strong><span class="nodecor">66.8</span></strong></td>
<td style="text-align: center;"><strong><span class="nodecor">69.3</span></strong></td>
<td style="text-align: center;"><strong><span class="nodecor">74.7</span></strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">CLIP-ES (lin2023clip)</td>
<td style="text-align: center;"><span class="nodecor">70.8</span></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><span class="nodecor">75.0</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">+CPAL (لنا)</td>
<td style="text-align: center;"><strong><span class="nodecor">71.9</span></strong></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;"><strong><span class="nodecor">75.8</span></strong></td>
</tr>
</tbody>
</table>
<p>[labelVOC]</p>
<h2 id="مقارنات-مع-الطرق-الحديثة">مقارنات مع الطرق الحديثة</h2>
<p><strong>تحسين خرائط التحديد:</strong> بما أن الطريقة المقترحة CPAL لا تعدل هندسة شبكة CAM، فإنها تدمج فرع CPAL كإشراف في طرق متعددة. الجدول [labelVOC] يعرض نتائج تطبيق CPAL على طرق معروفة (IRN (<span class="nodecor">ahn2019weakly</span>), AMN (<span class="nodecor">lee2022threshold</span>), MCTformer (<span class="nodecor">xu2022multi</span>)، و CLIP-ES (<span class="nodecor">lin2023clip</span>)) ويظهر تحسينات في خرائط التحديد على VOC <span class="nodecor">2012</span>. على سبيل المثال، دمج CPAL في AMN يحسن الأداء بنسبة <span class="nodecor">3.6</span>% في البذور و <span class="nodecor">2.1</span>% في الأقنعة الزائفة. عند دمج CPAL في نموذج CLIP-ES، هناك مكسب بنسبة <span class="nodecor">1.1</span>% في البذور.</p>
<p><strong>تحسين نتائج التجزئة:</strong> الجدول [miou_results] يظهر أداء نموذج التجزئة الدلالية المدرب بالتسميات الزائفة التي تم إنشاؤها بواسطة طريقتنا. التسميات الزائفة تُستخدم لتدريب نموذج التجزئة DeepLabV2. المقارنات مع الأعمال ذات الصلة. يحقق AMN+CPAL لدينا نتائج رائدة على VOC (mIoU بنسبة <span class="nodecor">72.5</span>% على مجموعة التحقق و <span class="nodecor">72.9</span>% على مجموعة الاختبار). على مجموعة البيانات MS COCO الأكثر تحدياً، يتفوق MCTformer+CPAL لدينا (مع ResNet-38 كالعَمُود الفقري) على النتيجة الرائدة AMN وجميع الأعمال ذات الصلة المبنية على ResNet-38. بالنسبة لـ CLIP-ES، يحسن CPAL الأداء (+<span class="nodecor">1.4</span>% mIoU على COCO val). تؤكد هذه النتائج المتفوقة على كلتا المجموعتين فعالية CPAL لدينا، والتي تلتقط بدقة الميزات الدلالية وهياكل الكائنات.</p>
<h1 id="الخلاصة">الخلاصة</h1>
<p>في هذا العمل، نقترح استراتيجية تعلم جديدة تعتمد على نماذج السياق الواعية بالنموذج (CPAL) لطرق WSSS، والتي تهدف إلى التخفيف من التحيز المعرفي بين الحالات والسياقات. تقوم هذه الطريقة بتعديل خصائص الميزات الفعالة في مجموعات السياق وتختار وتعدل نماذج السياق بشكل تكيفي لتعزيز قدرات التمثيل. جوهر هذه الطريقة هو الوعي بالنموذج، والذي يتحقق من خلال نماذج واعية بالسياق لالتقاط التباين داخل الفئة ومحاذاة توزيع الميزات بدقة. تظهر التجارب الموسعة تحت إعدادات مختلفة أن الطريقة المقترحة تتفوق على الطرق الحديثة الأخرى، وتكشف الدراسات التجريبية عن فعالية CPAL لدينا.</p>
</body>
</html>
