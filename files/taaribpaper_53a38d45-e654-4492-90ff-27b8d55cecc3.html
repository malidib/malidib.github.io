<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Xinwei Chen">
  <meta name="author" content="Kun Li">
  <meta name="author" content="Jiangjian Guo">
  <meta name="author" content="Tianyou Song">
  <title>التعرُّف على الكيانات المُسمّاة بعددٍ قليلٍ من الأمثلة في StackOverflow</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    html, body {
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      background: #f8f9fa;
      color: #222;
      font-size: 22px;
      line-height: 1.8;
      margin: 0;
      padding: 0;
      direction: rtl;
    }
    header {
      background: linear-gradient(90deg, #4e54c8 0%, #8f94fb 100%);
      color: #fff;
      padding: 2.5rem 1.5rem 1.5rem 1.5rem;
      text-align: center;
      border-bottom-left-radius: 40px;
      border-bottom-right-radius: 40px;
      box-shadow: 0 4px 16px rgba(78,84,200,0.08);
      margin-bottom: 2.5rem;
    }
    h1.title {
      font-size: 2.5rem;
      font-weight: 700;
      margin-bottom: 0.5rem;
      letter-spacing: 0.02em;
    }
    .author {
      font-size: 1.1rem;
      margin: 0.2rem 0;
      color: #e0e0e0;
      display: inline-block;
      margin-left: 0.5rem;
    }
    h1, h2, h3, h4 {
      color: #4e54c8;
      font-weight: 700;
      margin-top: 2.2rem;
      margin-bottom: 1rem;
      line-height: 1.3;
    }
    h1 {
      font-size: 2rem;
      border-bottom: 2px solid #e0e0e0;
      padding-bottom: 0.3rem;
    }
    h2 {
      font-size: 1.5rem;
      border-right: 4px solid #8f94fb;
      padding-right: 0.5rem;
      background: #f1f3fa;
      border-radius: 12px;
      margin-top: 2rem;
    }
    h3 {
      font-size: 1.2rem;
      color: #5a5a5a;
      margin-top: 1.5rem;
    }
    h4 {
      font-size: 1.1rem;
      color: #6c63ff;
      margin-top: 1.2rem;
    }
    p {
      margin: 1.2rem 0;
      text-align: justify;
    }
    ul, ol {
      margin: 1.2rem 2.5rem 1.2rem 0;
      padding-right: 1.5rem;
    }
    ul li, ol li {
      margin-bottom: 0.7rem;
      font-size: 1.02em;
    }
    em {
      color: #6c63ff;
      font-style: normal;
      font-weight: 600;
    }
    strong {
      color: #4e54c8;
      font-weight: 700;
    }
    code, .math.inline {
      background: #f3f3f3;
      color: #c7254e;
      padding: 0.15em 0.4em;
      border-radius: 5px;
      font-size: 0.95em;
      font-family: 'Cairo', 'Consolas', 'monospace';
      direction: ltr;
      unicode-bidi: embed;
    }
    pre {
      background: #23272f;
      color: #fff;
      padding: 1.2em;
      border-radius: 10px;
      overflow-x: auto;
      font-size: 1em;
      margin: 1.5em 0;
      direction: ltr;
      unicode-bidi: embed;
    }
    .nodecor {
      text-decoration: none;
      color: inherit;
      font-weight: 600;
      letter-spacing: 0.01em;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 2em 0;
      background: #fff;
      border-radius: 12px;
      overflow: hidden;
      box-shadow: 0 2px 8px rgba(78,84,200,0.06);
    }
    th, td {
      border: 1px solid #e0e0e0;
      padding: 0.7em 1em;
      text-align: center;
      font-size: 1em;
    }
    th {
      background: #f1f3fa;
      color: #4e54c8;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f8f9fa;
    }
    blockquote {
      border-right: 5px solid #8f94fb;
      background: #f1f3fa;
      color: #444;
      margin: 1.5em 0;
      padding: 1em 1.5em;
      border-radius: 10px;
      font-size: 1.05em;
    }
    @media (max-width: 700px) {
      header {
        padding: 1.2rem 0.5rem 1rem 0.5rem;
        border-bottom-left-radius: 20px;
        border-bottom-right-radius: 20px;
      }
      h1.title {
        font-size: 1.5rem;
      }
      h1, h2 {
        font-size: 1.2rem;
      }
      body {
        font-size: 18px;
      }
      table, th, td {
        font-size: 0.95em;
      }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">التعرُّف على الكيانات المُسمّاة بعددٍ قليلٍ من الأمثلة في <span class="nodecor">StackOverflow</span></h1>
  <p class="author"><span class="nodecor">Xinwei Chen</span></p>
  <p class="author"><span class="nodecor">Kun Li</span></p>
  <p class="author"><span class="nodecor">Jiangjian Guo</span></p>
  <p class="author"><span class="nodecor">Tianyou Song</span></p>
</header>
<main>
  <h1 id="ملخص">مُلخّص</h1>
  <p>يشكّل الرصيد الضخم من أسئلة <span class="nodecor">StackOverflow</span> تحدّياً لوسم البيانات بسبب محدودية الأمثلة المُوسومة. نعالج هذه الفجوة باستخدام <span class="nodecor">RoBERTa+MAML</span>، وهي طريقة للتعرُّف على الكيانات المُسمّاة بعددٍ قليلٍ من الأمثلة تستفيد من التعلُّم الفوقي. قيّمنا نهجنا على مجموعة بيانات <span class="nodecor">StackOverflow NER</span> (تضمّ 27 نوعاً من الكيانات)، وحققنا تحسّناً بنحو <span class="nodecor">5\%</span> في نقاط <span class="nodecor">F1</span> مقارنةً بالنموذج الأساسي. علاوةً على ذلك، عزّزنا الأداء إضافياً عبر التعامل مع العبارات المتخصِّصة بالمجال.</p>
  <h1 id="مقدمة">مقدمة</h1>
  <p>يطرح تَنَامِي محتوى البرمجة على الإنترنت تحدّياتٍ في فهم واستخراج المعلومات المتعلِّقة بالبرمجيّات. يُعَدّ منتدى <span class="nodecor">StackOverflow</span> أكبر منتدى للبرمجة، إذ يضمّ أكثر من <span class="nodecor">15</span> مليون سؤال. ولتفهم هذا الكمّ الهائل بفاعلية، يلزم تحديد الكيانات المُسمّاة (<span class="nodecor">NEs</span>). غير أنّ التعلُّم الخاضع للإشراف الكامل للتعرُّف على الكيانات المُسمّاة (<span class="nodecor">NER</span>) يتطلّب بيانات موسومة واسعة النطاق، وهو ما يستنزف الموارد. استجابةً لذلك، نقترح التعلُّم بعددٍ قليلٍ من الأمثلة لتمكين التعرُّف الدقيق على الكيانات بأقلّ قدرٍ من بيانات الوسم. ويمكن تطبيق نهجنا في مهامّ البرمجيات مثل استرجاع المعلومات، والإجابة عن الأسئلة، وتلخيص المحتوى.</p>
  <p>يشير <span class="nodecor">mai2018fine</span> إلى التعرُّف الدقيق على الكيانات المُسمّاة عبر تصنيفها إلى فئات أكثر تحديداً. وكثيراً ما تكون بُنى البيانات المُوسومة مُلتبِسة، ما يزيد صعوبة الوسم. ونظراً للتكلفة العالية للوسم اليدوي، يبدو التعلُّم بعددٍ قليلٍ من الأمثلة حلاً عملياً. عبر تدريب النماذج على أمثلة موسومة قليلة، نُحَقِّق تعرُّفاً دقيقاً وفعالاً للكيانات المُسمّاة. في هذه الورقة نُقدِّم دراسة حول التعلُّم بعددٍ قليلٍ من الأمثلة في مجال البرمجيات على مجموعة بيانات <span class="nodecor">StackOverflow NER</span> (<span class="nodecor">codener</span>). نقترح نموذجاً يستفيد من شبكة انتباه لاستخراج المعلومات من مقاطع الشيفرة والنص لتوليد نتائج أوليّة عند تحديد <span class="nodecor">27</span> نوعاً من الكيانات المتعلِّقة بالبرمجيات. تشمل مساهماتنا:</p>
  <ul>
    <li>نقترح إطاراً للتعرُّف على الكيانات المرتبطة بالبرمجيات بعددٍ قليلٍ من الأمثلة.</li>
    <li>نولّد النتائج الأوّلية على مجموعة بيانات <span class="nodecor">StackOverflow NER</span>.</li>
  </ul>
  <h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
  <p>شهد مجال معرفة البرمجيات العديد من الدراسات؛ من أمثلتها: تحسين قياس جودة الأسئلة في <span class="nodecor">StackOverflow</span> (<span class="nodecor">ravi</span>)، واستخراج الأسئلة والأجوبة ذات الصلة في <span class="nodecor">StackOverflow</span> (<span class="nodecor">shirani</span>). ومع ذلك، تفتقر هذه الأعمال إلى توظيفٍ منهجيّ لتقنيات معالجة اللغات الطبيعية من أجل تحديد الكيانات المُسمّاة في سياق البرمجيات.</p>
  <p>أُنجِزت أعمال كثيرة في التعرُّف على الكيانات المُسمّاة؛ إذ تناولت دراساتٌ مثل (<span class="nodecor">Li19, li2023deception</span>) قضايا استرجاع المعلومات وبناء الأنطولوجيا في نُطُق محدّدة. واقترح (<span class="nodecor">qun</span>) تعلّماً صفريّ اللقطة قابلَ النقل بين المجالات للتعرُّف على الكيانات. حديثاً، غدا التعلُّم العميق شائعاً في <span class="nodecor">NER</span>، ولا سيّما مع النماذج المُدرَّبة مُسبقاً مثل <span class="nodecor">BERT</span> (<span class="nodecor">bert</span>) و<span class="nodecor">RoBERTa</span> (<span class="nodecor">roberta</span>). على الرغم من ذلك، يظلّ التعرُّف على الكيانات مهمّةً تستغرق وقتاً وتحتاج خبراءَ مجال لوسم كمّيات ضخمة من البيانات. طُوِّرت عدّة منهجيات للتعرُّف الدقيق بعددٍ قليلٍ من الأمثلة، منها خطّ أنابيب <span class="nodecor">fgner</span>، وطرائق التوليد التبايني <span class="nodecor">fewshotner</span>، وطرائق التعلُّم التوليدي القائم على الرسوم البيانية (<span class="nodecor">pmlr-v202-zeng23c, zeng23acm</span>). ومع ذلك، لم يُعَالَج بعدُ بصورةٍ كافية التعرُّفُ الدقيق بعددٍ قليلٍ من الأمثلة في نطاق البرمجيات.</p>
  <p>يُعَدّ عمل <span class="nodecor">codener</span> من أوائل الجهود في التعرُّف على الكيانات المُسمّاة في ميدان البرمجة الحاسوبية؛ إذ استهدف تمييز الكيانات المُسمّاة في <span class="nodecor">StackOverflow</span>، ودُرِّب النموذج على <span class="nodecor">BERT</span> مع دمج التضمينات السياقية وتضمينات المجال الخاصّة.</p>
  <h4 id="تعلم-الاستدعاء">التعلُّم المُعتمِد على المُوجِّهات (Prompt-based):</h4>
  <p>يحوّل التعلُّم المُعتمِد على المُوجِّهات مهامّ معالجة اللغات الطبيعية التقليدية إلى مسائل تنبّؤ بملء مواضع مُقنَّعة. في مهامّ التعلُّم بعددٍ قليلٍ من الأمثلة، يضيف (<span class="nodecor">fgner</span>, <span class="nodecor">fewshotner</span>) قالباً مُوجِّهاً لاستخراج الكيان بعد الجملة الأصلية.</p>
  <h4 id="التعلم-البياني">التعلُّم الفوقي (Meta-learning):</h4>
  <p>يُوصَف التعلُّم الفوقي بأنّه "تعلُّمُ كيفيّةِ التعلُّم". ولأنّ تدريب نموذجٍ دقيقٍ يستلزم عادةً بياناتٍ موسومة وفيرة، في حين تكون الوسوم في نطاق البرمجيات محدودة، يتيح التعلُّم الفوقي تهيئةً أفضل لمعاملات النموذج بحيث يتكيّف سريعاً مع مهمّاتٍ جديدة. طبّق (<span class="nodecor">Decomposed</span>) خوارزمية <span class="nodecor">MAML</span> وقدّم نموذج <span class="nodecor">MAML-ProtoNet</span> للتعلُّم بعددٍ قليلٍ من الأمثلة.</p>
  <h1 id="الطريقة">الطريقة</h1>
  <p>بحسب علمنا، لم تُجرَ دراساتٌ كافية لتطبيق التعلُّم بعددٍ قليلٍ من الأمثلة على مجموعة بيانات <span class="nodecor">StackOverflow NER</span>. قدّم مؤلّفو <span class="nodecor">codener</span> دراسةً إشرافيةً منهجية على هذه المجموعة، لكنّها تواجه قيوداً: أوّلاً، ضمن مهامّ المجال، يحتاج الوسم إلى خبراء ويستغرق وقتاً طويلاً. ثانياً، يستغرق تدريب <span class="nodecor">BERT</span> داخل النطاق أكثر من شهرٍ على 152 مليون جملة من <span class="nodecor">StackOverflow</span>. نستكشف هنا نموذجين للتعلُّم بعددٍ قليلٍ من الأمثلة: الأوّل هو الضبطُ الدقيق المُعتمِد على المُوجِّهات، والثاني يدمج التعلُّم الفوقي للتكيّف مع مهامّ المجال المتخصِّصة.</p>
  <h2 id="تصنيف-الكيانات-بعدد-قليل-من-الأمثلة">تصنيف الكيانات بعددٍ قليلٍ من الأمثلة</h2>
  <p>نُعرِّف في هذا القسم مشكلةَ تصنيف الكيانات بعددٍ قليلٍ من الأمثلة: تحديدَ نوع الكيان في جملة باستخدام عددٍ محدودٍ من عيّنات التدريب. المُدخلات هي تسلسل من رموز النص <span class="math inline">\(\textbf{x}=\{t_{1},t_{2},\ldots,\textbf{m},\ldots,t_{T}\}\)</span>، حيث <span class="math inline">\(m = \{t_{i},\ldots,t_{j}\}\)</span> هو تسلسل كيان مكوّن من <span class="math inline">\((j-i+1)\)</span> رمزاً، و<span class="math inline">\(T\)</span> هو الطول الكلّي للجملة. المخرجات هي تسمية نوع الكيان <span class="math inline">\(y \in Y\)</span>، حيث <span class="math inline">\(Y\)</span> هي مجموعة التسميات <span class="math inline">\(\{y_{1},\ldots,y_{n}\}\)</span>. في التعلُّم ب<span class="math inline">\(K\)</span>-مثال، لدينا <span class="math inline">\(K\)</span> عيّنات تدريبية لكل فئة.</p>
  <h2 id="الضبط-الدقيق-بناء-على-الأوامر">الضبط الدقيق المُعتمِد على المُوجِّهات</h2>
  <p>اعتمدنا إطار العمل المقترَح من Huang بوصفه نموذجاً أساسياً (<span class="nodecor">fewshotner</span>). يوضّح الشكل المشار إليه الإطارَ العام للضبط الدقيق المُعتمِد على المُوجِّهات. في هذا الأسلوب، نُدخل الجملة مع قالبٍ يتضمّن موضعاً مُقنَّعاً <span class="math inline">\([MASK]\)</span> لتمثيل التنبّؤ المطلوب. بعد المعالجة بواسطة مُشفِّر النموذج المُدرَّب مُسبقاً <span class="math inline">\(\theta_{0}\)</span> (مثل <span class="nodecor">RoBERTa</span>)، نحصل على التمثيل السياقي <span class="math inline">\(h_{m}\)</span> لرمز <span class="math inline">\([MASK]\)</span>. ثم يُنتج رأسُ النموذج توزيعَ احتمال على المفردات <span class="math inline">\(\mathcal{V}\)</span>. نستخدم Softmax لحساب احتمالات الكلمات، ثم مُلفِّظاً (verbalizer) لتحويل احتمالات الكلمات إلى احتمالات التسميات، ونعتمد تنافُر <span class="nodecor">KL</span> دالّةَ خسارة لتقليل الفارق بين التنبّؤات والهدف.</p>
  <h2 id="التعلم-البيني-المستقل-عن-النموذج">التعلُّم الفوقي المُستقلّ عن النموذج</h2>
  <p>لتحسين الأداء في المهامّ المتخصِّصة بالمجال، ندمج التعلُّم الفوقي المُستقلّ عن النموذج (<span class="nodecor">finn2017model</span>). الفكرة العامّة هي تدريب النموذج على مهامّ متعدّدة للحصول على تهيئةٍ أفضل للمعاملات، بما يُتيح له التعلُّم بسرعة على مهامّ جديدة. نُطبّق خوارزمية <span class="nodecor">MAML</span> عبر مرحلتين: التدريب الفوقي على <span class="math inline">\(\xi_{\text{train}}\)</span> والاختبار الفوقي على <span class="math inline">\(\xi_{\text{test}}\)</span>، مع إعداد <span class="math inline">\(K\)</span>-<span class="nodecor">shot</span>-<span class="math inline">\(N\)</span>-<span class="nodecor">way</span>.</p>
  <h3 id="مرحلة-التدريب-الأولى">مرحلة التدريب الفوقي</h3>
  <p>في هذه المرحلة، ندرّب النموذج على مجموعة بياناتٍ عامّة عبر <span class="nodecor">N</span> مهام. لكلّ مهمّة، نأخذ عيّنة <span class="math inline">\((D_{i}^{\text{sup}} , D_{i}^{\text{query}})\)</span> من <span class="math inline">\(\xi_{\text{train}}\)</span> ونُحدِّث المعاملات داخليّاً بحسب المعادلة (7). ثم نقيس الأداء على <span class="math inline">\(D_{i}^{\text{query}}\)</span> ونجمع الخسائر عبر المهام لتحديث التهيئة العامّة كما في المعادلتين (8) و(9).</p>
  <h3 id="الاختبار-الفوقي">مرحلة الاختبار الفوقي</h3>
  <p>في هذه المرحلة، نستخدم المعاملات المُحدَّثة لتحسين النموذج على مجموعة تدريب <span class="nodecor">StackOverflow</span> ثم نُجري التنبّؤ على مجموعة الاختبار.</p>
  <h1 id="التجربة">التجربة</h1>
  <h2 id="مجموعة-البيانات">مجموعة البيانات</h2>
  <p>نستخدم في هذه المهمّة مجموعة بيانات <span class="nodecor">NER</span> من <span class="nodecor">StackOverflow</span> (المعروفة باسم <span class="nodecor">codener</span>)، التي تضمّ أكثر من <span class="nodecor">1,237</span> موضوعاً من الأسئلة والأجوبة على مدى <span class="nodecor">10</span> سنوات و<span class="nodecor">27</span> نوعاً من الكيانات. تشمل كيانات الشيفرة: <em>الصنف، المتغيِّر، الشيفرة المُضمَّنة، الدالّة، المكتبة، القيمة، نوع البيانات، والوسم (HTML/XML)</em>. وتشمل كيانات اللغة الطبيعية: <em>التطبيق، عنصر واجهة المستخدم، اللغة، بُنية البيانات، الخوارزمية، نوع الملف، اسم الملف، الإصدار، الجهاز، نظام التشغيل، موقع الويب، واسم المستخدم</em>. ونخطّط أيضاً لاستخدام بيانات إضافية مأخوذة عشوائياً من مستودعات <span class="nodecor">GitHub</span>.</p>
  <h2 id="إعدادات-التجربة">إعدادات التجربة</h2>
  <p><strong>أخذ العيّنات لسيناريو قليلِ الأمثلة.</strong> نجري تجربة التعلُّم بخمسة أمثلة لكلّ فئة (<span class="nodecor">5-shot</span>) على مجموعة بيانات <span class="nodecor">StackOverflow NER</span> باختيار خمس عيّنات تدريبية عشوائياً في كلّ تجربة. كما أجرينا تجربةً موازيةً تعتمد خمسَ عيّنات مُنتقاة يدوياً لتحسين دقّة النموذج.</p>
  <p><strong>التدريب الفوقي التمهيدي.</strong> تحتوي مجموعة بيانات <span class="nodecor">Few-NERD</span> على <span class="nodecor">66</span> نوعاً دقيقاً من الكيانات. نجري مرحلة التدريب الفوقي التمهيدي عبر <span class="nodecor">40</span> مهمّة؛ في كلّ مهمّة نُنشئ إعداد <span class="nodecor">27-way</span>، ثم نقسّمه إلى مجموعة دعم <span class="nodecor">5-shot</span> ومجموعة استعلام <span class="nodecor">15-shot</span>.</p>
  <p><strong>إعدادات المعاملات الفائقة.</strong> نعتمد نموذج <span class="nodecor">RoBERTa-base</span> المُدرَّب مُسبقاً. أقصى طولٍ للتسلسل <span class="nodecor">128</span>؛ حجم الدفعة الداخلي للضبط الدقيق <span class="nodecor">8</span>؛ حجم الدفعة الخارجي للتحديث الفوقي <span class="nodecor">32</span>؛ عدد عصور الضبط الدقيق في التدريب الفوقي <span class="nodecor">1</span>؛ عدد عصور الضبط في الاختبار الفوقي <span class="nodecor">10</span>؛ الحدّ الأقصى لخطوات التعلُّم الفوقي <span class="nodecor">15</span>؛ معدل التعلُّم للّفوقي <span class="math inline">\(5\text{e}{-3}\)</span>؛ ومعدل التعلُّم للتحديث الداخلي <span class="math inline">\(1\text{e}{-2}\)</span>. تمّ التدريب على <span class="nodecor">GPU</span> في <span class="nodecor">Google Colab</span>.</p>
  <p><strong>مقاييس التقييم.</strong> استخدمنا مقاييس <span class="nodecor">Micro-F1</span> و<span class="nodecor">Macro-F1</span>.</p>
  <h2 id="النتائج">النتائج</h2>
  <p>طبّقنا نموذج <span class="nodecor">RoBERTa</span> ونموذج <span class="nodecor">RoBERTa+MAML</span> على مجموعة بيانات التعرُّف على الكيانات المُسمّاة من <span class="nodecor">StackOverflow</span>. في التدريب، اخترنا عشوائياً خمس عيّنات لكلّ فئة من مجموعة البيانات. كما يظهر في الجدول [citation-guide]، بلغت درجة <span class="nodecor">Micro-F1</span> لنموذج <span class="nodecor">RoBERTa</span> <span class="nodecor">0.3091</span> ودرجة <span class="nodecor">Macro-F1</span> <span class="nodecor">0.2837</span>.</p>
  <p>وبالمنهجية نفسها، طبّقنا نموذج <span class="nodecor">RoBERTa+MAML</span>، فبلغت درجة <span class="nodecor">Micro-F1</span> <span class="nodecor">0.3578</span> ودرجة <span class="nodecor">Macro-F1</span> <span class="nodecor">0.3197</span>، ما يدلّ على زيادةٍ ملحوظة بفضل التعلُّم الفوقي.</p>
  <p>كما نرى في الشكلين ([fig:highperform], [fig:lowperform])، تحسّنت فئات <em>بُنية البيانات، عنصر واجهة المستخدم، نظام التشغيل، اسم المستخدم</em> و<em>نوع البيانات</em> باستخدام <span class="nodecor">RoBERTa+MAML</span>.</p>
  <h2 id="دراسة-حالة-لمجموعة-تدريب-من-5-لقطات">دراسة حالة لمجموعة تدريب من خمسة أمثلة</h2>
  <p>لاحظنا أنّ درجة <span class="nodecor">F1</span> لعدة فئات مثل نظام التشغيل، والمكتبة، واسم الدالّة، وعنوان <span class="nodecor">IP</span>، ولوحة المفاتيح، واللغة، واسم المتغيِّر، والخوارزمية كانت أدنى من المتوقّع. ويصعب التعرُّف على هذه الفئات عند تطبيق <span class="nodecor">NER</span> بخمسة أمثلة. وبعد فحص مجموعة التدريب العشوائية، وجدنا فيها كياناتٍ مكرّرة وأخرى غير ممثِّلة على نحوٍ كافٍ.</p>
  <p>ولتفادي التأثيرات السلبية، اخترنا يدوياً عيّناتٍ تدريبية ذات معنًى وتمثيلٍ أفضل. وطبّقنا النهج نفسه على نموذج <span class="nodecor">RoBERTa+MAML</span>. كما يوضّح الجدول، ارتفعت درجة <span class="nodecor">Micro-F1</span> بنحو <span class="nodecor">3\%</span> ودرجة <span class="nodecor">Macro-F1</span> بنحو <span class="nodecor">2\%</span>.</p>
  <p>يوضح هذا المثال أنّ فئة "المحتوى" غامضةٌ ضمن بيانات التدريب بخمسة أمثلة ولا تتمتّع بتمثيلٍ كافٍ، لذا قرّرنا اختيار عيّنات بديلة يدوياً لتعظيم أداء النموذج. أظهرت النتائج تفوّق <span class="nodecor">RoBERTa+MAML</span> مع البيانات المُنتقاة يدوياً على البيانات العشوائية، وعليه سنعتمد هذه البيانات في أعمالنا المستقبلية.</p>
  <h2 id="دراسة-حالة-لاستخراج-الأنماط-المعتمدة-على-المعرفة">دراسة حالة لاستخراج الأنماط المعتمِدة على المعرفة</h2>
  <p>لاحظنا وجود فئاتٍ يصعب التعرُّف عليها لكنّها تتّبع أنماطاً واضحة. مثلاً، تُصنَّف جميع امتدادات الملفات الشائعة ضمن فئة "نوع الملف". كما في الجدول [table:maual]، يمكن استخدام التعابير النمطية لاستخراج امتدادات مثل csv، وjpg، وdoc. ويشير الجدول [table:extraction] إلى تحسّن درجة <span class="nodecor">F1</span> لفئة "نوع الملف" من <span class="nodecor">0.345</span> إلى <span class="nodecor">0.49</span>، مع دقّة <span class="nodecor">0.716</span> واسترجاع <span class="nodecor">0.372</span>. ويمكن تعميم استخراج الأنماط المعتمِد على المعرفة على فئاتٍ أخرى لتحسين الدقّة الإجمالية ودرجة <span class="nodecor">F1</span>.</p>
  <h1 id="الخلاصة-والأعمال-المستقبلية">الخلاصة والأعمال المستقبلية</h1>
  <p>ركّزنا في دراستنا على التعرُّف على الكيانات المُسمّاة ضمن مجال برمجة الحاسوب باستخدام نموذج <span class="nodecor">RoBERTa+MAML</span> في مهمة <span class="nodecor">5-shot</span> على مجموعة بيانات <span class="nodecor">StackOverflow NER</span>، حيث حقّقنا تحسيناتٍ كبيرة مقارنةً بنموذج <span class="nodecor">RoBERTa</span> الأساسي. تُبيّن النتائج أنّ التعلُّم الفوقي أداةٌ قويّة لمهامّ التعرُّف المتخصِّصة والقليلة الأمثلة، وأنّ معالجة العبارات المتخصِّصة واستخراج الأنماط المعتمِدة على المعرفة يُعزّزان الدقّة. ونتوقّع أن تستفيد المهام المستقبلية في استخراج المعلومات المتعلّقة بالبرمجيات والإجابة عن الأسئلة من هذه التقنيات. ونعتزم مستقبلاً توسيع تنوّع البيانات واستكشاف تشكيلات مختلفة لمجموعات الدعم والاستعلام لتعزيز فاعلية التعلُّم الفوقي.</p>
</main>
</body>
</html>