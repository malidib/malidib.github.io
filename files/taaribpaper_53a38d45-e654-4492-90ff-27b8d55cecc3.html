<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Xinwei Chen">
  <meta name="author" content="Kun Li">
  <meta name="author" content="Jiangjian Guo">
  <meta name="author" content="Tianyou Song">
  <title>التعرّف على الكيانات الاسمية بعدد قليل من الأمثلة في StackOverflow</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">التعرّف على الكيانات الاسمية بعدد قليل من الأمثلة في <span class="nodecor">StackOverflow</span></h1>
<p class="author"><span class="nodecor">Xinwei Chen</span></p>
<p class="author"><span class="nodecor">Kun Li</span></p>
<p class="author"><span class="nodecor">Jiangjian Guo</span></p>
<p class="author"><span class="nodecor">Tianyou Song</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">مُلخّص</h1>
<p><span class="nodecor">StackOverflow</span>، بمخزونه الهائل من الأسئلة والأمثلة الموسومة المحدودة، يطرح تحدياً في التوسيم لدينا. نعالج هذه الفجوة من خلال اقتراح <span class="nodecor">RoBERTa+MAML</span>، وهي طريقة للتعرّف على الكيانات الاسمية بعدد قليل من الأمثلة تستفيد من التعلم البيني. تم تقييم نهجنا على مجموعة بيانات <span class="nodecor">StackOverflow NER</span> (27 نوعاً من الكيانات)، وقد حقق تحسناً بنسبة <span class="nodecor">5%</span> في معدل الدقة <span class="nodecor">F1</span> مقارنة بالأساس. كما قمنا بتحسين النتائج أكثر من خلال معالجة العبارات المحددة للمجال لتعزيز النتائج.</p>
<h1 id="مقدمة">مقدمة</h1>
<p>زيادة محتوى البرمجة على الإنترنت تطرح تحديات في فهم واستخراج المعلومات المتعلقة بالبرمجيات. منتدى <span class="nodecor">StackOverflow</span>، باعتباره أكبر منتدى برمجة، يحتوي على أكثر من <span class="nodecor">15</span> مليون سؤال متعلق بالبرمجيات. لفهم هذا الكم الهائل بفعالية، من الضروري تحديد الكيانات المسماة (<span class="nodecor">NEs</span>). ومع ذلك، يتطلب التعلم الخاضع للإشراف الكامل للتعرّف على الكيانات المسماة (<span class="nodecor">NER</span>) في سياقات محددة بيانات موسومة واسعة النطاق، وهو ما يستهلك الكثير من الموارد. رداً على ذلك، نقترح نهج التعلم بعدد قليل من الأمثلة للتعرّف الدقيق على الكيانات المسماة، مما يمكّن من التعرّف على الكيانات بفعالية مع الحد الأدنى من بيانات التدريب الموسومة. يمكن تطبيق طريقتنا على مهام مجال البرمجيات مثل استرجاع المعلومات، الإجابة على الأسئلة، وتلخيص المقالات.</p>
<p>التعرّف الدقيق على الكيانات المسماة (<span class="nodecor">mai2018fine</span>) يعني تصنيف الكيانات إلى فئات أكثر تحديداً. وأحياناً، تكون هيكلية البيانات الموسومة استنتاجية، مما يزيد من صعوبة توسيم الكيانات. نظراً للتكلفة العالية للتوسيم اليدوي، يظهر التعلم بعدد قليل من الأمثلة كحل عملي. من خلال تدريب النماذج بأمثلة موسومة قليلة، نحقق تعرّفاً دقيقاً وفعالاً للكيانات المسماة الدقيقة. في هذه الورقة، نقدم دراسة تحقق في تعرّف الكيانات المسماة بعدد قليل من الأمثلة في المجال المتعلق بالبرمجيات. نوضح فعالية طريقتنا على مجموعة بيانات <span class="nodecor">StackOverflow</span> (<span class="nodecor">codener</span>). اقترحنا نموذج تعرّف الكيانات المسماة بعدد قليل من الأمثلة المتعلق بالبرمجيات الذي يستخدم شبكة الانتباه لاستخراج المعلومات على مستوى النص من شظايا الكود وتوليد النتائج الأولية عند تحديد <span class="nodecor">20</span> نوعاً من الكيانات المسماة المتعلقة بالبرمجيات. تشمل مساهماتنا:</p>
<ul>
<li><p>نقترح وحدة تفسير تسمية الكيان مع عدد قليل من الأمثلة للكيانات المتعلقة بالبرمجيات</p></li>
<li><p>على مجموعة بيانات <span class="nodecor">StackOverflow</span> نولّد النتيجة الأولى</p></li>
</ul>
<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<p>تم إجراء العديد من الدراسات حول قاعدة المعرفة البرمجية. على سبيل المثال، البحث عن تحسين قياس جودة سؤال في StackOverflow (<span class="nodecor">ravi</span>)، وإيجاد الأسئلة والأجوبة ذات الصلة في StackOverflow (<span class="nodecor">shirani</span>). ومع ذلك، تفتقر أبحاثهم إلى تقنيات معالجة اللغات الطبيعية لتحديد الكيانات الاسمية المتعلقة بالبرمجيات مع مدونات اللغة الطبيعية.</p>
<p>كما تم إجراء العديد من الأعمال في التعرّف على الكيانات المسماة، حيث درس (<span class="nodecor">Li19, li2023deception</span>) استرجاع المعلومات والأونتولوجيا في مجال محدد. يقترح (<span class="nodecor">qun</span>) تعلماً بدون أمثلة قابلاً للنقل بين المجالات للتعرّف على الكيانات المسماة في الوكلاء الموجهين للمهام. مؤخراً، أصبح التعلم العميق شائعاً في التعرّف على الكيانات المسماة، خاصة مع نماذج اللغة المدربة مسبقاً مثل BERT (<span class="nodecor">bert</span>) وRoBERTa (<span class="nodecor">roberta</span>). على الرغم من نماذج اللغة المدربة مسبقاً، يُعتبر التعرّف على الكيانات المسماة مهمة تستغرق وقتاً طويلاً وتتطلب جهداً كبيراً، حيث يتطلب خبراء المعرفة المجالية لتوسيم مجموعة كبيرة من تسميات المجال كمجموعة تدريب واختبار لجعل النموذج جيداً. تم إجراء العديد من الدراسات في التعرّف على الكيانات المسماة الدقيقة بعدد قليل من الأمثلة. طوّر (<span class="nodecor">fgner</span>) خط أنابيب تعلم فعال للتنبؤ بنوع الكيانات. طريقة أخرى هي استخدام مولد الحالات المتناقضة القائم على النوع لتوسيع مجموعة التدريب واستخدام تباين كولباك-لايبلر لحساب خسارة الحالات المولدة حديثاً (<span class="nodecor">fewshotner</span>). يستخدم (<span class="nodecor">liu2023influence</span>) خوارزمية اكتشاف تستفيد من صلة الأعمال المنشورة. يستخدم (<span class="nodecor">pmlr-v202-zeng23c</span>), (<span class="nodecor">zeng23acm</span>) تعلم الرسم البياني التوليدي لإيجاد صلة الكيان. ومع ذلك، حسب علمنا، لم يتم إجراء أي عمل على التعرّف على الكيانات المسماة الدقيقة بعدد قليل من الأمثلة في مجال البرمجيات.</p>
<p>هناك أيضاً مدونة للتعرّف على الكيانات المسماة الدقيقة لمجال البرمجة الحاسوبية (<span class="nodecor">codener</span>). الهدف من التعرّف على الكيانات المسماة في StackOverflow هو تحديد الكيانات الاسمية في مجال البرمجيات. تم تدريبهم في مجال Bert ودمجه مع التضمينات الكلمية السياقية والتضمينات المحددة للمجال.</p>
<h4 id="تعلم-الاستدعاء">تعلم الاستدعاء: </h4>
<p>ينقل تعلم الاستدعاء المهام اللغوية الطبيعية التقليدية إلى مشاكل التنبؤ. يحتاج النموذج إلى التنبؤ بالمعلومات للفتحات غير المملوءة. لمهام كتابة الكيانات بعدد قليل من الأمثلة، يضيف (<span class="nodecor">fgner</span>), (<span class="nodecor">fewshotner</span>) قالب استدعاء يحتوي على الكيان المذكور بعد الجملة الأصلية.</p>
<h4 id="التعلم-البياني">التعلم البيني: </h4>
<p>يوصف التعلم البيني بأنه "تعلم التعلم" بينما يتطلب تدريب نموذج جيد بيانات موسومة وفيرة، للمهمة في المجال، التوسيمات محدودة. يمكن أن يمكّن أسلوب التعلم البيني من تكييف النموذج أو تعميمه لمهام جديدة يواجهها أثناء التدريب. طبّق (<span class="nodecor">Decomposed</span>) التعلم البيني على التعرّف على الكيانات المسماة بعدد قليل من الأمثلة. قاموا بتهيئة نماذج اكتشاف الكيانات باستخدام التعلم البيني غير المتحيز للنموذج (MAML) واقترحوا MAML-ProtoNet لكتابة الكيانات بعدد قليل من الأمثلة، مما يسد الفجوات بين المجالات.</p>
<h1 id="الطريقة">الطريقة</h1>
<p>حسب أفضل معرفتنا، لم يتم إجراء أي بحث حول تصنيف الكيانات بالطريقة قليلة الأمثلة على مجموعة بيانات StackOverflowNER. قام تبسم بدراسة الطريقة الإشرافية على هذه المجموعة (<span class="nodecor">codener</span>). ومع ذلك، هناك عدة قيود. أولاً، بالنسبة للمهام داخل النطاق، فهي تتطلب جهداً كبيراً وتستغرق وقتاً طويلاً، حيث نحتاج عادة إلى خبراء في المجال للقيام بالتوثيق. ثانياً، يستغرق الأمر أكثر من <span class="nodecor">1</span> شهر لتدريب BERT داخل النطاق على <span class="nodecor">152</span> مليون جملة من StackOverflow. نحن نستكشف نموذجين لإجراء التعلم قليل الأمثلة على هذه المجموعة. الأول هو نموذج الضبط الدقيق المبني على الأوامر. الثاني هو إضافة طريقة التعلم البيني للمهام المحددة بالمجال.</p>
<h2 id="تصنيف-الكيانات-بعدد-قليل-من-الأمثلة">تصنيف الكيانات بعدد قليل من الأمثلة</h2>
<p>في هذا القسم، سنعرّف مشكلة تصنيف الكيانات بعدد قليل من الأمثلة، والتي تعني تحديد نوع الكيانات في الجملة باستخدام بيانات تدريب قليلة. المدخلات هي تسلسل من رموز النص، <span class="math inline">\(\textbf{x}=\{t_{1},t_{2},...,\textbf{m},...,t_{T}\}\)</span>، حيث <span class="math inline">\(m = \{t_{i},...,t_{j}\}\)</span> هو تسلسل كيان يحتوي على <span class="math inline">\((j-i+1)\)</span> رمزاً، وT هو العدد الإجمالي للرموز في الجملة. المخرجات هي تسمية نوع الكيان <span class="math inline">\(y \in Y\)</span>، <span class="math inline">\( Y \)</span> هي مجموعة التسميات <span class="math inline">\(\{y_{1},...,y_{n}\}\)</span>، مما يشير إلى n فئات. التعلم ب <span class="math inline">\(K\)</span>-مثال يعني أن هناك <span class="math inline">\(K\)</span> أمثلة تدريبية لكل فئة. يمكننا تمثيل مجموعة البيانات التدريبية كما في المعادلة [eqn1]، حيث <span class="math inline">\(\textbf{m}\)</span> هو <span class="math inline">\([MASK]\)</span>.</p>
<h2 id="الضبط-الدقيق-بناء-على-الأوامر">الضبط الدقيق بناءً على الأوامر</h2>
<p>اخترنا الطريقة التي اقترحها هوانغ كنموذج أساسي (<span class="nodecor">fewshotner</span>). يوضح إطار العمل للضبط الدقيق بناءً على الأوامر في الشكل المحذوف. يقوم التعلم الخاضع للإشراف على تدريب نموذج باستخدام مدخلات معينة <span class="math inline">\(x\)</span> للتنبؤ بالمخرجات <span class="math inline">\(y\)</span> كـ <span class="math inline">\(P(y|x)\)</span>. بالنسبة لتعلم الأوامر، نحتاج إلى قالب يحتوي على معلومات غير مكتملة. نستخدم عادة رمز <span class="math inline">\([MASK]\)</span> لتمثيل الفتحة غير المملوءة. على سبيل المثال، يمكن أن يكون قالباً صالحاً كما في المعادلة [eqn2],</p>
<p>بعد إضافة القالب، تصبح مشكلة التصنيف مشكلة تنبؤ. نقوم بإدخال الجملة مع القالب المقابل إلى مشفّر نموذج اللغة المدرب مسبقاً <span class="math inline">\(\theta_{0}\)</span> مثل RoBERTa. يمكننا الحصول على التمثيل السياقي <span class="math inline">\(h_{m}\)</span> لرمز <span class="math inline">\([MASK]\)</span> كما هو موضح في المعادلة [eqn3],</p>
<p>نحتاج إلى اختيار الكلمة التي يمكن أن تحل محل رمز <span class="math inline">\([MASK]\)</span>. من خلال أخذ التمثيل السياقي <span class="math inline">\(h_{m}\)</span>، يمكن لرأس نموذج اللغة المقنع الحصول على توزيع الاحتمال على المفردات بأكملها <span class="math inline">\(\mathcal{V}\)</span>. يمكننا استخدام وظيفة Softmax لتطبيع توزيع الكلمات. يمكننا الحصول على احتمال كلمة معينة بإعطاء التمثيل السياقي <span class="math inline">\(h_m\)</span> لرمز <span class="math inline">\([MASK]\)</span> كما هو موضح في المعادلة [eqn4],</p>
<p>حيث <span class="math inline">\(E \in \mathbb{R}^{\left| V \right| \times h}\)</span> هي مصفوفة التضمين؛ <span class="math inline">\(\sigma(\cdot)\)</span> هي وظيفة التنشيط؛ <span class="math inline">\(W_{1} \in \mathbb{R}^{h \times h}\)</span> و <span class="math inline">\(b_{1} \in \mathbb{R}^{h}\)</span> من نموذج اللغة المقنع المدرب مسبقاً.</p>
<p>بعد الحصول على احتمال كل كلمة w في المفردات <span class="math inline">\(\mathcal{V}\)</span>. نستخدم معياراً لغوياً لتعيين التنبؤ على المفردات إلى التنبؤ على العلامات. يمكننا حساب احتمال كل علامة بناءً على احتمال كل كلمة في المفردات كما هو موضح في المعادلة [eqn5],</p>
<p>نستخدم التباين KL كدالة خسارة لدينا. الهدف من التدريب هو تقليل خسارة التباين KL كما هو موضح في [eqn6],</p>
<p>حيث <span class="math inline">\(Y^{pre}\)</span> هو التوزيع على العلامات، و <span class="math inline">\(Y^{target}\)</span> هو ترميز ساخن للعلامة المستهدفة.</p>
<h2 id="التعلم-البيني-المستقل-عن-النموذج">التعلم البيني المستقل عن النموذج</h2>
<p>لتحسين الأداء للمهمة المحددة بالمجال، نضيف وحدة التعلم البيني. الفلسفة العامة للتعلم البيني هي أن النموذج يتم تدريبه على مهام متعددة للحصول على تهيئة أفضل للمعاملات. وبالتالي، يكون النموذج قادراً على إحراز تقدم سريع في مهام المجالات الجديدة. لذلك، نطبق خوارزمية التعلم البيني المستقل عن النموذج (<span class="nodecor">finn2017model</span>) في مهمتنا. يتكون إجراء التعلم البيني من جزأين: التدريب البيني على <span class="math inline">\(\xi_{train}\)</span> والاختبار البيني على <span class="math inline">\(\xi_{test}\)</span>. تعني العبارة الشائعة <span class="math inline">\(K\)</span>-<span class="nodecor">shot</span>-<span class="math inline">\(N\)</span>-<span class="nodecor">way</span> التعلم أن هناك فئات <span class="math inline">\(N\)</span> وكل فئة تحتوي على <span class="math inline">\(K\)</span> أمثلة. بالنسبة للتعلم البيني، يجب أن يكون <span class="math inline">\(K\)</span> هو نفسه في مهمة التدريب البيني ومهمة الاختبار البيني. وللحصول على قدرة تعميم أفضل، يجب أن يكون <span class="math inline">\(N\)</span> في مهمة التدريب البيني هو نفسه أو أكبر من ذلك في مهمة الاختبار البيني.</p>
<h3 id="مرحلة-التدريب-الأولى">مرحلة التدريب الأولى</h3>
<p>في مرحلة التدريب الأولى، يتم تدريب النموذج على مجموعة البيانات العامة. يتم تحديد النموذج على <span class="nodecor">N</span> مهام. لكل مهمة، نقوم بأخذ عينة <span class="math inline">\((D_{i}^{sup} , D_{i}^{query})\)</span> من <span class="math inline">\(\xi_{train}\)</span> ونؤدي تحديثاً داخلياً كما هو موضح في المعادلة [eqn7],</p>
<p><span class="math display">\[\label{eqn7}
 \phi_{i} \leftarrow \theta - \alpha \nabla_{\theta} \mathcal{L}(\theta, D_{i}^{sup})\]</span></p>
<p>حيث <span class="math inline">\(\alpha\)</span> هو معدل التعلم للتحديث الداخلي</p>
<p>لتحسين سرعة الحوسبة، نستخدم تحديث التدرج الواحد. ثم نقيم <span class="math inline">\(\phi_{i}\)</span> على <span class="math inline">\(D_{i}^{query}\)</span> وننفذ التحديث الأولي بتجميع الخسائر في جميع المهام كما هو موضح في المعادلة [eqn8],</p>
<p><span class="math display">\[\label{eqn8}
\sum_{T_{i}} \mathcal{L}_{T_{i}}(\phi_{i}, D_{i}^{query})\]</span></p>
<p>ونحدث معاملات النموذج <span class="math inline">\(\nabla_{\theta}\)</span> كما يلي في المعادلة [eqn9],</p>
<p><span class="math display">\[\label{eqn9}
  \theta \leftarrow \theta - \beta \nabla_{\theta} \sum_{T_{i}} \mathcal{L}_{T_{i}}(\phi_{i}, D_{i}^{query})\]</span></p>
<p>حيث <span class="math inline">\(\beta\)</span> هو معدل التعلم للتعلم الأولي.</p>
<h3 id="الاختبار-الفوقي">الاختبار الفوقي</h3>
<p>في الاختبار الفوقي، نستخدم معاملات محدثة لتحسين النموذج على مجموعة بيانات تدريب StackOverflow وإجراء التنبؤ على مجموعة البيانات الاختبارية.</p>
<h1 id="التجربة">التجربة</h1>
<h2 id="مجموعة-البيانات">مجموعة البيانات</h2>
<p>في هذه المهمة، سنستخدم مجموعة بيانات (<span class="nodecor">NER</span>) من (<span class="nodecor">StackOverflow</span>) (<span class="nodecor">codener</span>)، والتي تحتوي على أكثر من <span class="nodecor">1,237</span> موضوعاً من الأسئلة والأجوبة من أرشيف (<span class="nodecor">StackOverflow</span>) لمدة <span class="nodecor">10</span> سنوات مع <span class="nodecor">27</span> نوعاً من الكيانات. نحن معطون كيانات الكود وكيانات اللغة الطبيعية. تشمل كيانات الكود <em>الفئة، المتغير، الكود داخل السطر، الوظيفة، المكتبة، القيمة، نوع البيانات</em>، و<em>علامة (<span class="nodecor">HTML XML</span>)</em>. تشمل كيانات اللغة الطبيعية <em>التطبيق، عنصر واجهة المستخدم، اللغة، هيكل البيانات، الخوارزمية، نوع الملف، اسم الملف، الإصدار، الجهاز، نظام التشغيل، الموقع الإلكتروني</em>، و<em>اسم المستخدم</em>. نخطط أيضاً لاستخدام بيانات إضافية من (<span class="nodecor">Github</span>) التي تم أخذ عينات عشوائية من المستودعات من (<span class="nodecor">GitHub</span>). يتم عرض مثال على مجموعة البيانات وتسميات الكيانات في الجدول [table:dataexp].</p>
<h2 id="إعدادات-التجربة">إعدادات التجربة</h2>
<p><strong>أخذ العينات لعدد قليل من الحالات.</strong> نقوم بإجراء التعلم ب <span class="nodecor">5</span>-حالات على مجموعة بيانات <span class="nodecor">StackOverflow NER</span> من خلال أخذ عينات من <span class="nodecor">5</span> حالات لمجموعة التدريب في كل تشغيل للتجربة. كما أجرينا تجربة حيث قمنا باختيار <span class="nodecor">5</span> حالات يدوياً على مجموعة التدريب بأكملها لجعل تنبؤ النموذج أكثر دقة.</p>
<p><strong>أخذ العينات للتدريب الأساسي.</strong> تحتوي مجموعة بيانات <span class="nodecor">Few-NERD</span> على <span class="nodecor">66</span> نوعاً دقيقاً من الكيانات في المجال العام. نقوم بمرحلة التدريب الأساسي في <span class="nodecor">40</span> مهمة. لكل مهمة، نقوم بأخذ عينات عشوائية من مجموعة بيانات <span class="nodecor">20</span>-حالة-<span class="nodecor">27</span>-طريقة من <span class="nodecor">Few-NERD</span>. ثم نقسمها إلى مجموعة دعم <span class="nodecor">5</span>-حالة-<span class="nodecor">27</span>-طريقة ومجموعة استفسار <span class="nodecor">15</span>-حالة-<span class="nodecor">27</span>-طريقة.</p>
<p><strong>إعدادات المعلمات الفائقة.</strong> نستخدم نموذج <span class="nodecor">RoBERTa-base</span> المدرب مسبقاً كنموذج أساسي. الطول الأقصى للتسلسل هو <span class="nodecor">128</span>؛ حجم الدفعة الداخلية للضبط الدقيق هو <span class="nodecor">8</span>؛ حجم الدفعة الخارجية للتحديث الأساسي هو <span class="nodecor">32</span>؛ عدد العصور للضبط الدقيق في التدريب الأساسي هو <span class="nodecor">1</span>؛ عدد العصور للضبط الدقيق في الاختبار الأساسي هو <span class="nodecor">10</span>؛ الحد الأقصى لخطوة التعلم الأساسي هو <span class="nodecor">15</span>؛ معدل التعلم للتعلم الأساسي هو <span class="nodecor">5e-3</span>؛ معدل التعلم للضبط الدقيق الداخلي هو <span class="nodecor">1e-2</span>. تم تدريب النموذج على <span class="nodecor">GPU</span> من <span class="nodecor">Google Colab</span>.</p>
<p><strong>مقاييس التقييم.</strong> نطبق درجة <span class="nodecor">micro-F1</span> ودرجة <span class="nodecor">macro-F1</span> الفضفاضة.</p>
<h2 id="النتائج">النتائج</h2>
<p>قمنا بتطبيق RoBERTa وRoBERTa+MAML على مجموعة بيانات التعرّف على الكيانات المسماة لـ StackOverflow. بالنسبة لمجموعة التدريب، قمنا باختيار خمس عينات عشوائياً لكل فئة من مجموعة بيانات التعرّف على الكيانات المسماة لـ StackOverflow كبيانات إدخال. كما هو موضح في الجدول [citation-guide]، فإن درجة Micro-F1 للنموذج الأساسي RoBERTa هي <span class="nodecor">0.3091</span> ودرجة Macro-F1 هي <span class="nodecor">0.2837</span>.</p>
<p>كما قمنا بتطبيق نفس الطريقة لنموذجنا RoBERTa+MAML. درجة Micro-F1 للنموذج الأساسي RoBERTa+MAML هي <span class="nodecor">0.3578</span> ودرجة Macro-F1 هي <span class="nodecor">0.3197</span>. لاحظنا زيادة كبيرة في درجة Micro-F1 باستخدام النموذج مع التعلم الفوقي.</p>
<p>كما يمكننا أن نرى في الشكل ([fig:highperform], [fig:lowperform])، فإن <em>هيكل البيانات، عنصر واجهة المستخدم، نظام التشغيل، اسم المستخدم</em>، و<em>أنواع البيانات</em> هي فئات يمكن لـ RoBERTa+MAML التعرّف عليها بشكل أفضل.</p>
<h2 id="دراسة-حالة-لمجموعة-تدريب-من-5-لقطات">دراسة حالة لمجموعة تدريب من 5 لقطات</h2>
<p>نلاحظ أن درجة <span class="nodecor">F1</span> للعديد من الفئات (مثل نظام التشغيل، فئة المكتبة، اسم الوظيفة، <span class="nodecor">IP</span> لوحة المفاتيح، اللغة، اسم المتغير، والخوارزمية) أقل من المعتاد. هذه الفئات يصعب التعرّف عليها في تعلم التعرّف على الكيانات المسماة المحددة بـ 5 لقطات. بعد استكشاف مجموعة التدريب العشوائية الخاصة بنا، وجدنا أن مجموعة التدريب تحتوي على العديد من الكيانات المكررة وبعض الكيانات غير الواضحة.</p>
<p>لتجنب التأثير السلبي لمجموعة التدريب من 5 لقطات، نختار يدوياً بيانات التدريب ذات المعنى والتمثيلية. نجعل كل بيانات التدريب فريدة وتمثيلية.</p>
<p>نطبق أيضاً نفس الطريقة لنموذجنا <span class="nodecor">RoBERTa+MAML</span>. كما هو موضح في الجدول، تحسنت درجة <span class="nodecor">Micro-F1</span> بحوالي 3% وتحسنت درجة <span class="nodecor">Macro-F1</span> بحوالي 2%.</p>
<p>هذا مثال يوضح أن "المحتوى" غير واضح، والذي لا يحتوي على العمومية في بيانات التدريب من 5 لقطات. وبالتالي، نختار يدوياً بيانات التدريب البديلة من أجل تعظيم أداء النموذج. وفقاً للنتيجة، يتفوق نموذج <span class="nodecor">RoBERTa+MAML</span> مع بيانات التدريب المختارة يدوياً على بيانات التدريب العشوائية. وبالتالي، نعتمد بيانات التدريب المختارة يدوياً للأعمال المستقبلية.</p>
<h2 id="دراسة-حالة-لاستخراج-الأنماط-المعتمدة-على-المعرفة">دراسة حالة لاستخراج الأنماط المعتمدة على المعرفة</h2>
<p>نلاحظ أن بعض الفئات يصعب التعرّف عليها ولكنها تحتوي على أنماط. على سبيل المثال، معظم أنواع الملفات موجودة ضمن مجموعات أنواع الملفات الشائعة. كما هو موضح في الجدول [table:maual]، يمكننا أداء استخراج الأنماط المعتمدة على المعرفة لاستخراج فئات معينة. على سبيل المثال، نستخدم التعبيرات النظامية لاستخراج امتدادات أسماء الملفات الشائعة، مثل csv، jpg، و doc. كما هو موضح في الجدول [table:extraction]، تم تحسين درجة <span class="nodecor">F1</span> لفئة نوع الملف من <span class="nodecor">0.345</span> إلى <span class="nodecor">0.49</span>. درجات الدقة والاسترجاع تصل إلى <span class="nodecor">0.716</span> و <span class="nodecor">0.372</span>. يمكننا أيضاً تطبيق استخراج الأنماط المعتمدة على المعرفة على العديد من الفئات الأخرى، مما سيحسن بشكل كبير دقة التنبؤ الشاملة ودرجة <span class="nodecor">F1</span>.</p>
<h1 id="الخلاصة-والأعمال-المستقبلية">الخلاصة والأعمال المستقبلية</h1>
<p>في دراستنا، ركزنا على التعرّف على الكيانات المسماة المحددة بمجال في مجال برمجة الحاسوب. من خلال استخدام RoBERTa + MAML على مجموعة بيانات التعرّف على الكيانات المسماة StackOverflow لـ <span class="nodecor">5</span>-shot، لاحظنا تحسينات كبيرة مقارنة بنموذج RoBERTa الأساسي. يأتي التعلم البيني كأداة قوية لمهام التعرّف على الكيانات المسماة المحددة بمجال وقليلة الأمثلة. بالإضافة إلى ذلك، ساهمت معالجة العبارات المحددة بالمجال واستخراج الأنماط المعتمدة على المعرفة في تحسين الدقة. نتوقع أن يستفيد التعلم البيني، ومعالجة العبارات المحددة بالمجال، والأنماط المعتمدة على المعرفة من المهام المستقبلية لاستخراج المعلومات المتعلقة بالبرمجيات والإجابة على الأسئلة. لتحسين منهجيتنا، نهدف إلى توسيع تنوع مجموعة بياناتنا واستكشاف مجموعات دعم العينات ومجموعات الاستعلام الإضافية، مما يعزز تأثير التعلم البيني.</p>
</body>
</html>
