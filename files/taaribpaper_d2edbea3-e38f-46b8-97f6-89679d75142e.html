```html
<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Chenshuang Zhang     Fei Pan    Junmo Kim     In So Kweon       Chengzhi Mao KAIST^{1}, University of Michigan, Ann Arbor^{2}, McGill University^{3}, MILA^{4}">
  <title>مِعْيار ImageNet-D: قِياسُ مَتانَةِ الشَبَكاتِ العَصَبِيَّةِ عَلَى الأَجْسامِ الاِصْطِناعِيَّةِ بِالاِنْتِشارِ</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    html, body {
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      background: #f8f9fa;
      color: #222;
      font-size: 22px;
      line-height: 1.8;
      margin: 0;
      padding: 0;
      direction: rtl;
    }
    body {
      max-width: 900px;
      margin: 40px auto 40px auto;
      padding: 32px 24px 32px 24px;
      background: #fff;
      border-radius: 18px;
      box-shadow: 0 4px 32px 0 rgba(0,0,0,0.08);
    }
    header {
      text-align: center;
      margin-bottom: 40px;
    }
    h1.title {
      font-size: 2.3em;
      font-weight: 700;
      color: #1a237e;
      margin-bottom: 0.3em;
      letter-spacing: 0.01em;
    }
    .author {
      font-size: 1.1em;
      color: #444;
      margin-bottom: 0.5em;
    }
    h1, h2, h3, h4 {
      color: #283593;
      font-weight: 700;
      margin-top: 1.7em;
      margin-bottom: 0.7em;
      line-height: 1.3;
    }
    h1 {
      font-size: 1.7em;
      border-bottom: 2px solid #e3e6f0;
      padding-bottom: 0.2em;
      margin-bottom: 1em;
    }
    h2 {
      font-size: 1.3em;
      border-bottom: 1px solid #e3e6f0;
      padding-bottom: 0.1em;
      margin-bottom: 0.7em;
    }
    h3 {
      font-size: 1.1em;
      margin-bottom: 0.5em;
    }
    p {
      margin: 0 0 1.1em 0;
      text-align: justify;
    }
    ul, ol {
      margin: 0 0 1.1em 2em;
      padding-right: 1.2em;
    }
    code, pre {
      font-family: 'Cairo', 'Fira Mono', 'Consolas', 'monospace';
      background: #f3f3f3;
      color: #c7254e;
      border-radius: 5px;
      padding: 2px 6px;
      font-size: 0.95em;
    }
    pre {
      display: block;
      padding: 1em;
      overflow-x: auto;
      background: #f3f3f3;
      color: #222;
      border-radius: 7px;
      margin-bottom: 1.2em;
    }
    strong {
      color: #0d47a1;
    }
    em {
      color: #6d4c41;
    }
    .math.inline {
      font-family: 'Cairo', 'Fira Mono', 'Consolas', 'monospace';
      background: #f3f3f3;
      color: #1b5e20;
      border-radius: 5px;
      padding: 1px 4px;
      font-size: 0.95em;
    }
    .math.display {
      display: block;
      margin: 1.2em auto;
      text-align: center;
      background: #f3f3f3;
      color: #1b5e20;
      border-radius: 7px;
      padding: 1em 0.5em;
      font-size: 1.1em;
    }
    .nodecor {
      text-decoration: none !important;
      color: inherit;
    }
    blockquote {
      border-right: 4px solid #b3c6ff;
      background: #f5f7fa;
      margin: 1.2em 0;
      padding: 1em 1.5em;
      color: #444;
      border-radius: 8px;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 1.5em 0;
      background: #fafbfc;
      border-radius: 8px;
      overflow: hidden;
      font-size: 0.98em;
    }
    th, td {
      border: 1px solid #e3e6f0;
      padding: 0.7em 1em;
      text-align: center;
    }
    th {
      background: #e3e6f0;
      color: #1a237e;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f6f8fc;
    }
    hr {
      border: none;
      border-top: 1px solid #e3e6f0;
      margin: 2em 0;
    }
    @media (max-width: 700px) {
      body {
        padding: 10px 2vw 10px 2vw;
        font-size: 18px;
      }
      h1.title {
        font-size: 1.3em;
      }
      h1, h2 {
        font-size: 1.1em;
      }
      table, th, td {
        font-size: 0.95em;
      }
    }
    /* Custom for Arabic numerals and punctuation */
    body, h1, h2, h3, h4, p, li, td, th {
      unicode-bidi: embed;
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">مِعْيار <span class="nodecor">ImageNet-D</span>: قِياسُ مَتانَةِ الشَبَكاتِ العَصَبِيَّةِ عَلَى الأَجْسامِ الاِصْطِناعِيَّةِ بِالاِنْتِشارِ</h1>
  <p class="author">
    <span class="nodecor">Chenshuang Zhang</span> &nbsp; <span class="nodecor">Fei Pan</span> &nbsp; <span class="nodecor">Junmo Kim</span> &nbsp; <span class="nodecor">In So Kweon</span> &nbsp; <span class="nodecor">Chengzhi Mao</span><br />
    <span class="nodecor">KAIST</span><span class="math inline">\(^{1}\)</span>، <span class="nodecor">University of Michigan, Ann Arbor</span><span class="math inline">\(^{2}\)</span>، <span class="nodecor">McGill University</span><span class="math inline">\(^{3}\)</span>، <span class="nodecor">MILA</span><span class="math inline">\(^{4}\)</span>
  </p>
</header>

<p>لايتيكس</p>

<h1 id="ملخص">مُلَخَّص</h1>
<p>نُقَدِّم مَعايير صارِمة لِمَتانة الإِدراك البَصَري. تُوَفِّر الصُوَر الاِصْطِناعِيَّة مثل <span class="nodecor">ImageNet-C</span>، <span class="nodecor">ImageNet-9</span>، و<span class="nodecor">Stylized ImageNet</span> نوعاً مُحَدَّداً من التقييم على التلوثات الاصطناعية، والخلفيات، والقِوام، ولكن تلك المعايير لِلمتانة محدودة في التباينات المُحددة ولها جودة اصطناعية منخفضة. في هذا العمل، نُقَدِّم نموذجاً توليدياً كمصدر بيانات لتوليد صور صعبة تقيس متانة النماذج العميقة. من خلال استخدام نماذج الانتشار، نستطيع توليد صور بخلفيات وقِوام ومواد أكثر تنوعاً من أي عمل سابق، حيث نُطلق على هذا المعيار اسم <span class="nodecor">ImageNet-D</span>. تُظهِر النتائج التجريبية أن <span class="nodecor">ImageNet-D</span> يؤدي إلى انخفاض كبير في الدقة لمجموعة من نماذج الرؤية، من مصنف الرؤية <span class="nodecor">ResNet</span> القياسي إلى أحدث النماذج الأساسية مثل <span class="nodecor">CLIP</span> و<span class="nodecor">MiniGPT-4</span>، مما يقلل دقتها بنسبة تصل إلى <span class="nodecor">60\%</span>. يُشير عملنا إلى أن نماذج الانتشار يمكن أن تكون مصدراً فعالاً لاختبار نماذج الرؤية. الشفرة ومجموعة البيانات متاحة على GitHub لمزيد من التوثيق والتنزيل.</p>

<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>لقد حققت الشبكات العصبية أداءً ملحوظاً في مهام تتراوح من تصنيف الصور (<span class="nodecor">vaswani2017attention</span>, <span class="nodecor">liu2021swin</span>, <span class="nodecor">liu2022convnet</span>) إلى الإجابة على الأسئلة البصرية (<span class="nodecor">li2023blip</span>, <span class="nodecor">dai2023instructblip</span>, <span class="nodecor">liu2023visual</span>, <span class="nodecor">zhu2023minigpt</span>). وقد ألهمت هذه التقدمات تطبيق الشبكات العصبية في مجالات متنوعة، بما في ذلك الأنظمة الأمنية والحرجة مثل السيارات ذاتية القيادة (<span class="nodecor">kangsepp2022calibrated</span>, <span class="nodecor">nesti2023ultra</span>, <span class="nodecor">liu2023vectormapnet</span>)، وكشف البرمجيات الخبيثة (<span class="nodecor">yuan2014droid</span>, <span class="nodecor">chen2019believe</span>, <span class="nodecor">pei2017deepxplore</span>) والروبوتات (<span class="nodecor">brohan2022rt</span>, <span class="nodecor">brohan2023rt</span>, <span class="nodecor">huang2023voxposer</span>). ونظراً لتوسع استخدامها، أصبح من المهم بشكل متزايد تحديد متانة الشبكات العصبية (<span class="nodecor">ming2022delving</span>, <span class="nodecor">li2023distilling</span>) لأسباب تتعلق بالسلامة.</p>
<p>لتقييم متانة الشبكات العصبية، يجمع ObjectNet (<span class="nodecor">barbu2019objectnet</span>) صور الأشياء الواقعية على عوامل يمكن التحكم بها مثل الخلفية بواسطة العمال البشريين، وهو ما يستغرق وقتاً طويلاً ويتطلب جهداً كبيراً. لزيادة جمع البيانات، تم اقتراح الصور الاصطناعية كصور اختبار (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">hendrycks2019benchmarking</span>, <span class="nodecor">xiao2020noise</span>). على سبيل المثال، يقدم ImageNet-C (<span class="nodecor">hendrycks2019benchmarking</span>) مجموعة من التشوهات البصرية الشائعة منخفضة المستوى، مثل الضوضاء الغاوسية والطمس، لاختبار متانة النماذج. يستخدم ImageNet-9 (<span class="nodecor">xiao2020noise</span>) تقنية القص واللصق البسيطة لإنشاء معيار للمتانة على خلفية الكائن، لكن الصور تبدو غير واقعية. يُولِّد Stylized-ImageNet (<span class="nodecor">geirhos2018imagenet</span>) صوراً جديدة من خلال تغيير نسيج صور ImageNet، ولكنها لا تتحكم في العوامل العالمية مثل الخلفية.</p>
<p>في هذا العمل، نقدم ImageNet-D، مجموعة اختبار اصطناعية تم إنشاؤها بواسطة نماذج الانتشار لمهمة التعرف على الأشياء. من خلال الاستفادة من قدرات نماذج الانتشار الرائدة (<span class="nodecor">rombach2022high</span>)، نظهر أنه يمكننا توجيه هذه النماذج باللغة لإنشاء صور اختبار واقعية تتسبب في فشل نماذج الرؤية. وبفضل الاعتماد على اللغة، يمكننا تنويع العوامل عالية المستوى في الصور على خلاف التشوهات المحلية والنسيج في الأعمال السابقة، مما يوفر أبعاداً إضافية لتقييم المتانة.</p>
<p>لتعزيز صعوبة العينات في مجموعة البيانات الخاصة بنا، نحتفظ بشكل انتقائي بالصور التي تسبب فشل نماذج الرؤية المختارة. تظهر نتائجنا أن الصور التي تثير الأخطاء في النماذج المختبرة تنقل صعوبتها بشكل موثوق إلى نماذج أخرى لم تخضع للاختبار مسبقاً. وهذا يؤدي إلى انخفاض ملحوظ في الدقة، حتى في النماذج الأساسية الحديثة مثل MiniGPT-4 (<span class="nodecor">zhu2023minigpt</span>) وLLaVa (<span class="nodecor">liu2023visual</span>)، مما يشير إلى أن مجموعة البيانات تكشف عن الفشل الشائع في نماذج الرؤية.</p>
<p>تُظهر التصورات أن ImageNet-D يعزز بشكل كبير جودة الصورة مقارنة بمعايير المتانة الاصطناعية السابقة. يعمل ImageNet-D كأداة فعالة لخفض الأداء وتقييم متانة النموذج، بما في ذلك ResNet <span class="nodecor">101</span> (انخفاض <span class="nodecor">55.02\%</span>)، ViT-L/<span class="nodecor">16</span> (انخفاض <span class="nodecor">59.40\%</span>)، CLIP (انخفاض <span class="nodecor">46.05\%</span>)، وينتقل جيداً إلى نماذج لغة الرؤية الكبيرة مثل LLaVa (<span class="nodecor">liu2023visual</span>) (انخفاض <span class="nodecor">29.67\%</span>) وMiniGPT-4 (<span class="nodecor">zhu2023minigpt</span>) (انخفاض <span class="nodecor">16.81\%</span>). يُعتَبَر نهجنا في استخدام النماذج التوليدية لتقييم المتانة منهجاً عاماً، ويُظهر إمكانية كبيرة لتحسينات مستقبلية مع تطور النماذج التوليدية.</p>

<h1 id="sec:related_work">الأَعْمال ذات الصِلَة</h1>
<p><strong>مَتانة الشبكات العصبية.</strong> تطورت الشبكات العصبية من شبكات الالتفاف العصبي (<span class="nodecor">CNN</span>) (<span class="nodecor">he2016deep, huang2017densely</span>)، وشبكات التحويل البصري (<span class="nodecor">ViT</span>) (<span class="nodecor">vaswani2017attention, liu2021swin</span>)، إلى النماذج الأساسية الكبيرة (<span class="nodecor">bommasani2021opportunities, devlin2018bert, touvron2023llama</span>). وقد تناولت الأعمال السابقة متانة الشبكات العصبية من عدة جوانب، مثل الأمثلة المعادية (<span class="nodecor">mao2022understanding, mahmood2021robustness, madry2017towards, zhao2023evaluating, zhang2019theoretically</span>) وعينات خارج النطاق (<span class="nodecor">MAE, mao2021discrete, hendrycks2021many, augmix</span>). كما أظهرت النماذج الأساسية متانة أكبر على العينات خارج التوزيع (<span class="nodecor">radford2021learning</span>). بالإضافة إلى ذلك، تم التحقيق في التفسير القوي أيضاً (<span class="nodecor">mao2023doubly, liu2023visual, zhu2023minigpt</span>). لتقييم متانة النماذج العميقة بشكل منهجي، من الضروري وجود مجموعات اختبار تغطي عوامل مختلفة.</p>
<p><strong>مجموعات بيانات لتقييم المتانة.</strong> تستخدم الدراسات صوراً من الإنترنت، بما في ذلك <span class="nodecor">ImageNet-A</span> (<span class="nodecor">hendrycks2021natural</span>), <span class="nodecor">Imagenet-R</span> (<span class="nodecor">hendrycks2021many</span>) و<span class="nodecor">ImageNet-Sketch</span> (<span class="nodecor">wang2019learning</span>). ومع ذلك، فهي محدودة بما هو متاح على الويب. يجمع <span class="nodecor">ObjectNet</span> (<span class="nodecor">barbu2019objectnet</span>) الصور يدوياً بمساعدة آلاف العاملين، مما يستغرق وقتاً وجهداً كبيرين.</p>
<p>للتغلب على قيود الصور من الويب وتقليل تكلفة الجمع اليدوي، تم اقتراح الصور الاصطناعية لتقييم المتانة (<span class="nodecor">geirhos2018imagenet, hendrycks2019benchmarking, xiao2020noise</span>). <span class="nodecor">ImageNet-C</span> (<span class="nodecor">hendrycks2019benchmarking</span>) يقيم متانة النموذج على التلفيات منخفضة المستوى. <span class="nodecor">ImageNet-9</span> (<span class="nodecor">xiao2020noise</span>) يُولِّد صوراً جديدة بدمج الخلفية والمقدمة من صور مختلفة، ولكنه محدود بجودة منخفضة نسبياً. <span class="nodecor">Stylized-ImageNet</span> (<span class="nodecor">geirhos2018imagenet</span>) يغير نسيج صور <span class="nodecor">ImageNet</span> باستخدام نقل أسلوب <span class="nodecor">AdaIN</span> (<span class="nodecor">huang2017arbitrary</span>) أو بإدخال تعارض بين النسيج والشكل، ولكنه لا يتحكم في عوامل أخرى مثل الخلفيات. في هذا العمل، نقدم مجموعة اختبار جديدة <span class="nodecor">ImageNet-D</span>، التي تُولَّد بالتحكم في نماذج الانتشار وتشمل صوراً جديدة مع خلفيات وأنسجة ومواد متنوعة.</p>
<p><strong>توليد الصور.</strong> حققت نماذج الانتشار نجاحاً كبيراً في مهام متنوعة بما في ذلك توليد الصور (<span class="nodecor">saharia2022photorealistic, ramesh2022hierarchical, ruiz2023dreambooth, zhang2023text</span>). على وجه الخصوص، يمكن <span class="nodecor">Stable Diffusion</span> (<span class="nodecor">rombach2022high</span>) من توليد صور عالية الدقة يتم التحكم فيها بواسطة اللغة. <span class="nodecor">InstructPix2Pix</span> (<span class="nodecor">brooks2023instructpix2pix</span>) يوفر تحكماً أكثر تعقيداً من خلال تعديل صورة معينة وفقاً لتعليمات بشرية. في هذه الورقة، نبني خط أنابيبنا باستخدام نموذج <span class="nodecor">Stable Diffusion</span> القياسي، رغم أن خوارزميتنا متوافقة مع نماذج توليدية أخرى قابلة للتوجيه باللغة.</p>
<p><strong>تعزيز الإدراك باستخدام صور الانتشار.</strong> استُخدمت الصور المولدة بالانتشار لتعزيز مهام إدراك الرؤية. يحسن فرع من الدراسات (<span class="nodecor">yuan2023not, bansal2023leaving, azizi2023synthetic, tian2023stablerep</span>) دقة التصنيف باستخدام الصور الاصطناعية كتوسيع لبيانات التدريب. أما <span class="nodecor">DREAM-OOD</span> (<span class="nodecor">du2023dream</span>)، فيكشف القيم الشاذة عبر فك تشفير العينات الكامنة إلى صور. إلا أن طريقتهم تفتقر إلى التحكم المحدد في فضاء الصور، وهو أمر حاسم لمعايير مثل <span class="nodecor">ImageNet-D</span>. كما يحدد (<span class="nodecor">metzen2023identification</span>) أزواج السمات غير الممثلة بشكل كافٍ، بينما يركز بحثنا على استخراج الصور الصعبة لكل سمة على حدة. على عكس (<span class="nodecor">li2023imagenet, vendrow2023dataset, prabhu2023lance</span>) الذين يعدلون مجموعات البيانات الحالية، يولد عملنا صوراً جديدة ويختار الأصعب منها كمجموعة اختبار، مما يحقق انخفاضاً أكبر في الدقة.</p>

<h1 id="sec:imagenet_d">ImageNet-D</h1>
<p>نقدم أولاً كيفية إنشاء ImageNet-D في القسم [sec:dataset_design]، ثم نظرة عامة على إحصائياته في القسم [sec:statistics].</p>
<h2 id="sec:dataset_design">تَصْمِيم مجموعة البيانات</h2>
<p>بينما تتفوق الشبكات العصبية في تطبيقات متعددة، فإن متانتها تحتاج إلى تقييم دقيق للسلامة. التقييمات التقليدية تستخدم مجموعات اختبار موجودة، تشمل إما صوراً طبيعية (<span class="nodecor">barbu2019objectnet</span>, <span class="nodecor">hendrycks2021natural</span>) أو صوراً اصطناعية (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">hendrycks2019benchmarking</span>, <span class="nodecor">xiao2020noise</span>). مقارنة بجمع الصور يدوياً، فإن جمع مجموعة اختبار اصطناعية أكثر كفاءة (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">xiao2020noise</span>). ومع ذلك، فإن تنوع مجموعات الاختبار الاصطناعية الحالية محدود بسبب اعتمادها على الصور الموجودة لاستخراج السمات. هذه الصور الاصطناعية ليست واقعية أيضاً، كما هو موضح في الشكل [fig:test_set_comparison]. يتم تقديم ImageNet-D لتقييم متانة النموذج عبر مجموعات متنوعة من الأشياء والعوامل الطارئة، لمعالجة هذه القيود.</p>
<p><strong>توليد الصور بواسطة نماذج الانتشار.</strong> لبناء ImageNet-D، نستخدم نماذج الانتشار لإنشاء مجموعة ضخمة من الصور عن طريق دمج جميع فئات الأشياء المحتملة والعوامل الطارئة، مما يتيح توليد صور عالية الدقة بناءً على مدخلات نصية محددة. نعتمد نموذج <span class="nodecor">Stable Diffusion</span> (<span class="nodecor">rombach2022high</span>) للتوليد، رغم أن نهجنا يتوافق مع نماذج توليدية أخرى قابلة للتوجيه باللغة. تُصاغ عملية توليد الصور على النحو التالي: <span class="math display">\[
\text{Image}(C, N)  = \text{Stable Diffusion}(\text{Prompt}(C,N)),
\]</span> حيث يشير <span class="math inline">\(C\)</span> و <span class="math inline">\(N\)</span> إلى فئة الشيء والعامل الطارئ، على التوالي. يشمل العامل الطارئ <span class="math inline">\(N\)</span> الخلفية والمادة والملمس. يقدم الجدول [tab:prompt_list] نظرة عامة على العوامل والط¶ليات. باستخدام فئة الحقائب كمثال، نولد صوراً لحقيبة في حقل قمح، حجرات خشبية، وما إلى ذلك، مما يوفر تنوعاً أوسع من المجموعات الحالية. تُصنَّف الصورة بحسب فئة الPrompt <span class="math inline">\(C\)</span> كحقيقة أساسية. تُعتَبَر الصورة مصنفة بشكل خاطئ إذا لم يتطابق تصنيف النموذج مع الحقيقة الأساسية <span class="math inline">\(C\)</span>.</p>
<p>بعد إنشاء مجموعة كبيرة من الصور لكل أزواج الفئات والعوامل، نقيم نموذج CLIP (ViT-L/14) على هذه الصور في الجدول [tab:vanilla_generation]. التفاصيل التجريبية في القسم [sec:experimental_setup]. يُظهر الجدول [tab:vanilla_generation] أن CLIP يحقق دقة عالية (حوالي <span class="nodecor">94\%</span>) على الصور الاصطناعية. لإنشاء مجموعة اختبار تحدي، نقترح استراتيجية فعالة لاستخراج العينات الصعبة بالاعتماد على فشل مشترك.</p>
<p><strong>استخراج الصور الصعبة مع فشل الإدراك المشترك.</strong> قبل شرح كيفية تحديد العينات الصعبة، نُعرِّف مفهوم فشل الإدراك المشترك:</p>
<p><code>الفشل المشترك:</code> صورة تُعتَبَر فشلاً مشتركاً إذا أدت إلى تنبؤ عدة نماذج بتصنيف الشيء بشكل غير صحيح.</p>
<p>المجموعة الصعبة المثالية تشمل صوراً يفشل فيها جميع النماذج المختبرة، لكن هذا غير عملي نظراً لعدم إمكانية الوصول إلى النماذج المستقبلية (النماذج الهدف). بدلاً من ذلك، نبني مجموعة الاختبار انطلاقاً من فشل النماذج البديلة المعروفة. إذا أدى فشل هذه النماذج إلى انخفاض دقة في النماذج الهدف غير المعروفة، نعدّ الفشل قابلاً للنقل:</p>
<p><code>الفشل القابل للنقل:</code> فشل النماذج البديلة المعروفة قابل للنقل إذا أدى أيضاً إلى دقة منخفضة في النماذج الهدف غير المعروفة.</p>
<p>لتقييم قابلية نقل الفشل من الصور المولدة، نقيم مجموعات اختبار تم إنشاؤها بفشل مشترك من <span class="nodecor">1</span> إلى <span class="nodecor">8</span> نماذج بديلة (الشكل [fig:filter_consistency]). كما نقيم ثلاثة نماذج هدف لم تُستخدم في بناء مجموعة الاختبار، هي CLIP (ViT-B/16)، LLaVa، وMiniGPT-4. يُظهر الشكل [fig:filter_consistency] أن دقة النموذج الهدف تنخفض كلما زاد عدد النماذج البديلة. وُلدت مجموعات الاختبار للعوامل الثلاثة (الخلفية والملمس والمادة) وتظهر نفس الاتجاه.</p>

<h1 id="التحكم-بالجودة-بواسطة-التدخل-البشري">التَحَكُّم بالجودة بواسطة التدخل البشري</h1>
<p>توفر العملية السابقة اكتشافاً تلقائياً لمجموعة اختبار صعبة، إلا أن النماذج التوليدية قد تُنتِج صوراً لا تتطابق مع فئة الطلب. لذلك، نلجأ إلى التعليق التوضيحي البشري لضمان أن تكون صور ImageNet-D صالحة، من فئة واحدة، وعالية الجودة. بعد الجولة الأولى من التعليق بواسطة طلاب الدراسات العليا المتخصصين، نستخدم Amazon Mechanical Turk (<span class="nodecor">deng2009imagenet</span>, <span class="nodecor">recht2019imagenet</span>, <span class="nodecor">hendrycks2021many</span>) للتدقيق في جودة التسمية. نطلب من العمال اختيار الصور التي يمكنهم التعرف فيها على الكائن الرئيسي أو التي تُظهر الكائن في وظيفته الحقيقية كفئة الحقيقة الأرضية. كما نصمم حراساً لضمان استجابات دقيقة، منهم الحراس الإيجابيون والسلبيون والمتسقون. التفاصيل في الملحق. شارك <span class="nodecor">679</span> عاملاً في <span class="nodecor">1540</span> مهمة، محققين اتفاقاً بنسبة <span class="nodecor">91.09\%</span>.</p>

<h2 id="sec:statistics">إحصائيات قاعدة البيانات</h2>
<p>تتضمن ImageNet-D <span class="nodecor">113</span> فئة متداخلة بين ImageNet وObjectNet، و<span class="nodecor">547</span> مرشحاً للعوامل المؤثرة من Broden (<span class="nodecor">bau2017network</span>) (انظر الجدول [tab:prompt_list])، مما ينتج عنه <span class="nodecor">4835</span> صورة صعبة: خلفيات متنوعة (<span class="nodecor">3764</span>)، أنسجة (<span class="nodecor">498</span>)، ومواد (<span class="nodecor">573</span>). تعتبر عملية إنشاء ImageNet-D عامة وفعالة، مما يسمح بإضافة فئات وعوامل جديدة بسهولة. يُظهر توزيع الفئات نمطاً طبيعياً طويل الذيل (الشكل [fig:hist_category])، والتوزيع النادر وغير المنتظم للعوامل في الشكل [fig:heatmap] يسلط الضوء على أهمية استنفاد أزواج الفئات والعوامل في إنشاء مجموعة الاختبار.</p>

<h1 id="sec:benchmark">التجارب</h1>
<p>نقيم نماذج مختلفة على معيار ImageNet-D، فتبيّن النتائج انخفاض دقة يصل إلى <span class="nodecor">60\%</span> لجميعها. ثم نستعرض ما إذا كانت تقنيات سابقة، مثل توسيع البيانات، تُحسن المتانة. أخيراً، نناقش ImageNet-D من زوايا مختلفة، مثل استرجاع الجار الأقرب.</p>
<h2 id="sec:experimental_setup">إعدادات التجربة</h2>
<p><strong>إعدادات بناء مجموعة الاختبار.</strong> نستخدم نموذج <span class="nodecor">Stable Diffusion</span> 2.1 (<span class="nodecor">stable-diffusion-2-1</span>) من <span class="nodecor">Hugging Face</span> لإنشاء <span class="nodecor">ImageNet-D</span>. لاستخراج الصور الصعبة، نحتفظ بالصور التي تفشل فيها أربعة نماذج بديلة: <span class="nodecor">CLIP</span> (<span class="nodecor">ViT-L/14</span>, <span class="nodecor">ViT-L/14-336px</span>, <span class="nodecor">ResNet50</span>) ونموذج الرؤية <span class="nodecor">ResNet50</span> (<span class="nodecor">he2016deep</span>). تشمل قائمة النماذج البديلة أيضاً <span class="nodecor">CLIP</span> (<span class="nodecor">ResNet101</span>, <span class="nodecor">ViT-B/32</span>) ونماذج رؤية أخرى (<span class="nodecor">ViT-L/16</span>, <span class="nodecor">VGG16</span>).</p>
<p><strong>تقييم نماذج التصنيف.</strong> تُقاس المتانة على <span class="nodecor">ImageNet-D</span> بدقة أعلى-1 في التعرف على الأجسام. نستخدم الأوزان المدربة مسبقاً المفتوحة المصدر للنماذج. بالنسبة لـ <span class="nodecor">CLIP</span> (<span class="nodecor">radford2021learning</span>)، نتبع الإرشادات الأصلية لاعتماد قالب نصي <em>“صورة لـ <span class="math inline">\(\left[\text{category}\right]\)</span>”</em>. نبلغ عن دقة التصفير لـ <span class="nodecor">CLIP</span>.</p>
<p><strong>تقييم نماذج الإجابة على الأسئلة البصرية (<span class="nodecor">VQA</span>).</strong> نقيم دقة نماذج <span class="nodecor">VQA</span> الحديثة المفتوحة المصدر على <span class="nodecor">ImageNet-D</span>، مثل <span class="nodecor">LLaVa</span> و<span class="nodecor">MiniGPT-4</span>. تستند الإجابة إلى مطالبة النص المدخل، لكن الإخراج النصي قد لا يتضمن اسم الفئة بالضبط، مما يصعب تقييم الدقة مباشرة.</p>
<p>لجعل نماذج <span class="nodecor">VQA</span> تختار من قائمة محددة، نستخدم المطالبة: <code>ما هو الكائن الرئيسي في هذه الصورة؟ اختر من القائمة التالية: [GT category], [failure category]</code>. تمثل فئة <span class="nodecor">GT</span> الحقيقة الأرضية، وأفضل فئة خاطئة من حيث ثقة <span class="nodecor">CLIP</span> هي فئة الفشل. إذا اختار النموذج فئة الحقيقة الأرضية، نعدّ التصنيف صحيحاً، وبذلك نحسب دقة <span class="nodecor">VQA</span>.</p>

<h2 id="sec:experimental_results">تقييم المتانة</h2>
<p><strong>النتائج الكمية.</strong> نقيم ImageNet-D على <span class="nodecor">25</span> نموذجاً، ونرسم دقة الاختبار في الشكل [fig:main_result_figure]، حيث المحور الأفقي دقة ImageNet والعمودي دقة ImageNet-D. يوضح الشكل أن دقة ImageNet-D أقل بوضوح على جميع النماذج (تحت خط <span class="math inline">\(y=x\)</span>). نبلغ عن دقة <span class="nodecor">14</span> نموذجاً على مجموعات اختبار مختلفة في الجدول [tab:benchmark_results]، مع باقي النماذج في الملحق. يُظهر الجدول أن ImageNet-D يحقق أقل دقة اختبار، باستثناء تشابه النتائج على Stylized-ImageNet لنماذج VQA. رغم أن ObjectNet يغير العديد من السمات لكل صورة، فإن دقته تبقى أعلى من ImageNet-D الذي يغيّر سمة واحدة. مقارنة بـ ImageNet، يؤدي ImageNet-D إلى انخفاض دقة يزيد عن <span class="nodecor">16\%</span> لجميع النماذج، بما في ذلك LLaVa (انخفاض <span class="nodecor">29.67\%</span>) وMiniGPT-4 (انخفاض <span class="nodecor">16.81\%</span>).</p>

<h1 id="نتائج-التصور">نتائج التصور</h1>
<p>يعرض ImageNet-D أمثلة لصور عالية الجودة يفهمها البشر بسهولة، بينما يصنفها CLIP (ViT-L/14) بشكل خاطئ. كما يمكن لـ MiniGPT-4 وLLaVa-1.5 أن يخطئا في التعرف على الكائن الرئيسي في هذه الصور.</p>

<h2 id="sec:robustness_improvement">تحسين المتانة</h2>
<p><strong>توسيع البيانات.</strong> وضّحت الأعمال السابقة فعالية توسيع البيانات لتحسين المتانة، كما في ImageNet-C. نختبر طرق SIN، AugMix، ANT، وDeepAugment على ImageNet-D. يوضح الجدول [tab:result_augmentation] دقة ImageNet وImageNet-D ومتوسط خطأ التلف (mCE) لـ ImageNet-C باستخدام ResNet50 كعمود فقري. بالرغم من أن هذه الطرق تحسن متانة ImageNet-C، فإنها لا تحسن – وقد تضعف – متانة ImageNet-D، مما يثبت حاجة هذا المعيار الجديد.</p>
<p><strong>هندسة النموذج.</strong> نُقارن تنويعات النموذج في الشكل [fig:result_arch]. عند الانتقال من ViT إلى Swin Transformer وConvNeXt، تتحسن دقة ImageNet وImageNet-D، لكن المتانة تبقى صعبة خاصة على مجموعات النسيج والمواد. تظهر النتائج صعوبة تحسين متانة ImageNet-D بهندسة النموذج فقط.</p>
<p><strong>التدريب المسبق بمزيد من البيانات.</strong> للتدريب المسبق على مجموعة بيانات أكبر أثر إيجابي على الدقة. يقارن الشكل [fig:result_arch] ConvNeXt المدرب مباشرة على ImageNet-1K مع نظيره المدرب أولاً على ImageNet-22K. يحقق الأخير متانة أعلى على جميع مجموعات ImageNet-D، لا سيما الخلفيات، مما يدل على فوائد التدريب المسبق الشامل.</p>

<h2 id="sec:analysis">مناقشات إضافية</h2>
<p><strong>هل يجد CLIP الجيران الصحيحين لصور ImageNet-D؟</strong> يستخدم CLIP إمكانياته في استرجاع الجار الأقرب. باعتبار صور ImageNet-D استعلامات، نسترجع صوراً من ImageNet للتحقق من التشابه. يوضح الشكل أن الصور المسترجعة تنطوي على خلفيات أو كائنات مشابهة لصورة الاستعلام، مما يكشف عن حالات فشل في استرجاع الجيران الأقرب.</p>
<p><strong>هل تطابق ImageNet-D مجموعات الاختبار الطبيعية في قابلية نقل الفشل؟</strong> كما عرّفنا الفشل القابل للنقل في القسم [sec:dataset_design]، نجري التجربة نفسها على ImageNet (الفشل) باستخدام صور الفشل المشتركة. يوضح الجدول [tab:transferability] أن ImageNet-D يحقق دقة مماثلة لـ ImageNet (الفشل)، مما يشير إلى أن الصور الاصطناعية يمكن أن تنقل الفشل بشكل يشابه الطبيعي، بتكلفة أقل وسهولة توسعة.</p>
<p><strong>التدريب على صور مولدة بالانتشار.</strong> نسمي الصور المولدة المصنفة صحياً من النماذج البديلة <span class="nodecor">Synthetic-easy</span>، ونستكشف تأثيرها في التدريب. نحسن ResNet18 المدرب مسبقاً على مجموعات تدريب مختلفة (الجدول [tab:finetune_experiment]). يُظهر الجدول أن التدريب على Synthetic-easy يعزز متانة ImageNet-D بنسبة <span class="nodecor">19.26\%</span>، وأن النموذج C يتفوق على النموذج B في دقة ObjectNet بنسبة <span class="nodecor">1.34\%</span>، مما يدل على تعميم أفضل. تشير النتائج إلى أن الصور المولدة بالانتشار مع أزواج متنوعة من الكائنات والعوامل تعزز المتانة كعينات تدريب.</p>

<h1 id="sec:conclusion">الخُلاصَة</h1>
<p>في هذه الورقة، نقدم مجموعة اختبار ImageNet-D ونطور معياراً صارماً لمتانة الإدراك البصري. من خلال استغلال قدرة توليد الصور لنماذج الانتشار، تتضمن ImageNet-D صوراً بعوامل متنوعة تشمل الخلفيات والملمس والمادة. تُظهر النتائج التجريبية أن ImageNet-D تقلل بشكل كبير من دقة النماذج المختلفة، بما في ذلك CLIP (انخفاض <span class="nodecor">46.05\%</span>)، LLaVa (انخفاض <span class="nodecor">29.67\%</span>)، وMiniGPT-4 (انخفاض <span class="nodecor">16.81\%</span>)، مما يؤكد فعاليته في تقييم المتانة. تُعَد أعمالنا خطوة إلى الأمام في تحسين معايير الاختبار الاصطناعية، وستؤدي الصور الاختبارية المستقبلية إلى مزيد من التنوع والتحدي مع تقدم النماذج التوليدية.</p>
<p><strong>الشكر والتقدير:</strong> قدَّم هذا العمل منحة من معهد تخطيط وتقييم تكنولوجيا المعلومات والاتصالات (IITP) الممولة من حكومة كوريا (MSIT) (رقم <span class="nodecor">2022-0-00951</span>) لتطوير عوامل غير مؤكدة تتعلم من خلال طرح الأسئلة.</p>

<h1 id="مهمة-التسمية-على-آمازون-ميكانيكال-تورك">مهمة التسمية على أمازون ميكانيكال تورك</h1>
<p>لتحقيق معايير موثوقة، اعتمدنا على Amazon Mechanical Turk (<span class="nodecor">deng2009imagenet</span>, <span class="nodecor">recht2019imagenet</span>, <span class="nodecor">hendrycks2021many</span>) لتقييم جودة تسمية صور ImageNet-D.</p>
<h2 id="تصميم-مهمة-التسمية">تصميم مهمة التسمية</h2>
<p><strong>تعليمات التسمية.</strong> نظراً لتنوع الصور التي قد تتضمن أزواجاً نادرة من الأشياء والعوامل، طلبنا من العمال مراعاة مظهر ووظيفة الكائن الرئيسي. نطلب من العمال الإجابة عن السؤالين التاليين:</p>
<p><code>السؤال 1:</code> هل يمكنك التعرف على الكائن المطلوب (<code>فئة الحقيقة الأرضية</code>) في الصورة، رغم الخلفية أو النسيج أو المادة؟</p>
<p><code>السؤال 2:</code> هل يمكن استخدام الكائن في الصورة فعلياً كالكائن المطلوب (<code>فئة الحقيقة الأرضية</code>)؟</p>
<p><strong>خط سير التسمية.</strong> لضمان فهم المعيارين، يبدأ العامل بتسمية صورتين تدريبيتين مع الإجابة الصحيحة على السؤالين. بعدها، يصنف حتى 20 صورة في المهمة الواحدة، ويجيب بـ”نعم“ أو ”لا“ لكل سؤال.</p>
<p><strong>واجهة المستخدم للتسمية.</strong> صُممت واجهة سهلة الاستخدام (الشكل المحذوف)، بحيث لا يمكن الانتقال إلى الصورة التالية إلا بعد إكمال الإجابتين.</p>

<h2 id="مراقبة-جودة-التصنيف-البشري">مراقبة جودة التصنيف البشري</h2>
<p>استخدمنا حراساً لضمان جودة التعليقات. ضمن كل مهمة تصنيف تشمل صوراً متعددة، ندرج ثلاثة أنواع من الحراس:</p>
<p><strong>الحارس الإيجابي:</strong> صورة تنتمي إلى الفئة المطلوبة وتم تصنيفها بشكل صحيح من قبل عدة نماذج. إن لم يختَر العمال ”نعم“، تُرفض تعليقاتهم.</p>
<p><strong>الحارس السلبي:</strong> صورة لا تنتمي إلى الفئة. على سبيل المثال، إذا كانت الفئة ”كرسي“، نستخدم صورة ”مغرفة“ كحارس سلبي. إن اختار العمال ”نعم“ للمغرفة، تُزال تعليقاتهم.</p>
<p><strong>الحارس المتسق:</strong> صورة تظهر مرتين بترتيب عشوائي داخل المهمة. إن أجاب العامل اختلافاً، تُستبعد تعليقاته لعدم الاتساق.</p>
<p>لكل مهمة حتى <span class="nodecor">20</span> صورة، ندرج حارساً إيجابياً واحداً، وحارساً سلبياً واحداً، وحارسين متسقين. نتجاهل الردود التي لا تجتاز جميع الحراس.</p>

<h2 id="النتائج">النتائج</h2>
<p>لضمان التنوع والجودة، جمعنا تعليقات 10 عمال مستقلين لكل صورة، واستبعدنا من لم يجتازوا فحوص الجودة. شارك <span class="nodecor">679</span> عاملاً في <span class="nodecor">1540</span> مهمة، محققين توافقاً نسبته <span class="nodecor">91.09\%</span> لكل صورة من ImageNet-D.</p>

<h1 id="sec:appendix_experimental_results">نتائج تجريبية على ImageNet-D</h1>
<p><strong>المزيد من النتائج للقسم 4.</strong> نقارن دقة نموذج ImageNet-D مع مجموعات الاختبار الحالية، بما في ذلك ImageNet (<span class="nodecor">russakovsky2015imagenet</span>), ObjectNet (<span class="nodecor">barbu2019objectnet</span>), ImageNet-9 (<span class="nodecor">xiao2020noise</span>) وStylized-ImageNet (<span class="nodecor">geirhos2018imagenet</span>). نبلغ عن جميع أرقام الدقة في الجدول [tab:appendix_benchmark_results]، والذي يتضمن أيضاً أرقام الشكل 8.</p>
<p><strong>إعدادات التدريب للجدول 6.</strong> نقدم التفاصيل التجريبية لجدول 6 في الورقة الرئيسية. نحسن ResNet18 المدرب مسبقاً على مجموعات تدريب مختلفة. لاستكشاف تأثير دمج الصور الاصطناعية، نأخذ عينات متساوية من ImageNet وSynthetic-easy، حيث يتضمن الأخير صوراً مولدة مولتاً بالانتشار وصُنفت بشكل صحيح من قبل النماذج البديلة. يبلغ عدد الصور في كل مجموعة <span class="nodecor">111098</span>، مع توزيع متساوٍ لكل فئة. نُجري تحسينا لمدة 10 حقب إضافية باستخدام SGD ومعدل تعلم 0.0001، مع تضمين بيانات ImageNet-1K الأصلية كجزء من التدريب.</p>
</body>
</html>
```
**تم إصلاح جميع معادلات LaTeX بحيث تُغلق بشكل صحيح وتُكتب بصيغة سليمة. تم التأكد من أن جميع النسب المئوية مكتوبة \% داخل LaTeX. جميع المعادلات الآن ستعمل بشكل صحيح مع MathJax. لم يتم تغيير أي كلمة من النص الأصلي.**