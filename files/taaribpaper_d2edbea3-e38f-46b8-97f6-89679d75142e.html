<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Chenshuang Zhang     Fei Pan    Junmo Kim     In So Kweon       Chengzhi Mao KAIST^{1}, University of Michigan, Ann Arbor^{2}, McGill University^{3}, MILA^{4}">
  <title>مِعيار ImageNet-D: قِياسُ مَتانةِ الشَّبكاتِ العَصبيّة على أشياءٍ مُولَّدةٍ بنماذجِ الاِنتشار</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    html, body {
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      background: #f8f9fa;
      color: #222;
      font-size: 22px;
      line-height: 1.8;
      margin: 0;
      padding: 0;
      direction: rtl;
    }
    body {
      max-width: 900px;
      margin: 40px auto 40px auto;
      padding: 32px 24px 32px 24px;
      background: #fff;
      border-radius: 18px;
      box-shadow: 0 4px 32px 0 rgba(0,0,0,0.08);
    }
    header {
      text-align: center;
      margin-bottom: 40px;
    }
    h1.title {
      font-size: 2.3em;
      font-weight: 700;
      color: #1a237e;
      margin-bottom: 0.3em;
      letter-spacing: 0.01em;
    }
    .author {
      font-size: 1.05em;
      color: #444;
      margin-bottom: 0.5em;
    }
    h1, h2, h3, h4 {
      color: #283593;
      font-weight: 700;
      margin-top: 1.7em;
      margin-bottom: 0.7em;
      line-height: 1.3;
    }
    h1 {
      font-size: 1.7em;
      border-bottom: 2px solid #e3e6f0;
      padding-bottom: 0.2em;
      margin-bottom: 1em;
    }
    h2 {
      font-size: 1.3em;
      border-bottom: 1px solid #e3e6f0;
      padding-bottom: 0.1em;
      margin-bottom: 0.7em;
    }
    h3 {
      font-size: 1.1em;
      margin-bottom: 0.5em;
    }
    p {
      margin: 0 0 1.1em 0;
      text-align: justify;
    }
    ul, ol {
      margin: 0 0 1.1em 2em;
      padding-right: 1.2em;
    }
    code, pre {
      font-family: 'Cairo', 'Fira Mono', 'Consolas', 'monospace';
      background: #f3f3f3;
      color: #c7254e;
      border-radius: 5px;
      padding: 2px 6px;
      font-size: 0.95em;
    }
    pre {
      display: block;
      padding: 1em;
      overflow-x: auto;
      background: #f3f3f3;
      color: #222;
      border-radius: 7px;
      margin-bottom: 1.2em;
    }
    strong {
      color: #0d47a1;
    }
    em {
      color: #6d4c41;
    }
    .math.inline {
      font-family: 'Cairo', 'Fira Mono', 'Consolas', 'monospace';
      background: #f3f3f3;
      color: #1b5e20;
      border-radius: 5px;
      padding: 1px 4px;
      font-size: 0.95em;
    }
    .math.display {
      display: block;
      margin: 1.2em auto;
      text-align: center;
      background: #f3f3f3;
      color: #1b5e20;
      border-radius: 7px;
      padding: 1em 0.5em;
      font-size: 1.1em;
    }
    .nodecor {
      text-decoration: none !important;
      color: inherit;
    }
    blockquote {
      border-right: 4px solid #b3c6ff;
      background: #f5f7fa;
      margin: 1.2em 0;
      padding: 1em 1.5em;
      color: #444;
      border-radius: 8px;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 1.5em 0;
      background: #fafbfc;
      border-radius: 8px;
      overflow: hidden;
      font-size: 0.98em;
    }
    th, td {
      border: 1px solid #e3e6f0;
      padding: 0.7em 1em;
      text-align: center;
    }
    th {
      background: #e3e6f0;
      color: #1a237e;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f6f8fc;
    }
    hr {
      border: none;
      border-top: 1px solid #e3e6f0;
      margin: 2em 0;
    }
    @media (max-width: 700px) {
      body {
        padding: 10px 2vw 10px 2vw;
        font-size: 18px;
      }
      h1.title {
        font-size: 1.3em;
      }
      h1, h2 {
        font-size: 1.1em;
      }
      table, th, td {
        font-size: 0.95em;
      }
    }
    /* دعم أحرف الأرقام وعلامات الترقيم العربية */
    body, h1, h2, h3, h4, p, li, td, th {
      unicode-bidi: embed;
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">مِعيار <span class="nodecor">ImageNet-D</span>: قِياسُ مَتانةِ الشَّبكاتِ العَصبيّة على أشياءٍ مُولَّدةٍ بنماذجِ الاِنتشار</h1>
  <p class="author">
    <span class="nodecor">Chenshuang Zhang</span> &nbsp; <span class="nodecor">Fei Pan</span> &nbsp; <span class="nodecor">Junmo Kim</span> &nbsp; <span class="nodecor">In So Kweon</span> &nbsp; <span class="nodecor">Chengzhi Mao</span><br />
    <span class="nodecor">KAIST</span><span class="math inline">\(^{1}\)</span>، <span class="nodecor">University of Michigan, Ann Arbor</span><span class="math inline">\(^{2}\)</span>، <span class="nodecor">McGill University</span><span class="math inline">\(^{3}\)</span>، <span class="nodecor">MILA</span><span class="math inline">\(^{4}\)</span>
  </p>
</header>

<h1 id="ملخص">مُلَخَّص</h1>
<p>نُقَدِّم مِعياراً صارِماً لِمَتانةِ الإدراكِ البَصري. تُوَفِّر المَعایيرُ القائِمة على صورٍ مُصطنَعة مثل <span class="nodecor">ImageNet-C</span> و<span class="nodecor">ImageNet-9</span> و<span class="nodecor">Stylized ImageNet</span> نوعاً مُحَدَّداً من التقييم على التشوُّهات الاصطناعية والخلفيّات والأنسجة، غير أنّ هذه المعايير محدودةٌ بتبايناتٍ مُعيَّنة وبواقعيّةٍ مُتدنّية. في هذا العمل، نَستخدِم نِموذجاً توليديّاً كمَصدرِ بياناتٍ لابتكار صورٍ صعبةٍ تقيس متانة النماذج العميقة. بالاعتماد على نماذجِ الاِنتشار نستطيع توليد صورٍ ذات خلفيّات وأنسجة وموادّ أكثر تنوُّعاً من أيّ عملٍ سابق، ونُطلِق على هذا المعيار اسم <span class="nodecor">ImageNet-D</span>. تُظهِر تجاربُنا أنّ <span class="nodecor">ImageNet-D</span> يُسبِّب انخفاضاً كبيراً في الدقّة عبر طيفٍ واسعٍ من نماذج الرؤية، من المُصنِّفات التقليدية مثل <span class="nodecor">ResNet</span> إلى النماذج الأساس الحديثة مثل <span class="nodecor">CLIP</span> و<span class="nodecor">MiniGPT-4</span>، مع تقليص الدقّة بما يصل إلى <span class="nodecor">60\%</span>. يُشير عملُنا إلى أنّ نماذج الاِنتشار يُمكِن أن تكون مَصدراً فعّالاً لاختبار نماذج الرؤية. الشيفرة والمجموعة مُتاحتان على GitHub للتوثيق والتنزيل.</p>

<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>حقَّقت الشبكاتُ العصبيّة أداءً ملحوظاً في مهامّ تمتدّ من تصنيف الصور (<span class="nodecor">vaswani2017attention</span>, <span class="nodecor">liu2021swin</span>, <span class="nodecor">liu2022convnet</span>) إلى الإجابةِ عن الأسئلةِ البصريّة (<span class="nodecor">li2023blip</span>, <span class="nodecor">dai2023instructblip</span>, <span class="nodecor">liu2023visual</span>, <span class="nodecor">zhu2023minigpt</span>). وقد ألهمت هذه التقدُّماتِ تطبيقَ الشبكات العصبيّة في مجالاتٍ متنوّعة، بما في ذلك الأنظمة الأمنيّة والحَرِجة مثل المركباتِ ذاتية القيادة (<span class="nodecor">kangsepp2022calibrated</span>, <span class="nodecor">nesti2023ultra</span>, <span class="nodecor">liu2023vectormapnet</span>)، وكشفِ البرمجياتِ الخبيثة (<span class="nodecor">yuan2014droid</span>, <span class="nodecor">chen2019believe</span>, <span class="nodecor">pei2017deepxplore</span>) والروبوتات (<span class="nodecor">brohan2022rt</span>, <span class="nodecor">brohan2023rt</span>, <span class="nodecor">huang2023voxposer</span>). ونظراً لاتّساع استخدامها، بات من المُهمّ أكثر فأكثر تحديدُ متانةِ الشبكات العصبيّة (<span class="nodecor">ming2022delving</span>, <span class="nodecor">li2023distilling</span>) لأسبابٍ تتعلّق بالسلامة.</p>
<p>لتقييم متانة الشبكات العصبيّة، يجمع <span class="nodecor">ObjectNet</span> (<span class="nodecor">barbu2019objectnet</span>) صورَ أشياءَ واقعيّة على عواملَ يمكن التحكُّم بها، مثل الخلفيّة، وذلك بواسطة عُمّال بشريّين، وهو ما يستهلك وقتاً وجهداً كبيرين. ولتوسيع جمع البيانات، استُخدمت الصورُ الاصطناعية كصور اختبار (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">hendrycks2019benchmarking</span>, <span class="nodecor">xiao2020noise</span>). على سبيل المثال، يُقدِّم <span class="nodecor">ImageNet-C</span> (<span class="nodecor">hendrycks2019benchmarking</span>) مجموعةً من التشوُّهات البصريّة الشائعة منخفضةِ المستوى، مثل الضوضاء الغاوسيّة والطمس، لاختبار متانة النماذج. ويستخدم <span class="nodecor">ImageNet-9</span> (<span class="nodecor">xiao2020noise</span>) تقنيةً بسيطةً للقصّ واللصق لإنشاء معيارٍ لمتانة الخلفيّة، لكن الصور تبدو أقلّ واقعيّة. أمّا <span class="nodecor">Stylized-ImageNet</span> (<span class="nodecor">geirhos2018imagenet</span>) فيُولِّد صوراً جديدة عبر تغيير نسيج صور ImageNet، لكنه لا يتحكّم في العوامل عاليةِ المستوى مثل الخلفيّة.</p>
<p>في هذا العمل، نُقدِّم <span class="nodecor">ImageNet-D</span>، وهي مجموعةُ اختبارٍ اصطناعية مُولَّدة بواسطة نماذج الاِنتشار لمَهمّةِ التعرّف على الأشياء. بالاستفادة من قدرات نماذج الاِنتشار الرائدة (<span class="nodecor">rombach2022high</span>)، نُظهِر أنه يمكن توجيه هذه النماذج باللغة الطبيعية لإنشاء صورِ اختبارٍ واقعيّة تُوقِع نماذج الرؤية في الخطأ. وبفضل قابليّتها للتوجيه باللغة، نستطيع تنويع العوامل عاليةِ المستوى في الصور، على خلاف التشوُّهات المحليّة والنسيج في الأعمال السابقة، بما يوفِّر أبعاداً إضافيّة لتقييم المتانة.</p>
<p>لِتعزيز صعوبة العيّنات في مجموعتنا، نحتفظ انتقائياً بالصور التي تُسبِّب فشلَ نماذجِ الرؤية المُختارة. وتُظهر نتائجُنا أنّ الصور التي تُثيرُ الأخطاء في النماذج المُستخدمة للاختيار تنقلُ صعوبتَها على نحوٍ موثوق إلى نماذج أخرى لم تُستَخدم مسبقاً، ممّا يؤدّي إلى انخفاضٍ ملحوظٍ في الدقّة حتى في النماذجِ الأساس الحديثة مثل <span class="nodecor">MiniGPT-4</span> (<span class="nodecor">zhu2023minigpt</span>) و<span class="nodecor">LLaVa</span> (<span class="nodecor">liu2023visual</span>)، وهذا يُشير إلى أنّ مجموعة البيانات تكشف فشلاً شائعاً في نماذج الرؤية.</p>
<p>تُظهر التصوُّرات أنّ <span class="nodecor">ImageNet-D</span> يُحسِّن بدرجةٍ كبيرة جودةَ الصورة مقارنةً بمعايير المتانة الاصطناعية السابقة. يعمل <span class="nodecor">ImageNet-D</span> كأداةٍ فعّالةٍ لكشف قصور الأداء وتقييم متانة النماذج، بما في ذلك <span class="nodecor">ResNet</span> 101 (انخفاض <span class="nodecor">55.02\%</span>)، و<span class="nodecor">ViT-L/16</span> (انخفاض <span class="nodecor">59.40\%</span>)، و<span class="nodecor">CLIP</span> (انخفاض <span class="nodecor">46.05\%</span>)، كما يتعمَّم جيداً إلى نماذج اللغة-الرؤية الكبيرة مثل <span class="nodecor">LLaVa</span> (<span class="nodecor">liu2023visual</span>) (انخفاض <span class="nodecor">29.67\%</span>) و<span class="nodecor">MiniGPT-4</span> (<span class="nodecor">zhu2023minigpt</span>) (انخفاض <span class="nodecor">16.81\%</span>). يُعتَبَر نهجُنا في استخدام النماذج التوليدية لتقييم المتانة منهجاً عامّاً، ويُظهِر إمكاناتٍ كبيرة لتحسيناتٍ مستقبليّة مع تطوُّر النماذج التوليدية.</p>

<h1 id="sec:related_work">الأَعْمال ذات الصِلَة</h1>
<p><strong>مَتانة الشبكات العصبيّة.</strong> تطوَّرت النماذج من الشبكاتِ الاِلتفافيّة (<span class="nodecor">CNN</span>) (<span class="nodecor">he2016deep, huang2017densely</span>)، إلى المحوِّلات الرؤيويّة (<span class="nodecor">ViT</span>) (<span class="nodecor">vaswani2017attention, liu2021swin</span>)، وصولاً إلى النماذج الأساس الكبيرة (<span class="nodecor">bommasani2021opportunities, devlin2018bert, touvron2023llama</span>). وقد تناولت أعمالٌ سابقة متانةَ الشبكات العصبيّة من جوانب عدّة، مثل الأمثلة المُعادية (<span class="nodecor">mao2022understanding, mahmood2021robustness, madry2017towards, zhao2023evaluating, zhang2019theoretically</span>) والعينات خارج التوزيع (<span class="nodecor">MAE, mao2021discrete, hendrycks2021many, augmix</span>). كما أظهرت النماذج الأساس متانةً أعلى على العينات خارج التوزيع (<span class="nodecor">radford2021learning</span>). وإلى جانب ذلك، جرى بحثُ التفسير المتين أيضاً (<span class="nodecor">mao2023doubly, liu2023visual, zhu2023minigpt</span>). ولتقييم متانة النماذج العميقة بشكلٍ منهجي، لا بُدّ من مجموعات اختبارٍ تُغطّي عوامل متنوّعة.</p>
<p><strong>مجموعات بيانات لتقييم المتانة.</strong> تستخدم دراساتٌ عديدة صوراً من الإنترنت، ومنها <span class="nodecor">ImageNet-A</span> (<span class="nodecor">hendrycks2021natural</span>) و<span class="nodecor">Imagenet-R</span> (<span class="nodecor">hendrycks2021many</span>) و<span class="nodecor">ImageNet-Sketch</span> (<span class="nodecor">wang2019learning</span>). غير أنّها مقيّدةٌ بما هو مُتاح على الويب. يجمع <span class="nodecor">ObjectNet</span> (<span class="nodecor">barbu2019objectnet</span>) الصورَ يدوياً بمساعدة آلاف العُمّال، ما يستغرق وقتاً وجهداً كبيرين.</p>
<p>لتجاوز قيودِ الاعتماد على الويب وتقليل كلفة الجمع اليدوي، اقتُرحت الصورُ الاصطناعية لتقييم المتانة (<span class="nodecor">geirhos2018imagenet, hendrycks2019benchmarking, xiao2020noise</span>). يُقيِّم <span class="nodecor">ImageNet-C</span> (<span class="nodecor">hendrycks2019benchmarking</span>) متانةَ النموذج حيال التشوُّهات منخفضة المستوى. ويُولّد <span class="nodecor">ImageNet-9</span> (<span class="nodecor">xiao2020noise</span>) صوراً جديدة بدمج الخلفيّة والمقدّمة من صورٍ مختلفة، لكنه محدودٌ بجودةٍ أدنى نسبياً. أمّا <span class="nodecor">Stylized-ImageNet</span> (<span class="nodecor">geirhos2018imagenet</span>) فيُغيِّر نسيجَ صور <span class="nodecor">ImageNet</span> باستخدام نقل الأسلوب <span class="nodecor">AdaIN</span> (<span class="nodecor">huang2017arbitrary</span>) أو بإدخال تعارضٍ بين النسيج والشكل، لكنه لا يتحكّم في عوامل أخرى مثل الخلفيات. في هذا العمل، نُقدِّم مجموعة اختبارٍ جديدة <span class="nodecor">ImageNet-D</span>، تُولَّد بالتحكُّم عبر نماذج الاِنتشار، وتَشمل صوراً جديدة مع خلفيّات وأنسجة وموادّ متنوّعة.</p>
<p><strong>توليد الصور.</strong> حقَّقت نماذجُ الاِنتشار نجاحاً كبيراً في مهامّ متعدّدة، منها توليدُ الصور (<span class="nodecor">saharia2022photorealistic, ramesh2022hierarchical, ruiz2023dreambooth, zhang2023text</span>). وبخاصةٍ، يُمكِّن <span class="nodecor">Stable Diffusion</span> (<span class="nodecor">rombach2022high</span>) من توليد صورٍ عاليةِ الدقّة مُوجَّهةٍ باللغة. كما يُتيح <span class="nodecor">InstructPix2Pix</span> (<span class="nodecor">brooks2023instructpix2pix</span>) تحكُّماً أدقّ عبر تعديل صورةٍ معيّنة وفق تعليماتٍ بشريّة. في هذه الورقة، نبني خطّ أنابيبنا باستخدام نموذج <span class="nodecor">Stable Diffusion</span> القياسي، مع أنّ طريقتنا متوافقةٌ مع نماذج توليديّة أخرى قابلةٍ للتوجيه باللغة.</p>
<p><strong>تعزيز الإدراك باستخدام صور الاِنتشار.</strong> استُخدمت الصورُ المُولَّدة بالاِنتشار لتعزيز مهامّ الإدراك الرؤيوي. فَقطّ حسَّن فرعٌ من الدراسات (<span class="nodecor">yuan2023not, bansal2023leaving, azizi2023synthetic, tian2023stablerep</span>) دقّةَ التصنيف باستعمال الصور الاصطناعية كتوسيعٍ لبيانات التدريب. بينما يكشف <span class="nodecor">DREAM-OOD</span> (<span class="nodecor">du2023dream</span>) القيمَ الشاذّة عبر فكّ ترميز العينات الكامنة إلى صور، لكن طريقتهم تفتقر إلى التحكّم الدقيق في فضاء الصور، وهو أمرٌ محوريّ لمِثل <span class="nodecor">ImageNet-D</span>. كما يُحدِّد (<span class="nodecor">metzen2023identification</span>) أزواج سماتٍ غير ممثّلةٍ بما يكفي، بينما يركّز بحثنا على استخراج الصور الصعبة لكلّ سمةٍ على حِدة. وعلى عكس (<span class="nodecor">li2023imagenet, vendrow2023dataset, prabhu2023lance</span>) الذين يُعدّلون مجموعات البيانات القائمة، يُولِّد عملُنا صوراً جديدة ويختار أصعبَها كمجموعة اختبار، مُحقِّقاً انخفاضاً أكبر في الدقّة.</p>

<h1 id="sec:imagenet_d">ImageNet-D</h1>
<p>نُقدِّم أولاً كيفيّة إنشاء <span class="nodecor">ImageNet-D</span> في <a class="nodecor" href="#sec:dataset_design">قسم تصميم مجموعة البيانات</a>، ثم نعرض لمحةً عن إحصاءاته في <a class="nodecor" href="#sec:statistics">قسم الإحصاءات</a>.</p>

<h2 id="sec:dataset_design">تَصْمِيم مجموعة البيانات</h2>
<p>بينما تتفوّق الشبكاتُ العصبيّة في تطبيقاتٍ متعدّدة، فإنّ متانتَها تحتاج إلى تقييمٍ دقيق لأغراض السلامة. تعتمد التقييمات التقليدية على مجموعات اختبارٍ قائمة تتضمن إمّا صوراً طبيعيّة (<span class="nodecor">barbu2019objectnet</span>, <span class="nodecor">hendrycks2021natural</span>) أو صوراً اصطناعية (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">hendrycks2019benchmarking</span>, <span class="nodecor">xiao2020noise</span>). ومقارنةً بجمع الصور يدوياً، فإن بناءَ مجموعة اختبارٍ اصطناعية أكثر كفاءةً (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">xiao2020noise</span>). غير أنّ تنوّع مجموعات الاختبار الاصطناعية الحالية محدودٌ لاعتمادها على صورٍ قائمةٍ لاستخراج السمات، كما أنّ واقعيّتَها منخفضةٌ أيضاً، كما هو موضّحٌ في الشكل [fig:test_set_comparison]. نُقدِّم <span class="nodecor">ImageNet-D</span> لمعالجة هذه القيود، عبر تقييم متانة النموذج على مجموعاتٍ متنوّعة من الأشياء والعوامل الطارئة.</p>
<p><strong>توليد الصور بواسطة نماذج الاِنتشار.</strong> لبناء <span class="nodecor">ImageNet-D</span>، نستخدم نماذجَ الاِنتشار لإنشاء مجموعةٍ ضخمةٍ من الصور عبر الجمع الطولي لجميع فئات الأشياء الممكنة والعوامل الطارئة، بما يُتيح توليد صورٍ عالية الدقّة انطلاقاً من مُدخلاتٍ نصّيةٍ مُحدّدة. نعتمد نموذج <span class="nodecor">Stable Diffusion</span> (<span class="nodecor">rombach2022high</span>) للتوليد، مع أنّ نهجَنا متوافقٌ مع نماذج توليديّة أخرى قابلةٍ للتوجيه باللغة. تُصاغ عمليةُ التوليد على النحو التالي:
<span class="math display">\[
\text{Image}(C, N)  = \text{Stable Diffusion}(\text{Prompt}(C,N)),
\]</span>
حيث يُشير <span class="math inline">\(C\)</span> و<span class="math inline">\(N\)</span> إلى فئةِ الشيء والعاملِ الطارئ، على التوالي. ويشمل العامل الطارئ <span class="math inline">\(N\)</span> الخلفيّةَ والنسيجَ والمادّة. يقدّم الجدول [tab:prompt_list] نظرةً عامّة على العوامل والقوالب النصّية. وبالاستعانة بفئةِ الحقيبة مثالاً، نُولِّد صوراً لـ«حقيبةٍ في حقلِ قمحٍ» و«حقيبةٍ في غرفةٍ خشبيّة» وغير ذلك، بما يوفّر تنوُّعاً أوسع من المجموعات القائمة. نَعُدّ فئةَ المُطالَبة النصّية <span class="math inline">\(C\)</span> هي الحقيقةَ الأرضية. وتُعدّ الصورةُ مُصنّفةً خطأً إذا لم يُطابِق تصنيفُ النموذج الحقيقةَ الأرضية <span class="math inline">\(C\)</span>.</p>
<p>بعد إنشاء مجموعةٍ كبيرةٍ من الصور لكلّ أزواجِ الفئات والعوامل، نقيّم نموذج <span class="nodecor">CLIP</span> (<span class="nodecor">ViT-L/14</span>) على هذه الصور في الجدول [tab:vanilla_generation] (التفاصيل التجريبية في <a class="nodecor" href="#sec:experimental_setup">قسم الإعدادات</a>). ويُظهر الجدول أنّ <span class="nodecor">CLIP</span> يُحقّق دقّةً مرتفعة (نحو <span class="nodecor">94\%</span>) على الصور الاصطناعية. ولإنشاء مجموعةِ اختبارٍ تحدٍّ، نقترح استراتيجيةً فعّالة لاستخراج العينات الصعبة بالاعتماد على الفشل المُشترَك.</p>
<p><strong>استخراج الصور الصعبة عبر فشلِ الإدراك المُشترَك.</strong> قبل شرحِ كيفية تحديدِ العينات الصعبة، نُعرِّف مفهومَ فشل الإدراك المُشترَك:</p>
<blockquote>
  <p><strong>الفشل المُشترَك:</strong> تُعَدّ صورةٌ ما فشلاً مُشترَكاً إذا أدّت إلى تنبؤٍ خاطئ بفئةِ الشيء لدى عدّةِ نماذج.</p>
</blockquote>
<p>المجموعةُ الصعبة المثاليّة تضمّ صوراً يفشل فيها جميعُ النماذج المُختبَرة، لكن ذلك غير عمليّ لعدم إمكان الوصول إلى النماذج المستقبلية (النماذج الهدف). بدلاً من ذلك، نبني مجموعةَ الاختبار انطلاقاً من فشلِ نماذج بديلةٍ معروفة. فإذا أدّى فشل هذه النماذج إلى انخفاضٍ في الدقّة لدى النماذج الهدف غير المعروفة، نَعُدّ الفشل «قابلاً للنقل»:</p>
<blockquote>
  <p><strong>الفشل القابل للنقل:</strong> فشلُ النماذج البديلة المعروفة يكون قابلاً للنقل إذا أدّى أيضاً إلى دقّةٍ متدنّية لدى النماذج الهدف غير المعروفة.</p>
</blockquote>
<p>لتقييم قابليّة نقل الفشل من الصور المُولَّدة، نُقيِّم مجموعاتِ اختبارٍ جرى إنشاؤها بفشلٍ مُشترَك من <span class="nodecor">1</span> إلى <span class="nodecor">8</span> نماذج بديلة (الشكل [fig:filter_consistency]). كما نُقيِّم ثلاثة نماذج هدف لم تُستخدَم في بناء مجموعة الاختبار، وهي <span class="nodecor">CLIP</span> (<span class="nodecor">ViT-B/16</span>) و<span class="nodecor">LLaVa</span> و<span class="nodecor">MiniGPT-4</span>. ويُظهر الشكل [fig:filter_consistency] أنّ دقّة النماذج الهدف تنخفض كلّما زاد عدد النماذج البديلة. وقد أُنتِجت مجموعاتُ الاختبار للعوامل الثلاثة (الخلفية والنسيج والمادّة) وتُظهِر الاتّجاه نفسه.</p>

<h1 id="التحكم-بالجودة-بواسطة-التدخل-البشري">التَحَكُّمُ بالجودة بواسطة تدخُّلٍ بشريّ</h1>
<p>تُوفِّر العمليةُ السابقة اكتشافاً تلقائيّاً لمجموعةِ اختبارٍ صعبة، غير أنّ النماذج التوليدية قد تُنتِج صوراً لا تتطابق مع فئةِ المُطالبة. لذا نَلجأ إلى التعليق التوضيحي البشري لضمان أن تكون صور <span class="nodecor">ImageNet-D</span> صالحةً ومن فئةٍ واحدة وعاليةَ الجودة. بعد الجولة الأولى من التعليق بواسطة طلاب دراساتٍ عليا متخصصين، نستخدم <span class="nodecor">Amazon Mechanical Turk</span> (<span class="nodecor">deng2009imagenet</span>, <span class="nodecor">recht2019imagenet</span>, <span class="nodecor">hendrycks2021many</span>) للتدقيق في جودة التسمية. نطلب من العُمّال اختيار الصور التي يمكنهم التعرّف فيها على الكائنِ الرئيسي أو التي تُظهر الكائن في وظيفته الحقيقيّة كفئةِ الحقيقة الأرضية. كما نُصمِّم عناصرَ تحكّمٍ بالجودة لضمان استجاباتٍ دقيقة، ومنها عناصرٌ إيجابيّة وسلبيّة واختباراتُ اتّساق. التفاصيل في الملحق. شارك <span class="nodecor">679</span> عاملاً في <span class="nodecor">1540</span> مهمّة، مُحقِّقين توافُقاً نسبته <span class="nodecor">91.09\%</span>.</p>

<h2 id="sec:statistics">إحصاءاتُ قاعدةِ البيانات</h2>
<p>تتضمن <span class="nodecor">ImageNet-D</span> <span class="nodecor">113</span> فئةً مشتركةً بين <span class="nodecor">ImageNet</span> و<span class="nodecor">ObjectNet</span>، و<span class="nodecor">547</span> مُرشِّحاً للعوامل المؤثّرة من <span class="nodecor">Broden</span> (<span class="nodecor">bau2017network</span>) (انظر الجدول [tab:prompt_list])، ما يُنتِج <span class="nodecor">4835</span> صورةً صعبة: خلفياتٌ متنوّعة (<span class="nodecor">3764</span>)، وأنسجة (<span class="nodecor">498</span>)، وموادّ (<span class="nodecor">573</span>). إنّ عملية إنشاء <span class="nodecor">ImageNet-D</span> عامّةٌ وفعّالة، وتُتيح إضافةَ فئاتٍ وعواملَ جديدة بسهولة. ويُظهِر توزيعُ الفئات نمطاً طبيعيّاً طويلَ الذيل (الشكل [fig:hist_category])، كما يُسلِّط التوزيعُ النادر وغيرُ المنتظم للعوامل في الشكل [fig:heatmap] الضوءَ على أهميّة استنفادِ أزواجِ الفئات والعوامل في إنشاء مجموعة الاختبار.</p>

<h1 id="sec:benchmark">التجارب</h1>
<p>نُقيِّم نماذج مختلفة على معيار <span class="nodecor">ImageNet-D</span>، فتُبيِّن النتائجُ انخفاضاً في الدقّة يصل إلى <span class="nodecor">60\%</span> لدى جميعها. ثم ندرس ما إذا كانت تقنياتٌ سابقة، مثل توسيع البيانات، تُحسِّن المتانة. أخيراً، نُناقش <span class="nodecor">ImageNet-D</span> من زوايا متنوّعة، مثل استرجاع الجار الأقرب.</p>

<h2 id="sec:experimental_setup">إعداداتُ التجربة</h2>
<p><strong>إعدادات بناء مجموعة الاختبار.</strong> نستخدم نموذج <span class="nodecor">Stable Diffusion</span> 2.1 (<span class="nodecor">stable-diffusion-2-1</span>) من <span class="nodecor">Hugging Face</span> لإنشاء <span class="nodecor">ImageNet-D</span>. لاستخراج الصور الصعبة، نحتفظ بالصور التي تفشل فيها أربعةُ نماذج بديلة: <span class="nodecor">CLIP</span> (<span class="nodecor">ViT-L/14</span>, <span class="nodecor">ViT-L/14-336px</span>, <span class="nodecor">ResNet50</span>) ونموذج الرؤية <span class="nodecor">ResNet50</span> (<span class="nodecor">he2016deep</span>). وتشمل قائمةُ النماذج البديلة أيضاً <span class="nodecor">CLIP</span> (<span class="nodecor">ResNet101</span>, <span class="nodecor">ViT-B/32</span>) ونماذجَ رؤيةٍ أخرى (<span class="nodecor">ViT-L/16</span>, <span class="nodecor">VGG16</span>).</p>
<p><strong>تقييم نماذج التصنيف.</strong> تُقاس المتانة على <span class="nodecor">ImageNet-D</span> بدقّة <span class="nodecor">Top-1</span> في التعرّف على الأشياء. نستخدم الأوزانَ المُدرَّبة مُسبقاً المفتوحة المصدر للنماذج. وبالنسبة إلى <span class="nodecor">CLIP</span> (<span class="nodecor">radford2021learning</span>)، نتبع الإرشاداتِ الأصليّة لاعتماد قالبٍ نصّيّ من قبيل: <em>«صورةٌ لـ <span class="math inline">\(\left[\text{category}\right]\)</span>»</em>. ونُبلِغ عن دقّة الضبط الصفري (<span class="nodecor">zero-shot</span>) لـ <span class="nodecor">CLIP</span>.</p>
<p><strong>تقييم نماذج الإجابة عن الأسئلة البصريّة (<span class="nodecor">VQA</span>).</strong> نُقيِّم دقّة نماذج <span class="nodecor">VQA</span> الحديثة المفتوحة المصدر على <span class="nodecor">ImageNet-D</span>، مثل <span class="nodecor">LLaVa</span> و<span class="nodecor">MiniGPT-4</span>. وبما أنّ الإخراج النصّي قد لا يتضمن اسمَ الفئة تماماً، يصعُب تقييم الدقّة مباشرة.</p>
<p>لجعل نماذج <span class="nodecor">VQA</span> تختار من قائمةٍ محدّدة، نستخدم المطالبة: <code>ما هو الكائن الرئيسي في هذه الصورة؟ اختر من القائمة التالية: [GT category], [failure category]</code>. تُمثِّل فئة <span class="nodecor">GT</span> الحقيقةَ الأرضية، وتكون «فئةُ الفشل» هي أفضل فئةٍ خاطئة وفقاً لثقة <span class="nodecor">CLIP</span>. إذا اختار النموذج فئةَ الحقيقة الأرضية، نَعُدّ التصنيف صحيحاً، وبذلك نحسب دقّة <span class="nodecor">VQA</span>.</p>

<h2 id="sec:experimental_results">تقييمُ المتانة</h2>
<p><strong>النتائج الكميّة.</strong> نُقيِّم <span class="nodecor">ImageNet-D</span> على <span class="nodecor">25</span> نموذجاً، ونرسم دقّة الاختبار في الشكل [fig:main_result_figure]، حيث يمثّل المحورُ الأفقي دقّةَ <span class="nodecor">ImageNet</span> والعمودي دقّة <span class="nodecor">ImageNet-D</span>. ويُظهر الشكل أنّ دقّة <span class="nodecor">ImageNet-D</span> أدنى بوضوح لدى جميع النماذج (تحت الخط <span class="math inline">\(y=x\)</span>). نُبلِغ عن دقّات <span class="nodecor">14</span> نموذجاً على مجموعاتِ اختبارٍ مختلفة في الجدول [tab:benchmark_results]، مع باقي النماذج في الملحق. ويُظهر الجدول أنّ <span class="nodecor">ImageNet-D</span> يُسجِّل أدنى دقّة اختبار، باستثناء تقارُبٍ في النتائج على <span class="nodecor">Stylized-ImageNet</span> لنماذج <span class="nodecor">VQA</span>. وعلى الرغم من أنّ <span class="nodecor">ObjectNet</span> يغيّر سماتٍ عديدة لكلّ صورة، تبقى دقّتُه أعلى من <span class="nodecor">ImageNet-D</span> الذي يغيّر سمةً واحدة. مقارنةً بـ <span class="nodecor">ImageNet</span>، يُفضي <span class="nodecor">ImageNet-D</span> إلى انخفاضٍ في الدقّة يزيد على <span class="nodecor">16\%</span> لدى جميع النماذج، بما في ذلك <span class="nodecor">LLaVa</span> (انخفاض <span class="nodecor">29.67\%</span>) و<span class="nodecor">MiniGPT-4</span> (انخفاض <span class="nodecor">16.81\%</span>).</p>

<h1 id="نتائج-التصور">نتائجُ التصوُّر</h1>
<p>يعرض <span class="nodecor">ImageNet-D</span> أمثلةً لصورٍ عاليةِ الجودة مفهومةٍ بسهولةٍ للبشر، بينما يُخطئ <span class="nodecor">CLIP</span> (<span class="nodecor">ViT-L/14</span>) في تصنيفها. كما قد يُخطئ كلٌّ من <span class="nodecor">MiniGPT-4</span> و<span class="nodecor">LLaVa-1.5</span> في التعرّف على الكائن الرئيسي في هذه الصور.</p>

<h2 id="sec:robustness_improvement">تحسينُ المتانة</h2>
<p><strong>توسيع البيانات.</strong> أوضحت أعمالٌ سابقة فعاليّةَ توسيع البيانات في تحسين المتانة، كما في <span class="nodecor">ImageNet-C</span>. نختبر طرق <span class="nodecor">SIN</span> و<span class="nodecor">AugMix</span> و<span class="nodecor">ANT</span> و<span class="nodecor">DeepAugment</span> على <span class="nodecor">ImageNet-D</span>. ويُبيِّن الجدول [tab:result_augmentation] دقّتَي <span class="nodecor">ImageNet</span> و<span class="nodecor">ImageNet-D</span> ومتوسط خطأ الفساد (<span class="nodecor">mCE</span>) لـ <span class="nodecor">ImageNet-C</span> باستخدام <span class="nodecor">ResNet50</span> كنموذجٍ أساسيّ. ورغم أنّ هذه الطرق تُحسِّن متانة <span class="nodecor">ImageNet-C</span>، فإنّها لا تُحسِّن – وقد تُضعِف – متانة <span class="nodecor">ImageNet-D</span>، ما يُبرهن الحاجةَ إلى هذا المعيار الجديد.</p>
<p><strong>هندسة النموذج.</strong> نُقارِن تنويعاتِ النموذج في الشكل [fig:result_arch]. عند الانتقال من <span class="nodecor">ViT</span> إلى <span class="nodecor">Swin Transformer</span> و<span class="nodecor">ConvNeXt</span> تتحسّن دقّتَا <span class="nodecor">ImageNet</span> و<span class="nodecor">ImageNet-D</span>، لكن تبقى المتانةُ صعبةً بخاصّة على مجموعاتِ النسيج والمواد. وتُظهِر النتائج صعوبةَ تحسين متانة <span class="nodecor">ImageNet-D</span> بهندسةِ النموذج وحدها.</p>
<p><strong>التدريب المسبق بمزيدٍ من البيانات.</strong> للتدريبِ المسبق على مجموعةِ بياناتٍ أكبر أثرٌ إيجابيّ على الدقّة. يُقارِن الشكل [fig:result_arch] بين <span class="nodecor">ConvNeXt</span> المُدرَّب مباشرةً على <span class="nodecor">ImageNet-1K</span> ونظيره المُدرَّب أوّلاً على <span class="nodecor">ImageNet-22K</span>. ويُحقّق الأخير متانةً أعلى على جميع مجموعات <span class="nodecor">ImageNet-D</span>، ولا سيّما الخلفيات، ما يدلّ على فائدة التدريب المسبق واسعِ النطاق.</p>

<h2 id="sec:analysis">مناقشاتٌ إضافيّة</h2>
<p><strong>هل يجد <span class="nodecor">CLIP</span> الجيرانَ الأقرب الصحيحين لصور <span class="nodecor">ImageNet-D</span>؟</strong> يستخدم <span class="nodecor">CLIP</span> قدراتِه في استرجاع الجار الأقرب. وباعتبار صور <span class="nodecor">ImageNet-D</span> استعلامات، نسترجع صوراً من <span class="nodecor">ImageNet</span> للتحقُّق من التشابه. ويُظهر الشكل أنّ الصور المُسترجَعة غالباً ما تحمل خلفياتٍ أو كائناتٍ مشابهة لصورة الاستعلام، ما يكشف حالاتِ فشلٍ في استرجاع الجيران الأقرب.</p>
<p><strong>هل تُضاهي <span class="nodecor">ImageNet-D</span> مجموعاتِ الاختبار الطبيعيّة في قابليّة نقل الفشل؟</strong> كما عرّفنا الفشل القابل للنقل في <a class="nodecor" href="#sec:dataset_design">قسم تصميم مجموعة البيانات</a>، نُجري التجربةَ نفسها على <span class="nodecor">ImageNet</span> (الفشل) باستخدام صور الفشل المُشترَك. ويُظهر الجدول [tab:transferability] أنّ <span class="nodecor">ImageNet-D</span> تُحقّق دقّةً مُشابهةً لـ <span class="nodecor">ImageNet</span> (الفشل)، ما يُشير إلى أنّ الصور الاصطناعية قادرةٌ على نقل الفشل كما في الطبيعي، لكن بتكلفةٍ أدنى وسهولةِ توسيعٍ أكبر.</p>
<p><strong>التدريب على صورٍ مُولَّدة بالاِنتشار.</strong> نُسمّي الصورَ المُولَّدة التي صُنِّفت تصنيفاً صحيحاً من النماذج البديلة <span class="nodecor">Synthetic-easy</span>، ونستكشف أثرَها في التدريب. نُحسِّن <span class="nodecor">ResNet18</span> المُدرَّب مسبقاً على مجموعاتِ تدريبٍ مختلفة (الجدول [tab:finetune_experiment]). ويُظهر الجدول أنّ التدريب على <span class="nodecor">Synthetic-easy</span> يعزّز متانةَ <span class="nodecor">ImageNet-D</span> بنسبة <span class="nodecor">19.26\%</span>، وأنّ «النموذج C» يتفوّق على «النموذج B» في دقّة <span class="nodecor">ObjectNet</span> بنسبة <span class="nodecor">1.34\%</span>، ما يدلّ على تعميمٍ أفضل. وتُشير النتائج إلى أنّ الصور المُولَّدة بالاِنتشار مع أزواجٍ متنوّعة من الكائنات والعوامل تُحسِّن المتانةَ كعيناتِ تدريب.</p>

<h1 id="sec:conclusion">الخُلاصَة</h1>
<p>نُقدِّم في هذه الورقة مجموعةَ اختبار <span class="nodecor">ImageNet-D</span> ونُطوِّر معياراً صارماً لمتانة الإدراك البصري. باستثمار قدرةِ توليد الصور لدى نماذج الاِنتشار، تتضمّن <span class="nodecor">ImageNet-D</span> صوراً بعواملَ متنوّعة تشمل الخلفياتِ والأنسجةَ والمواد. وتُظهِر النتائج التجريبية أنّ <span class="nodecor">ImageNet-D</span> تُقلِّص بدرجةٍ كبيرة دقّةَ نماذجَ متنوّعة، بما في ذلك <span class="nodecor">CLIP</span> (انخفاض <span class="nodecor">46.05\%</span>) و<span class="nodecor">LLaVa</span> (انخفاض <span class="nodecor">29.67\%</span>) و<span class="nodecor">MiniGPT-4</span> (انخفاض <span class="nodecor">16.81\%</span>)، مؤكِّدةً فاعليّتها في تقييم المتانة. يُمثّل عملُنا خطوةً إلى الأمام في تحسين معايير الاختبار الاصطناعية، ومع تقدُّم النماذج التوليدية ستزداد الصورُ الاختبارية تنوُّعاً وتحدّياً.</p>
<p><strong>الشكر والتقدير:</strong> حظِي هذا العملُ بدعمِ منحةٍ من معهد تخطيط وتقييم تكنولوجيا المعلومات والاتصالات (<span class="nodecor">IITP</span>) المموَّل من حكومة كوريا (<span class="nodecor">MSIT</span>) (رقم <span class="nodecor">2022-0-00951</span>) لتطوير عواملَ غيرِ مؤكَّدة تتعلّم عبر طرح الأسئلة.</p>

<h1 id="مهمة-التسمية-على-آمازون-ميكانيكال-تورك">مهمّة التسمية على أمازون ميكانيكال تورك</h1>
<p>لضمان معاييرَ موثوقة، اعتمدنا على <span class="nodecor">Amazon Mechanical Turk</span> (<span class="nodecor">deng2009imagenet</span>, <span class="nodecor">recht2019imagenet</span>, <span class="nodecor">hendrycks2021many</span>) لتقييم جودة تسمية صور <span class="nodecor">ImageNet-D</span>.</p>

<h2 id="تصميم-مهمة-التسمية">تصميمُ مهمّةِ التسمية</h2>
<p><strong>تعليمات التسمية.</strong> نظراً لتنوّع الصور التي قد تتضمّن أزواجاً نادرة من الأشياء والعوامل، طلبنا من العُمّال مُراعاة مظهرِ الكائن الرئيسي ووظيفته. ونطلب منهم الإجابة عن السؤالين التاليين:</p>
<p><code>السؤال 1:</code> هل يمكنك التعرّف على الكائن المطلوب (<code>فئة الحقيقة الأرضية</code>) في الصورة، على الرغم من الخلفية أو النسيج أو المادّة؟</p>
<p><code>السؤال 2:</code> هل يمكن استخدام الكائن في الصورة فعليّاً كالكائن المطلوب (<code>فئة الحقيقة الأرضية</code>)؟</p>
<p><strong>خطّ سير التسمية.</strong> لضمان فهم المعيارَين، يبدأ العاملُ بتسمية صورتين تدريبيّتَين مع الإجابة الصحيحة على السؤالين. بعدها يُصنِّف حتى 20 صورة في المهمّة الواحدة، ويُجيب بـ«نعم» أو «لا» لكلّ سؤال.</p>
<p><strong>واجهة المستخدم للتسمية.</strong> صُمِّمت واجهةٌ سهلةُ الاستخدام (الشكل محذوف)، بحيث لا يمكن الانتقال إلى الصورة التالية إلا بعد إكمال الإجابتَين.</p>

<h2 id="مراقبة-جودة-التصنيف-البشري">مراقبةُ جودةِ التصنيف البشري</h2>
<p>استخدمنا عناصرَ تحكّمٍ بالجودة لضمان جودة التعليقات. ضمن كلّ مهمّة تصنيف تشمل صوراً متعدّدة، نُدرِج ثلاثة أنواع:</p>
<p><strong>الحارس الإيجابي:</strong> صورةٌ تنتمي إلى الفئة المطلوبة وقد صُنِّفت بشكلٍ صحيح من قِبل عدّة نماذج. إن لم يختر العُمّال «نعم»، تُرفض تعليقاتُهم.</p>
<p><strong>الحارس السلبي:</strong> صورةٌ لا تنتمي إلى الفئة. على سبيل المثال، إذا كانت الفئة «كرسي»، نستخدم صورة «مِغرفة» كحارسٍ سلبي. إن اختار العُمّال «نعم» للمِغرفة، تُزال تعليقاتُهم.</p>
<p><strong>حارس الاتّساق:</strong> صورةٌ تظهر مرّتَين بترتيبٍ عشوائي داخل المهمّة. إن أجاب العامل إجابتَين مُتعارضتَين، تُستبعَد تعليقاته لعدم الاتّساق.</p>
<p>لكلّ مهمّةٍ تضمّ حتى <span class="nodecor">20</span> صورة، نُدرج حارساً إيجابيّاً واحداً، وحارساً سلبيّاً واحداً، وحارسين للاّتّساق. ونتجاهل الردود التي لا تجتاز جميع عناصر التحكّم.</p>

<h2 id="النتائج">النتائج</h2>
<p>لضمان التنوّع والجودة، جمعنا تعليقاتِ 10 عُمّالٍ مستقلّين لكلّ صورة، واستبعدنا من لم يجتز فحوصَ الجودة. وقد شارك <span class="nodecor">679</span> عاملاً في <span class="nodecor">1540</span> مهمّة، مُحقِّقين توافُقاً نسبته <span class="nodecor">91.09\%</span> لكلّ صورةٍ من <span class="nodecor">ImageNet-D</span>.</p>

<h1 id="sec:appendix_experimental_results">نتائجٌ تجريبية على ImageNet-D</h1>
<p><strong>المزيد من النتائج للقسم 4.</strong> نُقارِن دقّةَ النماذج على <span class="nodecor">ImageNet-D</span> مع مجموعاتِ الاختبار الحالية، بما فيها <span class="nodecor">ImageNet</span> (<span class="nodecor">russakovsky2015imagenet</span>) و<span class="nodecor">ObjectNet</span> (<span class="nodecor">barbu2019objectnet</span>) و<span class="nodecor">ImageNet-9</span> (<span class="nodecor">xiao2020noise</span>) و<span class="nodecor">Stylized-ImageNet</span> (<span class="nodecor">geirhos2018imagenet</span>). نُبلِغ عن جميع أرقام الدقّة في الجدول [tab:appendix_benchmark_results]، والذي يتضمّن أيضاً أرقام الشكل 8.</p>
<p><strong>إعداداتُ التدريب للجدول 6.</strong> نُقدِّم التفاصيلَ التجريبية لجدول 6 في الورقة الرئيسية. نُحسِّن <span class="nodecor">ResNet18</span> المُدرَّب مسبقاً على مجموعاتِ تدريبٍ مختلفة. ولاستكشاف أثر دمج الصور الاصطناعية، نأخذ عيناتٍ مُتساوية من <span class="nodecor">ImageNet</span> و<span class="nodecor">Synthetic-easy</span>، حيث يتضمّن الأخير صوراً مُولَّدةً بالاِنتشار وصُنِّفت تصنيفاً صحيحاً من النماذج البديلة. يبلغ عدد الصور في كلّ مجموعة <span class="nodecor">111098</span>، مع توزيعٍ مُتساوٍ لكلّ فئة. نُجري التحسين لمدة 10 حِقبٍ إضافية باستخدام <span class="nodecor">SGD</span> ومُعدّلِ تعلُّمٍ قدرُه 0.0001، مع تضمين بيانات <span class="nodecor">ImageNet-1K</span> الأصليّة كجزءٍ من التدريب.</p>
</body>
</html>