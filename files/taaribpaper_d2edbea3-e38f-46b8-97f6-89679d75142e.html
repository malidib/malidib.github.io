<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Chenshuang Zhang     Fei Pan    Junmo Kim     In So Kweon       Chengzhi Mao KAIST^{1}, University of Michigan, Ann Arbor^{2}, McGill University^{3}, MILA^{4}">
  <title>مِعْيار ImageNet-D: قياس متانة الشبكات العصبية على الصور الاصطناعية المولَّدة بنماذج الانتشار</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">مِعْيار <span class="nodecor">ImageNet-D</span>: قياس متانة الشبكات العصبية على الصور الاصطناعية المولَّدة بنماذج الانتشار</h1>
<p class="author"><span class="nodecor">Chenshuang Zhang</span>     <span class="nodecor">Fei Pan</span>    <span class="nodecor">Junmo Kim</span>     <span class="nodecor">In So Kweon</span>       <span class="nodecor">Chengzhi Mao</span><br />
<span class="nodecor">KAIST</span><span class="math inline">\(^{1}\)</span>, <span class="nodecor">University of Michigan, Ann Arbor</span><span class="math inline">\(^{2}\)</span>, <span class="nodecor">McGill University</span><span class="math inline">\(^{3}\)</span>, <span class="nodecor">MILA</span><span class="math inline">\(^{4}\)</span><br /></p>
</header>
<p>لاتيكس</p>
<h1 id="ملخص">مُلَخَّص</h1>
<p>نُقدِّم معايير صارمة لمتانة الإدراك البصري. توفر الصور الاصطناعية مثل <span class="nodecor">ImageNet-C</span>، <span class="nodecor">ImageNet-9</span> و<span class="nodecor">Stylized ImageNet</span> نوعاً محدّداً من التقييم للتشوهات الاصطناعية والخلفيات والملمس، لكنّ هذه المعايير تظل محدودة في تنوع التشوهات وجودتها الاصطناعية منخفضة الواقعية. في هذا العمل، نقدم نموذجاً توليدياً كمصدر بيانات لإنشاء صور صعبة تقيس متانة النماذج العميقة. من خلال الاستعانة بنماذج الانتشار، نستطيع توليد صور ذات خلفيات وملمس ومواد أكثر تنوّعاً من أي عمل سابق، لذا سمّينا هذا المعيار <span class="nodecor">ImageNet-D</span>. تُظهر النتائج التجريبية أن <span class="nodecor">ImageNet-D</span> يُسفر عن انخفاض كبير في الدقة عبر مجموعة من نماذج الرؤية، بدءاً من مصنف <span class="nodecor">ResNet</span> القياسي ووصولاً إلى النماذج الأساسية الأحدث مثل <span class="nodecor">CLIP</span> و<span class="nodecor">MiniGPT-4</span>، مع انخفاض يصل إلى <span class="nodecor">60%</span>. يشير عملنا إلى أن نماذج الانتشار يمكن أن تكون مصدراً فعالاً لاختبار نماذج الرؤية. الكود ومجموعة البيانات متاحة على GitHub للمزيد من التوثيق والتنزيل.</p>
<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>لقد حققت الشبكات العصبية أداءً ملحوظاً في مهام تتراوح من تصنيف الصور (<span class="nodecor">vaswani2017attention</span>, <span class="nodecor">liu2021swin</span>, <span class="nodecor">liu2022convnet</span>) إلى الإجابة على الأسئلة البصرية (<span class="nodecor">li2023blip</span>, <span class="nodecor">dai2023instructblip</span>, <span class="nodecor">liu2023visual</span>, <span class="nodecor">zhu2023minigpt</span>). وقد ألهمت هذه التطورات تطبيق الشبكات العصبية في مجالات متنوّعة، بما في ذلك الأنظمة الحرجة والآمنة مثل السيارات ذاتية القيادة (<span class="nodecor">kangsepp2022calibrated</span>, <span class="nodecor">nesti2023ultra</span>, <span class="nodecor">liu2023vectormapnet</span>) وكشف البرمجيات الخبيثة (<span class="nodecor">yuan2014droid</span>, <span class="nodecor">chen2019believe</span>, <span class="nodecor">pei2017deepxplore</span>) والروبوتات (<span class="nodecor">brohan2022rt</span>, <span class="nodecor">brohan2023rt</span>, <span class="nodecor">huang2023voxposer</span>). ونظراً لتوسّع استخدامها، أصبح من المهم بشكل متزايد تقييم متانة الشبكات العصبية (<span class="nodecor">ming2022delving</span>, <span class="nodecor">li2023distilling</span>) لأسباب تتعلق بالسلامة.</p>
<p>لتقييم المتانة، جمع <span class="nodecor">ObjectNet</span> (<span class="nodecor">barbu2019objectnet</span>) صوراً حقيقية لأشياء مع التحكم بعوامل مثل الخلفية بواسطة عمال بشريين، وهو ما يستغرق وقتاً طويلاً وجهداً كبيراً. ولزيادة فعالية الجمع، طُرحت الصور الاصطناعية كمعايير اختبار (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">hendrycks2019benchmarking</span>, <span class="nodecor">xiao2020noise</span>). على سبيل المثال، يقدم <span class="nodecor">ImageNet-C</span> (<span class="nodecor">hendrycks2019benchmarking</span>) مجموعة من التشوهات البصرية منخفضة المستوى، مثل الضوضاء الغاوسية والطمس، لاختبار المتانة. يستخدم <span class="nodecor">ImageNet-9</span> (<span class="nodecor">xiao2020noise</span>) تقنية القص واللصق لخلط الخلفيات، لكن الصور تظهر غير واقعية. ينتج <span class="nodecor">Stylized-ImageNet</span> (<span class="nodecor">geirhos2018imagenet</span>) صوراً جديدة عبر نقل الأسلوب، لكنه لا يتحكم في عوامل على مستوى الصورة ككل مثل الخلفية.</p>
<p>في هذا العمل، نقدم <span class="nodecor">ImageNet-D</span>، مجموعة اختبار اصطناعية مولَّدة بواسطة نماذج الانتشار لمهمة التعرف على الأشياء. من خلال الاستفادة من قدرات نماذج الانتشار الرائدة (<span class="nodecor">rombach2022high</span>)، نُظهر أنه يمكن توجيه هذه النماذج نصياً لإنشاء صور اختبار واقعية تُؤدِّي إلى فشل نماذج الرؤية. وبفضل التوجيه النصي، يمكننا تنويع العوامل عالية المستوى في الصور خلافاً للتشوهات المحلية والنسيج في الأعمال السابقة، مما يوفر أبعاداً إضافية لتقييم المتانة.</p>
<p>لتعزيز صعوبة العينات، نحافظ بشكل انتقائي على الصور التي تُسبّب فشل نماذج الرؤية المختارة. تُبيّن نتائجنا أن الصور التي تُثير أخطاءً في نماذج معينة تنقل صعوبتها بشكل موثوق إلى نماذج أخرى لم تُختَبَر سابقاً. يؤدي ذلك إلى انخفاض ملحوظ في الدقة، حتى في النماذج الأساسية الحديثة مثل <span class="nodecor">MiniGPT-4</span> (<span class="nodecor">zhu2023minigpt</span>) و<span class="nodecor">LLaVa</span> (<span class="nodecor">liu2023visual</span>)، مما يشير إلى أن المجموعة تكشف عن نقاط ضعف شائعة في نماذج الرؤية.</p>
<p>تُظهر التصوّرات أن <span class="nodecor">ImageNet-D</span> يعزّز جودة الصورة بشكل كبير مقارنة بالمعايير الاصطناعية السابقة. يعمل <span class="nodecor">ImageNet-D</span> كأداة فعّالة لخفض الأداء وتقييم المتانة عبر نماذج مختلفة، من <span class="nodecor">ResNet-101</span> (انخفاض <span class="nodecor">55.02%</span>) و<span class="nodecor">ViT-L/16</span> (انخفاض <span class="nodecor">59.40%</span>) إلى <span class="nodecor">CLIP</span> (انخفاض <span class="nodecor">46.05%</span>)، وينتقل جيداً إلى نماذج لغة الرؤية الضخمة مثل <span class="nodecor">LLaVa</span> (انخفاض <span class="nodecor">29.67%</span>) و<span class="nodecor">MiniGPT-4</span> (انخفاض <span class="nodecor">16.81%</span>). يُعتَبَر نهجنا في استخدام النماذج التوليدية لتقييم المتانة منهجاً عاماً، ويُظهر إمكانيات كبيرة للتحسين مع تقدّم النماذج التوليدية.</p>
<h1 id="sec:related_work">الأعمال ذات الصلة</h1>
<p><strong>متانة الشبكات العصبية.</strong> تحوّلت الشبكات العصبية من الشبكات الالتفافية (<span class="nodecor">CNN</span>) (<span class="nodecor">he2016deep, huang2017densely</span>) وشبكات التحويل البصري (<span class="nodecor">ViT</span>) (<span class="nodecor">vaswani2017attention, liu2021swin</span>) إلى النماذج الأساسية الكبيرة (<span class="nodecor">bommasani2021opportunities, devlin2018bert, touvron2023llama</span>). تناولت الأعمال السابقة متانة الشبكات من عدة جوانب، مثل الأمثلة العدائية (<span class="nodecor">mao2022understanding, mahmood2021robustness, madry2017towards, zhao2023evaluating, zhang2019theoretically</span>) وعينات خارج النطاق (<span class="nodecor">MAE, mao2021discrete, hendrycks2021many, augmix</span>). كما أظهرت النماذج الأساسية متانة أكبر على العينات خارج التوزيع (<span class="nodecor">radford2021learning</span>), واستُدرِس التفسير القوي أيضاً (<span class="nodecor">mao2023doubly, liu2023visual, zhu2023minigpt</span>). لتقييم المتانة بشكل منهجي، نحتاج إلى مجموعات اختبار تغطي عوامل متعددة.</p>
<p><strong>مجموعات بيانات لتقييم المتانة.</strong> تستخدم الدراسات صوراً من الإنترنت مثل <span class="nodecor">ImageNet-A</span> (<span class="nodecor">hendrycks2021natural</span>), <span class="nodecor">Imagenet-R</span> (<span class="nodecor">hendrycks2021many</span>) و<span class="nodecor">ImageNet-Sketch</span> (<span class="nodecor">wang2019learning</span>), لكنها محدودة بما هو متاح على الويب. يجمع <span class="nodecor">ObjectNet</span> (<span class="nodecor">barbu2019objectnet</span>) الصور يدوياً بواسطة آلاف العاملين، مما يستغرق وقتاً وجهداً كبيرين.</p>
<p>لتجاوز قيود الصور من الويب وتقليل تكلفة الجمع اليدوي، طُرحت الصور الاصطناعية لتقييم المتانة (<span class="nodecor">geirhos2018imagenet, hendrycks2019benchmarking, xiao2020noise</span>). يقيس <span class="nodecor">ImageNet-C</span> (<span class="nodecor">hendrycks2019benchmarking</span>) المتانة عبر تشوهات منخفضة المستوى. يُولّد <span class="nodecor">ImageNet-9</span> (<span class="nodecor">xiao2020noise</span>) صوراً بدمج الخلفية والمقدمة من صور مختلفة، لكنه محدود جودةً. يغيّر <span class="nodecor">Stylized-ImageNet</span> (<span class="nodecor">geirhos2018imagenet</span>) نسيج الصور باستخدام نقل الأسلوب (<span class="nodecor">AdaIN</span>), لكنه لا يتحكم في عوامل أخرى مثل الخلفية. في هذا العمل، نقدم مجموعة اختبار جديدة <span class="nodecor">ImageNet-D</span>، المولَّدة بواسطة نماذج الانتشار وتشتمل على صور بخلفيات وأنسجة ومواد متنوعة.</p>
<p><strong>توليد الصور.</strong> حققت نماذج الانتشار نجاحاً باهراً في مهام متعددة منها توليد الصور (<span class="nodecor">saharia2022photorealistic, ramesh2022hierarchical, ruiz2023dreambooth, zhang2023text</span>). على وجه الخصوص، يمكن لـ <span class="nodecor">Stable Diffusion</span> (<span class="nodecor">rombach2022high</span>) توليد صور عالية الدقة وفقاً لتوجيه نصي. يقدم <span class="nodecor">InstructPix2Pix</span> (<span class="nodecor">brooks2023instructpix2pix</span>) تحكماً أدق من خلال تعديل صورة معينة حسب تعليمات بشرية. في هذه الورقة، بنينا خط أنابيبنا باستخدام نموذج <span class="nodecor">Stable Diffusion</span> القياسي، رغم أن خوارزميتنا تتوافق مع نماذج توليدية أخرى قابلة للتوجيه باللغة.</p>
<p><strong>تعزيز الإدراك باستخدام صور الانتشار.</strong> استُخدمت الصور المولَّدة بنماذج الانتشار لتعزيز مهام إدراك الرؤية. يحسن بعض الأبحاث (<span class="nodecor">yuan2023not, bansal2023leaving, azizi2023synthetic, tian2023stablerep</span>) دقة التصنيف باستخدام الصور الاصطناعية كتوسيع لبيانات التدريب. بينما يكشف <span class="nodecor">DREAM-OOD</span> (<span class="nodecor">du2023dream</span>) الشواذ عبر فك تشفير العينات الكامنة إلى صور. إلا أن نهجهم يفتقر إلى التحكم الدقيق في فضاء الصور، وهو أمر حاسم لمعايير مثل <span class="nodecor">ImageNet-D</span>. كما يحدد (<span class="nodecor">metzen2023identification</span>) أزواج السمات غير ممثلة جيداً، بينما يركّز بحثنا على استخراج الصور الصعبة لكل سمة. بخلاف (<span class="nodecor">li2023imagenet, vendrow2023dataset, prabhu2023lance</span>) الذين يعدّلون مجموعات حالية، يولد عملنا صوراً جديدة ويختار الأصعب منها كمجموعة اختبار، مما يحقق انخفاضاً أكبر في الدقة.</p>
<h1 id="sec:imagenet_d">ImageNet-D</h1>
<p>نستعرض أولاً كيفية إنشاء <span class="nodecor">ImageNet-D</span> في القسم [sec:dataset_design]، ثم نظرة عامة على إحصائياته في القسم [sec:statistics].</p>
<h2 id="sec:dataset_design">تصميم مجموعة البيانات</h2>
<p>بينما تتفوق الشبكات العصبية في تطبيقات عدة، تحتاج متانتها إلى تقييم دقيق لأسباب تتعلق بالسلامة. التقييمات التقليدية تستخدم مجموعات اختبار موجودة، تشمل صوراً طبيعية (<span class="nodecor">barbu2019objectnet</span>, <span class="nodecor">hendrycks2021natural</span>) أو اصطناعية (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">hendrycks2019benchmarking</span>, <span class="nodecor">xiao2020noise</span>). مقارنةً بجمع الصور يدوياً، يعد جمع مجموعة اختبار اصطناعية أكثر كفاءة (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">xiao2020noise</span>). مع ذلك، يقين تنوع هذه المجموعات الاصطناعية محدود بسبب اعتمادها على الصور الحالية لاستخراج السمات، وهي كذلك تفتقر إلى الطابع الواقعي كما في الشكل [fig:test_set_comparison]. يهدف <span class="nodecor">ImageNet-D</span> إلى تقييم متانة النموذج عبر مجموعات متنوعة من الفئات والعوامل الطارئة، لمعالجة هذه القيود.</p>
<p><strong>توليد الصور بنماذج الانتشار.</strong> لبناء <span class="nodecor">ImageNet-D</span>، نستخدم نماذج الانتشار لإنشاء مجموعة ضخمة من الصور بدمج جميع فئات الأشياء والعوامل الطارئة، مما يتيح توليد صور عالية الدقة بناءً على مدخلات نصية محدّدة. نعتمد نموذج <span class="nodecor">Stable Diffusion</span> (<span class="nodecor">rombach2022high</span>) للتوليد، لكن نهجنا يتوافق مع نماذج أخرى قابلة للتوجيه النصي. تصاغ عملية التوليد كما يلي: <span class="math display">\[\text{Image}(C, N)  = \text{Stable Diffusion}(\text{Prompt}(C,N)),
    \label{eq:image_generation}\]</span> حيث يشير كل من <span class="math inline">\(C\)</span> و<span class="math inline">\(N\)</span> إلى فئة الشيء والعامل الطارئ على التوالي، ويشمل العامل الطارئ الخلفية والمادة والملمس. يوضح الجدول [tab:prompt_list] العوامل والإعدادات. باستخدام فئة الحقائب مثالاً، نولد صوراً لحقيبة في حقل قمح، وغرف خشبية، وما إلى ذلك، مما يوفر تنوعاً أوسع من المجموعات الحالية. تُصنَّف الصورة حسب فئة <span class="math inline">\(C\)</span> كحقيقة أساسية وتعتبر مصنّفة خاطئة إذا لم يتطابق تصنيف النموذج مع الحقيقة الأساسية.</p>
<p>بعد إنشاء مجموعة كبيرة لكل أزواج الفئات والعوامل، نقيم نموذج <span class="nodecor">CLIP</span> (ViT-L/14) على هذه الصور في الجدول [tab:vanilla_generation]. التفاصيل في القسم [sec:experimental_setup]. يبين الجدول أن CLIP يحقق دقة عالية تقريباً (<span class="nodecor">94%</span>) على الصور الاصطناعية. لإنشاء مجموعة اختبار تحدّي، نقترح استراتيجية فعّالة لاستخراج العينات الصعبة استناداً إلى فشل مشترك.</p>
<p><strong>استخراج الصور الصعبة عبر الفشل المشترك.</strong> قبل شرح كيفية تحديد العينات الصعبة، نعرف مفهوم الفشل المشترك:</p>
<p><code>الفشل المشترك:</code> صورة تُعد فاشلة بشكل مشترك إذا أدت إلى تصنيف غير صحيح لعدة نماذج.</p>
<p>المجموعة الصعبة المثالية تشمل صوراً يفشل فيها جميع النماذج المختبرة، لكن ذلك غير عملي نظراً لعدم إمكانية الوصول إلى النماذج المستقبلية (النماذج الهدف). بدلاً من ذلك، نبني مجموعة الاختبار انطلاقاً من فشل النماذج البديلة المعروفة. إذا أدى فشل هذه النماذج إلى انخفاض دقة في النماذج الهدف غير المعروفة، نعتبر الفشل قابلاً للنقل:</p>
<p><code>الفشل القابل للنقل:</code> فشل النماذج البديلة المعروفة يعتبر قابلاً للنقل إذا أدّى أيضاً إلى دقة منخفضة في النماذج الهدف غير المعروفة.</p>
<p>لتقييم قابلية نقل الفشل، نقيم مجموعات اختبار مولّدة بفشل مشترك من 1 إلى 8 نماذج بديلة (الشكل [fig:filter_consistency]). كذلك نقيّم ثلاثة نماذج هدف لم تُستخدم في بناء مجموعة الاختبار: <span class="nodecor">CLIP</span> (ViT-B/16)، <span class="nodecor">LLaVa</span> و<span class="nodecor">MiniGPT-4</span>. يوضح الشكل أن دقة النموذج الهدف تنخفض كلما زاد عدد النماذج البديلة. وُلدت مجموعات الاختبار لكل من الخلفية والملمس والمواد وتظهر نفس الاتجاه.</p>
<h1 id="التحكم-بالجودة-بواسطة-التدخل-البشري">التحكم بالجودة بواسطة التدخل البشري</h1>
<p>توفر العملية السابقة اكتشافاً آلياً لمجموعة اختبار صعبة، لكن النماذج التوليدية قد تنتج صوراً لا تتطابق مع فئة الطلب. لذا، نلجأ إلى التعليق التوضيحي البشري لضمان صحة ووضوح صور <span class="nodecor">ImageNet-D</span>. بعد الجولة الأولى من التعليق بواسطة طلاب الدراسات العليا المتخصصين، نستخدم Amazon Mechanical Turk (<span class="nodecor">deng2009imagenet</span>, <span class="nodecor">recht2019imagenet</span>, <span class="nodecor">hendrycks2021many</span>) للتدقيق في جودة التسمية. نطلب من العمال اختيار الصور التي يمكنهم التعرف فيها على الكائن الرئيسي أو التي تظهر وظيفته الحقيقية كفئة الحقيقة الأرضية. كما صممنا حراساً لضمان استجابات دقيقة: الحراس الإيجابيون والسلب…
</body>
</html>