<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Chenshuang Zhang     Fei Pan    Junmo Kim     In So Kweon       Chengzhi Mao KAIST^{1}, University of Michigan, Ann Arbor^{2}, McGill University^{3}, MILA^{4}">
  <title>مِعْيار ImageNet-D: قِياسُ مَتانَةِ الشَبَكاتِ العَصَبِيَّةِ عَلَى الأَجْسامِ الاِصْطِناعِيَّةِ بِالاِنْتِشارِ</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">مِعْيار <span class="nodecor">ImageNet-D</span>: قِياسُ مَتانَةِ الشَبَكاتِ العَصَبِيَّةِ عَلَى الأَجْسامِ الاِصْطِناعِيَّةِ بِالاِنْتِشارِ</h1>
<p class="author"><span class="nodecor">Chenshuang Zhang</span>     <span class="nodecor">Fei Pan</span>    <span class="nodecor">Junmo Kim</span>     <span class="nodecor">In So Kweon</span>       <span class="nodecor">Chengzhi Mao</span><br />
<span class="nodecor">KAIST</span><span class="math inline">\(^{1}\)</span>, <span class="nodecor">University of Michigan, Ann Arbor</span><span class="math inline">\(^{2}\)</span>, <span class="nodecor">McGill University</span><span class="math inline">\(^{3}\)</span>, <span class="nodecor">MILA</span><span class="math inline">\(^{4}\)</span><br /></p>
</header>
<p>latex</p>
<h1 id="ملخص">مُلَخَّص</h1>
<p>نُقَدِّم مَعايير صارِمة لِمَتانة الإِدراك البَصَري. تُوَفِّر الصُوَر الاِصْطِناعِيَّة مثل <span class="nodecor">ImageNet-C</span>، <span class="nodecor">ImageNet-9</span>، و<span class="nodecor">Stylized ImageNet</span> نوعاً مُحَدَّداً من التقييم على التلوثات الاصطناعية، والخلفيات، والقِوام، ولكن تلك المعايير لِلمتانة محدودة في التباينات المُحددة ولها جودة اصطناعية منخفضة. في هذا العمل، نُقَدِّم نموذجاً توليدياً كمصدر بيانات لتوليد صور صعبة تقيس متانة النماذج العميقة. من خلال استخدام نماذج الانتشار، نستطيع توليد صور بخلفيات وقِوام ومواد أكثر تنوعاً من أي عمل سابق، حيث نُطلق على هذا المعيار اسم <span class="nodecor">ImageNet-D</span>. تُظهِر النتائج التجريبية أن <span class="nodecor">ImageNet-D</span> يؤدي إلى انخفاض كبير في الدقة لمجموعة من نماذج الرؤية، من مصنف الرؤية <span class="nodecor">ResNet</span> القياسي إلى أحدث النماذج الأساسية مثل <span class="nodecor">CLIP</span> و<span class="nodecor">MiniGPT-4</span>، مما يقلل دقتها بنسبة تصل إلى <span class="nodecor">60%</span>. يُشير عملنا إلى أن نماذج الانتشار يمكن أن تكون مصدراً فعالاً لاختبار نماذج الرؤية. الشفرة ومجموعة البيانات متاحة على .</p>
<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>لقد حققت الشبكات العصبية أداءً ملحوظاً في مهام تتراوح من تصنيف الصور (<span class="nodecor">vaswani2017attention</span>, <span class="nodecor">liu2021swin</span>, <span class="nodecor">liu2022convnet</span>) إلى الإجابة على الأسئلة البصرية (<span class="nodecor">li2023blip</span>, <span class="nodecor">dai2023instructblip</span>, <span class="nodecor">liu2023visual</span>, <span class="nodecor">zhu2023minigpt</span>). وقد ألهمت هذه التقدمات تطبيق الشبكات العصبية في مجالات متنوعة، بما في ذلك الأنظمة الأمنية والحرجة مثل السيارات ذاتية القيادة (<span class="nodecor">kangsepp2022calibrated</span>, <span class="nodecor">nesti2023ultra</span>, <span class="nodecor">liu2023vectormapnet</span>)، وكشف البرمجيات الخبيثة (<span class="nodecor">yuan2014droid</span>, <span class="nodecor">chen2019believe</span>, <span class="nodecor">pei2017deepxplore</span>) والروبوتات (<span class="nodecor">brohan2022rt</span>, <span class="nodecor">brohan2023rt</span>, <span class="nodecor">huang2023voxposer</span>). ونظراً لتوسع استخدامها، أصبح من المهم بشكل متزايد تحديد متانة الشبكات العصبية (<span class="nodecor">ming2022delving</span>, <span class="nodecor">li2023distilling</span>) لأسباب تتعلق بالسلامة.</p>
<p>لتقييم متانة الشبكات العصبية، يجمع ObjectNet (<span class="nodecor">barbu2019objectnet</span>) صور الأشياء الواقعية على عوامل يمكن التحكم بها مثل الخلفية بواسطة العمال البشريين، وهو ما يستغرق وقتاً طويلاً ويتطلب جهداً كبيراً. لزيادة جمع البيانات، تم اقتراح الصور الاصطناعية كصور اختبار (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">hendrycks2019benchmarking</span>, <span class="nodecor">xiao2020noise</span>). على سبيل المثال، يقدم ImageNet-C (<span class="nodecor">hendrycks2019benchmarking</span>) مجموعة من التشوهات البصرية الشائعة منخفضة المستوى، مثل الضوضاء الغاوسية والضبابية، لاختبار متانة النماذج. يستخدم ImageNet-9 (<span class="nodecor">xiao2020noise</span>) تقنية القص واللصق البسيطة لإنشاء معيار للمتانة على خلفية الكائن، لكن الصور ليست واقعية. يُولِّد Stylized-ImageNet (<span class="nodecor">geirhos2018imagenet</span>) صوراً جديدة من خلال تغيير نسيج صور ImageNet، والتي لا يمكنها التحكم في العوامل العالمية مثل الخلفية.</p>
<p>في هذا العمل، نقدم ImageNet-D، مجموعة اختبار اصطناعية تم إنشاؤها بواسطة نماذج الانتشار لمهمة التعرف على الأشياء. من خلال الاستفادة من قدرات نماذج الانتشار الرائدة (<span class="nodecor">rombach2022high</span>)، نظهر أنه يمكننا توجيه نماذج الانتشار باللغة لإنشاء صور اختبار واقعية تتسبب في فشل نماذج الرؤية. ونظراً لاعتمادنا على اللغة لإنشاء الصور، يمكننا تنويع العوامل عالية المستوى في الصور على عكس التشوهات المحلية والنسيج في الأعمال السابقة، مما يوفر عوامل إضافية يمكن تقييم المتانة عليها.</p>
<p>لتعزيز صعوبة العينات في مجموعة البيانات الخاصة بنا، نحتفظ بشكل انتقائي بالصور التي تسببت في فشل نماذج الرؤية المختارة. تظهر نتائجنا أن الصور التي تثير الأخطاء في النماذج المختارة يمكن أن تنقل طبيعتها الصعبة بشكل موثوق إلى نماذج أخرى لم يتم اختبارها سابقاً. وهذا يؤدي إلى انخفاض ملحوظ في الدقة، حتى في النماذج الأساسية الحديثة مثل MiniGPT-4 (<span class="nodecor">zhu2023minigpt</span>) و LLaVa (<span class="nodecor">liu2023visual</span>)، مما يشير إلى أن مجموعة البيانات الخاصة بنا تكشف عن الفشل الشائع في نماذج الرؤية.</p>
<p>تُظهر التصورات أن ImageNet-D يعزز بشكل كبير جودة الصورة مقارنة بمعايير المتانة الاصطناعية السابقة. يعمل ImageNet-D كأداة فعالة لتقليل الأداء وتقييم متانة النموذج، بما في ذلك ResNet <span class="nodecor">101</span> (انخفاض <span class="nodecor">55.02</span>%)، ViT-L/<span class="nodecor">16</span> (انخفاض <span class="nodecor">59.40</span>%)، CLIP (انخفاض <span class="nodecor">46.05</span>%)، وينتقل جيداً إلى نماذج لغة الرؤية الكبيرة غير المتوقعة مثل LLaVa (<span class="nodecor">liu2023visual</span>) (انخفاض <span class="nodecor">29.67</span>%)، و MiniGPT-4 (<span class="nodecor">zhu2023minigpt</span>) (انخفاض <span class="nodecor">16.81</span>%). يُعتَبَر نهجنا في استخدام النماذج التوليدية لتقييم متانة النموذج عاماً، ويُظهِر إمكانية كبيرة لفعالية أكبر مع التقدم المستقبلي في النماذج التوليدية.</p>
<h1 id="sec:related_work">الأَعْمال ذات الصِلَة</h1>
<p><strong>مَتانة الشبكات العصبية.</strong> تطورت الشبكات العصبية من شبكات الالتفاف العصبي (<span class="nodecor">CNN</span>) (<span class="nodecor">he2016deep, huang2017densely</span>)، وشبكات التحويل البصري (<span class="nodecor">ViT</span>) (<span class="nodecor">vaswani2017attention, liu2021swin</span>)، إلى النماذج الأساسية الكبيرة (<span class="nodecor">bommasani2021opportunities, devlin2018bert, touvron2023llama</span>). وقد تناولت الأعمال السابقة متانة الشبكات العصبية من عدة جوانب، مثل الأمثلة المعادية (<span class="nodecor">mao2022understanding, mahmood2021robustness, madry2017towards, zhao2023evaluating, zhang2019theoretically</span>) وعينات خارج النطاق (<span class="nodecor">MAE, mao2021discrete, hendrycks2021many, augmix</span>). وقد أظهرت النماذج الأساسية متانة أكبر على عينات خارج التوزيع (<span class="nodecor">radford2021learning</span>). كما تم التحقيق في التفسير القوي أيضاً (<span class="nodecor">mao2023doubly, liu2023visual, zhu2023minigpt</span>). لتقييم متانة النماذج العميقة بشكل منهجي، من الضروري وجود مجموعات اختبار تغطي عوامل مختلفة.</p>
<p><strong>مجموعات بيانات لتقييم المتانة.</strong> لتقييم متانة الشبكات العصبية، يستخدم فرع من الدراسات صوراً من الإنترنت، بما في ذلك <span class="nodecor">ImageNet-A</span> (<span class="nodecor">hendrycks2021natural</span>), <span class="nodecor">Imagenet-R</span> (<span class="nodecor">hendrycks2021many</span>) و<span class="nodecor">ImageNet-Sketch</span> (<span class="nodecor">wang2019learning</span>). ومع ذلك، فهي محدودة بالصور الموجودة على الويب. <span class="nodecor">ObjectNet</span> (<span class="nodecor">barbu2019objectnet</span>) يجمع الصور يدوياً بمساعدة 5982 عاملاً، وهو ما يستغرق وقتاً طويلاً ويتطلب موارد كبيرة.</p>
<p>للتغلب على قيود الصور من الويب وتقليل تكلفة الجمع اليدوي، تم اقتراح الصور الاصطناعية لتقييم المتانة (<span class="nodecor">geirhos2018imagenet, hendrycks2019benchmarking, xiao2020noise</span>). <span class="nodecor">ImageNet-C</span> (<span class="nodecor">hendrycks2019benchmarking</span>) يقيم متانة النموذج على التلفيات منخفضة المستوى. <span class="nodecor">ImageNet-9</span> (<span class="nodecor">xiao2020noise</span>) يُولِّد صوراً جديدة بدمج الخلفية والمقدمة من صور مختلفة، لكنه محدود بضعف جودة الصورة. <span class="nodecor">Stylized-ImageNet</span> (<span class="nodecor">geirhos2018imagenet</span>) يغير نسيج صور <span class="nodecor">ImageNet</span> باستخدام نقل أسلوب <span class="nodecor">AdaIN</span> (<span class="nodecor">huang2017arbitrary</span>) أو بإدخال تعارض بين النسيج والشكل، والذي لا يمكنه التحكم في عوامل أخرى مثل الخلفيات. في هذا العمل، نقدم مجموعة اختبار جديدة <span class="nodecor">ImageNet-D</span>، والتي يتم توليدها بالتحكم في نماذج الانتشار وتشمل صوراً جديدة مع خلفيات وأنسجة ومواد متنوعة.</p>
<p><strong>توليد الصور.</strong> حققت نماذج الانتشار نجاحاً كبيراً في مهام متنوعة بما في ذلك توليد الصور (<span class="nodecor">saharia2022photorealistic, ramesh2022hierarchical, ruiz2023dreambooth, zhang2023text</span>). كعمل رائد، يمكن <span class="nodecor">Stable Diffusion</span> (<span class="nodecor">rombach2022high</span>) من توليد صور عالية الدقة يتم التحكم فيها بواسطة اللغة. <span class="nodecor">InstructPix2Pix</span> (<span class="nodecor">brooks2023instructpix2pix</span>) يوفر تحكماً أكثر تعقيداً من خلال تعديل صورة معينة وفقاً لتعليمات بشرية. في هذه الورقة، نبني خط أنابيبنا باستخدام نموذج <span class="nodecor">Stable Diffusion</span> القياسي، ومع ذلك، فإن خوارزميتنا متوافقة مع نماذج توليدية أخرى يمكن توجيهها باللغة.</p>
<p><strong>تعزيز الإدراك باستخدام صور الانتشار.</strong> تم استخدام الصور المولدة بالانتشار لمهام إدراك الرؤية. فرع من الدراسات (<span class="nodecor">yuan2023not, bansal2023leaving, azizi2023synthetic, tian2023stablerep</span>) يحسن دقة التصنيف باستخدام الصور الاصطناعية كتوسيع لبيانات التدريب. <span class="nodecor">DREAM-OOD</span> (<span class="nodecor">du2023dream</span>) يكتشف القيم الشاذة من خلال فك تشفير العينات الكامنة المستخلصة إلى صور. ومع ذلك، فإن طريقتهم تفتقر إلى التحكم المحدد في فضاء الصور، وهو أمر حاسم لمعايير مثل <span class="nodecor">ImageNet-D</span>. (<span class="nodecor">metzen2023identification</span>) يحدد أزواج السمات غير الممثلة بشكل كافٍ، بينما يركز بحثنا على الصور الصعبة ذات السمة الواحدة. على عكس (<span class="nodecor">li2023imagenet, vendrow2023dataset, prabhu2023lance</span>) الذين يعدلون مجموعات البيانات الحالية، يولد عملنا صوراً جديدة ويستخرج الأكثر تحدياً كمجموعة اختبار، مما يحقق انخفاضاً أكبر في الدقة مقارنة بـ (<span class="nodecor">li2023imagenet, vendrow2023dataset, prabhu2023lance</span>).</p>
<h1 id="sec:imagenet_d">ImageNet-D</h1>
<p>نقدم أولاً كيفية إنشاء ImageNet-D في القسم [sec:dataset_design]، يليه نظرة عامة على إحصائياته في القسم [sec:statistics].</p>
<h2 id="sec:dataset_design">تَصْمِيم مجموعة البيانات</h2>
<p>بينما تتفوق الشبكات العصبية في تطبيقات متعددة، فإن متانتها تحتاج إلى تقييم دقيق للسلامة. التقييمات التقليدية تستخدم مجموعات اختبار موجودة، تشمل إما صوراً طبيعية (<span class="nodecor">barbu2019objectnet</span>, <span class="nodecor">hendrycks2021natural</span>) أو صوراً اصطناعية (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">hendrycks2019benchmarking</span>, <span class="nodecor">xiao2020noise</span>). مقارنة بجمع الصور يدوياً، فإن جمع مجموعة اختبار اصطناعية أكثر كفاءة (<span class="nodecor">geirhos2018imagenet</span>, <span class="nodecor">xiao2020noise</span>). ومع ذلك، فإن تنوع مجموعات الاختبار الاصطناعية الحالية محدود بسبب اعتمادها على الصور الموجودة لاستخراج السمات. هذه الصور الاصطناعية ليست واقعية أيضاً، كما هو موضح في الشكل [fig:test_set_comparison]. يتم تقديم ImageNet-D لتقييم متانة النموذج عبر مجموعات متنوعة من الأشياء والمتغيرات الطارئة، لمعالجة هذه القيود.</p>
<p><strong>توليد الصور بواسطة نماذج الانتشار.</strong> لبناء ImageNet-D، يتم استخدام نماذج الانتشار لإنشاء مجموعة ضخمة من الصور عن طريق دمج جميع الأشياء الممكنة والمتغيرات الطارئة، مما يتيح توليد صور عالية الدقة بناءً على مدخلات نصية محددة من المستخدم. نستخدم نموذج الانتشار المستقر (<span class="nodecor">rombach2022high</span>) لتوليد الصور، بينما يتوافق نهجنا مع نماذج توليدية أخرى يمكن توجيهها باللغة. يتم صياغة عملية توليد الصور على النحو التالي: <span class="math display">\[\text{Image}(C, N)  = \text{Stable Diffusion}(\text{Prompt}(C,N)),
    \label{eq:image_generation}\]</span> حيث يشير <span class="math inline">\(C\)</span> و <span class="math inline">\(N\)</span> إلى فئة الشيء والمتغير الطارئ، على التوالي. المتغير الطارئ <span class="math inline">\(N\)</span> يشمل الخلفية، المادة، والملمس في هذا العمل. الجدول [tab:prompt_list] يقدم نظرة عامة على المتغيرات الطارئة والمطالبات لنماذج الانتشار. باستخدام فئة الحقائب كمثال، نولد أولاً صوراً للحقائب مع خلفيات متنوعة، ومواد، وملمس (مثلاً، حقيبة في حقل قمح)، مما يوفر مجموعة أوسع من المجموعات مقارنة بمجموعات الاختبار الحالية. يتم تصنيف كل صورة بفئة المطالبة <span class="math inline">\(C\)</span> كحقيقة أساسية للتصنيف. تعتبر الصورة مصنفة بشكل خاطئ إذا لم يتطابق تصنيف النموذج المتوقع مع الحقيقة الأساسية <span class="math inline">\(C\)</span>.</p>
<p>بعد إنشاء مجموعة كبيرة من الصور مع جميع أزواج فئات الأشياء والمتغيرات الطارئة، نقوم بتقييم نموذج CLIP (ViT-L/14) على هذه الصور في الجدول [tab:vanilla_generation]. يتم الإبلاغ عن التفاصيل التجريبية في القسم [sec:experimental_setup]. الجدول [tab:vanilla_generation] يُظهِر أن CLIP يحقق دقة عالية على جميع مجموعات الاختبار، بدقة حوالي <span class="nodecor">94%</span> على مجموعة الصور الاصطناعية. لإنشاء مجموعة اختبار تحدي لتقييم المتانة، نقترح استراتيجية فعالة للعثور على العينات الصعبة من جميع الصور المولدة على النحو التالي.</p>
<p><strong>استخراج الصور الصعبة مع فشل الإدراك المشترك.</strong> قبل تقديم كيفية تحديد العينات الصعبة من مجموعة الصور الاصطناعية، نعرّف أولاً مفهوم فشل الإدراك المشترك على النحو التالي.</p>
<p><code>الفشل المشترك:</code> صورة تُعتَبَر فشلاً مشتركاً إذا أدت إلى تنبؤ عدة نماذج بتصنيف الشيء بشكل غير صحيح.</p>
<p>يجب أن تشمل مجموعة الاختبار الصعبة المثالية صوراً تفشل جميع النماذج المختبرة في تحديدها. ومع ذلك، فإن هذا غير عملي بسبب عدم إمكانية الوصول إلى نماذج الاختبار المستقبلية، المسماة بالنموذج الهدف. بدلاً من ذلك، نقوم ببناء مجموعة الاختبار باستخدام فشل النماذج البديلة المعروفة. إذا أدى فشل النماذج البديلة إلى دقة منخفضة في النماذج غير المعروفة، فإن مجموعة الاختبار تعتبر تحدياً. يتم تعريف هذا على أنه فشل قابل للنقل على النحو التالي:</p>
<p><code>الفشل القابل للنقل:</code> فشل النماذج البديلة المعروفة قابل للنقل إذا أدى أيضاً إلى دقة منخفضة للنماذج الهدف غير المعروفة.</p>
<p>لتقييم ما إذا كان فشل النماذج البديلة المعروفة قابلاً للنقل للصور المولدة بالانتشار، نقوم بتقييم مجموعات الاختبار المنشأة بفشل مشترك من <span class="nodecor">1</span> إلى <span class="nodecor">8</span> نماذج بديلة في الشكل [fig:filter_consistency]. نقوم بتقييم دقة ثلاثة نماذج هدف لم تُستخدم أثناء بناء مجموعة الاختبار، بما في ذلك CLIP(ViT-B/16), LLaVa، و MiniGPT-4. الشكل [fig:filter_consistency] يُظهِر أن دقة النموذج الهدف تقل كلما زاد عدد النماذج البديلة المستخدمة. تم إنشاء مجموعات اختبار مع خلفيات متنوعة، بينما تظهر التجارب للملمس والمادة نفس الاتجاه.</p>
<h1 id="التحكم-بالجودة-بواسطة-التدخل-البشري">التَحَكُّم بالجودة بواسطة التدخل البشري</h1>
<p>تتيح لنا العملية المذكورة أعلاه العثور تلقائياً على مجموعة اختبار صعبة للنماذج غير المرئية. ومع ذلك، يمكن للنماذج التوليدية أن تنتج صوراً غير صحيحة لا تتطابق مع فئة المطالبة. نلجأ إلى التعليق التوضيحي البشري لضمان أن تكون صور ImageNet-D صالحة ومن فئة واحدة وعالية الجودة في الوقت نفسه. بعد الجولة الأولى من التعليق التوضيحي بواسطة طلاب الدراسات العليا، نستخدم Amazon Mechanical Turk (<span class="nodecor">deng2009imagenet</span>, <span class="nodecor">recht2019imagenet</span>, <span class="nodecor">hendrycks2021many</span>) لتقييم جودة التسمية. نطلب من العمال اختيار الصور التي يمكنهم إما التعرف على الكائن الرئيسي أو يمكن استخدام الكائن الرئيسي وظيفياً كفئة الحقيقة الأرضية. بالإضافة إلى ذلك، نصمم حراساً لضمان استجابات عالية الجودة، بما في ذلك الحراس الإيجابيين والسلبيين والمتسقين. نقدم تفاصيل مهمة التسمية في الملحق. شارك ما مجموعه <span class="nodecor">679</span> عاملاً مؤهلاً في <span class="nodecor">1540</span> مهمة تسمية، محققين اتفاقاً بنسبة <span class="nodecor">91.09</span>%.</p>
<h2 id="sec:statistics">إحصائيات قاعدة البيانات</h2>
<p>تتضمن ImageNet-D 113 فئة متداخلة بين ImageNet وObjectNet، و547 مرشحاً للعوامل المؤثرة من مجموعة بيانات Broden (<span class="nodecor">bau2017network</span>) (انظر الجدول [tab:prompt_list])، مما ينتج عنه 4835 صورة صعبة تتميز بخلفيات متنوعة (<span class="nodecor">3764</span>)، وأنسجة (<span class="nodecor">498</span>)، ومواد (<span class="nodecor">573</span>). تعتبر عملية إنشاء ImageNet-D عامة وفعالة، مما يسمح بإضافة فئات وعوامل مؤثرة جديدة بسهولة. يُظهر توزيع فئات ImageNet-D نمطاً طبيعياً طويل الذيل، كما هو موضح في الشكل [fig:hist_category]. يُظهر التوزيع النادر وغير المنتظم لفئات الصفات في الشكل [fig:heatmap] ضرورة استنفاد جميع أزواج الفئات والعوامل المؤثرة في إنشاء مجموعة الاختبار.</p>
<h1 id="sec:benchmark">التجارب</h1>
<p>نقوم بتقييم نماذج مختلفة على معيار ImageNet-D. نجد أن ImageNet-D يقلل بشكل كبير من دقة جميع النماذج بما يصل إلى <span class="nodecor">60%</span>. ثم نعرض ما إذا كانت التطورات السابقة يمكن أن تحسن من قوة ImageNet-D، مثل توسيع البيانات. أخيراً، نناقش ImageNet-D من جوانب مختلفة، مثل استرجاع الجار الأقرب.</p>
<h2 id="sec:experimental_setup">إعدادات التجربة</h2>
<p><strong>إعدادات بناء مجموعة الاختبار.</strong> نستخدم الانتشار الثابت (<span class="nodecor">Stable Diffusion</span>) من (<span class="nodecor">rombach2022high</span>) لإنشاء <span class="nodecor">ImageNet-D</span>، ونعتمد على الوزن المدرب مسبقاً للإصدار <span class="nodecor">stable-diffusion-2-1</span> من <span class="nodecor">Hugging Face</span>. للعثور على الصور الصعبة، ننهي <span class="nodecor">ImageNet-D</span> بالفشل المشترك لأربعة نماذج بديلة، بما في ذلك <span class="nodecor">CLIP</span> (<span class="nodecor">radford2021learning</span>) (<span class="nodecor">ViT-L/14</span>, <span class="nodecor">ViT-L/14-336px</span> و <span class="nodecor">ResNet50</span>)، ونموذج الرؤية (<span class="nodecor">ResNet50</span> (<span class="nodecor">he2016deep</span>)). تشمل مجموعة المرشحين للنماذج البديلة في الشكل المحذوف أيضاً <span class="nodecor">CLIP</span> (<span class="nodecor">Resnet101</span>,<span class="nodecor">ViT-B/32</span>) ونموذج الرؤية (<span class="nodecor">ViT-L/16</span> (<span class="nodecor">dosovitskiy2010image</span>) و <span class="nodecor">VGG16</span> (<span class="nodecor">simonyan2014very</span>)).</p>
<p><strong>تقييم نماذج التصنيف.</strong> يُقاس الثبات على <span class="nodecor">ImageNet-D</span> بدقة الأعلى-1 في التعرف على الأشياء، نسبة الصور المصنفة بشكل صحيح إلى إجمالي الصور. نقوم بتقييم نماذج التصنيف باستخدام الأوزان المدربة مسبقاً المفتوحة المصدر. بالنسبة لـ <span class="nodecor">CLIP</span> (<span class="nodecor">radford2021learning</span>)، نتبع الورقة الأصلية (<span class="nodecor">radford2021learning</span>) لاعتماد <em>صورة لـ <span class="math inline">\(\left[ \text{category} \right]\)</span></em> كقالب نصي. يتم الإبلاغ عن دقة الصفر لـ <span class="nodecor">CLIP</span>.</p>
<p><strong>تقييم نماذج الإجابة على الأسئلة البصرية (<span class="nodecor">VQA</span>).</strong> نقيم دقة نماذج <span class="nodecor">VQA</span> الحديثة المفتوحة المصدر على <span class="nodecor">ImageNet-D</span>، بما في ذلك <span class="nodecor">LLaVa</span> (<span class="nodecor">liu2023visual</span>)، و <span class="nodecor">MiniGPT-4</span> (<span class="nodecor">zhu2023minigpt</span>). بناءً على مدخلات الصورة، تخرج نماذج <span class="nodecor">VQA</span> الإجابات استناداً إلى مطالبة النص المدخل. ومع ذلك، لا يقتصر الإخراج النصي لنماذج <span class="nodecor">VQA</span> على قالب معين، وبالتالي قد لا يتضمن اسم الفئة في قائمة الفئات المحددة مسبقاً لمهام التعرف على الأشياء. هذا يجعل من الصعب تقييم الدقة استناداً إلى الإجابات المتنوعة.</p>
<p>المطالبة الشائعة التي تطلب من نماذج <span class="nodecor">VQA</span> التعرف على الكائن هي: <code>ما هو الكائن الرئيسي في هذه الصورة؟</code> لجعل نماذج <span class="nodecor">VQA</span> تختار من قائمة الفئات المحددة مسبقاً، نسأل نماذج <span class="nodecor">VQA</span> كما يلي: <code>ما هو الكائن الرئيسي في هذه الصورة؟ اختر من القائمة التالية: \left[ \text{GT category} \right], \left[ \text{failure category} \right]</code>. تشير فئة <span class="nodecor">GT</span> إلى فئة الحقيقة الأرضية للصورة. أما بالنسبة لفئة الفشل، فإننا نعتمد الفئة التي تحقق أعلى ثقة <span class="nodecor">CLIP</span> (<span class="nodecor">ViT-L/14</span>) بين جميع الفئات الخاطئة. مع هذه المطالبة، نجد أن كلاً من <span class="nodecor">MiniGPT-4</span> و <span class="nodecor">LLaVa</span> يمكنهما اختيار من قائمة الفئات المقدمة في إخراجهما. إذا اختار النموذج فئة الحقيقة الأرضية، يعتبر هذا الكائن معترفاً به بشكل صحيح. لذلك، يمكننا حساب دقة نماذج <span class="nodecor">VQA</span>.</p>
<h2 id="sec:experimental_results">تقييم المتانة</h2>
<p><strong>النتائج الكمية.</strong> نقوم بتقييم ImageNet-D على <span class="nodecor">25</span> نموذجاً، ونرسم اتجاه دقة الاختبار في الشكل [fig:main_result_figure]. المحور الأفقي والمحور العمودي يشيران إلى دقة الاختبار على ImageNet وImageNet-D على التوالي. يُظهر الشكل [fig:main_result_figure] أنه كلما زادت دقة ImageNet، زادت دقة ImageNet-D أيضاً. دقة ImageNet-D أقل بكثير من دقة ImageNet لجميع النماذج، كما يتضح من التوزيع الأدنى أسفل خط الإشارة <span class="math inline">\(y=x\)</span>. نبلغ عن دقة <span class="nodecor">14</span> نموذجاً على مجموعات اختبار مختلفة في الجدول [tab:benchmark_results]، ودقة جميع النماذج في الملحق. يُظهر الجدول [tab:benchmark_results] أن ImageNet-D يحقق أدنى دقة اختبار لجميع النماذج، باستثناء النتيجة المماثلة على Stylized-ImageNet لنماذج VQA. لاحظ أن ImageNet-D يحقق جودة صورة أعلى من Stylized-ImageNet كما هو موضح في الشكل [fig:test_set_comparison]. على الرغم من أن ObjectNet يغير العديد من السمات لكل صورة، إلا أنه لا يزال يؤدي إلى دقة أعلى من ImageNet-D التي تحدد سمة واحدة فقط لكل صورة. مقارنة بـ ImageNet، يؤدي ImageNet-D إلى انخفاض في دقة الاختبار بأكثر من <span class="nodecor">16%</span> لجميع النماذج، بما في ذلك LLaVa (انخفاض <span class="nodecor">29.67%</span>) وMiniGPT-4 (انخفاض <span class="nodecor">16.81%</span>).</p>
<h1 id="نتائج-التصور">نتائج التصور</h1>
<p>يعرض ImageNet-D أمثلة صور تُظهِر جودة عالية. على الرغم من أن البشر يمكنهم التعرف بسهولة على الكائن الرئيسي، إلا أن CLIP (ViT-L/14) يصنف هذه الصور بشكل خاطئ في فئة غير صحيحة. يظهر أن MiniGPT-4 و (<span class="nodecor">LLaVa-1.5</span>) يمكن أن يفشلا أيضاً في التعرف على الكائن الرئيسي من صور ImageNet-D.</p>
<h2 id="sec:robustness_improvement">تحسين المتانة</h2>
<p><strong>توسيع البيانات.</strong> تكشف الدراسات السابقة أن توسيع البيانات فعال لتحسين متانة النموذج، كما هو الحال في ImageNet-C (<span class="nodecor">hendrycks2019benchmarking</span>). نقوم بتقييم طرق توسيع البيانات على ImageNet-D، بما في ذلك SIN (<span class="nodecor">geirhos2018imagenet</span>)، AugMix (<span class="nodecor">hendrycks2019augmix</span>)، ANT (<span class="nodecor">rusak2020simple</span>) و DeepAugment (<span class="nodecor">hendrycks2021many</span>). يُظهر الجدول [tab:result_augmentation] النتائج باستخدام العمود الفقري ResNet50 لجميع الطرق. نقدم دقة الاختبار لـ ImageNet و ImageNet-D، ومتوسط خطأ التلف (mCE) لـ ImageNet-C باتباع (<span class="nodecor">hendrycks2019benchmarking,hendrycks2019augmix,rusak2020simple,hendrycks2021many</span>). على الرغم من أن هذه الطرق تحسن متانة ImageNet-C (انخفاض mCE) مقارنة بالنموذج الأساسي، يُظهر الجدول [tab:result_augmentation] أن متانة ImageNet-D الخاصة بها مماثلة أو حتى أسوأ من النموذج الأساسي. هذا يشير إلى أن المعايير الحالية مثل ImageNet-C لا تمثل بدقة المتانة الحقيقية للشبكات العصبية في إعداداتنا، مما يجعل ImageNet-D معياراً ضرورياً لتقييم المتانة.</p>
<p><strong>هندسة النموذج.</strong> نقارن متانة ImageNet-D لهندسات نموذج مختلفة في الشكل [fig:result_arch]. عندما نغير النموذج من ViT إلى Swin Transformer (<span class="nodecor">liu2021swin</span>) و ConvNeXt (<span class="nodecor">liu2022convnet</span>)، تتحسن دقة الاختبار على كل من ImageNet-D (الخلفية) و ImageNet. ومع ذلك، تنخفض المتانة قليلاً حتى على مجموعة اختبار النسيج والمواد. تُظهر هذه النتائج صعوبة تحسين متانة ImageNet-D من خلال هندسة النموذج.</p>
<p><strong>التدريب المسبق بمزيد من البيانات.</strong> التدريب المسبق على مجموعة بيانات كبيرة فعال لتحسين أداء النموذج، مثل دقة ImageNet (<span class="nodecor">he2022masked</span>). يقارن الشكل [fig:result_arch] بين ConvNext، الذي يتم تدريبه مباشرة على ImageNet-1K، و ConvNext (المدرب مسبقاً) الذي يتم تدريبه أولاً على ImageNet-22K. نجد أن ConvNext (المدرب مسبقاً) يحقق متانة أعلى من ConvNext على جميع مجموعات ImageNet-D الثلاث، خاصة لمجموعة الخلفية. تُظهر هذه النتائج أن التدريب المسبق على مجموعة بيانات كبيرة يساعد في تحسين المتانة على ImageNet-D.</p>
<h2 id="sec:analysis">مناقشات إضافية</h2>
<p><strong>هل يمكن لنموذج <span class="nodecor">CLIP</span> أن يجد الجيران الصحيحين لصور <span class="nodecor">ImageNet-D</span>؟</strong> يُظهر نموذج <span class="nodecor">CLIP</span> (<span class="nodecor">radford2021learning</span>) إمكانيات في مهام البحث عن الجيران الأقرب. باستخدام صور <span class="nodecor">ImageNet-D</span> كصور استعلام، نسترجع الصور الأكثر تشابهاً من <span class="nodecor">ImageNet</span> للتحقق مما إذا كان <span class="nodecor">CLIP</span> يمكن أن يجد الجيران الصحيحين، كما هو موضح في الشكل. خذ الخلفية على سبيل المثال، قد تحتوي الصور المسترجعة إما على خلفية مشابهة لصورة الاستعلام أو تشمل الكائن الذي يتعلق بخلفية صورة الاستعلام. تظهر نتائجنا أن <span class="nodecor">ImageNet-D</span> يمكن أن يجد حالات الفشل للشبكات العصبية في استرجاع الجيران الأقرب.</p>
<p><strong>هل يمكن لـ <span class="nodecor">ImageNet-D</span> مطابقة مجموعات الاختبار الطبيعية في قابلية نقل الفشل؟</strong> يعرف القسم [sec:dataset_design] <code>الفشل القابل للنقل</code> وينهي <span class="nodecor">ImageNet-D</span> بفشل مشترك لنماذج البديل. نجري نفس التجربة على <span class="nodecor">ImageNet</span>، مقدمين <span class="nodecor">ImageNet</span> (الفشل) مع صور الفشل المشتركة لنماذج البديل. يُظهر الجدول [tab:transferability] أن <span class="nodecor">ImageNet-D</span> يحقق دقة مماثلة لـ <span class="nodecor">ImageNet</span> (الفشل)، مما يشير إلى أن الصور الاصطناعية يمكن أن تحقق قابلية نقل فشل مماثلة للصور الطبيعية. على عكس مجموعات البيانات الطبيعية مثل <span class="nodecor">ImageNet</span>، يتمتع <span class="nodecor">ImageNet-D</span> بتكلفة أقل في جمع البيانات ويمكن توسيعه بكفاءة.</p>
<p><strong>التدريب على صور مولدة بالانتشار.</strong> بالمقارنة مع صور الفشل المشتركة في <span class="nodecor">ImageNet-D</span>، نطلق على الصور المولدة التي تم تصنيفها بشكل صحيح من قبل نماذج البديل <span class="nodecor">Synthetic-easy</span>، ونستكشف تأثيرها كبيانات تدريب. نقوم بتحسين نموذج <span class="nodecor">ResNet18</span> المدرب مسبقاً على مجموعات تدريب مختلفة في الجدول [tab:finetune_experiment]. يُظهر الجدول [tab:finetune_experiment] أن التدريب على <span class="nodecor">Synthetic-easy</span> يحسن بشكل كبير من متانة <span class="nodecor">ImageNet-D</span> بنسبة 19.26%. بشكل ملحوظ، يتفوق النموذج C على النموذج B في دقة <span class="nodecor">ObjectNet</span> بنسبة 1.34%، مما يشير إلى تعميم أفضل للنموذج C. تشير هذه النتائج إلى أن الصور المولدة بالانتشار مع أزواج الكائنات والمتغيرات المتنوعة يمكن أن تعزز متانة النموذج كعينات تدريب.</p>
<h1 id="sec:conclusion">الخُلاصَة</h1>
<p>في هذه الورقة، نقدم مجموعة اختبار ImageNet-D ونقيم معياراً صارماً لمتانة الإدراك البصري. من خلال استغلال قدرة توليد الصور لنماذج الانتشار، تتضمن ImageNet-D صوراً بعوامل متنوعة تشمل الخلفية والملمس والمادة. تُظهر النتائج التجريبية أن ImageNet-D تقلل بشكل كبير من دقة النماذج المختلفة، بما في ذلك CLIP (انخفاض <span class="nodecor">46.05</span>%)، LLaVa (<span class="nodecor">liu2023visual</span>) (انخفاض <span class="nodecor">29.67</span>%)، و MiniGPT-4 (<span class="nodecor">zhu2023minigpt</span>) (انخفاض <span class="nodecor">16.81</span>%)، مما يدل على فعاليتها في تقييم النماذج. تُعَد أعمالنا خطوة إلى الأمام في تحسين مجموعات الاختبار الاصطناعية، وستخلق صور اختبار أكثر تنوعاً وتحدياً مع تحسن النماذج التوليدية.</p>
<p><strong>الشكر والتقدير:</strong> تم دعم هذا العمل من قبل منحة معهد تخطيط وتقييم تكنولوجيا المعلومات والاتصالات (IITP) الممولة من حكومة كوريا (MSIT) (رقم <span class="nodecor">2022-0-00951</span>، تطوير عوامل غير مؤكدة تتعلم من خلال طرح الأسئلة).</p>
<h1 id="مهمة-التسمية-على-آمازون-ميكانيكال-تورك">مهمة التسمية على أمازون ميكانيكال تورك</h1>
<p>للحصول على معايير موثوقة، نستخدم أمازون ميكانيكال تورك (<span class="nodecor">deng2009imagenet</span>, <span class="nodecor">recht2019imagenet</span>, <span class="nodecor">hendrycks2021many</span>) لتقييم جودة التسمية لـ ImageNet-D.</p>
<h2 id="تصميم-مهمة-التسمية">تصميم مهمة التسمية</h2>
<p><strong>تعليمات التسمية.</strong> نظراً لأن ImageNet-D يتضمن صوراً تحتوي على أزواج من الأشياء والمعوّقات التي قد تكون نادرة في العالم الحقيقي، فإننا نأخذ في الاعتبار كل من المظهر والوظيفة للكائن الرئيسي كمعايير للتسمية. على وجه التحديد، نطلب من العمال من MTurk الإجابة على السؤالين التاليين:</p>
<p><code>السؤال 1:</code> هل يمكنك التعرف على الكائن المطلوب (<code>فئة الحقيقة الأرضية</code>) في الصورة؟ قد تحتوي على خلفيات أو أنسجة أو مواد أو أنماط نادرة.</p>
<p><code>السؤال 2:</code> هل يمكن استخدام الكائن في الصورة كالكائن المطلوب (<code>فئة الحقيقة الأرضية</code>)؟</p>
<p><strong>خط سير التسمية.</strong> لضمان فهم العمال لهذين المعيارين، نطلب من العمال تسمية صورتين تمثيليتين للتدريب، والتي توفر الإجابة الصحيحة للسؤالين المذكورين أعلاه. بعد جلسة التدريب، يُطلب من العمال تسمية ما يصل إلى 20 صورة في مهمة واحدة، والإجابة على كلا السؤالين لكل صورة. يختار العامل "نعم" أو "لا" لكل سؤال.</p>
<p><strong>واجهة المستخدم للتسمية.</strong> تم تصميم صفحة التسمية كما في الشكل المحذوف. يمكن للعمال الانتقال إلى الصورة التالية فقط إذا أنهوا الإجابة على كلا السؤالين في الصفحة الحالية.</p>
<h2 id="مراقبة-جودة-التصنيف-البشري">مراقبة جودة التصنيف البشري</h2>
<p>نستخدم الحراس لضمان التعليقات التوضيحية عالية الجودة. لكل مهمة تصنيف تشمل صوراً متعددة، نصمم ثلاثة أنواع من الحراس كما يلي.</p>
<p><strong>الحارس الإيجابي:</strong> صورة تنتمي إلى الفئة المطلوبة وتم تصنيفها بشكل صحيح من قبل عدة نماذج. إذا لم يختَر العمال "نعم" لهذه الصورة، فقد لا يفهمون المفهوم جيداً وسيتم إزالة تعليقاتهم التوضيحية.</p>
<p><strong>الحارس السلبي:</strong> صورة لا تنتمي إلى الفئة المطلوبة. على سبيل المثال، إذا كانت الفئة المطلوبة هي كرسي، فقد يكون الحارس السلبي مغرفة. إذا اختار العمال "نعم" لصورة المغرفة، فقد لا يجيبون على الأسئلة بجدية وسيتم إزالة تعليقاتهم التوضيحية.</p>
<p><strong>الحارس المتسق.</strong> نفترض أن العمال يجب أن يختاروا نفس الإجابة لنفس الصورة إذا ظهرت عدة مرات. الحراس المتسقون هم صور تظهر مرتين بترتيب عشوائي. إذا أجاب العمال بشكل مختلف عن نفس الصورة، فإن تعليقاتهم التوضيحية ليست متسقة وسيتم إزالتها.</p>
<p>لكل مهمة تصنيف تشمل ما يصل إلى <span class="nodecor">20</span> صورة، نشمل حارساً إيجابياً واحداً، وحارساً سلبياً واحداً، وحارسين متسقين. نتجاهل الردود إذا لم يجتز العمال جميع فحوصات الحراس.</p>
<h2 id="النتائج">النتائج</h2>
<p>لكل صورة، نجمع تعليقات مستقلة من <span class="nodecor">10</span> عمال ونقوم بتصفية الردود من العمال الذين لا يجتازون فحص الجودة. بلغ مجموع العمال المؤهلين <span class="nodecor">679</span> عاملاً قدموا <span class="nodecor">1540</span> مهمة تصنيف، مما أسفر عن توافق نسبته <span class="nodecor">91.09</span>% على صورة معينة من ImageNet-D.</p>
<h1 id="sec:appendix_experimental_results">نتائج تجريبية على ImageNet-D</h1>
<p><strong>المزيد من النتائج للقسم 4.</strong> نقارن دقة النموذج لـ Image-D مع مجموعات الاختبار الحالية، بما في ذلك ImageNet (<span class="nodecor">russakovsky2015imagenet</span>)، ObjectNet (<span class="nodecor">barbu2019objectnet</span>)، ImageNet-9 (<span class="nodecor">xiao2020noise</span>) و Stylized-ImageNet (<span class="nodecor">geirhos2018imagenet</span>). يتم الإبلاغ عن جميع أرقام الدقة في الجدول [tab:appendix_benchmark_results]، والذي يتضمن أيضاً أرقام الشكل 8 في المخطوطة الرئيسية.</p>
<p><strong>إعدادات التدريب للجدول 6.</strong> نقدم تفاصيل تجريبية للجدول 6 في المخطوطة الرئيسية. نقوم بتحسين نموذج ResNet18 المدرب مسبقاً على مجموعات تدريب متنوعة. لفحص تأثير دمج الصور الاصطناعية في مجموعة تدريب التحسين، نقوم بأخذ عينات من ImageNet و Synthetic-easy لتوزيعات بيانات متماثلة، حيث يتضمن Synthetic-easy صوراً مولدة بالانتشار تم تصنيفها بشكل صحيح من قبل نماذج بديلة. يحتوي كل مجموعة على 111098 صورة، وكلا المجموعتين لديهما نفس عدد الصور لكل فئة. يتم تحسين جميع النماذج على ResNet18 المدرب مسبقاً في الحقبة 90 لـ 10 حقب إضافية، باستخدام محسن SGD بمعدل تعلم 0.0001. بخلاف ImageNet المأخوذة و Synthetic-easy، نُضمِّن ImageNet-1K الأصلي كبيانات تدريب للتدريب السلس.</p>
</body>
</html>
