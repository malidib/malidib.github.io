<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Zhenwen Dai, Federico Tomasi, Sina Ghiassian">
  <title>استكشاف السياق والاستغلال في تعلم التعزيز</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">استكشاف السياق والاستغلال في تعلم التعزيز</h1>
<p class="author"><span class="nodecor">Zhenwen Dai</span>, <span class="nodecor">Federico Tomasi</span>, <span class="nodecor">Sina Ghiassian</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">مُلخّص</h1>
<p>التعلم في السياق هو نهج واعد لتعلم السياسات عبر الإنترنت لطرق تعلم التعزيز (<span class="nodecor">RL</span>) دون الاتصال، والذي يمكن تحقيقه في وقت الاستدلال دون الحاجة إلى تحسين تدريجي. ومع ذلك، يعيق هذا الأسلوب تكاليف حسابية كبيرة ناتجة عن جمع مجموعات كبيرة من مسارات التدريب والحاجة إلى تدريب نماذج <span class="nodecor">Transformer</span> ضخمة. نعالج هذا التحدي من خلال تقديم خوارزمية استكشاف السياق والاستغلال (<span class="nodecor">ICEE</span>)، المصممة لتحسين كفاءة تعلم السياسات في السياق. على عكس النماذج الحالية، تحقق <span class="nodecor">ICEE</span> توازناً بين الاستكشاف والاستغلال في وقت الاستدلال داخل نموذج <span class="nodecor">Transformer</span>، دون الحاجة إلى استدلال <span class="nodecor">Bayesian</span> صريح. ونتيجة لذلك، يمكن لـ<span class="nodecor">ICEE</span> حل مشاكل التحسين <span class="nodecor">Bayesian</span> بكفاءة تعادل طرق المعالجة المعتمدة على عملية <span class="nodecor">Gaussian</span>، ولكن في وقت أقل بكثير. من خلال التجارب في بيئات العالم الشبكي، نظهر أن <span class="nodecor">ICEE</span> يمكن أن تتعلم حل مهام تعلم التعزيز الجديدة باستخدام عشرات الحلقات فقط، مما يمثل تحسناً كبيراً عن المئات من الحلقات التي تحتاجها طريقة التعلم في السياق السابقة.</p>
<h1 id="مقدمة">مقدمة</h1>
<p>تمثل النماذج المحولة نهجاً فعالاً للغاية في نمذجة التسلسل، مع تطبيقات تمتد عبر مجالات متعددة مثل النصوص والصور والصوت. في مجال التعلم المعزز (<span class="nodecor">Reinforcement Learning (RL)</span>)، اقترح (<span class="nodecor">NEURIPS2021_7f489f64</span>) و(<span class="nodecor">NEURIPS2021_099fe6b0</span>) مفهوم معالجة التعلم المعزز دون اتصال كمشكلة تنبؤ تسلسلي باستخدام النموذج المحول. لقد أثبت هذا الأسلوب نجاحه في التعامل مع مجموعة من المهام باستخدام تقنيات نمذجة التسلسل على نطاق واسع فقط (<span class="nodecor">NEURIPS2022_b2cac94f, Reed2022-lj</span>). يكمن العيب البارز في عدم قدرة السياسة على تحسين نفسها عند استخدامها في بيئات عبر الإنترنت. للتغلب على ذلك، تم تقديم طرق التنعيم مثل (<span class="nodecor">Zheng2022-kr</span>)، التي تمكن من تحسين السياسة بشكل مستمر. ومع ذلك، غالباً ما تعتمد هذه الطرق على التحسين القائم على التدرج البطيء والمكلف حسابياً.</p>
<p>من ناحية أخرى، يمكن للتعلم في السياق، وهو خاصية ملحوظة في نماذج اللغة الكبيرة (<span class="nodecor">Large Language Models (LLMs)</span>)، التعامل مع المهام الجديدة من خلال توفير تفاصيل المهمة عبر تلميحات لغوية، مما يلغي الحاجة إلى التنعيم. يقترح (<span class="nodecor">laskin2023incontext</span>) خوارزمية تعلم في السياق للتعلم المعزز، والتي تستخدم نموذج تسلسل لتقطير خوارزمية تعلم السياسة من مسارات تدريب التعلم المعزز. النموذج الناتج قادر على إجراء تعلم السياسة في وقت الاستدلال من خلال عملية تكرارية لأخذ العينات من الإجراءات وزيادة التلميح. تتكبد هذه الطريقة تكاليف حسابية كبيرة في جمع مجموعات واسعة من مسارات التدريب وتدريب نماذج المحولات الكبيرة التي تحتاج إلى نمذجة جزء كبير من مسار التدريب. السبب الرئيسي لهذه التكلفة الحسابية العالية هو مسارات التدريب الطويلة الناتجة عن عملية التجربة والخطأ البطيئة لخوارزميات تعلم سياسة التعلم المعزز.</p>
<p>تهدف هذه الورقة إلى تحسين كفاءة تعلم السياسة في السياق من خلال القضاء على الحاجة إلى التعلم من مسارات تعلم السياسة. في سيناريو مثالي، يمكن تحقيق تعلم سياسة فعال من خلال عملية تجربة وخطأ فعالة. بالنسبة لمشاكل التعلم المعزز المبسطة مثل الأذرع المتعددة (<span class="nodecor">Multi-Armed Bandits (MAB)</span>)، تم إثبات وجود عملية تجربة وخطأ فعالة مثل عينة تومسون والحدود العليا للثقة. تعتمد هذه العملية، والتي غالباً ما يشار إليها باسم تجارة الاستكشاف-الاستغلال (<span class="nodecor">Exploration-Exploitation (EE)</span>)، بشكل كبير على عدم اليقين المعرفي المستمد من الاعتقاد البايزي. ومع ذلك، من الصعب استنتاج عدم اليقين المعرفي الدقيق لمشاكل التعلم المعزز التسلسلي باستخدام الطرق البايزية التقليدية. في ضوء الدراسات الحديثة حول تقدير عدم اليقين لنماذج اللغة الكبيرة (<span class="nodecor">yin-etal-2023-large</span>)، نفحص التوزيعات التنبؤية لنماذج التسلسل، مما يظهر أنه، من خلال التدريب بالتعلم الإشرافي البحت على البيانات دون اتصال، يمكن لنموذج التسلسل التقاط عدم اليقين المعرفي في التنبؤ بالتسلسل. هذا يوحي بإمكانية تنفيذ الاستكشاف-الاستغلال في التعلم المعزز دون اتصال.</p>
<p>استناداً إلى هذه الملاحظة، نطور خوارزمية الاستكشاف-الاستغلال في السياق (<span class="nodecor"></span>) لتعلم السياسة. تأخذ <span class="nodecor"></span> كمدخلات سلسلة من الحلقات المتعددة لنفس المهمة وتتنبأ بالإجراء المقابل في كل خطوة مشروطة ببعض المعلومات بأثر رجعي. يشبه تصميم التعلم المعزز دون اتصال هذا المحول القراري (<span class="nodecor">Decision Transformer (DT)</span>)، ولكن <span class="nodecor"></span> يتعامل مع تعلم السياسة في السياق من خلال نمذجة الحلقات المتعددة لمهمة بينما <span class="nodecor">DT</span> ينمذج حلقة واحدة فقط. علاوة على ذلك، لا تحتاج هذه الحلقات إلى النشأة من مسار تدريب، مما يتجنب التكاليف الحسابية العالية المرتبطة بتوليد واستهلاك مسارات التعلم. تتجه توزيعات الإجراءات المتعلمة في <span class="nodecor">DT</span> نحو سياسة جمع البيانات، والتي قد لا تكون مثالية عندما تكون دون المستوى الأمثل. لمعالجة هذا التحيز، نقدم هدفاً غير متحيز ونطور شكلاً معيناً من المعلومات بأثر رجعي للاستكشاف-الاستغلال الفعال عبر الحلقات.</p>
<p>من خلال التجارب، نوضح أن سلوك الاستكشاف-الاستغلال يظهر في <span class="nodecor"></span> أثناء الاستدلال بفضل عدم اليقين المعرفي في التنبؤ بالإجراء. هذا واضح بشكل خاص عند تطبيق <span class="nodecor"></span> على التحسين البايزي (<span class="nodecor">Bayesian Optimization (BO)</span>)، حيث أن أداء <span class="nodecor"></span> يضاهي طريقة تعتمد على عملية غاوسية في مهام <span class="nodecor">BO</span> المنفصلة. نوضح أيضاً أن <span class="nodecor"></span> يمكن أن يحسن بنجاح السياسة لمهمة جديدة مع التجارب والأخطاء من الصفر لمشاكل التعلم المعزز التسلسلي. حسب علمنا، <span class="nodecor"></span> هي الطريقة الأولى التي تدمج بنجاح الاستكشاف-الاستغلال في السياق في التعلم المعزز من خلال النمذجة التسلسلية دون اتصال.</p>
<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<p><strong>التعلم البياني.</strong> لقد زاد الاهتمام مؤخراً بخوارزميات <em>التعلم البياني</em> أو <em>تعلم التعلم</em>. بينما يكون المتعلم عبارة عن وكيل يتعلم حل مهمة باستخدام البيانات المرصودة، يتضمن خوارزمية تعلم التعلم وجود <em>متعلم بياني</em> يحسن باستمرار من عملية التعلم للمتعلم (<span class="nodecor">schmidhuber1996simple, thrun2012learning, hospedales2021meta, sutton2022history</span>). تم إجراء الكثير من الأعمال في مجال التعلم البياني. على سبيل المثال، اقترح (<span class="nodecor">finn2017model</span>) خوارزمية تعلم بياني عامة لا تعتمد على النموذج تدرب المعلمات الأولية للنموذج بحيث يكون للنموذج أداء أقصى في مهمة جديدة بعد تحديث معلمات النموذج من خلال بضع خطوات تدريجية محسوبة بكمية صغيرة من البيانات من المهمة الجديدة. تشمل الأعمال الأخرى في التعلم البياني تحسين المحسنات (<span class="nodecor">andrychowicz2016learning, li2016learning, ravi2016optimization, wichrowska2017learned</span>)، تحسين التعلم القليل الأمثلة (<span class="nodecor">mishra2017simple, duan2017one</span>)، تعلم الاستكشاف (<span class="nodecor">stadie2018some</span>)، والتعلم غير المشرف عليه (<span class="nodecor">hsu2018unsupervised</span>).</p>
<p>في مجال التعلم البياني العميق لتعزيز التعلم (<span class="nodecor">wang2016learning</span>)، ركزت بعض الأعمال على شكل خاص من التعلم البياني يسمى التدرجات البيانية. في التدرجات البيانية، يتم تدريب المتعلم البياني بواسطة التدرجات من خلال قياس تأثير المعلمات البيانية على متعلم يتم تدريبه أيضاً باستخدام خوارزمية التدرج (<span class="nodecor">xu2018meta</span>). في عمل آخر، استخدم (<span class="nodecor">zheng2018learning</span>) التدرجات البيانية لتعلم المكافآت. ركز (<span class="nodecor">gupta2018unsupervised</span>) على أتمتة عملية تصميم المهام في تعزيز التعلم، لتحرير الخبير من عبء التصميم اليدوي لمهام التعلم البياني. بالمثل، قدم (<span class="nodecor">veeriah2019discovery</span>) طريقة لوكيل تعزيز التعلم لاكتشاف الأسئلة المصاغة كوظائف قيمة عامة من خلال استخدام التدرجات البيانية غير القصيرة النظر. ومؤخراً، شهد تعلم تعزيز التدرجات البيانية تقدماً كبيراً من مكاسب الأداء في المعايير الشعبية إلى خوارزميات هجينة للتعلم البياني لتعزيز التعلم عبر الإنترنت وغير المتصل (<span class="nodecor">xu2020meta, zahavy2020self, flennerhag2021bootstrapped, mitchell2021offline, yin-etal-2023-large, pong2022offline</span>). تمت دراسة دور الشك في تعزيز التعلم البياني من قبل (<span class="nodecor">JMLR:v22:21-0657</span>)، والذي أسفر عن طريقة فعالة لتعزيز التعلم البياني عبر الإنترنت. ثم تم توسيع هذا العمل من قبل (<span class="nodecor">NEURIPS2021_24802454</span>) إلى الإعداد غير المتصل بالسياسة.</p>
<p><strong>تعلم التعزيز غير المتصل.</strong> بشكل عام، تم اقتراح تعلم التعزيز كنموذج أساسي عبر الإنترنت (<span class="nodecor">sutton1988learning, sutton1999policy, sutton2018reinforcement</span>). تأتي هذه الطبيعة التعليمية عبر الإنترنت مع بعض القيود مثل صعوبة تبنيها في العديد من التطبيقات التي من المستحيل جمع البيانات عبر الإنترنت والتعلم في نفس الوقت، مثل القيادة الذاتية وأحياناً ليست فعالة من حيث البيانات كما يمكن أن تكون، حيث قد يختار التعلم من عينة ثم التخلص من العينة والانتقال إلى العينة التالية (<span class="nodecor">levine2020offline</span>). إحدى الأفكار للحصول على المزيد من الخبرة المجمعة هي استخدام مخازن إعادة التشغيل. عند استخدام المخازن، يتم الاحتفاظ بجزء من العينات في الذاكرة ثم يتم إعادة استخدامها عدة مرات بحيث يمكن للوكيل التعلم أكثر منها (<span class="nodecor">lin1992self, mnih2015human</span>). يشير متغير من تعلم التعزيز، يعرف باسم <em>تعلم التعزيز غير المتصل</em>، إلى خوارزميات تعلم التعزيز التي يمكن أن تتعلم بالكامل غير متصل، من مجموعة ثابتة من البيانات التي تم جمعها مسبقاً دون جمع بيانات جديدة في وقت التعلم (<span class="nodecor">ernst2005tree, riedmiller2005neural, lange2012batch, fujimoto2019off, siegel2020keep, gulcehre2020rl, nair2020awac</span>). تركز الأدبيات الحديثة على محولات القرار أيضاً على تعلم التعزيز غير المتصل (<span class="nodecor">NEURIPS2021_7f489f64</span>) لأنها تحتاج إلى حساب <em>العائد المتبقي</em> في وقت التدريب، والذي بدوره يتطلب بيانات تم جمعها مسبقاً.</p>
<p><strong>التعلم في السياق.</strong> خوارزميات تعلم التعزيز في السياق هي تلك التي تحسن سياستها بالكامل <em>في السياق</em> دون تحديث معلمات الشبكة أو دون أي تعديل دقيق للنموذج (<span class="nodecor">lu2021pretrained</span>). تم إجراء بعض الأعمال لدراسة ظاهرة التعلم في السياق في محاولة لشرح كيف قد يكون التعلم في السياق ممكناً (<span class="nodecor">abernethy2023mechanism, min2022rethinking</span>). يعمل الوكيل "جاتو" الذي طوره (<span class="nodecor">reed2022generalist</span>) كوكيل عام متعدد النماذج ومتعدد المهام ومتعدد الأجسام، بمعنى أن نفس الوكيل المدرب يمكنه لعب أتاري، ووضع تعليقات توضيحية على الصور، والدردشة، وتكديس الكتل باستخدام ذراع روبوت حقيقي فقط بناءً على سياقه. من خلال تدريب وكيل تعلم التعزيز على نطاق واسع، أظهر (<span class="nodecor">team2023human</span>) أن وكيلاً في السياق يمكنه التكيف مع بيئات ثلاثية الأبعاد جديدة ومفتوحة النهايات. من الاهتمام الخاص بالنسبة لنا هو تقطير الخوارزمية (AD)، وهي طريقة تعلم تعزيز بياني في السياق (<span class="nodecor">laskin2023incontext</span>). على وجه التحديد، AD هي طريقة تعلم تعزيز بياني في السياق غير متصل. بشكل أساسي، AD خالٍ من التدرجات—يتكيف مع المهام اللاحقة دون تحديث معلمات شبكته.</p>
<h1 id="عدم-اليقين-المعرفي-في-تنبؤ-نموذج-التسلسل">عدم اليقين المعرفي في تنبؤ نموذج التسلسل</h1>
<p>يعالج DT، المعروف أيضاً باسم RL المقلوب، مشكلة تعلم السياسة دون اتصال كمشكلة في نمذجة التسلسل. في هذا القسم، ننظر في نموذج تسلسل عام ونحلل عدم اليقين التنبؤي له.</p>
<p>لتكن <span class="math inline">\(\mX_{1:T}=(\vx_1, \ldots, \vx_T)\)</span> تسلسلاً من المدخلات بطول <span class="math inline">\(T\)</span> و<span class="math inline">\(\mY_{1:T} = (\vy_1, \ldots, \vy_T)\)</span> تسلسلاً مقابلاً من المخرجات. نفترض أن تسلسل المخرجات يتم توليده وفقاً لتوزيع احتمالي خطي بمعامل <span class="math inline">\(\vtheta\)</span>، <span class="math inline">\(\vy_t \sim p(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}, \vtheta)\)</span>. يتم توليد كل تسلسل بمعامل مختلف مأخوذ من توزيعه الأولي، <span class="math inline">\(\vtheta \sim p(\vtheta)\)</span>. هذه تحدد توزيعاً توليدياً لتسلسل: <span class="math display">\[p(\mY_{1:T}, \vtheta |  \mX_{1:T}) = p(\vtheta) p(\vy_1| \vx_1)\prod_{t=2}^Tp(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}, \vtheta).\]</span> غالباً ما تعرف مهمة نمذجة التسلسل بأنها تدريب نموذج ذاتي الارتداد بمعامل <span class="math inline">\(\vpsi\)</span>، <span class="math inline">\(p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})\)</span>، بناءً على مجموعة بيانات من التسلسلات <span class="math inline">\(\mathcal{D} = \{\mX^{(i)}, \mY^{(i)}\}_i\)</span> المولدة من التوزيع التوليدي المجهول أعلاه. في حد البيانات اللانهائية، يمكن صياغة هدف التعلم بالاحتمال الأقصى لنموذج التسلسل أعلاه كـ <span class="math inline">\(\vpsi* = \argmax_{\vpsi} \mathcal{L}_\vpsi\)</span>, <span class="math display">\[\label{eqn:ml_objective}
\begin{split}
\mathcal{L}_\vpsi =&amp; - \sum_t \int p(\mY_{1:t-1} |  \mX_{1:t-1}) \\
&amp;\KL\left(p(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})|| p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})\right) d \mY_{1:t-1} +C, 
\end{split}\]</span> حيث <span class="math inline">\(\KL(\cdot || \cdot)\)</span> يشير إلى انحراف كولباك ليبلر و<span class="math inline">\(C\)</span> ثابت بالنسبة لـ<span class="math inline">\(\vpsi\)</span>.</p>
<p>التوزيع في الجانب الأيسر في مصطلح الانتروبيا المتقاطعة <span class="math inline">\(p(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})\)</span> هو التوزيع التنبؤي <em>الحقيقي</em> لـ<span class="math inline">\(\vy_t|\vx_t\)</span> مشروطاً بالتاريخ الملحوظ <span class="math inline">\(\mY_{1:t-1}\)</span> و<span class="math inline">\(\mX_{1:t-1}\)</span>، والذي يمكن كتابته كالتالي: <span class="math display">\[p(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) = \int p(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}, \vtheta) p(\vtheta | \mX_{1:t-1}, \mY_{1:t-1}) d \vtheta,\]</span> حيث <span class="math display">\[p(\vtheta| \mX_{1:t-1}, \mY_{1:t-1}) = \frac{p(\vtheta) p(\mY_{1:t-1} |  \mX_{1:t-1}, \vtheta)}{\int p(\vtheta&#39;) p(\mY_{1:t-1} |  \mX_{1:t-1}, \vtheta&#39;) d\vtheta&#39;}.\]</span> كما هو موضح أعلاه، يحتوي التوزيع التنبؤي <em>الحقيقي</em> لـ<span class="math inline">\(\vy_t|\vx_t\)</span> على كل من عدم اليقين العشوائي وعدم اليقين المعرفي، حيث يساهم <span class="math inline">\(p(\vtheta | \mX_{1:t-1}, \mY_{1:t-1})\)</span> في ذلك. مع بيانات كافية وقدرة نموذجية، سيتم تدريب التوزيع التوليدي في نموذج التسلسل <span class="math inline">\(p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})\)</span> ليتطابق مع التوزيع التنبؤي <em>الحقيقي</em>. نتيجة لذلك، يمكننا توقع أن يتضمن عدم اليقين المعرفي في التوزيع التنبؤي لنموذج التسلسل. لاحظ أن التوزيع التنبؤي يمكن أن يلتقط عدم اليقين المعرفي فيما يتعلق بمعاملات التسلسل <span class="math inline">\(\vtheta\)</span>، ولكنه لا يشمل عدم اليقين المعرفي بشأن المعاملات الفائقة (إذا كانت موجودة).</p>
<h1 id="تعلم-السياسات-في-سياق-محدد">تعلم السياسات في سياق محدد</h1>
<p>الشك الابستمولوجي هو المكون الأساسي لـEE. مع ملاحظة أن نموذج توزيع التنبؤ يحتوي على شك ابستمولوجي، نصمم خوارزمية تعلم سياسات في سياق محدد مع EE.</p>
<p>نعتبر مشكلة حل مجموعة من ألعاب التعلم المعزز بناءً على بيانات غير متصلة بالشبكة. من كل لعبة، يتم جمع مجموعة من المسارات من عدد من السياسات، حيث أن مسار الحلقة <span class="math inline">\(k\)</span> للعبة <span class="math inline">\(i\)</span> و<span class="math inline">\(\vo\)</span>، <span class="math inline">\(\va\)</span>، <span class="math inline">\(r\)</span> تدل على الحالة المرصودة، الفعل والمكافأة على التوالي. السياسة المستخدمة لجمع <span class="math inline">\(\tau_k^{(i)}\)</span> تعرف بـ<span class="math inline">\(\pi_k^{(i)}(\va_{k,t}^{(i)}|\vo_{k,t}^{(i)})\)</span>. نقوم بدمج جميع الحلقات للعبة <span class="math inline">\(i\)</span> في تسلسل واحد <span class="math inline">\(\vtau^{(i)} = (\tau_1^{(i)}, \ldots, \tau_K^{(i)})\)</span>. للتسهيل، سيتم حذف الأسطر العليا <span class="math inline">\(^{(i)}\)</span> في النص التالي ما لم يشار صراحة إلى اللعبة <span class="math inline">\(i\)</span>.</p>
<p>نقترح نموذج تسلسل يتم تدريبه للتنبؤ خطوة بخطوة بـ<span class="math inline">\(p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t})\)</span>، حيث <span class="math inline">\(R_{k,t}\)</span> هو <em>العائد المتبقي</em> في الحلقة <span class="math inline">\(k\)</span> والخطوة الزمنية <span class="math inline">\(t\)</span> و<span class="math inline">\(\mH_{k,t}=(\tau_{k, 1:t-1}, \vtau_{1:k-1})\)</span> هو التاريخ حتى الخطوة الزمنية <span class="math inline">\(t\)</span> بما في ذلك الحلقات السابقة. صياغة النموذج أعلاه مشابهة لـDT ولكن التسلسل في DT يحتوي فقط على حلقة واحدة. علماً بأنه، على عكس AD، لا يلزم أن تكون المسارات المتتالية من خوارزمية تعلم التعلم المعزز.</p>
<p>كما هو موضح في القسم السابق، من خلال القيام بالتعلم بالاحتمال الأقصى على المسارات المجمعة، سيتم تدريب التوزيع التنبؤي ليتطابق مع التوزيع اللاحق <em>الحقيقي</em> لفعل سياسة جمع البيانات، <span class="math display">\[\label{eqn:true_action_posterior}
p(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t}) = \frac{p(R_{k,t}| \va_{k,t}, \vo_{k,t}, \mH_{k,t})\pi_k(\va_{k,t}|\vo_{k,t})}{\int p(R_{k,t}| \va_{k,t}&#39;, \vo_{k,t}, \mH_{k,t})\pi_k(\va_{k,t}&#39;|\vo_{k,t}) d \va_{k,t}&#39;},\]</span> حيث <span class="math inline">\(p(R_{k,t}| \va_{k,t}, \vo_{k,t}, \mH_{k,t})\)</span> هو توزيع العائد بعد الخطوة الزمنية <span class="math inline">\(t\)</span> التالية لـ<span class="math inline">\(\pi_k\)</span>.</p>
<p>كما هو موضح في ([eqn:true_action_posterior])، التوزيع اللاحق للفعل متحيز نحو سياسة جمع البيانات. اتباع مثل هذا التوزيع للفعل يسمح لنا بإعادة إنتاج المسارات التي تم إنشاؤها بواسطة سياسة جمع البيانات ولكن سيؤدي إلى إعادة إنشاء مسارات غير مثالية إذا لم تكن سياسة جمع البيانات مثالية. توزيع الفعل الأكثر ملاءمة هو توزيع الفعل الذي يتوافق مع العائد المحدد دون تأثير سياسة جمع البيانات، أي <span class="math display">\[\label{eqn:unbiased_action_posterior}
\hat{p}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t}) = \frac{p(R_{k,t}| \va_{k,t}, \vo_{k,t}, \mH_{k,t})\mathcal{U}(\va_{k,t})}{\int p(R_{k,t}| \va_{k,t}&#39;, \vo_{k,t}, \mH_{k,t}) \mathcal{U}(\va_{k,t}&#39;) d \va_{k,t}&#39;},\]</span> حيث <span class="math inline">\(\mathcal{U}(\va_{k,t})\)</span> هي السياسة العشوائية الموحدة، التي تعطي جميع الأفعال احتمالات متساوية. لتمكين نموذج التسلسل من تعلم توزيع الفعل غير المتحيز، يجب تعريف الهدف الاحتمالي الأقصى على النحو التالي <span class="math display">\[\mathcal{L}_{\vpsi} =\sum_{k,t} \int \hat{p}(R_{k,t}, \va_{k,t} |\vo_{k,t}, \mH_{k,t}) \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t}) dR_{k,t} d \va_{k,t}.\]</span> بعد تطبيق حيلة أخذ العينات حسب الأهمية، يمكن استنتاج تقريب مونت كارلو للهدف أعلاه كما يلي <span class="math display">\[\label{eqn:action_correction_obj}
\mathcal{L}_{\vpsi} \approx  \sum_{k,t}  \frac{\mathcal{U}(\va_{k,t})}{\pi_k(\va_{k,t}|\vo_{k,t})} \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t}),\]</span> حيث <span class="math inline">\(\va_{k,t} \sim \pi_k(\va_{k,t}|\vo_{k,t})\)</span> و<span class="math inline">\(R_{k,t} \sim p(R_{k,t}| \va_{k,t}, \vo_{k,t}, \mH_{k,t})\)</span>، أي أن <span class="math inline">\(\va_{k,t}\)</span> و<span class="math inline">\(R_{k,t}\)</span> يتم أخذهما من سياسة جمع البيانات <span class="math inline">\(\pi_k\)</span>.</p>
<h1 id="تصميم-العودة-إلى-الهدف">تصميم العودة إلى الهدف</h1>
<p>العودة إلى الهدف هي مكون حاسم في (<span class="nodecor">DT</span>) لحل مهام (<span class="nodecor">RL</span>) في الاستدلال باستخدام نموذج تسلسلي مدرب. تم تصميم نظام العودة إلى الهدف لحساب إشارة العائد المتوقعة من حلقة واحدة. من أجل تحقيق تعلم السياسات في سياقها، نصمم العودة إلى الهدف عبر الحلقات.</p>
<p>تتكون العودة إلى الهدف في (<span class="nodecor">DT</span>) من مكونين: واحد للخطوات الفردية داخل حلقة والآخر للسلوك عبر الحلقات، <span class="math inline">\(R_{k,t} = (c_{k,t}, \tilde{c}_k)\)</span>. تتبع العودة إلى الهدف داخل الحلقة <span class="math inline">\(c_{k,t}\)</span> التصميم المستخدم في (<span class="nodecor">NEURIPS2021_7f489f64</span>)، والذي يعرف بأنه المكافآت التراكمية ابتداءً من الخطوة الحالية <span class="math inline">\(c_{k,t} = \sum_{t&#39;&gt;t} r_{k, t&#39;}\)</span>. يستعير هذا التصميم مفهوم المكافأة التراكمية لـ(<span class="nodecor">RL</span>) وله فائدة تضمين معلومات المكافآت المستقبلية التي تتبع السياسة. هذا مفيد جداً عندما تعتمد نتائج الخطوات المستقبلية بشدة على حالة وفعل الخطوة الحالية. يسمح ذلك بتمييز الفعل الذي يؤدي إلى نتيجة مستقبلية جيدة عن الفعل الذي يؤدي إلى نتيجة سيئة في نموذج التسلسل. الجانب السلبي هو أنه مع سياسة جمع بيانات غير خبيرة، غالباً ما لا تلاحظ العودة إلى الهدف المثالي في كل حالة. هذا سيحد من قدرة نموذج التسلسل على تحقيق أداء أفضل من سياسة جمع البيانات في وقت الاستدلال.</p>
<p>في تصميم العودة إلى الهدف عبر الحلقات، الوضع مختلف. الحالات الأولية للحلقات الفردية مستقلة عن بعضها البعض. ما يحدد المكافآت التراكمية للحلقات الفردية هو تسلسل الأفعال. إذا اعتبرنا مجمل فضاء السياسات كفضاء الأفعال لكل حلقة، فإن اتخاذ القرارات عبر الحلقات يكون أقرب إلى (<span class="nodecor">MAB</span>)، حيث السياسة هي الفعل وعائد الحلقة هو مكافأة (<span class="nodecor">MAB</span>). مدفوعاً بهذه الملاحظة، نعرف العودة إلى الهدف بناءً على تحسين عائد الحلقة الحالية مقارنة بجميع الحلقات السابقة. على وجه التحديد، نعرف العودة إلى الهدف عبر الحلقات كما يلي: <span class="math display">\[\tilde{c}_k =  \begin{cases}
    1       &amp; \quad  \bar{r}_k &gt; \max_{1\leq j \leq k-1} \bar{r}_j,\\
    0  &amp; \quad \text{otherwise}.
  \end{cases}\]</span> حيث <span class="math inline">\(\bar{r}_k = \sum_t r_{k,t}\)</span> هو المكافأة التراكمية للحلقة <span class="math inline">\(k\)</span>. بديهياً، في وقت الاستدلال، من خلال الشرط على <span class="math inline">\(\tilde{c}_k =1\)</span>، نتخذ أفعالاً من سياسة "معينة" وفقاً لاحتمالية كونها أفضل من جميع الحلقات السابقة. هذا يشجع نموذج التسلسل على تقديم أداء أفضل بعد جمع المزيد والمزيد من الحلقات. يتجنب هذا التصميم القيود المتعلقة بالحاجة إلى مراقبة مسارات تعلم السياسة المثلى.</p>
<p><strong>استدلال الفعل.</strong> بعد تدريب نموذج التسلسل، يمكن استخدام النموذج لأداء تعلم السياسة من الصفر. في كل خطوة، نأخذ عينة من فعل من نموذج التسلسل مشروطاً على المسار حتى الآن وعودة إلى الهدف للخطوة. تعرف العودة إلى الهدف لأخذ عينة الفعل على النحو التالي. دائماً ما يتم ضبط العودة إلى الهدف عبر الحلقات <span class="math inline">\(\tilde{c}_k\)</span> على واحد لتشجيع تحسينات السياسة. بالنسبة للعودة إلى الهدف داخل الحلقة، نتبع استدلال الفعل المقترح بواسطة (<span class="nodecor">NEURIPS2022_b2cac94f</span>). خلال تدريب (<span class="nodecor">DT</span>)، يتم تدريب نموذج تسلسل منفصل للتنبؤ بالعائد المنقسم من المسارات، <span class="math inline">\(p_{\vphi}(c_{k,t} | \tilde{c}_k, \vo_{k,t}, \mH_{k,t})\)</span>. في وقت الاستدلال، يتم أخذ عينة من العودة إلى الهدف داخل الحلقة لكل خطوة من توزيع معزز <span class="math display">\[q(c_{k, t}) \propto p_{\vphi}(c_{k,t} | \tilde{c}_k, \vo_{k,t}, \mH_{k,t})(\frac{c_{k,t} - c_{\min}}{c_{\max}- c_{\min}})^{\kappa}.\]</span> يميل هذا التعزيز توزيع العودة إلى الهدف نحو القيم الأعلى، مما يشجع العامل على اتخاذ أفعال تؤدي إلى عوائد أفضل. لا يتم دمج التنبؤ بالعائد في نموذج التسلسل الرئيسي كما في (<span class="nodecor">NEURIPS2022_b2cac94f</span>)، لأن العوائد تدخل أيضاً في المدخلات. بهذه الطريقة، يكتشف النموذج بسرعة أن <span class="math inline">\(c_{k,t}\)</span> يمكن التنبؤ به من <span class="math inline">\(c_{k, t-1}\)</span>. هذه مشكلة لأن العائد الحقيقي لا يمكن ملاحظته حتى نهاية الحلقة.</p>
<p>بعد أخذ عينة من <span class="math inline">\(c_{k,t}\)</span>، يتم أخذ عينة من فعل مشروط على العودة إلى الهدف المجمعة <span class="math inline">\(R_{k,t}\)</span>. يتم دمج الحالة الناتجة والمكافأة في المسار للتنبؤ بفعل الخطوة التالية. في نهاية الحلقة، سنعيد حساب العودة إلى الهدف الحقيقية <span class="math inline">\(c_{k,t}\)</span> و<span class="math inline">\(\tilde{c}_k\)</span> بناءً على المكافآت من الحلقة بأكملها وتحديث العودة إلى الهدف في المسار بالقيم المعاد حسابها. هذا يجعل المسار في وقت الاستدلال قريباً قدر الإمكان من مسارات التدريب. يمكن العثور على وصف خوارزمية استدلال الفعل في الخوارزمية (<span class="nodecor">alg:action_infer</span>).</p>
<h1 id="تجارب-التحسين-البيزي">تجارب التحسين البايزي</h1>
<p>التحسين البايزي (BO) هو تطبيق ناجح جداً لاستكشاف الاستغلال (EE). يمكنه البحث عن الأمثل لدالة بأقل عدد من تقييمات الدالة. تم استخدام طرق التحسين البايزي المعتمدة على عملية غاوس (GP) على نطاق واسع في مجالات مختلفة مثل ضبط المعلمات الفائقة، واكتشاف الأدوية، وتحسين الديناميكا الهوائية. لتقييم أداء EE لـ<span class="nodecor">ours</span>، نطبقه على BO ونقارنه بطريقة معتمدة على GP باستخدام واحدة من أكثر وظائف الاستحواذ استخداماً، التحسين المتوقع (EI).</p>
<p>نعتبر مشكلة التحسين البايزي المنفصلة. المهمة هي العثور على الموقع من مجموعة ثابتة من النقاط التي لديها أقل قيمة وظيفية بأقل عدد من تقييمات الدالة قدر الإمكان. يمكن اعتبار BO كنوع خاص من العصابات متعددة الأذرع (MAB)، حيث يرتبط كل عمل بموقع في مساحة محدودة. لحل BO باستخدام <span class="nodecor">ours</span>، نقوم بترميز مسار البحث التكراري لدالة كتسلسل واحد، حيث <span class="math inline">\(\va_t\)</span> هو موقع يتم فيه جمع قيمة الدالة في الخطوة <span class="math inline">\(t\)</span>، <span class="math inline">\(r_t\)</span> هي قيمة الدالة المقابلة و<span class="math inline">\(R_t\)</span> هو العائد المتبقي. يمكن استخدام الملاحظات <span class="math inline">\(\{\vo_t\}\)</span> لترميز أي معلومات جانبية معروفة عن الدالة والحد الأدنى. نظراً لعدم توفر مثل هذه المعلومات لـBO العام، فإننا لا نستخدم <span class="math inline">\(\{\vo_t\}\)</span> في تجربتنا. نظراً لأن قيمة الدالة يمكن اعتبارها المكافأة الفورية المتاحة، فإننا نعامل كل عمل كحلقة مختلفة ونستخدم فقط العائد المتبقي كـ<span class="math inline">\(R_t\)</span> هنا. نظراً لأن كل عمل مرتبط بموقع، فإننا نقوم بتضمين الإجراءات من خلال تعلم إسقاط خطي بين مساحة الموقع ومساحة التضمين. عند فك تشفير إخراج Transformer لإجراء، يتم إنشاء لوغاريتم الإجراء باستخدام MLP الذي يأخذ كمدخلات إخراج Transformer جنباً إلى جنب مع التضمين المرتبط بالإجراء. التصميم هو لمواجهة التحدي الذي قد تكون مجموعة المواقع لكل دالة مختلفة.</p>
<p>لتدريب <span class="nodecor">ours</span> لحل مشكلة التحسين البايزي المنفصلة، نحتاج إلى توليد بيانات تدريب تتكون من أزواج إدخال-إخراج لدوال تم أخذ عينات منها بشكل عشوائي. أثناء التدريب، يتم توليد أزواج الإدخال-الإخراج في المواقع العشوائية على الفور. نستخدم GP مع نواة Matérn 5/2 لأخذ عينات من <span class="nodecor">1024</span> نقطة لكل دالة. يتم أخذ عينات من مواقع هذه النقاط من توزيع موحد على <span class="math inline">\([0, 1]\)</span>. يتم أخذ عينات من مقاييس الطول للنواة من توزيع موحد على <span class="math inline">\([0.05, 0.3]\)</span>.</p>
<p>بعد تدريب <span class="nodecor">ours</span>، نقوم بتقييم أدائه على مجموعة من وظائف المعيار 2D. نستخدم <span class="nodecor">16</span> وظيفة 2D التي تم تنفيذها في (<span class="nodecor">KimJ2017bayeso</span>). يتم تطبيع مساحة الإدخال لكل دالة لتكون بين 0 و1. نقوم بأخذ عينات من مجموعة مختلفة من <span class="nodecor">1024</span> نقطة من كل دالة وتطبيع قيم الدالة الناتجة لتكون بمتوسط صفر وتباين وحدة. تم إعطاء كل دالة خمس تجارب بتصاميم أولية مختلفة. في كل خطوة من البحث، نحسب الفرق في قيمة الدالة بين التقدير الحالي الأفضل والحد الأدنى الحقيقي. يتم عرض الأداء العام لجميع الوظائف المقيمة في الشكل [fig:bo_exp]. تتم مقارنة أداء <span class="nodecor">ours</span> مع طريقة التحسين البايزي المعتمدة على GP باستخدام وظيفة الاستحواذ EI والأساس العشوائي الذي يختار المواقع وفقاً لاحتمالية عشوائية موحدة. "<span class="nodecor">ours</span>-biased" يشير إلى متغير من <span class="nodecor">ours</span> الذي لا يستخدم هدف تصحيح التحيز العملي كما هو موضح في ([eqn:action_correction_obj]). كفاءة البحث لـ<span class="nodecor">ours</span> مماثلة لطريقة التحسين البايزي المعتمدة على GP مع EI. كلاهما أفضل بشكل ملحوظ من العشوائية وأفضل بشكل ملحوظ من <span class="nodecor">ours</span>-biased. الفجوة في الأداء بين <span class="nodecor">ours</span> و<span class="nodecor">ours</span>-biased تظهر فقدان الكفاءة بسبب التوزيع المتعلم المتحيز للإجراءات. القدرة على الأداء بمستوى مماثل لطريقة التحسين البايزي المتقدمة تظهر أن <span class="nodecor">ours</span> قادر على أداء EE المتقدم مع الاستدلال في السياق.</p>
<p>ميزة واضحة لـ<span class="nodecor">ours</span> هي أن البحث بأكمله يتم من خلال استدلال النموذج دون الحاجة إلى أي تحسين تدريجي. في المقابل، تحتاج طرق التحسين البايزي المعتمدة على GP إلى ملاءمة دالة بديلة GP في كل خطوة، مما يؤدي إلى فرق كبير في السرعة. يظهر الشكل [fig:bo_exp_time] نفس نتائج البحث مع المحور السيني كونه الوقت المنقضي. تعمل جميع الطرق على وحدة معالجة الرسومات A100 واحدة. بفضل الاستدلال في السياق، <span class="nodecor">ours</span> أسرع بكثير من طرق التحسين البايزي التقليدية.</p>
<h1 id="التفاصيل-الإضافية-لتجارب-ال-bayesian-optimization">التفاصيل الإضافية لتجارب Bayesian Optimization</h1>
<p>يمكن العثور على المزيد من التفاصيل حول تجارب Bayesian Optimization في الملحق [sec:appendix_bo_exp].</p>
<h1 id="تجارب-التعلم-المعزز">تجارب التعلم المعزز</h1>
<p>نحن نستكشف قدرة التعلم السياقي لـ<span class="nodecor">ours</span> في مشاكل التعلم المعزز التسلسلية. لإظهار قدرة التعلم في السياق، نركز على عائلات البيئات التي لا يمكن حلها من خلال التعميم الفوري لنموذج مدرب مسبقاً، لذا فإن التعلم السياقي للسياسة ضروري لحل المهام. هذا يعني أن بعض المعلومات المهمة لنجاح المهمة مفقودة من تمثيل الحالة ويجب اكتشافها من قبل الوكيل. نستخدم بيئتين من بيئات العالم الشبكي في (<span class="nodecor">NEURIPS2022_b2cac94f</span>): غرفة مظلمة ومفتاح إلى باب مظلم. تفاصيلها كالتالي.</p>
<p><strong>الغرفة المظلمة.</strong> تجرى التجربة في نموذج قرار جزئي المراقبة ثنائي الأبعاد، حيث يتم وضع وكيل داخل غرفة لتحديد موقع نقطة هدف. يمكن للوكيل الوصول إلى إحداثيات موقعه <span class="math inline">\((x,y)\)</span>، لكنه غير مدرك لمكان الهدف مما يتطلب منه استنتاج ذلك من المكافآت المستلمة. أبعاد الغرفة هي 9x9 مع الإجراءات الممكنة من قبل الوكيل تشمل التحرك خطوة واحدة إما إلى اليسار، اليمين، الأعلى أو الأسفل، أو البقاء ثابتاً، كل ذلك ضمن طول حلقة من 20. عند الانتهاء، يتم وضع الوكيل مرة أخرى في منتصف الخريطة. يتم النظر في نوعين من البيئات لهذه التجربة: حالة الغرفة المظلمة حيث يحصل الوكيل على مكافأة (r=1) في كل مرة يتم فيها تحقيق الهدف، وحالة الغرفة المظلمة الصعبة حيث المكافآت نادرة (r=1 مرة واحدة فقط لتحقيق الهدف). كلما لم تكن قيمة المكافأة 1، ستعتبر 0. بخلاف (<span class="nodecor">NEURIPS2022_b2cac94f</span>)، نحافظ على حجم الغرفة في الحالة الصعبة ليكون 9 x 9.</p>
<p><strong>المفتاح إلى الباب المظلم.</strong> هذا الإعداد مشابه للغرفة المظلمة، ولكن مع ميزات تحدي إضافية. مهمة الوكيل هي تحديد موقع مفتاح غير مرئي لتلقي مكافأة لمرة واحدة بقيمة r=1، وبعد ذلك، تحديد موقع باب غير مرئي للحصول على مكافأة أخرى لمرة واحدة بقيمة r=1. خلاف ذلك، تظل المكافأة عند r=0. موقع الوكيل الأول في كل حلقة يتم إعادة تعيينه بشكل عشوائي. حجم الغرفة لا يزال 9 x 9 ولكن طول الحلقة يزيد إلى 50 خطوة.</p>
<p>لجمع البيانات للتدريب غير المتصل، نقوم بأخذ عينات من مجموعة من الألعاب الجديدة لكل دفعة صغيرة. نجمع <span class="math inline">\(K\)</span> حلقات من كل لعبة. بفضل قدرة <span class="nodecor">EE</span> لـ<span class="nodecor">ours</span>، لا تحتاج البيانات التدريبية أن تكون من خوارزمية تعلم RL حقيقية مثل شبكة Q العميقة (<span class="nodecor">DQN</span>)، والتي تكون مكلفة للتشغيل. بدلاً من ذلك، نسمح لسياسة جمع البيانات الرخيصة بالعمل لـ<span class="math inline">\(K\)</span> حلقات بشكل مستقل ونقوم بدمج الحلقات الناتجة في تسلسل واحد. نستخدم نسخة <span class="math inline">\(\epsilon\)</span>-الجشعة من السياسة المثلى "الغش". تعرف السياسة موقع الهدف الذي لا يعرفه الوكيل وستتحرك مباشرة نحو الهدف باحتمال <span class="math inline">\(1-\epsilon\)</span> وباحتمال <span class="math inline">\(\epsilon\)</span> ستتخذ إجراء لا يقرب الوكيل من الهدف. لكل حلقة، يتم أخذ عينة <span class="math inline">\(\epsilon\)</span> من توزيع موحد بين 0 و1. بديهياً، تمتلك هذه السياسة بعض الفرص لحل اللعبة بكفاءة عندما يكون <span class="math inline">\(\epsilon\)</span> صغيراً ولكن في المتوسط لا تقدم أداء جيداً. لتجارب الغرفة المظلمة، يتكون كل تسلسل من 50 حلقة وللمفتاح إلى الباب المظلم، يتكون من 20 حلقة.</p>
<p><strong>الغرفة المظلمة (متحيزة).</strong> لإظهار فوائد <span class="nodecor">EE</span> لمشكلة التعلم المعزز التسلسلي عندما لا يمكن أن تكون سياسة جمع البيانات مثالية، نقوم بإنشاء نسخة متغيرة من بيئة الغرفة المظلمة. في كل خطوة، تتخذ سياسة جمع البيانات إجراء "اليسار" باحتمال <span class="math inline">\(2/3\)</span> وباحتمال <span class="math inline">\(1/3\)</span> تتصرف كما هو موضح أعلاه. في وقت التدريب، يمكن أن يكون الهدف في أي مكان في الغرفة و، في وقت التقييم، سيظهر الهدف فقط على الجانب الأيمن حيث <span class="math inline">\(x&gt;5\)</span>.</p>
<p>لمشاكل التعلم المعزز التسلسلية، يتكون <span class="nodecor">ours</span> من نموذجين تسلسليين: واحد لتوقع الإجراء والآخر لتوقع العائد المتبقي داخل الحلقة. يأخذ نموذج تسلسل العائد المتبقي كمدخلات تسلسل الثلاثيات الحالة، الإجراء، المكافأة ويتنبأ بالعائد المتبقي داخل الحلقة. يأخذ نموذج توقع الإجراء كمدخلات تسلسل الثلاثيات والعائدين المتبقيين <span class="math inline">\(R_{k,t}\)</span> ويتنبأ بتسلسل الإجراءات. يتم تدريب النموذجين معاً بنفس محسن التدرج. لتشجيع <span class="nodecor">ours</span> على حل الألعاب بسرعة، عند حساب العائد المتبقي داخل الحلقة، يعطى مكافأة سالبة، <span class="math inline">\(-1/T\)</span>، لكل خطوة لا تتلقى مكافأة، حيث <span class="math inline">\(T\)</span> هو طول الحلقة. كل من <span class="math inline">\(\tilde{c}_k\)</span> و<span class="math inline">\(c_{k,t}\)</span> متقطعان ومميزان.</p>
<h2 id="طرق-الأساس">طرق الأساس</h2>
<p><strong>المصدر.</strong> نستخدم سياسة جمع البيانات كأساس للمقارنة. حيث تحل سياسة جمع البيانات كل حلقة بشكل مستقل، نحسب العائد المتوسط عبر عدة حلقات.</p>
<p><strong>تقطير الخوارزمية (<span class="nodecor">laskin2023incontext</span>).</strong> خوارزمية التعلم في السياق التي تقطر خوارزميات التعلم المعزز من مسارات تدريب التعلم المعزز. تقطير الخوارزمية يتنبأ بالفعل استناداً فقط إلى الحالات الحالية وتاريخ الثلاثيات الحالة، الفعل والمكافأة. نحن نقوم بتكرار تنفيذ تقطير الخوارزمية باستخدام هندسة المحول كما في <span class="nodecor">ours</span>. نطبق تقطير الخوارزمية على نفس بيانات التدريب التي يستخدمها <span class="nodecor">ours</span> (متجاهلين إشارات العودة إلى الذهاب)، على الرغم من أنها مولدة من مسارات تعلم التعلم المعزز.</p>
<p><strong>تقطير الخوارزمية المرتب.</strong> تم تصميم تقطير الخوارزمية ليتم تدريبه على مسارات تعلم التعلم المعزز. خاصية مهمة لمسارات تعلم التعلم المعزز هي أن أداء العامل يزداد تدريجياً خلال التدريب. لتقليد مثل هذه المسارات باستخدام بياناتنا، نقوم بفرز الحلقات في تسلسل وفقاً لـ<span class="math inline">\(\epsilon\)</span> المعين من سياسة جمع البيانات بترتيب تنازلي. <span class="math inline">\(\epsilon\)</span> يحدد مدى قرب سياسة جمع البيانات من السياسة المثلى. في هذا الترتيب، تميل الحلقات في موضع لاحق من التسلسل إلى أن يكون لها عائد أعلى. نقوم بتدريب تقطير الخوارزمية باستخدام هذه التسلسلات المرتبة بدلاً من الأصلية.</p>
<p><strong>محول القرار متعدد الألعاب (<span class="nodecor">NEURIPS2022_b2cac94f</span>).</strong> محول القرار متعدد الألعاب ليس خوارزمية تعلم في السياق. نقوم بتدريب محول القرار متعدد الألعاب باستخدام حلقة واحدة فقط من كل لعبة معينة. أداء محول القرار متعدد الألعاب يظهر ما هو أداء العامل عندما لا يوجد تعلم سياسة في السياق.</p>
<h2 id="التقييم-والنتائج">التقييم والنتائج</h2>
<p>بعد التدريب، سيتم تقييم <span class="nodecor">OURS</span> في حل مجموعة من الألعاب المعينة. يتم وصف خوارزمية الاستدلال في <span class="nodecor">Alg. [alg:action_infer]</span>. لا يتم إجراء تحديث للنموذج عبر الإنترنت بواسطة <span class="nodecor">OURS</span> وجميع الطرق الأساسية في وقت التقييم. لكل لعبة معينة، سيتصرف <span class="nodecor">OURS</span> ونوعان من <span class="nodecor">AD</span> لمدة <span class="nodecor"><span class="math inline">\(K\)</span></span> حلقات متتالية. في كل حلقة، يتم استخدام المسارات من الحلقات السابقة كما هو موضح في تمثيل التاريخ. من المثالي أن يحدد العامل الأداء الجيد المعلومات المفقودة بأقل عدد ممكن من الحلقات ثم يعظم العائد في الحلقات التالية. لكل مشكلة، نقوم بأخذ عينة من <span class="nodecor">100</span> لعبة و<span class="nodecor"><span class="math inline">\(K\)</span></span> هو <span class="nodecor">50</span> لغرفة الظلام و<span class="nodecor">20</span> لمفتاح الباب.</p>
<p>تظهر نتائج التجربة في <span class="nodecor">Fig. [fig:rl_exp]</span>. يمكن لـ<span class="nodecor">OURS</span> حل الألعاب المعينة بكفاءة مقارنة بالطرق الأساسية. تسمح قدرة <span class="nodecor">EE</span> لـ<span class="nodecor">OURS</span> بالبحث عن المعلومات المفقودة بكفاءة ثم يتصرف بثقة بمجرد العثور على المعلومات المفقودة. مؤشر لهذا السلوك هو الانخفاض المستمر لتشتت الأفعال كما يختبر العامل المزيد من الحلقات.</p>
<p>كما هو متوقع، يتعلم <span class="nodecor">AD</span> الأصلي تقليد سياسة جمع البيانات، مما يؤدي إلى أداء متوسط يقل قليلاً عن سياسة جمع البيانات. يفشل <span class="nodecor">MGDT</span> في حل معظم الألعاب بسبب المعلومات المفقودة. من المثير للاهتمام، على الرغم من أن بيانات التدريب لم تتولد من خوارزمية تعلم <span class="nodecor">RL</span>، فإن <span class="nodecor">AD-sorted</span> قادر على استنساخ سلوك سياسة جمع البيانات بـ<span class="nodecor"><span class="math inline">\(\epsilon\)</span></span> مختلف في مراحل مختلفة، مما يسمح له بحل الألعاب في نهاية السلسلة.</p>
<p>لم يتم عرض <span class="nodecor">OURS-biased</span> في <span class="nodecor">Fig. [fig:rl_dark_room_easy]</span>, <span class="nodecor">Fig. [fig:rl_dark_room_hard]</span> و<span class="nodecor">Fig. [fig:rl_key2door]</span> حيث أنه يحقق أداء مماثلاً لـ<span class="nodecor">OURS</span>. السبب هو أنه لا يوجد تحيز واضح في توزيع الأفعال لسياسة جمع البيانات. ومع ذلك، كما هو موضح في <span class="nodecor">Fig. [fig:dark_room_easy_biased]</span>، بالنسبة لبيئة غرفة الظلام (متحيزة)، يتفوق <span class="nodecor">OURS</span> بوضوح على <span class="nodecor">OURS-biased</span>، حيث يمكنه التغلب على التحيز في سياسة جمع البيانات والحفاظ على عدم اليقين الكافي في توزيع الأفعال لاستكشاف الجانب الأيمن من الغرفة. يفشل <span class="nodecor">AD-sorted</span> في المهمة لأنه يستنسخ سياسة جمع البيانات، والتي من غير المرجح أن تحل المهام بسبب التحيز في الأفعال.</p>
<h1 id="الخلاصة">الخلاصة</h1>
<p>في هذه الورقة، نحلل التوزيع التنبؤي لنماذج التسلسل ونظهر أن التوزيع التنبؤي يمكن أن يحتوي على عدم اليقين الابستمولوجي، مما يلهم خلق خوارزمية EE. نقدم خوارزمية EE في السياق بتوسيع صيغة DT إلى تعلم السياسات في السياق واشتقاق هدف تدريبي غير متحيز. من خلال التجارب على مشاكل BO وRL المنفصلة، نظهر أن: (i) <span class="nodecor">ours</span> يمكن أن يؤدي EE في التعلم في السياق دون الحاجة إلى استدلال بايزي صريح؛ (ii) أداء <span class="nodecor">ours</span> يعادل أفضل طرق BO دون الحاجة إلى تحسين التدرج، مما يؤدي إلى تسريع كبير؛ (iii) يمكن حل مهام RL جديدة في غضون عشرات الحلقات.</p>
<h1 id="اشتقاق-هدف-نموذج-التسلسل">اشتقاق هدف نموذج التسلسل</h1>
<p>يمكن اشتقاق هدف الاحتمال الأقصى لنموذج التسلسل في ([eqn:ml_objective]) بالخطوات التالية. <span class="math display">\[\begin{aligned}
\mathcal{L}_\vpsi =&amp; \int p(\mY_{1:T}, \vtheta |  \mX_{1:T}) \log p_{\vpsi}(\mY_{1:T} |  \mX_{1:T}) d \mY_{1:T} d\vtheta\\
=&amp; \int p(\mY_{1:T}, \vtheta |  \mX_{1:T}) \log \prod_{t=1}^T p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) d \mY_{1:T} d\vtheta\\
=&amp;  \sum_{t=1}^T \int p(\mY_{1:T}, \vtheta |  \mX_{1:T}) \log p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) d \mY_{1:T} d\vtheta\\
=&amp;  \sum_{t=1}^T \int p(\mY_{1:t} |  \mX_{1:t}) \log p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) d \mY_{1:t} \\
=&amp;  \sum_{t=1}^T \int p(\mY_{1:t-1} |  \mX_{1:t-1}) p(\vy_{t} |   \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) \log p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) d \mY_{1:t} \\
=&amp;  \sum_{t=1}^T \int p(\mY_{1:t-1} |  \mX_{1:t-1}) \Big( p(\vy_{t} |   \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) \log p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) \\
&amp; - p(\vy_{t} |   \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) \log p(\vy_{t} |   \vx_t, \mX_{1:t-1}, \mY_{1:t-1})   \Big) d \mY_{1:t} \\
&amp; +  \int p(\mY_{1:t-1} |  \mX_{1:t-1}) p(\vy_{t} |   \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) \log p(\vy_{t} |   \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) d \mY_{1:t} \\
=&amp;  \sum_{t=1}^T \int p(\mY_{1:t-1} |  \mX_{1:t-1}) \Big( -  \KL\left(p(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})|| p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})\right) \Big)d \mY_{1:t-1} \\
&amp; + \int p(\mY_{1:t-1} |  \mX_{1:t-1}) H\Big(p(\vy_{t} |   \vx_t, \mX_{1:t-1}, \mY_{1:t-1}) \Big) d \mY_{1:t-1}\\
=&amp; - \sum_t \int p(\mY_{1:t-1} |  \mX_{1:t-1}) \KL\left(p(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})|| p_{\vpsi}(\vy_t | \vx_t, \mX_{1:t-1}, \mY_{1:t-1})\right) d \mY_{1:t-1} +C, \end{aligned}\]</span></p>
<h1 id="اشتقاق-الهدف-غير-المتحيز">اشتقاق الهدف غير المتحيز</h1>
<p>يشمل نهج محول القرار في التعلم المعزز غير المتصل تدريب توزيع الفعل مشروطاً بالعائد، مما يسمح بأخذ عينة من الفعل في وقت الاستدلال من خلال تقديم العائد المتوقع (العائد المتبقي). بما أن العائد هو نتيجة الأفعال الحالية واللاحقة، يمكن إعادة صياغة توزيع الفعل الذي يحاول النموذج تعلمه على أنه توزيع بعدي للفعل، كما هو مقدم في المعادلة ([eqn:true_action_posterior]). لاحظ أن المعادلة ([eqn:true_action_posterior]) تحدد توزيع البيانات، والذي ينبغي تمييزه عن نموذج الشبكة العصبية <span class="math inline">\(p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t})\)</span>. كما هو ملاحظ في المعادلة ([eqn:true_action_posterior])، فإن توزيع الفعل متناسب مع توزيع العائد، ويتم ترجيح ذلك بواسطة احتمال الفعل من سياسة جمع البيانات. وبما أنه التوزيع البعدي المشتق بواسطة قاعدة بايز، نسميه بتوزيع الفعل البعدي "الحقيقي".</p>
<p>لوضع هذا بشكل بديهي، إذا استطاع النموذج مطابقة المعادلة ([eqn:true_action_posterior]) بدقة، فسيؤدي ذلك إلى تحيز توزيع الفعل نحو سياسة جمع البيانات. على سبيل المثال، في تسلسل مسجل مسبقاً، إذا تم اختيار فعل بشكل عشوائي باحتمالية منخفضة جداً من سياسة جمع البيانات، ولكنه يحقق عائداً عالياً، فإن توزيع الفعل اللاحق في المعادلة ([eqn:true_action_posterior]) سيعطي احتمالاً ضئيلاً للفعل المعني، بالنظر إلى العائد العالي. على الرغم من ملاحظة عائد عالٍ بعد الفعل، مما قد يشير إلى احتمالية عالية لـ<span class="math inline">\(p(R_{k,t} | \va_{k,t}, \vo_{k,t}, \mH_{k,t})\)</span>، فإن احتمالية الفعل الناتجة مرجحة بواسطة احتمالية الفعل في سياسة جمع البيانات، <span class="math inline">\(\pi_k(\va_{k,t}|\vo_{k,t})\)</span>، مما يؤدي إلى قيمة صغيرة. لذلك، على الرغم من أن ([eqn:true_action_posterior]) هو التوزيع البعدي الحقيقي للفعل، إلا أنه ليس التوزيع المرغوب فيه لنموذجنا.</p>
<p>من الناحية المثالية، ينبغي أن يكون توزيع الفعل، كما هو موضح في المعادلة ([eqn:unbiased_action_posterior])، متناسباً فقط مع توزيع العائد وغير متأثر بسياسة جمع البيانات. مع مثل هذا التوزيع، سيتم القضاء على التقليل غير المرغوب فيه بسبب سياسة جمع البيانات، مما يحل المشكلة المذكورة.</p>
<p>كما هو موضح أعلاه، نود أن نتعلم توزيع الفعل في ([eqn:unbiased_action_posterior]) بدلاً من توزيع الفعل في ([eqn:true_action_posterior]). ومع ذلك، نظراً لأن ([eqn:true_action_posterior]) هو توزيع الفعل الحقيقي للبيانات، فإن الهدف التدريبي الشائع للأقصى درجة الإمكانية سيجعل النموذج يطابق توزيع الفعل في ([eqn:true_action_posterior]).</p>
<p>يمكن اشتقاق الهدف غير المتحيز لتعلم توزيع الفعل في ([eqn:action_correction_obj]) بالخطوات التالية.</p>
<p>لتمكين النموذج من تعلم توزيع الفعل في ([eqn:unbiased_action_posterior]) بدلاً من ذلك، نبدأ بتحديد الهدف التدريبي المرغوب كما لو أن البيانات تتبع التوزيع ([eqn:unbiased_action_posterior]): <span class="math display">\[\begin{aligned}
\mathcal{L}_{\vpsi} =&amp; \sum_{k,t} \int \hat{p}(R_{k,t}, \va_{k,t} |\vo_{k,t}, \mH_{k,t}) \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t}) dR_{k,t} d \va_{k,t} \\
=&amp; \sum_{k,t} \int\hat{p}(R_{k,t} |\vo_{k,t}, \mH_{k,t}) \Big( \int \hat{p}(\va_{k,t} |R_{k,t}, \vo_{k,t}, \mH_{k,t})  \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t})  d \va_{k,t} \Big) dR_{k,t} \\\end{aligned}\]</span></p>
<p>ثم نطبق حيلة أخذ العينات حسب الأهمية لإدخال التوزيع البعدي الحقيقي للفعل في المعادلة: <span class="math display">\[\begin{aligned}
\mathcal{L}_{\vpsi} =&amp; \sum_{k,t} \int\hat{p}(R_{k,t} |\vo_{k,t}, \mH_{k,t}) \Big( \int p(\va_{k,t} |R_{k,t}, \vo_{k,t}, \mH_{k,t}) \frac{\hat{p}(\va_{k,t} |R_{k,t}, \vo_{k,t}, \mH_{k,t})}{p(\va_{k,t} |R_{k,t}, \vo_{k,t}, \mH_{k,t})}\\
&amp;\log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t})  d \va_{k,t} \Big) dR_{k,t}\end{aligned}\]</span></p>
<p>بعد إعادة ترتيب المعادلة، نحصل على صياغة أوضح للهدف: <span class="math display">\[\begin{aligned}
\mathcal{L}_{\vpsi} =&amp; \sum_{k,t} \int\hat{p}(R_{k,t} |\vo_{k,t}, \mH_{k,t}) \Big( \int p(\va_{k,t} |R_{k,t}, \vo_{k,t}, \mH_{k,t}) \frac{\frac{p(R_{k,t}| \va_{k,t}, \vo_{k,t}, \mH_{k,t})\mathcal{U}(\va_{k,t})}{ \hat{p}(R_{k,t}| \vo_{k,t}, \mH_{k,t})}}{\frac{p(R_{k,t}| \va_{k,t}, \vo_{k,t}, \mH_{k,t})\pi_k(\va_{k,t}|\vo_{k,t})}{ p(R_{k,t}| \vo_{k,t}, \mH_{k,t})}} \\
&amp; \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t})  d \va_{k,t} \Big) dR_{k,t} \\
=&amp; \sum_{k,t} \int\hat{p}(R_{k,t} |\vo_{k,t}, \mH_{k,t}) \Big( \int p(\va_{k,t} |R_{k,t}, \vo_{k,t}, \mH_{k,t}) \frac{\mathcal{U}(\va_{k,t})p(R_{k,t}| \vo_{k,t}, \mH_{k,t})}{\pi_k(\va_{k,t}|\vo_{k,t})\hat{p}(R_{k,t}| \vo_{k,t}, \mH_{k,t})} \\
&amp; \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t})  d \va_{k,t} \Big) dR_{k,t} \\
=&amp; \sum_{k,t} \int p(R_{k,t} |\vo_{k,t}, \mH_{k,t}) \Big( \int p(\va_{k,t} |R_{k,t}, \vo_{k,t}, \mH_{k,t}) \frac{\mathcal{U}(\va_{k,t})}{\pi_k(\va_{k,t}|\vo_{k,t})} \\
&amp; \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t})  d \va_{k,t} \Big) dR_{k,t} \\
=&amp; \sum_{k,t}  \int p(R_{k,t}, \va_{k,t} | \vo_{k,t}, \mH_{k,t}) \frac{\mathcal{U}(\va_{k,t})}{\pi_k(\va_{k,t}|\vo_{k,t})}  \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t})  d \va_{k,t} dR_{k,t} \end{aligned}\]</span></p>
<p>لاحظ أن التوزيع الاحتمالي في المقدمة هو الآن التوزيع المشترك للعائد والفعل في توزيع البيانات. نطبق تقريب مونت كارلو للتكامل من خلال النظر في أن البيانات المسجلة هي عينات من توزيع البيانات. نحصل على الهدف التدريبي المقترح. <span class="math display">\[\begin{aligned}
\mathcal{L}_{\vpsi} \approx&amp; \sum_{k,t} \frac{\mathcal{U}(\va_{k,t})}{\pi_k(\va_{k,t}|\vo_{k,t})}  \log p_{\vpsi}(\va_{k,t} | R_{k,t}, \vo_{k,t}, \mH_{k,t}), \quad R_{k,t}, \va_{k,t} \sim p(R_{k,t}, \va_{k,t} | \vo_{k,t}, \mH_{k,t})  \end{aligned}\]</span></p>
<h1 id="تفاصيل-التنفيذ">تفاصيل التنفيذ</h1>
<p>تم تنفيذ <span class="nodecor">ours</span> بناءً على <span class="nodecor">nanoGPT</span><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. بالنسبة لتجارب التعلم بالتعزيز، يحتوي <span class="nodecor">ours</span> على 12 طبقة مع تضمينات بعدية مقدارها 128. هناك 4 رؤوس في الانتباه المتعدد الرؤوس. نستخدم محسن <span class="nodecor">Adam</span> مع معدل تعلم <span class="math inline">\(10^{-5}\)</span>.</p>
<h1 id="تجارب-التحسين-البيزي-1">تجارب التحسين البايزي</h1>
<p>نعتبر مشكلة التحسين البايزي المتقطعة. المهمة هي إيجاد الموقع من مجموعة ثابتة من النقاط التي تحتوي على أقل قيمة للدالة بأقل عدد ممكن من تقييمات الدالة. في بداية البحث، يتم إعطاء قيم الدالة لبعض المواقع المختارة عشوائياً. سيطلب من خوارزمية التحسين البايزي اقتراح موقع يكمن فيه الحد الأدنى للدالة، ثم يتم جمع قيمة الدالة من الموقع المقترح. سيتم تكرار حلقة الاقتراح والتقييم عدداً ثابتاً من المرات. يتم تقييم أداء خوارزمية التحسين البايزي بناءً على سرعتها في إيجاد الحد الأدنى للدالة.</p>
<p>قائمة الدوال ثنائية الأبعاد المستخدمة للتقييمات هي: برانين، بيل، بوهاتشيفسكي، بوكين <span class="nodecor">6</span>، ديجونج <span class="nodecor">5</span>، دروبويف، ايجهولدر، جولدستين برايس، هولدر تيبل، كيم <span class="nodecor">1</span>، كيم <span class="nodecor">2</span>، كيم <span class="nodecor">3</span>، ميشاليفيتش، شوبرت، سيكس هيمب كيميل، ثري هيمب كيميل.</p>
<p>تم تنفيذ خط الأساس للتحسين المتوقع باستخدام (<span class="nodecor">balandat2020botorch</span>). استخدمنا فئة “SingleTaskGP” لنموذج البديل GP، الذي يستخدم نواة ماترن <span class="nodecor">5/2</span> مع أولوية جاما على مقاييس الطول.</p>
<h1 id="النتائج-الكمية-لتجارب-التعلم-المعزز-المنفصلة">النتائج الكمية لتجارب التعلم المعزز المنفصلة</h1>
<p>يرجى العثور أدناه على المقارنة الكمية لتجارب التعلم المعزز المعروضة في الشكل. تظهر القيم المعروضة العوائد المتوسطة على مدى <span class="nodecor">100</span> لعبة معينة والقيم الموجودة بين الأقواس هي فترات الثقة لتقديرات المتوسط، والتي تتوافق مع المنطقة المظللة في الشكل. نأخذ ثلاث نقاط زمنية على طول مسارات التعلم السياقي للسياسة. بما أن <span class="nodecor">MGDT</span> لا يستطيع تحديث السياسة في وقت الاستدلال، فإننا نقدر عائداً متوسطاً واحداً لكل لعبة.</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">الغرفة المظلمة (الحلقة العاشرة)</th>
<th style="text-align: center;">الغرفة المظلمة (الحلقة الثلاثون)</th>
<th style="text-align: center;">الغرفة المظلمة (الحلقة الخمسون)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="nodecor">ICEE</span></td>
<td style="text-align: center;"><span class="nodecor">8.15</span> (<span class="nodecor">1.29</span>)</td>
<td style="text-align: center;"><span class="nodecor">12.37</span> (<span class="nodecor">1.14</span>)</td>
<td style="text-align: center;"><span class="nodecor">13.61</span> (<span class="nodecor">0.86</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="nodecor">AD</span></td>
<td style="text-align: center;"><span class="nodecor">3.74</span> (<span class="nodecor">1.15</span>)</td>
<td style="text-align: center;"><span class="nodecor">4.51</span> (<span class="nodecor">1.17</span>)</td>
<td style="text-align: center;"><span class="nodecor">4.03</span> (<span class="nodecor">1.15</span>)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="nodecor">AD-sorted</span></td>
<td style="text-align: center;"><span class="nodecor">0.05</span> (<span class="nodecor">0.05</span>)</td>
<td style="text-align: center;"><span class="nodecor">3.83</span> (<span class="nodecor">0.87</span>)</td>
<td style="text-align: center;"><span class="nodecor">12.48</span> (<span class="nodecor">1.37</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="nodecor">MGDT</span></td>
<td style="text-align: center;"><span class="nodecor">1.86</span> (<span class="nodecor">0.93</span>)</td>
<td style="text-align: center;"><span class="nodecor">1.86</span> (<span class="nodecor">0.93</span>)</td>
<td style="text-align: center;"><span class="nodecor">1.86</span> (<span class="nodecor">0.93</span>)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">المصدر</td>
<td style="text-align: center;"><span class="nodecor">5.13</span> (<span class="nodecor">1.19</span>)</td>
<td style="text-align: center;"><span class="nodecor">5.13</span> (<span class="nodecor">1.19</span>)</td>
<td style="text-align: center;"><span class="nodecor">5.13</span> (<span class="nodecor">1.19</span>)</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">الغرفة المظلمة (صعبة) (العاشرة)</th>
<th style="text-align: center;">الغرفة المظلمة (صعبة) (الثلاثون)</th>
<th style="text-align: center;">الغرفة المظلمة (صعبة) (الخمسون)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="nodecor">ICEE</span></td>
<td style="text-align: center;"><span class="nodecor">0.48</span> (<span class="nodecor">0.10</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.74</span> (<span class="nodecor">0.09</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.79</span> (<span class="nodecor">0.08</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="nodecor">AD</span></td>
<td style="text-align: center;"><span class="nodecor">0.33</span> (<span class="nodecor">0.09</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.43</span> (<span class="nodecor">0.10</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.43</span> (<span class="nodecor">0.10</span>)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="nodecor">AD-sorted</span></td>
<td style="text-align: center;"><span class="nodecor">0.08</span> (<span class="nodecor">0.05</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.55</span> (<span class="nodecor">0.10</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.75</span> (<span class="nodecor">0.08</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="nodecor">MGDT</span></td>
<td style="text-align: center;"><span class="nodecor">0.09</span> (<span class="nodecor">0.06</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.09</span> (<span class="nodecor">0.06</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.09</span> (<span class="nodecor">0.06</span>)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">المصدر</td>
<td style="text-align: center;"><span class="nodecor">0.51</span> (<span class="nodecor">0.10</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.51</span> (<span class="nodecor">0.10</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.51</span> (<span class="nodecor">0.10</span>)</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">المفتاح إلى الباب المظلم (الخامسة)</th>
<th style="text-align: center;">المفتاح إلى الباب المظلم (العاشرة)</th>
<th style="text-align: center;">المفتاح إلى الباب المظلم (العشرون)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="nodecor">ICEE</span></td>
<td style="text-align: center;"><span class="nodecor">1.04</span> (<span class="nodecor">0.15</span>)</td>
<td style="text-align: center;"><span class="nodecor">1.50</span> (<span class="nodecor">0.12</span>)</td>
<td style="text-align: center;"><span class="nodecor">1.84</span> (<span class="nodecor">0.08</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="nodecor">AD</span></td>
<td style="text-align: center;"><span class="nodecor">0.67</span> (<span class="nodecor">0.15</span>)</td>
<td style="text-align: center;"><span class="nodecor">1.02</span> (<span class="nodecor">0.17</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.94</span> (<span class="nodecor">0.17</span>)</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="nodecor">AD-sorted</span></td>
<td style="text-align: center;"><span class="nodecor">0.17</span> (<span class="nodecor">0.08</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.84</span> (<span class="nodecor">0.14</span>)</td>
<td style="text-align: center;"><span class="nodecor">1.77</span> (<span class="nodecor">0.09</span>)</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="nodecor">MGDT</span></td>
<td style="text-align: center;"><span class="nodecor">0.34</span> (<span class="nodecor">0.11</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.34</span> (<span class="nodecor">0.11</span>)</td>
<td style="text-align: center;"><span class="nodecor">0.34</span> (<span class="nodecor">0.11</span>)</td>
</tr>
<tr class="odd">
<td style="text-align: center;">المصدر</td>
<td style="text-align: center;"><span class="nodecor">1.10</span> (<span class="nodecor">0.19</span>)</td>
<td style="text-align: center;"><span class="nodecor">1.10</span> (<span class="nodecor">0.19</span>)</td>
<td style="text-align: center;"><span class="nodecor">1.10</span> (<span class="nodecor">0.19</span>)</td>
</tr>
</tbody>
</table>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://github.com/karpathy/nanoGPT" class="uri">https://github.com/karpathy/nanoGPT</a><a href="#fnref1">↩</a></p></li>
</ol>
</section>
</body>
</html>
