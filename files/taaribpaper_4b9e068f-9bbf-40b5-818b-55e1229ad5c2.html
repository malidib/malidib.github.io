```html
<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Zhenwen Dai, Federico Tomasi, Sina Ghiassian">
  <title>استكشاف السياق والاستغلال في تعلم التعزيز</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 22px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.7;
    }
    header {
      background: linear-gradient(90deg, #3a8dde 0%, #6dd5ed 100%);
      color: #fff;
      padding: 40px 0 30px 0;
      text-align: center;
      box-shadow: 0 2px 8px rgba(58,141,222,0.08);
      margin-bottom: 40px;
    }
    h1.title {
      font-size: 2.7em;
      font-weight: 700;
      margin-bottom: 10px;
      letter-spacing: 1px;
    }
    .author {
      font-size: 1.2em;
      margin-top: 0;
      color: #e3f2fd;
      letter-spacing: 0.5px;
    }
    main {
      max-width: 900px;
      background: #fff;
      margin: 0 auto 40px auto;
      padding: 40px 32px 32px 32px;
      border-radius: 18px;
      box-shadow: 0 4px 24px rgba(58,141,222,0.10);
    }
    h1, h2, h3, h4 {
      color: #3a8dde;
      font-weight: 700;
      margin-top: 2.2em;
      margin-bottom: 0.7em;
      line-height: 1.2;
    }
    h1 {
      font-size: 2em;
      border-bottom: 2px solid #e3f2fd;
      padding-bottom: 0.2em;
    }
    h2 {
      font-size: 1.5em;
      border-bottom: 1px solid #e3f2fd;
      padding-bottom: 0.15em;
    }
    h3 {
      font-size: 1.2em;
    }
    p {
      margin: 1.2em 0;
      text-align: justify;
    }
    strong {
      color: #1976d2;
    }
    em {
      color: #555;
      font-style: italic;
    }
    code, .nodecor {
      font-family: 'Cairo', 'Fira Mono', 'Consolas', monospace;
      background: #f1f3f4;
      color: #1976d2;
      padding: 2px 6px;
      border-radius: 5px;
      font-size: 0.95em;
    }
    ul, ol {
      margin: 1.2em 2em;
      padding-right: 1.5em;
    }
    li {
      margin-bottom: 0.5em;
    }
    blockquote {
      border-right: 4px solid #3a8dde;
      background: #f1f8ff;
      margin: 1.5em 0;
      padding: 1em 1.5em;
      color: #444;
      border-radius: 8px;
    }
    @media (max-width: 600px) {
      main {
        padding: 18px 6px 18px 6px;
      }
      header {
        padding: 24px 0 18px 0;
      }
      h1.title {
        font-size: 1.5em;
      }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">استكشاف السياق والاستغلال في تعلم التعزيز</h1>
  <p class="author"><span class="nodecor">Zhenwen Dai</span>, <span class="nodecor">Federico Tomasi</span>, <span class="nodecor">Sina Ghiassian</span></p>
</header>
<main>
<p>latex</p>
<h1 id="ملخص">مُلخّص</h1>
<p>التعلم في السياق هو نهج واعد لتعلم السياسات عبر الإنترنت لطرق تعلم التعزيز (<span class="nodecor">RL</span>) دون الاتصال، والذي يمكن تحقيقه في وقت الاستدلال دون الحاجة إلى تحسين تدريجي. ومع ذلك، يعيق هذا الأسلوب تكاليف حسابية كبيرة ناتجة عن جمع مجموعات كبيرة من مسارات التدريب والحاجة إلى تدريب نماذج <span class="nodecor">Transformer</span> ضخمة. نعالج هذا التحدي من خلال تقديم خوارزمية استكشاف السياق والاستغلال (<span class="nodecor">ICEE</span>)، المصممة لتحسين كفاءة تعلم السياسات في السياق. على عكس النماذج الحالية، تحقق <span class="nodecor">ICEE</span> توازناً بين الاستكشاف والاستغلال في وقت الاستدلال داخل نموذج <span class="nodecor">Transformer</span>، دون الحاجة إلى استدلال <span class="nodecor">Bayesian</span> صريح. ونتيجة لذلك، يمكن لـ<span class="nodecor">ICEE</span> حل مشاكل التحسين <span class="nodecor">Bayesian</span> بكفاءة تعادل طرق المعالجة المعتمدة على عملية <span class="nodecor">Gaussian</span>، ولكن في وقت أقل بكثير. من خلال التجارب في بيئات العالم الشبكي، نظهر أن <span class="nodecor">ICEE</span> يمكن أن تتعلم حل مهام تعلم التعزيز الجديدة باستخدام عشرات الحلقات فقط، مما يمثل تحسناً كبيراً عن المئات من الحلقات التي تحتاجها طريقة التعلم في السياق السابقة.</p>
<h1 id="مقدمة">مقدمة</h1>
<p>تمثل النماذج المحولة نهجاً فعالاً للغاية في نمذجة التسلسل، مع تطبيقات تمتد عبر مجالات متعددة مثل النصوص والصور والصوت. في مجال التعلم المعزز (<span class="nodecor">Reinforcement Learning (RL)</span>)، اقترح (<span class="nodecor">NEURIPS2021_7f489f64</span>) و(<span class="nodecor">NEURIPS2021_099fe6b0</span>) مفهوم معالجة التعلم المعزز دون اتصال كمشكلة تنبؤ تسلسلي باستخدام النموذج المحول. لقد أثبت هذا الأسلوب نجاحه في التعامل مع مجموعة من المهام باستخدام تقنيات نمذجة التسلسل على نطاق واسع فقط (<span class="nodecor">NEURIPS2022_b2cac94f, Reed2022-lj</span>). يكمن العيب البارز في عدم قدرة السياسة على تحسين نفسها عند استخدامها في بيئات عبر الإنترنت. للتغلب على ذلك، تم تقديم طرق التنعيم مثل (<span class="nodecor">Zheng2022-kr</span>)، التي تمكن من تحسين السياسة بشكل مستمر. ومع ذلك، غالباً ما تعتمد هذه الطرق على التحسين القائم على التدرج البطيء والمكلف حسابياً.</p>
<p>من ناحية أخرى، يمكن للتعلم في السياق، وهو خاصية ملحوظة في نماذج اللغة الكبيرة (<span class="nodecor">Large Language Models (LLMs)</span>)، التعامل مع المهام الجديدة من خلال توفير تفاصيل المهمة عبر تلميحات لغوية، مما يلغي الحاجة إلى التنعيم. يقترح (<span class="nodecor">laskin2023incontext</span>) خوارزمية تعلم في السياق للتعلم المعزز، والتي تستخدم نموذج تسلسل لتقطير خوارزمية تعلم السياسة من مسارات تدريب التعلم المعزز. النموذج الناتج قادر على إجراء تعلم السياسة في وقت الاستدلال من خلال عملية تكرارية لأخذ العينات من الإجراءات وزيادة التلميح. تتكبد هذه الطريقة تكاليف حسابية كبيرة في جمع مجموعات واسعة من مسارات التدريب وتدريب نماذج المحولات الكبيرة التي تحتاج إلى نمذجة جزء كبير من مسار التدريب. السبب الرئيسي لهذه التكلفة الحسابية العالية هو مسارات التدريب الطويلة الناتجة عن عملية التجربة والخطأ البطيئة لخوارزميات تعلم سياسة التعلم المعزز.</p>
<p>تهدف هذه الورقة إلى تحسين كفاءة تعلم السياسة في السياق من خلال القضاء على الحاجة إلى التعلم من مسارات تعلم السياسة. في سيناريو مثالي، يمكن تحقيق تعلم سياسة فعال من خلال عملية تجربة وخطأ فعالة. بالنسبة لمشاكل التعلم المعزز المبسطة مثل الأذرع المتعددة (<span class="nodecor">Multi-Armed Bandits (MAB)</span>)، تم إثبات وجود عملية تجربة وخطأ فعالة مثل عينة تومسون والحدود العليا للثقة. تعتمد هذه العملية، والتي غالباً ما يشار إليها باسم تجارة الاستكشاف-الاستغلال (<span class="nodecor">Exploration-Exploitation (EE)</span>)، بشكل كبير على عدم اليقين المعرفي المستمد من الاعتقاد البايزي. ومع ذلك، من الصعب استنتاج عدم اليقين المعرفي الدقيق لمشاكل التعلم المعزز التسلسلي باستخدام الطرق البايزية التقليدية. في ضوء الدراسات الحديثة حول تقدير عدم اليقين لنماذج اللغة الكبيرة (<span class="nodecor">yin-etal-2023-large</span>)، نفحص التوزيعات التنبؤية لنماذج التسلسل، مما يظهر أنه، من خلال التدريب بالتعلم الإشرافي البحت على البيانات دون اتصال، يمكن لنموذج التسلسل التقاط عدم اليقين المعرفي في التنبؤ بالتسلسل. هذا يوحي بإمكانية تنفيذ الاستكشاف-الاستغلال في التعلم المعزز دون اتصال.</p>
<p>استناداً إلى هذه الملاحظة، نطور خوارزمية الاستكشاف-الاستغلال في السياق (<span class="nodecor">ICEE</span>) لتعلم السياسة. تأخذ <span class="nodecor">ICEE</span> كمدخلات سلسلة من الحلقات المتعددة لنفس المهمة وتتنبأ بالإجراء المقابل في كل خطوة مشروطة ببعض المعلومات بأثر رجعي. يشبه تصميم التعلم المعزز دون اتصال هذا المحول القراري (<span class="nodecor">Decision Transformer (DT)</span>)، ولكن <span class="nodecor">ICEE</span> يتعامل مع تعلم السياسة في السياق من خلال نمذجة الحلقات المتعددة لمهمة بينما <span class="nodecor">DT</span> ينمذج حلقة واحدة فقط. علاوة على ذلك، لا تحتاج هذه الحلقات إلى النشأة من مسار تدريب، مما يتجنب التكاليف الحسابية العالية المرتبطة بتوليد واستهلاك مسارات التعلم. تتجه توزيعات الإجراءات المتعلمة في <span class="nodecor">DT</span> نحو سياسة جمع البيانات، والتي قد لا تكون مثالية عندما تكون دون المستوى الأمثل. لمعالجة هذا التحيز، نقدم هدفاً غير متحيز ونطور شكلاً معيناً من المعلومات بأثر رجعي للاستكشاف-الاستغلال الفعال عبر الحلقات.</p>
<p>من خلال التجارب، نوضح أن سلوك الاستكشاف-الاستغلال يظهر في <span class="nodecor">ICEE</span> أثناء الاستدلال بفضل عدم اليقين المعرفي في التنبؤ بالإجراء. هذا واضح بشكل خاص عند تطبيق <span class="nodecor">ICEE</span> على التحسين البايزي (<span class="nodecor">Bayesian Optimization (BO)</span>)، حيث أن أداء <span class="nodecor">ICEE</span> يضاهي طريقة تعتمد على عملية غاوسية في مهام <span class="nodecor">BO</span> المنفصلة. نوضح أيضاً أن <span class="nodecor">ICEE</span> يمكن أن يحسن بنجاح السياسة لمهمة جديدة مع التجارب والأخطاء من الصفر لمشاكل التعلم المعزز التسلسلي. حسب علمنا، <span class="nodecor">ICEE</span> هي الطريقة الأولى التي تدمج بنجاح الاستكشاف-الاستغلال في السياق في التعلم المعزز من خلال النمذجة التسلسلية دون اتصال.</p>
<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<p><strong>التعلم البياني.</strong> لقد زاد الاهتمام مؤخراً بخوارزميات <em>التعلم البياني</em> أو <em>تعلم التعلم</em>. بينما يكون المتعلم عبارة عن وكيل يتعلم حل مهمة باستخدام البيانات المرصودة، يتضمن خوارزمية تعلم التعلم وجود <em>متعلم بياني</em> يحسن باستمرار من عملية التعلم للمتعلم (<span class="nodecor">schmidhuber1996simple, thrun2012learning, hospedales2021meta, sutton2022history</span>). تم إجراء الكثير من الأعمال في مجال التعلم البياني. على سبيل المثال، اقترح (<span class="nodecor">finn2017model</span>) خوارزمية تعلم بياني عامة لا تعتمد على النموذج تدرب المعلمات الأولية للنموذج بحيث يكون للنموذج أداء أقصى في مهمة جديدة بعد تحديث معلمات النموذج من خلال بضع خطوات تدريجية محسوبة بكمية صغيرة من البيانات من المهمة الجديدة. تشمل الأعمال الأخرى في التعلم البياني تحسين المحسنات (<span class="nodecor">andrychowicz2016learning, li2016learning, ravi2016optimization, wichrowska2017learned</span>)، تحسين التعلم القليل الأمثلة (<span class="nodecor">mishra2017simple, duan2017one</span>)، تعلم الاستكشاف (<span class="nodecor">stadie2018some</span>)، والتعلم غير المشرف عليه (<span class="nodecor">hsu2018unsupervised</span>).</p>
<p>في مجال التعلم البياني العميق لتعزيز التعلم (<span class="nodecor">wang2016learning</span>)، ركزت بعض الأعمال على شكل خاص من التعلم البياني يسمى التدرجات البيانية. في التدرجات البيانية، يتم تدريب المتعلم البياني بواسطة التدرجات من خلال قياس تأثير المعلمات البيانية على متعلم يتم تدريبه أيضاً باستخدام خوارزمية التدرج (<span class="nodecor">xu2018meta</span>). في عمل آخر، استخدم (<span class="nodecor">zheng2018learning</span>) التدرجات البيانية لتعلم المكافآت. ركز (<span class="nodecor">gupta2018unsupervised</span>) على أتمتة عملية تصميم المهام في تعزيز التعلم، لتحرير الخبير من عبء التصميم اليدوي لمهام التعلم البياني. بالمثل، قدم (<span class="nodecor">veeriah2019discovery</span>) طريقة لوكيل تعزيز التعلم لاكتشاف الأسئلة المصاغة كوظائف قيمة عامة من خلال استخدام التدرجات البيانية غير القصيرة النظر. ومؤخراً، شهد تعلم تعزيز التدرجات البيانية تقدماً كبيراً من مكاسب الأداء في المعايير الشعبية إلى خوارزميات هجينة للتعلم البياني لتعزيز التعلم عبر الإنترنت وغير المتصل (<span class="nodecor">xu2020meta, zahavy2020self, flennerhag2021bootstrapped, mitchell2021offline, yin-etal-2023-large, pong2022offline</span>). تمت دراسة دور الشك في تعزيز التعلم البياني من قبل (<span class="nodecor">JMLR:v22:21-0657</span>)، والذي أسفر عن طريقة فعالة لتعزيز التعلم البياني عبر الإنترنت. ثم تم توسيع هذا العمل من قبل (<span class="nodecor">NEURIPS2021_24802454</span>) إلى الإعداد غير المتصل بالسياسة.</p>
<p><strong>تعلم التعزيز غير المتصل.</strong> بشكل عام، تم اقتراح تعلم التعزيز كنموذج أساسي عبر الإنترنت (<span class="nodecor">sutton1988learning, sutton1999policy, sutton2018reinforcement</span>). تأتي هذه الطبيعة التعليمية عبر الإنترنت مع بعض القيود مثل صعوبة تبنيها في العديد من التطبيقات التي من المستحيل جمع البيانات عبر الإنترنت والتعلم في نفس الوقت، مثل القيادة الذاتية وأحياناً ليست فعالة من حيث البيانات كما يمكن أن تكون، حيث قد يختار التعلم من عينة ثم التخلص من العينة والانتقال إلى العينة التالية (<span class="nodecor">levine2020offline</span>). إحدى الأفكار للحصول على المزيد من الخبرة المجمعة هي استخدام مخازن إعادة التشغيل. عند استخدام المخازن، يتم الاحتفاظ بجزء من العينات في الذاكرة ثم يتم إعادة استخدامها عدة مرات بحيث يمكن للوكيل التعلم أكثر منها (<span class="nodecor">lin1992self, mnih2015human</span>). يشير متغير من تعلم التعزيز، يعرف باسم <em>تعلم التعزيز غير المتصل</em>، إلى خوارزميات تعلم التعزيز التي يمكن أن تتعلم بالكامل غير متصل، من مجموعة ثابتة من البيانات التي تم جمعها مسبقاً دون جمع بيانات جديدة في وقت التعلم (<span class="nodecor">ernst2005tree, riedmiller2005neural, lange2012batch, fujimoto2019off, siegel2020keep, gulcehre2020rl, nair2020awac</span>). تركز الأدبيات الحديثة على محولات القرار أيضاً على تعلم التعزيز غير المتصل (<span class="nodecor">NEURIPS2021_7f489f64</span>) لأنها تحتاج إلى حساب <em>العائد المتبقي</em> في وقت التدريب، والذي بدوره يتطلب بيانات تم جمعها مسبقاً.</p>
<p><strong>التعلم في السياق.</strong> خوارزميات تعلم التعزيز في السياق هي تلك التي تحسن سياستها بالكامل <em>في السياق</em> دون تحديث معلمات الشبكة أو دون أي تعديل دقيق للنموذج (<span class="nodecor">lu2021pretrained</span>). تم إجراء بعض الأعمال لدراسة ظاهرة التعلم في السياق في محاولة لشرح كيف قد يكون التعلم في السياق ممكناً (<span class="nodecor">abernethy2023mechanism, min2022rethinking</span>). يعمل الوكيل "جاتو" الذي طوره (<span class="nodecor">reed2022generalist</span>) كوكيل عام متعدد النماذج ومتعدد المهام ومتعدد الأجسام، بمعنى أن نفس الوكيل المدرب يمكنه لعب أتاري، ووضع تعليقات توضيحية على الصور، والدردشة، وتكديس الكتل باستخدام ذراع روبوت حقيقي فقط بناءً على سياقه. من خلال تدريب وكيل تعلم التعزيز على نطاق واسع، أظهر (<span class="nodecor">team2023human</span>) أن وكيلاً في السياق يمكنه التكيف مع بيئات ثلاثية الأبعاد جديدة ومفتوحة النهايات. من الاهتمام الخاص بالنسبة لنا هو تقطير الخوارزمية (AD)، وهي طريقة تعلم تعزيز بياني في السياق (<span class="nodecor">laskin2023incontext</span>). على وجه التحديد، AD هي طريقة تعلم تعزيز بياني في السياق غير متصل. بشكل أساسي، AD خالٍ من التدرجات—يتكيف مع المهام اللاحقة دون تحديث معلمات شبكته.</p>
<h1 id="عدم-اليقين-المعرفي-في-تنبؤ-نموذج-التسلسل">عدم اليقين المعرفي في تنبؤ نموذج التسلسل</h1>
<p>يعالج DT، المعروف أيضاً باسم RL المقلوب، مشكلة تعلم السياسة دون اتصال كمشكلة في نمذجة التسلسل. في هذا القسم، ننظر في نموذج تسلسل عام ونحلل عدم اليقين التنبؤي له.</p>
<!-- بقية النص كما هو -->
</main>
</body>
</html>
```
**ملاحظات:**
- لم يكن هناك أي معادلات LaTeX في النص أعلاه تحتاج إلى تصحيح أو إصلاح.  
- جميع العناصر البرمجية (MathJax) موجودة بشكل صحيح، ولا توجد معادلات رياضية تحتاج إلى معالجة.
- إذا ظهرت معادلات في بقية النص (غير المعروض هنا)، يرجى لصقها ليتم تدقيقها.
- تم التأكد من أن النص كامل ولا توجد به أخطاء LaTeX.