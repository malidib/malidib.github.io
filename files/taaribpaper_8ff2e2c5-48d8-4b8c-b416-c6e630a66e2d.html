<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque, University of Montreal &amp; Mila firstname.lastname@umontreal.ca Shunichi Akatsuka, Hitachi, Ltd. shunichi.akatsuka.bo@hitachi.com Aaron Courville University of Montreal &amp; Mila aaron.courville@umontreal.ca">
  <title>تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</h1>
<p class="author"><span class="nodecor">Milad Aghajohari</span>, <span class="nodecor">Tim Cooijmans</span>, <span class="nodecor">Juan Agustin Duque</span>,<br />
<span class="nodecor">University of Montreal &amp; Mila</span><br />
<code>firstname.lastname@umontreal.ca</code><br />
<strong><span class="nodecor">Shunichi Akatsuka</span></strong>,<br />
<span class="nodecor">Hitachi, Ltd.</span><br />
<span class="nodecor">shunichi.akatsuka.bo@hitachi.com</span><br />
<strong><span class="nodecor">Aaron Courville</span></strong><br />
<span class="nodecor">University of Montreal &amp; Mila</span><br />
<code>aaron.courville@umontreal.ca</code><br /></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>نستكشف تحدي تعلم التعزيز العميق متعدد الوكلاء في بيئات تنافسية جزئية، حيث تواجه الأساليب التقليدية صعوبات في تعزيز التعاون القائم على المعاملة بالمثل. يتعلم وكلاء <span class="nodecor">LOLA</span> و<span class="nodecor">POLA</span> سياسات تعاونية قائمة على المعاملة بالمثل من خلال التفاضل على عدد محدود من خطوات تحديث الخصم المستقبلية. إلا أن لهذه التقنيات قيدًا أساسيًا: نظرًا لاعتمادها على عدد قليل من خطوات التحسين، قد يستغلها خصم قادر على اتخاذ خطوات إضافية لتعظيم عائده. استجابةً لذلك، نقدم نهجًا جديدًا يسمى تشكيل الاستجابة المثلى (<span class="nodecor">BRS</span>)، الذي يوظف خصمًا يحاكي الاستجابة المثلى، ويُطلق عليه "المحقِّق". لتكييف المحقِّق مع سياسة الوكيل في الألعاب المعقدة، نقترح آلية تكيف قابلة للتفاضل تعتمد على الحالة، ميسّرة عبر "الإجابة على الأسئلة" لاستخراج تمثيل للوكيل بناءً على سلوكه في مواقف بيئية محددة. للتحقق من صحة طريقتنا تجريبيًا، نعرض أداء نماذجنا المحسّن مقابل خصم <span class="nodecor">Monte Carlo Tree Search (MCTS)</span> الذي يعمل كتقريب للاستجابة المثلى في لعبة القطع النقدية. يوسّع هذا العمل نطاق تطبيق تعلم التعزيز متعدد الوكلاء في البيئات التنافسية الجزئية ويمهّد طريقًا جديدًا نحو تحقيق رفاهية اجتماعية أفضل في الألعاب ذات المنفعة الجماعية.</p>
<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>مكنت خوارزميات التعلم المعزز متعدد الوكلاء من تحقيق أداء متميز في ألعاب معقدة وعالية الأبعاد مثل لعبة الذهاب (<span class="nodecor">AlphaGo</span>) وستار كرافت (<span class="nodecor">AlphaStar</span>). الهدف الأسمى من التعلم المعزز هو تدريب وكلاء قادرين على مساعدة البشر في حل المشكلات الصعبة. لا محالة، سيحتاج هؤلاء الوكلاء إلى الاندماج في سيناريوهات الحياة الواقعية التي تتطلب التفاعل مع البشر ووكلاء تعلم آخرين. فعلى الرغم من تفوق التدريب متعدد الوكلاء في البيئات التعاونية أو التنافسية الكاملة، غالبًا ما يفشل في اكتشاف تعاون قائم على المعاملة بالمثل في البيئات التنافسية الجزئية. مثال بارز على ذلك غياب قدرة وكلاء MARL التقليديين على تعلم استراتيجيات كالرد بالمثل في معضلة السجين المتكررة.</p>
<p>رغم الطابع التمثيلي لألعاب المنفعة الجماعية الشائعة مثل معضلة السجين، تتكرر مثل هذه المشكلات في المجتمع والطبيعة. تخيل سيناريو تحاول فيه دولتان (وكلاء) تعظيم إنتاجهما الصناعي، مع ضمان مناخ عمل مناسب يحدّ من الانبعاثات الكربونية. من ناحية، ترغب كل دولة في أن تفي الأخرى بالتزاماتها البيئية؛ ومن ناحية أخرى، قد يغريهما التزايد في الانبعاثات لتحقيق عوائد صناعية أكبر. تلزم المعاهدة الفعّالة كل دولة—من خلال تهديد بالعقوبات—بالالتزام بالحدود المتفق عليها للانبعاثات. وإذا أخفق الوكلاء في تطوير استراتيجيات كالمعاملة بالمثل، فمن المرجح أن ينتهي بهما المطاف بتصعيد متبادل مؤسف في استهلاك الطاقة والانبعاثات.</p>
<p>قدمت (<span class="nodecor">LOLA</span>) خوارزمية تراعي تعلم الخصم، نجحت في اكتشاف سلوك المعاملة بالمثل في معضلة السجين المتكررة من خلال التفاضل بالنسبة لخطوة تحديث واحدة بسيطة يقوم بها الخصم. بناءً على ذلك، قدمت (<span class="nodecor">POLA</span>) طريقة "تحديث سياسة الخصم القريب" التي تعزز LOLA عبر افتراض تحديث مبسّط لسياسة الخصم، ما أتاح تدريب الشبكات العصبية في ألعاب أكثر تعقيدًا مثل لعبة القطع النقدية. وإلى حد علمنا، يُعدّ أسلوب POLA الطريقة الوحيدة التي تتيح تدريب وكلاء يتعاونون بمعاملة بالمثل في هذه اللعبة بشكل موثوق.</p>
<p>وعلى الرغم من نجاحه في لعبة القطع النقدية، فإن لأسلوب POLA حدودًا. فاستمداده لمعرفة الخصم يقتصر على عدد محدود من خطوات التحسين المستقبلية، مما يجعله عرضة للاستغلال من قبل خصوم قادرين على اتخاذ مزيد من التحسينات. تُظهر تحليلاتنا أن خصمًا يتعلم خصيصًا لتعظيم عائده ضد سياسة وكيل مدربة بواسطة POLA يستغله لتحقيق عوائد أعلى. كما تعيق هذه القيود قابلية التوسع حين يكون الخصم شبكة عصبية معقدة تتطلب العديد من خطوات التحسين لمحاكاة تعلمها الحقيقي.</p>
<p>في هذه الورقة نقدم نهجًا جديدًا نسميه تشكيل الاستجابة المثلى. يعتمد على تصميم خصم يحاكي سياسة الاستجابة المثلى ضد وكيل معين، وهو ما نسميه "المحقِّق". يوضّح الشكل [fig:cobalt] الإطار العام: يخضع المحقِّق للتدريب ضد مجموعة متنوعة من وكلاء التدريب، ثم ندرّب الوكيل عبر التفاضل عبر المحقِّق. على عكس LOLA وPOLA اللتين تفترضان عددًا قليلاً من خطوات التحسين المستقبلية للخصم، يعتمد أسلوبنا على أن ينشئ المحقِّق استجابة مثلى للوكيل الحالي من خلال التكيف الديناميكي للسياسة.</p>
<p>نعتمد على التجارب في معضلة السجين المتكررة ولعبة القطع النقدية. وبما أن نتائج الوكيل تعتمد على قدرته في مواجهة خصم معتدل الاستجابة، تكون المقارنة المنطقية مع خصم يستجيب بأفضل شكل ممكن، وهو ما نقربه عبر بحث شجرة مونت كارلو. نظهر أنه بينما لا يتعاون <span class="nodecor">MCTS</span> تمامًا مع وكلاء LOLA/POLA، فإنه يتعاون بالكامل مع وكلاء تشكيل الاستجابة المثلى.</p>
<p><strong>المُسَاهَمَات الرَئِيسِيَّة</strong>: نُلَخِّص مساهماتنا فيما يلي:</p>
<ul>
<li><p>نُبيِّن أن الخصم المُحسَّن بواسطة بحث شجرة مونت كارلو لا يتعاون مع وكلاء LOLA/POLA، بل يستغلهم لتحقيق عوائد أعلى من التعاون الكامل.</p></li>
<li><p>لتجاوز هذا الضعف، نقدم أسلوب تشكيل الاستجابة المثلى، الذي يدرب وكيلًا عبر التفاضل عبر خصم يحاكي الاستجابة المثلى ("المحقِّق"). نُثبت تجريبيًا أن وكلاء BRS يتعاونون بصورة كاملة كما هو مبين في الشكل [fig:coin_main_compare].</p></li>
<li><p>نقترح كذلك آلية تكيف قابلة للتفاضل وواعية بالحالة للمحقِّق، تُمكّنه من التكيف مع سياسة الوكيل.</p></li>
</ul>
<h1 id="sec:background">الخَلْفِيَّة</h1>
<h2 id="تعلم-التعزيز-المتعدد-العوامل">تَعَلُّم التَّعْزِيز المُتَعَدِّد العَوامِل</h2>
<p>تُعَرَّف لعبة ماركوف متعددة العوامل بالرمز <span class="math inline">\(\bm{(} N, \mathcal{S},\left\{\mathcal{A}^i\right\}_{i=1}^N, \mathbb{P},\left\{r^i\right\}_{i = 1}^N, \gamma \bm{)}\)</span>. هنا، <span class="math inline">\(N\)</span> تمثل عدد العوامل، <span class="math inline">\(\mathcal{S}\)</span> فضاء الحالات، و<span class="math inline">\(\mathcal{A}:=\mathcal{A}^1 \times \cdots \times \mathcal{A}^N\)</span> مجموعة الأفعال لكل عامل. احتمالات انتقال الحالة ممثلة بـ <span class="math inline">\(\mathbb{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\)</span> والمكافأة بـ <span class="math inline">\(r^i: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span>. أخيرًا، <span class="math inline">\(\gamma \in [0,1]\)</span> هو عامل الخصم. يحاول كل عامل تعظيم عائده <span class="math inline">\(R^i = \sum_{t=0}^\infty \gamma^t r^i_t\)</span>. تمثل سياسة العامل <span class="math inline">\(i\)</span> بـ <span class="math inline">\(\pi^{i}_{\theta_{i}}\)</span> حيث <span class="math inline">\(\theta_i\)</span> معاملات الشبكة العصبية. يتم تدريب هذه السياسات باستخدام مقدرات التدرج مثل REINFORCE (<span class="nodecor">reinforce</span>).</p>
<h2 id="المعضلات-الاجتماعية-ومعضلة-السجين-المتكررة">المُعْضِلات الاِجْتِمَاعِيَّة ومُعْضِلَة السَّجِين المُتَكَرِّرَة</h2>
<p>في الألعاب ذات المنفعة الجماعية تظهر معضلات اجتماعية عندما يسعى كل وكيل لتعظيم مكافأته الشخصية فيقوّض الناتج الجماعي أو الرفاهية الاجتماعية. يصبح هذا جليًا حين تكون النتيجة الجماعية أدنى من تلك التي يمكن تحقيقها بالتعاون الكامل. توضح نماذج نظرية مثل معضلة السجين كيف أن كل مشارك، رغم أنه أفضل حالًا حين يعترف، يؤدي اعترافه إلى مكافأة جماعية أقل مما لو بقي صامتًا.</p>
<p>في معضلة السجين المتكررة (<span class="nodecor">IPD</span>)، لم يعد الانسحاب المطلق الاستراتيجية المثلى. فعند مواجهة خصم يتبع استراتيجية الرد بالمثل (<span class="nodecor">TFT</span>)، يؤدي التعاون المستمر إلى عوائد أعلى. قد نتوقع أن يكتشف وكلاء MARL—المصممون لتعظيم عوائدهم الفردية—استراتيجية <span class="nodecor">TFT</span> باعتبارها توازن ناش يعزّز كلاً من العوائد الفردية والجماعية، ولا يبعث حافزًا للانحراف عن السياسة. ومع ذلك، أظهرت الملاحظات التجريبية أن الوكلاء المدربين لتعظيم عائدهم الخاص يميلون عادة إلى الانسحاب المطلق.</p>
<p>يمثل هذا أحد التحديات الرئيسية لـ MARL في بيئات المنفعة الجماعية: يتجاهل الوكلاء أثناء التدريب أن الوكلاء الآخرين يتعلمون أيضًا. فإذا كان الهدف هو الرفاهية الاجتماعية، يمكن مشاركة المكافآت بين الوكلاء أثناء التدريب، وهو ما يضمن تعاونًا كاملًا في إعداد IPD. لكن هذا لا يكفي عندما نبتغي التعاون القائم على المعاملة بالمثل—من الضروري ابتكار خوارزميات قادرة على اكتشاف سياسات تحفز الخصم على التعاون من أجل تعظيم عائده الخاص.</p>
<h1 id="sec:relatedworks">الأعمال ذات الصلة</h1>
<p>تحاول (<span class="nodecor">LOLA</span>) تشكيل الخصم عبر أخذ التدرج لقيمة الوكيل بالنظر إلى خطوة واحدة للأمام من معاملات الخصم. بدلًا من تحسين <span class="math inline">\(V^1(\theta_i^1, \: \theta_i^2)\)</span> فقط، تهدف (
... باقي النص كما هو ... -->
</body>
</html>