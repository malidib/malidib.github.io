<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque, University of Montreal &amp; Mila firstname.lastname@umontreal.ca Shunichi Akatsuka, Hitachi, Ltd. shunichi.akatsuka.bo@hitachi.com Aaron Courville University of Montreal &amp; Mila aaron.courville@umontreal.ca">
  <title>تَشْكِيلُ الاسْتِجَابَةِ الأَمْثَلِ</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 22px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.7;
    }
    header {
      background: linear-gradient(90deg, #3a6073 0%, #16222a 100%);
      color: #fff;
      padding: 40px 0 30px 0;
      text-align: center;
      margin-bottom: 40px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.07);
    }
    h1.title {
      font-size: 2.7em;
      font-weight: 700;
      margin-bottom: 0.2em;
      letter-spacing: 1px;
    }
    .author {
      font-size: 1.05em;
      margin-top: 1.2em;
      color: #e6e6e6;
      line-height: 1.6;
    }
    .author strong {
      color: #ffd700;
    }
    .author code {
      background: #222;
      color: #ffd700;
      padding: 2px 8px;
      border-radius: 6px;
      font-size: 0.95em;
      margin: 0 2px;
    }
    main {
      max-width: 900px;
      background: #fff;
      margin: 0 auto 40px auto;
      padding: 40px 40px 60px 40px;
      border-radius: 18px;
      box-shadow: 0 4px 24px rgba(0,0,0,0.08);
    }
    h1, h2, h3 {
      color: #2a3b4c;
      font-weight: 700;
      margin-top: 1.7em;
      margin-bottom: 0.7em;
      line-height: 1.2;
    }
    h1 {
      font-size: 2em;
      border-bottom: 2px solid #e0e0e0;
      padding-bottom: 0.2em;
    }
    h2 {
      font-size: 1.4em;
      border-right: 4px solid #3a6073;
      padding-right: 12px;
      margin-top: 2em;
    }
    h3 {
      font-size: 1.15em;
      color: #3a6073;
      margin-top: 1.5em;
    }
    p {
      margin: 1.1em 0;
      text-align: justify;
    }
    ul, ol {
      margin: 1.2em 2em 1.2em 0;
      padding-right: 1.5em;
    }
    ul li, ol li {
      margin-bottom: 0.7em;
      font-size: 1em;
    }
    strong {
      color: #3a6073;
    }
    code, .math.inline {
      background: #f3f3f3;
      color: #c7254e;
      padding: 2px 6px;
      border-radius: 5px;
      font-size: 0.98em;
      font-family: 'Cairo', 'Consolas', monospace;
    }
    .nodecor {
      text-decoration: none;
      color: inherit;
    }
    .ref {
      background: #f1f6fa;
      padding: 0 6px;
      border-radius: 4px;
      color: #355c7d;
      white-space: nowrap;
    }
    blockquote {
      border-right: 5px solid #3a6073;
      background: #f1f6fa;
      margin: 1.5em 0;
      padding: 1em 1.5em;
      color: #444;
      border-radius: 8px;
      font-size: 1.05em;
    }
    @media (max-width: 700px) {
      main {
        padding: 18px 8px 30px 8px;
      }
      header {
        padding: 25px 0 18px 0;
      }
      h1.title {
        font-size: 2em;
      }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">تَشْكِيلُ الاسْتِجَابَةِ الأَمْثَلِ</h1>
  <p class="author">
    <span class="nodecor">Milad Aghajohari</span>, 
    <span class="nodecor">Tim Cooijmans</span>, 
    <span class="nodecor">Juan Agustin Duque</span><br>
    <span class="nodecor">University of Montreal &amp; Mila</span><br>
    <code>firstname.lastname@umontreal.ca</code><br>
    <strong><span class="nodecor">Shunichi Akatsuka</span></strong><br>
    <span class="nodecor">Hitachi, Ltd.</span><br>
    <span class="nodecor">shunichi.akatsuka.bo@hitachi.com</span><br>
    <strong><span class="nodecor">Aaron Courville</span></strong><br>
    <span class="nodecor">University of Montreal &amp; Mila</span><br>
    <code>aaron.courville@umontreal.ca</code>
  </p>
</header>
<main>
  <h1 id="ملخص">مُلَخَّص</h1>
  <p>نستكشف تحدّي تعلُّم التعزيز العميق متعدِّد الوكلاء في بيئاتٍ تنافسيّة جزئيًّا، حيث تواجه الأساليب التقليديّة صعوباتٍ في تعزيز التعاون المبنيّ على المعاملة بالمِثل. يتعلّم وكلاء <span class="nodecor">LOLA</span> و<span class="nodecor">POLA</span> سياساتٍ تعاونيّة قائمة على المعاملة بالمِثل بالتفاضل خلال عددٍ محدود من خطوات تحديث الخصم المستقبلية. غير أنّ لهذه التقنيات قيدًا أساسيًّا: لاعتمادها على عددٍ قليل من خطوات التحسين، قد يستغلّها خصمٌ قادرٌ على إجراء مزيدٍ من التحديثات لتعظيم عائده. استجابةً لذلك، نقدّم نهجًا جديدًا نسمّيه تشكيل الاستجابة المثلى (<span class="nodecor">BRS</span>)، يُوظِّف خصمًا يُقارِب الاستجابة المثلى ويُشار إليه بـ"المُحَقِّق". ولِتكييف المُحَقِّق مع سياسة الوكيل في الألعاب المعقّدة، نقترح آليّة تكيُّف قابلةً للتفاضل ومُشْتَرَطةً بالحالة، تُيَسِّرها آليّة "طَرْح الأسئلة" لاستخلاص تمثيلٍ للوكيل انطلاقًا من سلوكه في حالاتٍ بيئيّة مُعيَّنة. وللتحقُّق تجريبيًّا من صحّة طريقتنا، نُبيِّن تفوُّق أدائها في مواجهة خصمٍ قائمٍ على <span class="nodecor">بحث شجرة مونتِ كارلو (MCTS)</span>، الذي يعمل كتقريبٍ للاستجابة المثلى في لعبة القطع النقديّة. يُوسِّع هذا العمل نطاق تطبيق تعلُّم التعزيز متعدِّد الوكلاء في البيئات التنافسيّة الجزئيّة، ويمهِّد مسارًا جديدًا نحو تحقيق رفاهيّة اجتماعيّة أفضل في الألعاب ذات المصلحة الجماعيّة.</p>

  <h1 id="sec:intro">مُقَدِّمَة</h1>
  <p>مكَّنَت خوارزميّات تعلُّم التعزيز متعدِّد الوكلاء من تحقيق أداءٍ مُتميِّز في ألعابٍ معقّدة وعالية الأبعاد مثل لعبة الغو (<span class="nodecor">AlphaGo</span>) وستاركرافت (<span class="nodecor">AlphaStar</span>). الهدف الأسمى من تعلُّم التعزيز هو تدريب وكلاء قادرين على مساعدة البشر في حلّ المشكلات الصعبة. ولا محالة، سيحتاج هؤلاء الوكلاء إلى الاندماج في سيناريوهاتٍ واقعيّة تتطلّب التفاعل مع البشر ووكلاء تعلُّمٍ آخرين. فعلى الرغم من تفوُّق التدريب متعدِّد الوكلاء في البيئات التعاونيّة أو التنافسيّة الخالصة، فإنّه كثيرًا ما يعجز عن اكتشاف تعاونٍ قائمٍ على المعاملة بالمثل في البيئات التنافسيّة الجزئيّة. ومثالٌ بارزٌ على ذلك فشل وكلاء MARL في تعلُّم سياساتٍ على غرار المعاملة بالمثل في معضلة السجين المتكرّرة (<span class="nodecor">IPD</span>)، وهو ما عالجته <span class="nodecor">LOLA</span>.</p>

  <p>رغم الطابع التمثيلي لمعضلات المصلحة الجماعيّة مثل معضلة السجين، تتكرّر هذه الإشكالات في المجتمع والطبيعة. تخيَّل سيناريو تحاول فيه دولتان (وكلاء) تعظيم إنتاجهما الصناعي مع الإبقاء على مناخ عملٍ يحدّ من الانبعاثات الكربونيّة. من جهة، ترغب كلّ دولة في التزام الدولة الأخرى بتعهّداتها البيئيّة؛ ومن جهةٍ أخرى، يَغريهما إطلاقُ مزيدٍ من الكربون لتحصيل عوائد صناعيّة أكبر. ستُرغِم معاهدةٌ فعّالة كلّ دولة—بواسطة التهديد بالعقوبات—على التزام حدود الانبعاثات المتّفق عليها. وإذا أخفق الوكلاء في تطوير استراتيجياتٍ كالمعاملة بالمثل، فالأرجح أن ينتهي الأمر بتصعيدٍ مُتبادَلٍ مؤسفٍ في استهلاك الطاقة والانبعاثات.</p>

  <p>قدَّمت <span class="nodecor">LOLA</span> خوارزميّة "التعلُّم مع الوعي بتعلُّم الخصم"، ونجحت في إظهار سلوك المعاملة بالمثل في <span class="nodecor">IPD</span> عبر التفاضل خلال خطوة تحديثٍ واحدة يتّخذها الخصم. وبالبناء على ذلك، قدَّمت <span class="nodecor">POLA</span> أسلوب "الوعي بتعلُّم الخصم التقاربي"، الذي يُقرِّب تحديثات سياسة الخصم على نحوٍ مُقيَّد ويُمكِّن من تدريب الشبكات العصبيّة في ألعابٍ أكثر تعقيدًا، مثل لعبة القطع النقديّة. وبحسب علمنا، تُعدّ <span class="nodecor">POLA</span> النهج الوحيد الذي يدرِّب بصورةٍ موثوقة وكلاء يَلتزمون تعاونًا قائمًا على المعاملة بالمثل في هذه اللعبة.</p>

  <p>وعلى الرغم من نجاحها في لعبة القطع النقديّة، فإنّ لـ<span class="nodecor">POLA</span> حدودًا واضحة. إذ يقتصر استشرافها لتعلُّم الخصم على عددٍ محدود من خطوات التحديث المستقبلية، ما يجعلها عرضةً للاستغلال من قِبل خصوم يمكنهم إجراء مزيدٍ من التحسين. تُظهر تحليلاتُنا أنّ خصمًا يُدرِّب نفسه خصّيصًا لتعظيم عائده ضدّ سياسةٍ ثابتة مُدرَّبة بهذه الطريقة يستطيع استغلال الوكيل القائم على <span class="nodecor">LOLA/POLA</span>. كما أنّ هذه القيود تُعيق قابلية التوسّع حين يكون الخصم شبكةً عصبيّةً معقّدة تتطلّب العديد من خطوات التحسين لمحاكاة تعلّمها الفعلي.</p>

  <p>في هذه الورقة نُقدِّم نهجًا جديدًا نسمّيه تشكيل الاستجابة المثلى. يرتكز على بناء خصمٍ يُقارِب سياسة الاستجابة المثلى ضدّ وكيلٍ معيّن، نُسمّيه "المُحَقِّق". يُوضِّح <span class="ref">[fig:cobalt]</span> الإطار العام: نُدرِّب المُحَقِّق ضدّ مجموعةٍ متنوّعة من وكلاء التدريب، ثم نُدرِّب الوكيل بالتفاضل خلال المُحَقِّق. وعلى خلاف <span class="nodecor">LOLA</span> و<span class="nodecor">POLA</span> اللتين تفترضان عددًا يسيرًا من خطوات التحسين المستقبلية للخصم، يفترض أسلوبُنا أن يُنتِج المُحَقِّق استجابةً مُثلى للوكيل الحالي عبر تكييفٍ ديناميّ لسياسته.</p>

  <p>نختبر طريقتنا في معضلة السجين المتكرّرة ولعبة القطع النقديّة. وبما أنّ تقييم الوكيل يتوقّف على قدرته في مواجهة خصمٍ مُجيد الاستجابة، فإنّ المقارنة المنطقيّة تكون مقابل خصمٍ يستجيب بأفضل شكلٍ ممكن، وهو ما نُقرِّبه عبر <span class="nodecor">بحث شجرة مونتِ كارلو</span>. نُظهر أنّه بينما لا يتعاون <span class="nodecor">MCTS</span> تعاونًا كاملًا مع وكلاء <span class="nodecor">LOLA/POLA</span>، فإنّه يتعاون بالكامل مع وكلاء تشكيل الاستجابة المثلى.</p>

  <p><strong>المُسَاهَمَات الرَّئِيسِيَّة</strong>: نُلخِّص مساهماتنا فيما يلي:</p>
  <ul>
    <li><p>نُبيّن أنّ خصمًا مُحسَّنًا بواسطة <span class="nodecor">بحث شجرة مونتِ كارلو</span> لا يتعاون مع وكلاء <span class="nodecor">LOLA/POLA</span>، بل يستغلّهم لتحصيل عوائد أعلى من عائد التعاون الكامل.</p></li>
    <li><p>لتجاوز هذا الضعف، نُقدِّم أسلوب تشكيل الاستجابة المثلى، الذي يُدرِّب وكيلًا بالتفاضل عبر خصمٍ يُحاكي الاستجابة المثلى ("المُحَقِّق"). ونُبرهن تجريبيًّا أنّ وكلاء <span class="nodecor">BRS</span> يتعاونون تعاونًا كاملًا، كما هو مُبيَّن في <span class="ref">[fig:coin_main_compare]</span>.</p></li>
    <li><p>نقترح كذلك آليّة تكيُّف للمُحَقِّق قابلةً للتفاضل ومُشْتَرَطةً بالحالة، تُمكِّنه من التكيُّف مع سياسة الوكيل.</p></li>
  </ul>

  <h1 id="sec:background">الخَلْفِيَّة</h1>

  <h2 id="تعلم-التعزيز-المتعدد-العوامل">تَعَلُّمُ التَّعْزِيزِ مُتَعَدِّدُ الوُكَلاءِ</h2>
  <p>تُعرَّف لعبةُ ماركوف متعدِّدةُ الوكلاء بالرمز <span class="math inline">\(\left( N, \mathcal{S},\left\{\mathcal{A}^i\right\}_{i=1}^N, \mathbb{P},\left\{r^i\right\}_{i = 1}^N, \gamma \right)\)</span>. هنا، <span class="math inline">\(N\)</span> يمثّل عددَ الوكلاء، و<span class="math inline">\(\mathcal{S}\)</span> فضاء الحالات، و<span class="math inline">\(\mathcal{A}:=\mathcal{A}^1 \times \cdots \times \mathcal{A}^N\)</span> مجموعةُ الأفعال المشتركة. احتمالاتُ انتقال الحالة ممثّلةٌ بـ<span class="math inline">\(\mathbb{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\)</span>، ودالّةُ المكافأة لكلّ وكيلٍ <span class="math inline">\(i\)</span> هي <span class="math inline">\(r^i: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span>. أخيرًا، <span class="math inline">\(\gamma \in [0,1]\)</span> هو مُعامِلُ الخصم. يحاول كلّ وكيلٍ تعظيم عائده <span class="math inline">\(R^i = \sum_{t=0}^\infty \gamma^t r^i_t\)</span>. تُعرَّف سياسة الوكيل <span class="math inline">\(i\)</span> بـ<span class="math inline">\(\pi^{i}_{\theta_{i}}\)</span> حيث <span class="math inline">\(\theta_i\)</span> معاملاتُ الشبكة العصبيّة. وتُدرَّب هذه السياسات باستخدام مُقَدِّرات التدرُّج مثل <span class="nodecor">REINFORCE</span>.</p>

  <h2 id="المعضلات-الاجتماعية-ومعضلة-السجين-المتكررة">المُعْضِلاتُ الاِجْتِمَاعِيَّةُ وَمُعْضِلَةُ السَّجِينِ المُتَكَرِّرَةُ</h2>
  <p>في الألعاب ذات المصلحة الجماعيّة، تنشأ معضلاتٌ اجتماعيّة حين يسعى كلّ وكيلٍ إلى تعظيم مكافأته الشخصيّة بما يُقوِّض الناتج الجماعي أو الرفاهيّة الاجتماعيّة. يتبدّى ذلك حين تكون النتيجةُ الجماعيّة أدنى ممّا يمكن بلوغُه عبر التعاون الكامل. وتُظهر نماذجُ نظريّةٌ كمعضلة السجين أنّ كلّ مشاركٍ، رغم أنّ الخيانة قد ترفع عائده الفردي الآني، فإنّ الخيانة المُتبادَلة تؤدّي إلى مكافأةٍ جماعيّةٍ أدنى ممّا لو تعاوَن الطرفان.</p>

  <p>في معضلة السجين المتكرّرة (<span class="nodecor">IPD</span>)، لا تَعود الخيانةُ الدائمةُ استراتيجيةً مُثلى. فعند مواجهة خصمٍ يتّبع استراتيجية <span class="nodecor">الْمُعامَلَةِ بِالْمِثْل</span> (<span class="nodecor">TFT</span>)، يؤدّي التعاونُ المُستمرّ إلى عوائدَ أعلى. وقد نتوقّع أن يكتشف وكلاءُ MARL—المُصمَّمون لتعظيم عائدهم الفردي—استراتيجياتٍ على غرار <span class="nodecor">TFT</span> لكونها تُحسِّن العوائدَ الفرديّة والجماعيّة معًا ولا تُغري بالانحراف عنها. ومع ذلك، تُظهِر المُشاهداتُ التجريبيّة أنّ الوكلاء المُدرَّبين على تعظيم عائدهم الخاص يميلون عادةً إلى الخيانة الدائمة.</p>

  <p>يُمثّل هذا أحدَ التحدّيات الرئيسة لتعلُّم التعزيز متعدِّد الوكلاء في بيئات المصلحة الجماعيّة: إذ يتجاهل الوكلاء أثناء التدريب أنّ الآخرين يتعلّمون أيضًا. فإذا كان الهدفُ رفاهيّةً اجتماعيّة، أمكن تشاركُ المكافآت بين الوكلاء أثناء التدريب، وهو ما يضمن تعاونًا كاملًا في إعداد <span class="nodecor">IPD</span>. لكنّ ذلك لا يكفي حين نبتغي تعاونًا مبنيًّا على المعاملة بالمثل—إذ يلزم ابتكارُ خوارزميّاتٍ قادرةٍ على اكتشاف سياساتٍ تُحفِّز الخصم على التعاون لتعظيم عائده الخاص.</p>

  <h1 id="sec:relatedworks">الأَعْمَالُ ذاتُ الصِّلَة</h1>
  <p>تحاول <span class="nodecor">LOLA</span> تشكيلَ الخصم عبر أخذ تدرُّج قيمة الوكيل مع مراعاة خطوةٍ واحدةٍ مُستقبليّة من تحديثات الخصم. فبدلًا من تحسين <span class="math inline">\(V^1(\theta^1, \theta^2)\)</span> فحسب، تسعى <span class="nodecor">LOLA</span> إلى احتساب تأثير تحديث الخصم المرتقَب في هدف التحسين.</p>
  <!-- ... باقي النص كما هو ... -->
</main>
</body>
</html>