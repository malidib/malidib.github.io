<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Rui Wang, Dengpan Ye, Long Tang, Yunming Zhang, Jiacheng Deng">
  <title>تحسين كشف التزييف العميق باستخدام الدمج السمعي البصري واستراتيجيات التوزين الديناميكي</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">تحسين كشف التزييف العميق باستخدام الدمج السمعي البصري واستراتيجيات التوزين الديناميكي</h1>
<p class="author"><span class="nodecor">Rui Wang</span>, <span class="nodecor">Dengpan Ye</span>, <span class="nodecor">Long Tang</span>, <span class="nodecor">Yunming Zhang</span>, <span class="nodecor">Jiacheng Deng</span></p>
</header>
<p>latex</p>
<h1 id="ملخص">مُلَخَّص</h1>
<p>مع التحسينات المستمرة في تقنيات التزييف العميق، انتقلت الرسائل المزيفة من أحادية الوسائط إلى دمج متعدد الوسائط، مما يطرح تحديات جديدة أمام خوارزميات كشف التزييف الحالية. في هذه الورقة، نقترح <strong>AVT<span class="math inline">\(^2\)</span>-DWF</strong>، وهو دمج سمعي بصري مزدوج يعتمد على <strong>التوزين الديناميكي</strong>، ويهدف إلى تضخيم كل من الإشارات المزيفة داخل وعبر الوسائط، مما يعزز قدرات الكشف. يعتمد AVT<span class="math inline">\(^2\)</span>-DWF على نهج ثنائي المراحل لالتقاط كل من الخصائص المكانية والديناميكيات الزمنية لتعبيرات الوجه. يتم تحقيق ذلك من خلال محول الوجه مع مشفر استراتيجية ترميز الإطارات <span class="math inline">\(n\)</span> ومشفر محول سمعي. بعد ذلك، يستخدم التحويل متعدد الوسائط مع التوزين الديناميكي لمعالجة تحدي دمج المعلومات المتجانسة بين الوسائط السمعية والبصرية. تشير التجارب على مجموعات بيانات DeepfakeTIMIT وFakeAVCeleb وDFDC إلى أن AVT<span class="math inline">\(^2\)</span>-DWF يحقق أداءً رائداً في كشف التزييف العميق داخل وعبر مجموعات البيانات. الكود متاح في <a href="https://github.com/raining-dev/AVT2-DWF" class="uri">https://github.com/raining-dev/AVT2-DWF</a>.</p>
<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>مع التقدم المستمر في تكنولوجيا إنتاج المحتوى بواسطة الذكاء الاصطناعي، لم يعد إنتاج المحتوى مقتصراً على وسيط واحد فقط. مؤخراً، تم استخدام أداة “HeyGen” لإنتاج فيديو تظهر فيه المغنية تايلور سويفت وهي تتحدث الصينية، باستخدام حركات شفاه وصوت مزيفين. تشكل هذه الأعمال المعقدة والمتنوعة تحديات كبيرة للكشف عنها. لذلك، هناك حاجة ملحة لطرق متقدمة للكشف عن هذه الفيديوهات العميقة المتطورة.</p>
<p>الطرق السابقة (<span class="nodecor">verdoliva2020media</span>, <span class="nodecor">rossler2019faceforensics++</span>) ركزت بشكل أساسي على الكشف ضمن وسيط واحد، باستخدام تقنيات التلاعب بالوجه المعروفة للتعرف على الآثار البصرية والتنبؤ بها. ومع ذلك، كان أداؤها عبر مجموعات البيانات ضعيفاً. حاولت بعض الطرق الحالية استخدام إشارات الزمان والمكان على مستوى البقع لتعزيز متانة النموذج وقدرته على التعميم (<span class="nodecor">zhang2022deepfake</span>, <span class="nodecor">heo2023deepfake</span>). تقوم هذه الطرق ببناء الفيديو المدخل إلى نماذج بقع تتم معالجتها بواسطة محول بصري، كما هو موضح في الصورة العلوية. ومع ذلك، فإن هذا يعيق الارتباط الطبيعي بين مكونات الوجه، مما يعيق الكشف عن عدم الاتساق المكاني. علاوة على ذلك، يمكن تزوير المحتوى الصوتي، والتركيز حصرياً على الكشف عن صحة المستوى البصري سيؤدي إلى التحيز. ونتيجة لذلك، فقد جذب مجال الكشف عن التزوير السمعي البصري متعدد الوسائط اهتماماً كبيراً في البحث.</p>
<p>توجد حالياً عدة طرق للكشف عن التزوير متعدد الوسائط. على سبيل المثال، يركز EmoForen (<span class="nodecor">mittal2020emotions</span>) على اكتشاف عدم التناسق العاطفي، بينما يقدم MDS (<span class="nodecor">chugh2020not</span>) درجة التنافر الوسيطي لقياس التنافر السمعي البصري. يستخدم VFD (<span class="nodecor">cheng2023voice</span>) طريقة مطابقة الصوت والوجه للكشف عن الفيديوهات المزيفة. يستفيد AVA-CL (<span class="nodecor">zhang2023joint</span>) من الانتباه السمعي البصري والتعلم التبايني لتعزيز دمج ومطابقة السمات السمعية والبصرية، مما يلتقط الارتباطات الجوهرية بفعالية. ومع ذلك، ركزت الأبحاث السابقة بشكل كبير على دمج السمات بين الوسائط وتجاهلت تحسين مخططات استخراج السمات داخل الوسيط. لحل هذه المشكلة، يعمل هذا البحث على تحسين استخراج السمات داخل الوسيط من خلال بقع الإطار-<span class="math inline">\(n\)</span> ويستخدم وحدة DWF لموازنة دمج أدلة التزوير عبر الوسائط لتعزيز قدرات الكشف.</p>
<p>في هذا العمل، نقترح محولاً سمعياً بصرياً متعدد الوسائط يعتمد على مبدأ دمج الوزن الديناميكي <strong>AVT<span class="math inline">\(^2\)</span>-DWF</strong>، بهدف التقاط السمات المحددة لكل وسيط وتحقيق التناسق بين الوسائط. لتعزيز قدرات التمثيل للنموذج واستكشاف الاتساق المكاني والزماني في الفيديوهات المعالجة، نعتمد استراتيجية ترميز بقع الإطار-<span class="math inline">\(n\)</span> المركزة على ملامح الوجه داخل إطارات الفيديو، مدمجة في مشفر المحول. يتم تطبيق عملية موازية في المجال السمعي لاستخراج السمات. ولمعالجة الحاجة الملحة لالتقاط السمات المشتركة عبر الوسائط المتميزة، نقترح محولاً متعدد الوسائط مع دمج الوزن الديناميكي (<strong>DWF</strong>). تتنبأ هذه الآلية المبتكرة بأوزان الوسائط السمعية والبصرية ديناميكياً، مما يسهل دمجًا أكثر فعالية لميزات أثر التزوير والسمات المشتركة، وبالتالي تعزيز قدرات الكشف.</p>
<p>باختصار، تشمل مساهماتنا:</p>
<ul>
<li><p>نستخدم استراتيجية ترميز بقع الإطار-<span class="math inline">\(n\)</span> التي تعزز استخراج ملامح الوجه الشاملة داخل إطارات الفيديو، بما في ذلك الفروق الدقيقة في تعبيرات الوجه، الحركات، والتفاعلات.</p></li>
<li><p>نقترح محولاً متعدد الوسائط مع دمج الوزن الديناميكي (DWF) لتعزيز دمج المعلومات المتباينة من الوسائط السمعية والبصرية.</p></li>
<li><p>نُدمج الطريقتين المذكورتين أعلاه ونقترح طريقة تُسمى AVT<span class="math inline">\(^2\)</span>-DWF. من خلال تقييم شامل لمعايير عامة معترف بها على نطاق واسع، نظهر التطبيق الواسع والفعالية الملحوظة لـ AVT<span class="math inline">\(^2\)</span>-DWF.</p></li>
</ul>
<h1 id="الطريقة">الطريقة</h1>
<p>يعمل نهجنا على تضخيم إشارات التزييف داخل الوسيط وعبر الوسائط، مما يعزز قدرات الكشف بمعلومات عملية. يتضمن الأسلوب المقترح AVT<span class="math inline">\(^2\)</span>-DWF ثلاثة مكونات رئيسية: مشفر محول الوجه، مشفر محول الصوت، ووحدة دمج الأوزان الديناميكية (DWF). أولاً، يقوم مشفر محول الوجه ومشفر محول الصوت بتحليل الخصائص البصرية والصوتية للحصول على درجة الارتباط داخل الوسيط. بعد ذلك، يتم دمج المخرجات من كلا المشفرين وتغذيتها إلى وحدة دمج الأوزان الديناميكية (DWF) لتدريب أوزان الارتباط بين الوسيطين، مما يسهل عمليات الدمج ومهام الكشف.</p>
<h2 id="مشفر-تحويل-الوجه">مشفر تحويل الوجه</h2>
<p>يتميز مشفر تحويل الوجه عن الأبحاث السابقة (<span class="nodecor">zhang2022deepfake</span>, <span class="nodecor">heo2023deepfake</span>) من خلال استخدام استراتيجية ترميز جديدة تغطي <span class="nodecor"><span class="math inline">\(n\)</span></span> إطارات، كما هو موضح في الجزء السفلي من الشكل <span class="nodecor">1</span>. توجه هذه الاستراتيجية تركيز النموذج نحو المعلومات الزمانية-المكانية الجوهرية عبر إطارات مختلفة داخل الفيديو. بالنسبة لفيديو معين <span class="nodecor"><span class="math inline">\(V\)</span></span>، يتم استخراج كتلة الوجه <span class="nodecor"><span class="math inline">\(\mathbf{F} \in \mathbb{R}^{T \times C \times H \times W}\)</span></span>. <span class="nodecor"><span class="math inline">\(T\)</span></span> تمثل طول الإطار، <span class="nodecor"><span class="math inline">\(C\)</span></span> تدل على عدد القنوات، و <span class="nodecor"><span class="math inline">\(H \times W\)</span></span> تتوافق مع دقة الإطار. يتم إعادة تنظيم الإطارات بترتيب زمني، مما يؤدي إلى تمثيل جديد كـ <span class="nodecor"><span class="math inline">\(C \times (T \times H) \times W\)</span></span>. مشابهاً لرمز [class] في ViT (<span class="nodecor">dosovitskiy2020image</span>)، يتم دمج مضمن قابل للتعلم <span class="nodecor"><span class="math inline">\(\mathbf{F}_{class}\)</span></span> في السلسلة، بينما يتم إضافة تضمينات الموضع القابلة للتعلم <span class="nodecor"><span class="math inline">\(\mathbf{E}_{p}\)</span></span>. يتم تعيين ميزات كل قطعة صورة خطياً إلى فضاء بأبعاد <span class="nodecor"><span class="math inline">\(D\)</span></span> قبل الدخول إلى مشفر التحويل. يتضمن مشفر التحويل طبقة انتباه ذاتي متعدد الرؤوس (MSA)، مما يمكن النموذج من تمييز الارتباطات بين المواقع المختلفة والجوانب المكانية داخل إطار الفيديو. يتم تطبيق تطبيع الطبقة (LN) قبل كل كتلة، ويتم تطبيق الاتصالات المتبقية (RC) بعد كل كتلة. يمكن التعبير عن العملية بأكملها رسمياً كما يلي:</p>
<p><span class="math display">\[\begin{aligned}
    \mathbf{F}_0 &amp;= [ \mathbf{F}_{class}\mathbf{E}_{p}; \, \mathbf{f}_1 \mathbf{E}_{p}; \, \mathbf{f}_2 \mathbf{E}_{p}; \cdots; \, \mathbf{f}_T \mathbf{E}_{p} ],   \\
    \mathbf{F}_\ell &amp;= \text{MSA}(\text{LN}(\mathbf{F}_{\ell-1})) + \mathbf{F}_{\ell-1},\quad \ell = 1, \dots, L ,\end{aligned}\]</span></p>
<p>حيث يمثل <span class="nodecor"><span class="math inline">\(\mathbf{f} \in \mathbb{R}^{(H \times W\times C) \times D}\)</span></span> الميزة البصرية و <span class="nodecor"><span class="math inline">\(\mathbf{E}_{p} \in \mathbb{R}^{(T + 1) \times D}\)</span></span> هو تضمين الموضع القابل للتعلم.</p>
<h2 id="مشفر-تحويل-الصوت">مشفر تحويل الصوت</h2>
<p>للتعامل مع مكونات الصوت، يستخدم النموذج محولاً مشابهاً لمشفر تحويل الوجه، مستفيداً من آلية الانتباه الذاتي لالتقاط الاعتماديات طويلة المدى الداخلية ضمن الصوت. تقوم الدراسة باستخراج الأنماط الصوتية، والديناميكيات الزمنية، والميزات الأخرى المحددة للصوت من إشارات الصوت بشكل منهجي. يتم حساب ميزة MFCC من إشارة الصوت، مما ينتج مكونات يشار إليها بـ <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{T \times M}\)</span>، حيث <span class="math inline">\(T\)</span> تمثل الزمن و<span class="math inline">\(M\)</span> تمثل عناصر التردد، والتي يتم بعد ذلك إسقاطها خطياً إلى تضمين أحادي البعد. لالتقاط الارتباطات الهيكلية الجوهرية من الطيفيات الصوتية، يتم دمج رمز فئة مضمن قابل للتعلم <span class="math inline">\(\mathbf{A}_{\text{class}}\)</span> في التسلسل. بالإضافة إلى ذلك، يتم تقديم تضمينات موضعية قابلة للتدريب. يتم توضيح العملية بأكملها في الصيغة التالية.</p>
<p><span class="math display">\[\begin{aligned}
    \mathbf{A}_0 &amp;= [ \mathbf{A}_{class} \mathbf{E}_{p}; \, \mathbf{a}_1 \mathbf{E}_{p}; \, \mathbf{a}_2 \mathbf{E}_{p}; \cdots; \, \mathbf{a}_T \mathbf{E}_{p} ],  \\
    \mathbf{A}_\ell &amp;= \text{MSA}(\text{LN}(\mathbf{A}_{\ell-1})) + \mathbf{A}_{\ell-1},\quad \ell = 1, \dots, L  .\end{aligned}\]</span></p>
<p>حيث <span class="math inline">\(\mathbf{a} \in \mathbb{R}^{(H \times W \times C) \times D}\)</span> يمثل ميزة الصوت و<span class="math inline">\(\mathbf{E}_{p} \in \mathbb{R}^{(T + 1) \times D}\)</span> هو أيضاً التضمين الموضعي القابل للتعلم. تشمل المخرجات <span class="math inline">\(\mathbf{F}_{class}\)</span> و<span class="math inline">\(\mathbf{A}_{class}\)</span> من مشفر تحويل الوجه ومشفر تحويل الصوت مجموعة متنوعة من المعلومات داخل الفيديو مثل التفاصيل البصرية المكانية، التحولات الزمنية في الأوضاع السمعية البصرية، ومحتوى الصوت.</p>
<h2 id="المحول-متعدد-الوسائط-مع-دمج-الاوزان-الديناميكي">المحول متعدد الوسائط مع دمج الأوزان الديناميكي</h2>
<p>بعد استخراج ميزة الصوت <span class="math inline">\(\mathbf{A}_{class}\)</span> وميزة الفيديو <span class="math inline">\(\mathbf{F}_{class}\)</span>، تولد وحدة دمج الأوزان الديناميكي (DWF) أوزاناً على مستوى الكيان <span class="math inline">\(W_A\)</span> و<span class="math inline">\(W_F\)</span> لكل وسيط، كما هو موضح في الشكل المحذوف. مستلهمين من (<span class="nodecor">chen2023meaformer</span>)، يتضمن تصميمنا كتلة انتباه متقاطع متعدد الرؤوس ثنائية الطبقات (MHCA) لحساب هذه الأوزان. تُستخدم الطبقة التالية، MHCA، أوزان الطبقة السابقة ولا تتطلب تهيئة. تعمل MHCA بوظيفة الانتباه في <span class="math inline">\(N_h\)</span> رؤوس متوازية، مما يسمح للنموذج بالانتباه المشترك للمعلومات من فضاءات تمثيل فرعية مختلفة في مواقع مختلفة. يتم تحديد الرأس <span class="math inline">\(i\)</span> بواسطة مصفوفات المشاركة الموضعية <span class="math inline">\(W_q^{(i)}\)</span>, <span class="math inline">\(W_k^{(i)}\)</span>, <span class="math inline">\(W_v^{(i)} \in \mathbb{R}^{d \times d_h}\)</span>، التي تحول المدخلات متعددة الوسائط <span class="math inline">\(\mathbf{A}_{class}\)</span>, <span class="math inline">\(\mathbf{F}_{class}\)</span> إلى استفسارات واعية بالوضع <span class="math inline">\(Q_{f/a}^{(i)}\)</span>، مفاتيح <span class="math inline">\(K_{f/a}^{(i)}\)</span>، وقيم <span class="math inline">\(V_{f/a}^{(i)}\)</span>. <span class="math inline">\(d\)</span> يمثل بعد ميزات الإدخال، بينما <span class="math inline">\(d_h\)</span> يمثل بعد الطبقات الخفية. لكل ميزة من الوسائط، الناتج هو:</p>
<p><span class="math display">\[\begin{gathered}
\text{MHCA}(\mathbf{F}_{class}) = \text{Concat}(W^i_F V_f \cdot W_o), \\
\text{MHCA}(\mathbf{A}_{class}) = \text{Concat}(W^i_A V_a \cdot W_o), \\
W^i_F = \bar{\beta}^{(i)}_{ff} + \bar{\beta}^{(i)}_{fa},  \hspace{0.6cm} W_F= {\textstyle \sum_{i=1}^{N_h}} W_F^i/N_h,\\
W^i_A = \bar{\beta}^{(i)}_{aa} + \bar{\beta}^{(i)}_{af}, \hspace{0.6cm} W_A= {\textstyle \sum_{i=1}^{N_h}} W_A^i/N_h,\end{gathered}\]</span></p>
<p>حيث <span class="math inline">\(W_o \in \mathbb{R}^{d \times d}\)</span>, <span class="math inline">\({\bar\beta}^{(i)}_{*}\)</span> يمثل وزن الانتباه للرأس <span class="math inline">\(i\)</span>. يُعرف وزن الانتباه لكل رأس <span class="math inline">\({\bar\beta}^{(i)}_{fa}\)</span> بين <span class="math inline">\(f\)</span> و<span class="math inline">\(a\)</span> في كل رأس كما يلي:</p>
<p><span class="math display">\[\begin{aligned}
{\bar\beta}^{(i)}_{fa} = \frac{\exp(Q_f K^{\top}_a / \sqrt{d_h})}  {\textstyle \sum_{n\in {f,a}}\exp(Q_f K^{\top}_n / \sqrt{d_h}) },\quad \end{aligned}\]</span></p>
<p>حيث يتم حساب <span class="math inline">\({\bar\beta}^{(i)}_{ff}\)</span>، <span class="math inline">\({\bar\beta}^{(i)}_{af}\)</span>، و<span class="math inline">\({\bar\beta}^{(i)}_{aa}\)</span> بطريقة مماثلة، مع <span class="math inline">\(d_h=d/N_h\)</span>. LN وRC تستقر أيضاً أثناء التدريب. <span class="math display">\[\begin{aligned}
h_v=\text{LN} (\text{MHCA}(\mathbf{F}_{\ell-1})+\mathbf{F}_{\ell-1}),\\
h_a=\text{LN} (\text{MHCA}(\mathbf{A}_{\ell-1})+\mathbf{A}_{\ell-1}),\end{aligned}\]</span></p>
<p>حيث يتم بعد ذلك تمرير <span class="math inline">\(h_v\)</span> و<span class="math inline">\(h_a\)</span> إلى الطبقة التالية من وحدة DWF لمزيد من التدريب.</p>
<p><strong>دمج الوسائط.</strong> لتعظيم استخدام الميزات بين الوسائط السمعية والبصرية، نضرب الميزات السمعية المستخرجة مسبقاً <span class="math inline">\(\mathbf{A}_{class}\)</span>، وميزات الفيديو <span class="math inline">\(\mathbf{F}_{class}\)</span> بأوزان على مستوى الكيان <span class="math inline">\(W_A\)</span> و<span class="math inline">\(W_F\)</span> في قطاع دمج الوسائط. يضمن هذا النهج تنوع الوسائط ويتجنب التركيز الذاتي المفرط. <span class="math display">\[\begin{aligned}
V = W_F \mathbf{F}_{class}\oplus W_A \mathbf{A}_{class}.\end{aligned}\]</span></p>
<h1 id="التجربة">التجربة</h1>
<h2 id="مجموعة-البيانات">مجموعة البيانات</h2>
<p>تشمل التجارب ثلاث مجموعات بيانات: (<span class="nodecor">korshunov1812deepfakes</span>)، (<span class="nodecor">dolhansky2020deepfake</span>)، و(<span class="nodecor">khalid2021fakeavceleb</span>). نظراً لأن نسبة الفيديوهات الحقيقية والمزيفة في هذه المجموعات غير متوازنة بشكل كبير، فإننا نستخدم طرقاً متنوعة لموازنة البيانات الحقيقية والمزيفة. يوضح الجدول [tab:tab0] التغير في نسبة البيانات الحقيقية والمزيفة قبل وبعد التوازن. تم دمج الفيديوهات الأصلية لـ(<span class="nodecor">sanderson2002vidtimit</span>) في مجموعة بيانات (<span class="nodecor">korshunov1812deepfakes</span>). استخرجت مجموعة بيانات (<span class="nodecor">dolhansky2020deepfake</span>) إطارات متتالية جزئية من كل فيديو (<span class="nodecor">Deepfake</span>). بالمقابل، تم استخدام جميع الإطارات لتدريب الفيديوهات الحقيقية. لمعالجة مشكلة عدم التوازن في البيانات في مجموعة بيانات (<span class="nodecor">khalid2021fakeavceleb</span>)، تم اختيار <span class="nodecor">19,000</span> فيديو حقيقي من (<span class="nodecor">chung2018voxceleb2</span>). تم تقسيم المجموعات إلى مجموعات تدريب، تحقق، واختبار بنسبة <span class="nodecor">7:1:2</span>. كانت نسبة توازن البيانات الحقيقية والمزيفة في مجموعة الاختبار <span class="nodecor">1:1</span>. تم إجراء جميع التقييمات التجريبية حصرياً على مجموعة الاختبار.</p>
<h2 id="التنفيذ">التنفيذ</h2>
<p>خلال التدريب، يتم تقسيم كل من الفيديوهات الأصلية والمزيفة إلى كتل بطول <span class="math inline">\(T\)</span> (القيمة الافتراضية هي <span class="nodecor">30</span>). لكشف الوجوه، يتم استخدام كاشف الوجوه المقاوم للتغيرات القياسية بطلقة واحدة (<span class="nodecor">Single Shot Scale-invariant Face Detector (S<span class="math inline">\(^3\)</span>FD)</span> (<span class="nodecor">zhang2017s3fd</span>)). ثم يتم محاذاة الوجوه المكتشفة وحفظها كصور بأبعاد <span class="math inline">\(224\times224\)</span>. في معالجة الصوت، يتم حساب ميزات <span class="nodecor">MFCC</span> كمدخلات باستخدام نافذة <span class="nodecor">Hanning</span> مدتها <span class="nodecor">15</span> مللي ثانية وانتقال النافذة <span class="nodecor">4</span> مللي ثانية لتحليل الطيف بدقة. تم إجراء جميع التجارب تحت نفس الإعدادات لضمان قابلية مقارنة النتائج التجريبية.</p>
<h2 id="مقارنات-مع-الأحدث-في-المجال">مقارنات مع الأحدث في المجال</h2>
<p>في تجارب شاملة، تم تقييم فعالية AVT<span class="math inline">\(^2\)</span>-DWF مقابل النماذج الأساسية الأحدث في المجال باستخدام مقاييس الأداء مثل الدقة (<span class="nodecor">Accuracy</span>) ومساحة تحت المنحنى (<span class="nodecor">Area Under the Curve</span>). تم تصنيف النماذج الأساسية إلى مجموعتين: الوضع البصري (<span class="nodecor">V</span>) والوضع متعدد الوسائط (<span class="nodecor">AV</span>). تم إجراء تحليل مقارن على ثلاث مجموعات بيانات، وتم عرض النتائج في الجدول [tab:tab1]. تم التأكيد على النتائج الأكثر بروزاً بالخط العريض، وينطبق الأمر نفسه فيما بعد. بسبب الكمية المحدودة من الفيديوهات، تظهر معظم الطرق الأساسية أداءً مرتفعاً في الكشف عن DF-TIMIT. يبرز AVT<span class="math inline">\(^2\)</span>-DWF وAVA-CL بدقة <span class="nodecor">99.99%</span> و <span class="nodecor">100%</span> على DF-TIMIT (LQ)، متفوقين بشكل ملحوظ على الطرق الأخرى. في مجموعة بيانات FakeAVCeleb الصعبة، المصممة لتزوير الفيديو المعقد، يظهر AVA-CL، الذي يستخدم طريقة التعلم بالتباين للانتباه السمعي البصري، أداءً مماثلاً لطريقتنا AVT<span class="math inline">\(^2\)</span>-DWF. يُلاحظ أن طريقتنا أكثر موثوقية بسبب مجموعة الاختبار المتوازنة. في مجموعة البيانات الواسعة DFDC، يتفوق AVT<span class="math inline">\(^2\)</span>-DWF على طرق الكشف الأخرى المبنية على الرؤية والسمع البصري، محققاً دقة <span class="nodecor">88.02%</span> ومساحة تحت المنحنى <span class="nodecor">89.20%</span>، مظهراً أداءً استثنائياً.</p>
<h2 id="تقييم-البيانات-المتقاطعة">تقييم البيانات المتقاطعة</h2>
<p>تعطى الأولوية في هذه المرحلة لتقييم متانة نموذج AVT<span class="math inline">\(^2\)</span>-DWF. لضمان التعميم عبر مجموعات البيانات المختلفة، يتم مقارنة منهجنا مع أربعة نماذج بارزة: Xception (<span class="nodecor">rossler2019faceforensics++</span>), CViT (<span class="nodecor">wodajo2021deepfake</span>), Lipforensis (<span class="nodecor">haliassos2021lips</span>)، و MDS (<span class="nodecor">mittal2020emotions</span>). تمتد التقييمات عبر البيانات المتقاطعة على ثلاث مجموعات بيانات معيارية. على وجه التحديد، تشتمل FakeAVCeleb على أربع طرق تزييف عميق متميزة، وDFDC تشمل ثماني تقنيات، وDF-TIMIT تشمل عمليتين—حيث تقدم كل مجموعة بيانات تحديات تزييف عميق فريدة من نوعها. تلخص نتائج التقييم عبر البيانات لهذه المعايير الثلاثة في الجدول [tab:tab2]. تظهر الطرق التقليدية أداءً ضعيفاً عند مواجهة مزيفات عميقة غير مرئية. على الرغم من أن CViT، الذي يستفيد من المحولات كمكتشفات، يحقق نتائج مشرفة، إلا أن نموذجنا AVT<span class="math inline">\(^2\)</span>-DWF يتفوق على أدائه، مما يظهر فعاليته المحسنة في كشف المزيف العميق.</p>
<h2 id="دراسة-الاستئصال">دراسة الاستئصال</h2>
<h3 id="فائدة-وحدة-dwf">فائدة وحدة DWF</h3>
<p>في تقييم شامل لوحدة AVT2-DWF، أجرينا تجارب استئصالية، حيث فحصنا نسخة بصرية بحتة، ونسخة AV (من خلال دمج مستخرجات الكلام والوجه ببساطة)، وAVT2-DWF التي تجمع بين وحدتي AV وDWF (VA-DWF). تم عرض نتائج الاختبار على مجموعتي بيانات DFDC وFakeAVCeleb في الجدول [tab:tab3]. في مجموعة بيانات DFDC، حيث لم يتم تزوير الصوت، يؤدي الاعتماد فقط على ميزات الصوت والصورة المدمجة للتصنيف إلى انخفاض كبير في نتائج الكشف. على العكس، بالنسبة لمجموعة بيانات FakeAVCeleb، حيث أن الوضع البصري لبعض الفيديوهات حقيقي بينما يتم التلاعب بالوضع الصوتي، فإن وحدة الصوت والصورة تعزز الأداء بشكل ملحوظ. مع إدخال وحدة DWF، تحسنت نتائج الكشف بنسبة <span class="nodecor">11.55%</span> و <span class="nodecor">12.89%</span> على التوالي، مما يبرز المزايا الكبيرة لوحدة DWF لدينا في التقاط الميزات المشتركة عبر الوسائط المختلفة.</p>
<h3 id="فائدة-ترميز-الإطارات-n">فائدة ترميز الإطارات <span class="math inline">\(n\)</span></h3>
<p>لتقييم مزايا استراتيجية ترميز الإطارات <span class="math inline">\(n\)</span>، يتم استخراج بقع غير متكررة بشكل عشوائي من تسلسل متتابع لإطارات الوجه. ثم يتم تجميع هذه البقع في صور كاملة للإدخال. تعرض نتائج الاختبار في مجموعتي البيانات DFDC وFakeAVCeleb في الجدول [tab:tab4]. في هذين المعيارين، تحسن أداء استراتيجية ترميز الإطارات <span class="math inline">\(n\)</span> بنسبة <span class="nodecor">22.45</span>% و <span class="nodecor">3.74</span>% على التوالي، مقارنة بطريقة البقع التقليدية، مما يظهر فعالية نظامنا في الحفاظ على المعلومات المستمرة للوجه بالكامل.</p>
<h1 id="الخلاصة">الخلاصة</h1>
<p>تقترح هذه الورقة إطار عمل AVT<span class="math inline">\(^2\)</span>-DWF لمعالجة التباينات المكانية الدقيقة والاتساق الزمني داخل محتوى الفيديو. يتم تسليط الضوء على الخصائص الفريدة لكل وضعية باستخدام مشفرات محول الوجه ومحول الصوت التي تستخدم استراتيجية ترميز الإطار <span class="math inline">\(n\)</span>. بعد ذلك، تقوم تقنية الدمج الموزون ديناميكياً (DWF) باستخراج الخصائص المشتركة من الوضعيات السمعية البصرية. تشير نتائج تجاربنا إلى أداء متفوق لـ AVT<span class="math inline">\(^2\)</span>-DWF في كل من التنفيذات داخل وعبر مجموعات البيانات مقارنة بطرق أخرى لكشف التزييف العميق. توحي هذه النتائج بأن تحقيق الاتساق الشامل عبر وضعيات متعددة يمكن أن يكون مؤشراً حاسماً لكشف التزييف العميق في سيناريوهات العالم الحقيقي.</p>
</body>
</html>
