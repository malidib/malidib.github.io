<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Rui Wang, Dengpan Ye, Long Tang, Yunming Zhang, Jiacheng Deng">
  <title>تحسين كشف التزييف العميق باستخدام الدمج السمعي البصري واستراتيجيات التوزين الديناميكي</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 22px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.8;
    }
    header {
      background: linear-gradient(90deg, #3a8dde 0%, #6dd5ed 100%);
      color: #fff;
      padding: 40px 0 20px 0;
      text-align: center;
      box-shadow: 0 2px 8px rgba(58,141,222,0.08);
      margin-bottom: 40px;
    }
    h1.title {
      font-size: 2.5em;
      font-weight: 700;
      margin: 0 0 10px 0;
      letter-spacing: 1px;
    }
    .author {
      font-size: 1.1em;
      margin: 0;
      color: #e3f2fd;
      letter-spacing: 0.5px;
    }
    main, .main-content {
      max-width: 900px;
      background: #fff;
      margin: 0 auto 40px auto;
      padding: 40px 32px 32px 32px;
      border-radius: 18px;
      box-shadow: 0 4px 24px rgba(58,141,222,0.10);
    }
    h1, h2, h3 {
      color: #3a8dde;
      font-weight: 700;
      margin-top: 2.2em;
      margin-bottom: 0.7em;
      line-height: 1.3;
    }
    h1 {
      font-size: 2em;
      border-bottom: 2px solid #e3f2fd;
      padding-bottom: 0.2em;
    }
    h2 {
      font-size: 1.4em;
      border-right: 4px solid #6dd5ed;
      padding-right: 12px;
      margin-top: 2em;
    }
    h3 {
      font-size: 1.1em;
      color: #1976d2;
      margin-top: 1.5em;
    }
    p {
      margin: 1.2em 0;
      text-align: justify;
    }
    ul, ol {
      margin: 1.2em 2em 1.2em 0;
      padding-right: 1.5em;
    }
    li {
      margin-bottom: 0.7em;
    }
    strong {
      color: #1976d2;
      font-weight: 700;
    }
    a, a:visited {
      color: #1565c0;
      text-decoration: underline;
      word-break: break-all;
    }
    a:hover {
      color: #0d47a1;
      text-decoration: none;
    }
    code, pre {
      background: #f1f3f4;
      color: #c7254e;
      font-family: 'Cairo', 'Consolas', 'monospace';
      font-size: 0.95em;
      border-radius: 6px;
      padding: 2px 6px;
    }
    pre {
      display: block;
      padding: 16px;
      overflow-x: auto;
      margin: 1.5em 0;
      background: #f1f3f4;
      border-radius: 8px;
      border: 1px solid #e3f2fd;
    }
    blockquote {
      border-right: 5px solid #6dd5ed;
      background: #f1f8fb;
      color: #1976d2;
      margin: 1.5em 0;
      padding: 1em 1.5em;
      border-radius: 8px;
      font-size: 1.05em;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin: 2em 0;
      background: #fafbfc;
      border-radius: 8px;
      overflow: hidden;
      font-size: 0.98em;
    }
    th, td {
      border: 1px solid #e3f2fd;
      padding: 10px 14px;
      text-align: center;
    }
    th {
      background: #e3f2fd;
      color: #1976d2;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f6fbff;
    }
    .math.display {
      display: block;
      margin: 1.5em 0;
      direction: ltr;
      unicode-bidi: embed;
      text-align: center;
    }
    .math.inline {
      font-size: 1em;
      direction: ltr;
      unicode-bidi: embed;
      background: none;
      color: #1976d2;
    }
    ul > li > p { margin: 0.2em 0; }
    @media (max-width: 700px) {
      main, .main-content { padding: 18px 6vw 18px 6vw; }
      header { padding: 24px 0 12px 0; }
      h1.title { font-size: 1.5em; }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">تحسين كشف التزييف العميق باستخدام الدمج السمعي البصري واستراتيجيات التوزين الديناميكي</h1>
  <p class="author">
    <span class="nodecor">Rui Wang</span>,
    <span class="nodecor">Dengpan Ye</span>,
    <span class="nodecor">Long Tang</span>,
    <span class="nodecor">Yunming Zhang</span>,
    <span class="nodecor">Jiacheng Deng</span>
  </p>
</header>
<main class="main-content">
<h1 id="ملخص">مُلخَّص</h1>
<p>مع التحسينات المستمرة في تقنيات التزييف العميق، تطوّر المحتوى المُزَيَّف من أحادي الوسائط إلى مُتعدِّد الوسائط المُندمج، مما طرح تحديات جديدة أمام خوارزميات الكشف التقليدية. في هذه الورقة، نقترح <strong>AVT<span class="math inline">^{2}</span>-DWF</strong>، وهو إطار سمعي-بصري يعتمد على <strong>التوزين الديناميكي</strong>، يهدف إلى تضخيم الإشارات المُزَيَّفة محليًّا وعبر الوسائط لتعزيز قدرات الكشف. يستند AVT<span class="math inline">^{2}</span>-DWF إلى نهج ثنائي المراحل لالتقاط الخصائص المكانية والديناميكيات الزمنية لتعابير الوجه. يتحقق ذلك عبر استخدام مُشَفِّر مُحَوِّل بصري مع استراتيجية ترميز الإطار<span class="math inline">–n</span> ومُشَفِّر مُحَوِّل سمعي. ثم يُطبَّق مُحَوِّل مُتعدِّد الوسائط مع التوزين الديناميكي لمواجهة تحدّي دمج المعلومات المشتركة بين الصوت والصورة. تُشير التجارب على مجموعات بيانات DeepfakeTIMIT وFakeAVCeleb وDFDC إلى أن AVT<span class="math inline">^{2}</span>-DWF يُحقِّق أداءً رائدًا في كشف التزييف داخل وخارج مجموعات البيانات. يتوفر الكود المصدري على <a href="https://github.com/raining-dev/AVT2-DWF" class="uri">https://github.com/raining-dev/AVT2-DWF</a>.</p>

<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>مع التقدّم المستمر في تقنيات الذكاء الاصطناعي لإنتاج المحتوى، لم يَعُد الإنتاج مُقتصرًا على وسيط واحد. فقد استُخدِمت مؤخرًا أداة “HeyGen” لإنشاء مقطع فيديو يُصوِّر المُغنِّية تايلور سويفت وهي تتحدث الصينية، عبر مزج حركات شفاه اصطناعية وصوت مُزَيَّف. تُشكِّل هذه السيناريوهات المُعقَّدة ومتعدّدة الوسائط تحدّيًا كبيرًا لأساليب الكشف الحالية؛ لذا تبرز الحاجة إلى تقنيات متقدّمة لرصد هذه الفيديوهات العميقة المتطورة.</p>
<p>ركّزت الطرائق السابقة (<span class="nodecor">verdoliva2020media</span>، <span class="nodecor">rossler2019faceforensics++</span>) أساسًا على كشف التزييف في وسيط واحد، بالاعتماد على تقنيات التلاعب بالوجه المعروفة لكشف البصمات البصرية. غير أنّ أداءها يَضعُف عند الانتقال عبر مجموعات بيانات مختلفة. حاولت بعض الأساليب الأحدث استغلال إشارات مكان-زمان على مستوى البقع لتعزيز متانة النموذج وقدرته على التعميم (<span class="nodecor">zhang2022deepfake</span>، <span class="nodecor">heo2023deepfake</span>)؛ إذ تُقسِّم هذه الطرائق الفيديو إلى بقع تُعالَج بواسطة مُحَوِّل بصري. لكن ذلك يقطع الارتباط الطبيعي بين مكوّنات الوجه، مما يحدّ من قدرة الكشف على عدم الاتساق المكاني. علاوة على ذلك، مع إمكانية تزوير المحتوى الصوتي، فإن التركيز الحصري على القناة البصرية قد يُدخِل تحيّزًا. لذلك، اكتسب مجال الكشف السمعي-البصري مُتعدِّد الوسائط اهتمامًا متزايدًا.</p>
<p>توجد حاليًا عدة أساليب للكشف عن التزييف مُتعدِّد الوسائط. على سبيل المثال، يركّز EmoForen (<span class="nodecor">mittal2020emotions</span>) على اكتشاف التباين العاطفي، بينما يُقدِّم MDS (<span class="nodecor">chugh2020not</span>) مقياس التنافر الوسيطي لقياس التوافق السمعي-البصري. يستخدم VFD (<span class="nodecor">cheng2023voice</span>) آلية مطابقة بين الصوت والوجه لرصد الفيديوهات المُزَيَّفة. ويستفيد AVA-CL (<span class="nodecor">zhang2023joint</span>) من الانتباه السمعي-البصري والتعلّم التبايني لتعزيز دمج ومطابقة السمات من كلا الوسيطين، بما يلتقط الارتباطات الجوهرية بفعالية. ومع ذلك، ركّزت الأبحاث السابقة أساسًا على دمج السمات عبر الوسائط، مع إغفال تحسين استخراج السمات داخل كل وسيط. ولمعالجة هذا القصور، نقترح استراتيجية <strong>ترميز الإطار<span class="math inline">–n</span></strong> لتحسين استخراج السمات الموضعية، إلى جانب وحدة <strong>DWF</strong> لموازنة دمج أدلة التزييف عبر الوسائط وتعزيز قدرات الكشف.</p>
<p>في هذه الدراسة، نُقدِّم مُحَوِّلًا سمعيًّا-بصريًّا مُتعدِّد الوسائط يُعرف بـ<strong>AVT<span class="math inline">^{2}</span>-DWF</strong>، يستهدف التقاط السمات المُميِّزة لكل وسيط وتحقيق تناغم فعّال بينها. ولتعزيز القدرة التمثيلية واستكشاف الاتساق المكاني والزمني ضمن الفيديو، نعتمد استراتيجية ترميز الإطار<span class="math inline">–n</span> التي تُركِّز على ملامح الوجه داخل الإطارات، ضمن مُشَفِّر المُحَوِّل البصري. وبالموازاة، نُطبِّق عملية مماثلة في المجال السمعي لاستخراج السمات الصوتية. ثم نقترح وحدة <strong>دمج الأوزان الديناميكي (DWF)</strong> داخل المُحَوِّل مُتعدِّد الوسائط، حيث تتنبّأ هذه الآلية بأوزان الوسائط السمعية والبصرية ديناميكيًا، مما يُسهِّل اندماجًا أكثر فعالية لميزات التزييف والسمات المشتركة، وبالتالي يُعزِّز قدرات الكشف.</p>
<p>باختصار، تشمل مساهماتنا:</p>
<ul>
  <li>اعتماد استراتيجية ترميز الإطار<span class="math inline">–n</span> لتعزيز استخراج ملامح الوجه داخل الإطارات، بما في ذلك تفاصيل التعابير وحركات الوجه الدقيقة.</li>
  <li>اقتراح مُحَوِّل مُتعدِّد الوسائط مع دمج الأوزان الديناميكي (DWF) لتحسين دمج المعلومات المُتباينة من الوسائط السمعية والبصرية.</li>
  <li>دمج الطريقتين ضمن إطار AVT<span class="math inline">^{2}</span>-DWF، وإظهار فعاليته العالية عبر تقييم شامل باستخدام معايير مُعتَرَف بها على نطاق واسع.</li>
</ul>

<h1 id="الطريقة">الطريقة</h1>
<p>يهدف نهجنا إلى تضخيم إشارات التزييف داخل كل وسيط وعبر الوسائط، بما يعزّز قدرات الكشف بمعلومات أكثر دقة. يتكوّن إطار AVT<span class="math inline">^{2}</span>-DWF من ثلاثة مكوّنات رئيسية: مُشَفِّر المُحَوِّل البصري للوجه، ومُشَفِّر المُحَوِّل السمعي، ووحدة دمج الأوزان الديناميكي (DWF). أولًا، يستخلص مُشَفِّر الوجه والمُشَفِّر السمعي الخصائص البصرية والصوتية لاجتناء دلائل التزييف داخل كل وسيط. ثم تُدمَج مخرجاتهما وتُمرَّر إلى وحدة DWF التي تتعلّم أوزان الارتباط بين الوسائط لِتَيسير الدمج وتحسين نتائج الكشف.</p>

<h2 id="مشفر-تحويل-الوجه">مُشَفِّر المُحَوِّل البصري للوجه</h2>
<p>يتميّز مُشَفِّر المُحَوِّل البصري عن الأبحاث السابقة (<span class="nodecor">zhang2022deepfake</span>، <span class="nodecor">heo2023deepfake</span>) باعتماد استراتيجية ترميز جديدة تُغطي <span class="math inline">n</span> إطارات. تُوجِّه هذه الاستراتيجية تركيز النموذج نحو المعلومات الزمكانية الجوهرية عبر إطارات مختلفة في الفيديو. لفيديو مُعطى <span class="math inline">\mathbf{V}</span>، نَستخرج موتر الوجه <span class="math inline">\mathbf{F} \in \mathbb{R}^{T \times C \times H \times W}</span>، حيث <span class="math inline">T</span> طول المقطع الزمني، و<span class="math inline">C</span> عدد القنوات، و<span class="math inline">H \times W</span> دقّة الإطار. تُقسَّم الإطارات إلى بقع وتُسقَط خطيًّا إلى فَضاء بُعده <span class="math inline">D</span>. مُماثلةً لرمز [class] في ViT (<span class="nodecor">dosovitskiy2020image</span>)، نُدرِج رمز فئة قابِلًا للتعلُّم <span class="math inline">\mathbf{F}_{\text{class}}</span> ضمن التسلسل، ونُضيف تضمينات موضعية قابلة للتعلّم <span class="math inline">\mathbf{E}_{p}</span>. يُمكن صياغة العملية كما يلي:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbf{F}_0 &= [ \mathbf{F}_{\text{class}} + \mathbf{E}_{p}; \, \mathbf{f}_1 + \mathbf{E}_{p}; \, \mathbf{f}_2 + \mathbf{E}_{p}; \cdots; \, \mathbf{f}_T + \mathbf{E}_{p} ],\\
    \mathbf{F}_\ell &= \text{MSA}(\text{LN}(\mathbf{F}_{\ell-1})) + \mathbf{F}_{\ell-1}, \quad \ell = 1, \dots, L .
\end{aligned}
\]</span></p>
<p>حيث تُمثِّل المتجهات <span class="math inline">\mathbf{f}_t \in \mathbb{R}^{D}</span> تمثيلاتٍ للإطارات بعد الإسقاط الخطي، و<span class="math inline">\mathbf{E}_{p} \in \mathbb{R}^{(T + 1) \times D}</span> هو تضمين موضعي قابل للتعلّم. تُستخدم طبقات الانتباه الذاتي متعدّد الرؤوس MSA مع <span class="math inline">\text{LN}</span> قبل كل كتلة ووصلةٍ مُتبقية بعد كل كتلة.</p>

<h2 id="مشفر-تحويل-الصوت">مُشَفِّر المُحَوِّل السمعي</h2>
<p>للتعامل مع مُكوِّن الصوت، نستخدم مُحَوِّلًا مشابهًا لمُشَفِّر الوجه، مُستفيدين من آلية الانتباه الذاتي لالتقاط الاعتماديات طويلة المدى داخل الإشارة. نَستخرج ميزات <span class="nodecor">MFCC</span> من الإشارة، مُنتِجِين مصفوفة <span class="math inline">\mathbf{A} \in \mathbb{R}^{T \times M}</span>، حيث <span class="math inline">T</span> هو الزمن و<span class="math inline">M</span> عدد معاملات التردد. تُسقَط هذه الميزات خطيًّا إلى فَضاء بُعده <span class="math inline">D</span>. لالتقاط الارتباطات البنيوية من الطيفيات الصوتية، نُدرِج رمز فئة قابِلًا للتعلّم <span class="math inline">\mathbf{A}_{\text{class}}</span> في التسلسل ونُضيف تضمينًا موضعيًّا قابِلًا للتدريب. تُوضّح المعادلات التالية العملية:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbf{A}_0 &= [ \mathbf{A}_{\text{class}} + \mathbf{E}_{p}; \, \mathbf{a}_1 + \mathbf{E}_{p}; \, \mathbf{a}_2 + \mathbf{E}_{p}; \cdots; \, \mathbf{a}_T + \mathbf{E}_{p} ],\\
    \mathbf{A}_\ell &= \text{MSA}(\text{LN}(\mathbf{A}_{\ell-1})) + \mathbf{A}_{\ell-1}, \quad \ell = 1, \dots, L .
\end{aligned}
\]</span></p>
<p>حيث <span class="math inline">\mathbf{a}_t \in \mathbb{R}^{D}</span> تمثّل متجهات الإطار الزمني المُسقَطة، و<span class="math inline">\mathbf{E}_{p} \in \mathbb{R}^{(T + 1) \times D}</span> هو التضمين الموضعي القابل للتعلّم. تُلخِّص <span class="math inline">\mathbf{F}_{\text{class}}</span> و<span class="math inline">\mathbf{A}_{\text{class}}</span> المعلومات البصرية المكانية والديناميكيات الزمنية السمعية والمحتوى الصوتي.</p>

<h2 id="المحول-متعدد-الوسائط-مع-دمج-الاوزان-الديناميكي">المُحَوِّل مُتعدِّد الوسائط مع دمج الأوزان الديناميكي</h2>
<p>بعد استخراج ميزة الصوت <span class="math inline">\mathbf{A}_{\text{class}}</span> وميزة الفيديو <span class="math inline">\mathbf{F}_{\text{class}}</span>، تُولِّد وحدة دمج الأوزان الديناميكي (DWF) أوزانًا على مستوى الكيان <span class="math inline">W_A</span> و<span class="math inline">W_F</span> لكل وسيط. استلهامًا من (<span class="nodecor">chen2023meaformer</span>)، يتضمّن تصميمنا كتلة انتباهٍ مُتقاطع مُتعدّد الرؤوس ثنائية الطبقات (MHCA) لحساب هذه الأوزان. تعمل MHCA بوظيفة الانتباه في <span class="math inline">N_h</span> رؤوس متوازية، مما يسمح للنموذج بالانتباه المشترك للمعلومات من فضاءات تمثيل فرعية مختلفة وفي مواقع متعددة. يُحدَّد الرأس <span class="math inline">i</span> بمصفوفات إسقاط <span class="math inline">W_q^{(i)}</span>، <span class="math inline">W_k^{(i)}</span>، <span class="math inline">W_v^{(i)} \in \mathbb{R}^{d \times d_h}</span>، التي تُحوِّل المدخلات مُتعدِّدة الوسائط <span class="math inline">\mathbf{A}_{\text{class}}</span>، <span class="math inline">\mathbf{F}_{\text{class}}</span> إلى استفسارات <span class="math inline">Q_{f/a}^{(i)}</span> ومفاتيح <span class="math inline">K_{f/a}^{(i)}</span> وقيم <span class="math inline">V_{f/a}^{(i)}</span> مُدرِكة للوضع. يرمز <span class="math inline">d</span> إلى بُعد ميزات الإدخال، و<span class="math inline">d_h</span> إلى بُعد الرأس الخفي. لكل وسيط، يكون ناتج MHCA:</p>
<p><span class="math display">\[
\begin{gathered}
\text{MHCA}(\mathbf{F}_{\text{class}}) = \text{Concat}(W^i_F \, V_f)\, W_o, \\
\text{MHCA}(\mathbf{A}_{\text{class}}) = \text{Concat}(W^i_A \, V_a)\, W_o, \\
W^i_F = \bar{\beta}^{(i)}_{ff} + \bar{\beta}^{(i)}_{fa},\quad\ \ \ W_F= {\textstyle \sum_{i=1}^{N_h}} W_F^i/N_h,\\
W^i_A = \bar{\beta}^{(i)}_{aa} + \bar{\beta}^{(i)}_{af},\quad\ \ \ W_A= {\textstyle \sum_{i=1}^{N_h}} W_A^i/N_h,
\end{gathered}
\]</span></p>
<p>حيث <span class="math inline">W_o \in \mathbb{R}^{d \times d}</span> و<span class="math inline">\bar\beta^{(i)}_{*}</span> تمثّل أوزان الانتباه للرأس <span class="math inline">i</span>. ويُحسب وزن الانتباه المُشترك بين الصوت والصورة <span class="math inline">\bar\beta^{(i)}_{fa}</span> كما يلي:</p>
<p><span class="math display">\[
\bar\beta^{(i)}_{fa} = \frac{\exp(Q_f K^{\top}_a / \sqrt{d_h})}  {\textstyle \sum_{n\in \{f,a\}}\exp(Q_f K^{\top}_n / \sqrt{d_h}) },
\]</span></p>
<p>وتُحسب القيم <span class="math inline">\bar\beta^{(i)}_{ff}</span> و<span class="math inline">\bar\beta^{(i)}_{af}</span> و<span class="math inline">\bar\beta^{(i)}_{aa}</span> بطريقة مماثلة، مع <span class="math inline">d_h=d/N_h</span>. كما نُطبِّق <span class="math inline">\text{LN}</span> والوصلة المُتبقية خلال التعلّم:</p>
<p><span class="math display">\[
\begin{aligned}
h_v &= \text{LN} \big(\text{MHCA}(\mathbf{F}_{\ell-1})+\mathbf{F}_{\ell-1}\big),\\
h_a &= \text{LN} \big(\text{MHCA}(\mathbf{A}_{\ell-1})+\mathbf{A}_{\ell-1}\big),
\end{aligned}
\]</span></p>
<p>حيث يُمرَّر <span class="math inline">h_v</span> و<span class="math inline">h_a</span> إلى الطبقة التالية من وحدة DWF لمزيد من المعالجة.</p>
<p><strong>دمج الوسائط.</strong> لتعظيم الاستفادة من الميزات بين الوسيطين السمعي والبصري، نَضرب الميزات السمعية المستخرَجة <span class="math inline">\mathbf{A}_{\text{class}}</span> وميزات الفيديو <span class="math inline">\mathbf{F}_{\text{class}}</span> في أوزان المستوى الكياني <span class="math inline">W_A</span> و<span class="math inline">W_F</span> ضمن مرحلة الدمج، بما يضمن تنوّعًا بين الوسائط ويحدّ من التمركز الذاتي المُفرِط:</p>
<p><span class="math display">\[
V = W_F \,\mathbf{F}_{\text{class}}\ \oplus\ W_A \,\mathbf{A}_{\text{class}}.
\]</span></p>
<p>حيث ترمز <span class="math inline">\oplus</span> إلى عملية الربط/الدمج البسيط للمتجهات.</p>

<h1 id="التجربة">التجربة</h1>
<h2 id="مجموعة-البيانات">مجموعة البيانات</h2>
<p>شملت تجاربنا ثلاث مجموعات بيانات: (<span class="nodecor">korshunov1812deepfakes</span>)، (<span class="nodecor">dolhansky2020deepfake</span>)، و(<span class="nodecor">khalid2021fakeavceleb</span>). ونظرًا للاختلال الكبير في نسبة الفيديوهات الحقيقية إلى المُزَيَّفة، استخدمنا استراتيجيات مختلفة لموازنة العينات. يوضّح الجدول [tab:tab0] نسب البيانات قبل وبعد المعالجة. في مجموعة (<span class="nodecor">korshunov1812deepfakes</span>) جُمِعَت فيديوهات حقيقية أصلية من (<span class="nodecor">sanderson2002vidtimit</span>)، بينما استُخرِجت مقاطع/إطارات متتابعة جزئية من فيديوهات (<span class="nodecor">dolhansky2020deepfake</span>)، وبالمقابل ضمّت الأخيرة جميع الإطارات للفيديوهات الحقيقية. وللتخفيف من عدم التوازن في (<span class="nodecor">khalid2021fakeavceleb</span>)، اختيرت <span class="nodecor">19,000</span> فيديو حقيقي إضافي من (<span class="nodecor">chung2018voxceleb2</span>). قُسِّمت البيانات إلى مجموعات تدريب وتحقّق واختبار بنسبة <span class="nodecor">7:1:2</span>، مع نسبة متوازنة <span class="nodecor">1:1</span> بين الحقيقية والمزوّرة في مجموعة الاختبار. وأُجريت جميع التقييمات على هذه المجموعة الاختبارية حصريًّا.</p>

<h2 id="التنفيذ">التنفيذ</h2>
<p>أثناء التدريب، نقسم الفيديوهات الحقيقية والمزوّرة إلى كُتَل بطول <span class="math inline">T</span> (القيمة الافتراضية <span class="nodecor">30</span>). لاكتشاف الوجوه، نستخدم كاشف الوجوه المُقاوِم لتغيُّر المقياس أحادي الطلقة <span class="nodecor">Single Shot Scale-invariant Face Detector</span> (<span class="math inline">S^{3}</span><span class="nodecor">FD</span>) (<span class="nodecor">zhang2017s3fd</span>). ثم نُحاذي الوجوه المكتشفة ونحفظها كصور بأبعاد <span class="math inline">224\times224</span>. في الجانب السمعي، نحسب ميزات <span class="nodecor">MFCC</span> باستخدام نافذة <span class="nodecor">Hanning</span> مدّتها <span class="nodecor">15</span> مللي ثانية مع انتقال <span class="nodecor">4</span> مللي ثانية، مما يضمن تحليلًا طيفيًا دقيقًا. جرت جميع التجارب تحت إعدادات موحّدة لضمان مقارنةٍ عادلة للنتائج.</p>

<h2 id="مقارنات-مع-الأحدث-في-المجال">مقارنات مع الأحدث في المجال</h2>
<p>في سلسلة من التجارب الشاملة، قارنّا فعالية AVT<span class="math inline">^{2}</span>-DWF بعدد من النماذج الرائدة باستخدام مقاييس الأداء (الدقة <span class="nodecor">Accuracy</span> ومساحة تحت المنحنى <span class="nodecor">Area Under the Curve</span>). قسّمنا النماذج الأساسية إلى فئتين: بصرية (<span class="nodecor">V</span>) ومُتعدِّدة الوسائط (<span class="nodecor">AV</span>). أُجري التحليل على ثلاث مجموعات بيانات، كما هو مبين في الجدول [tab:tab1]، وتم تمييز القيم الأفضل بالخط العريض. في مجموعة DF‑TIMIT منخفضة الجودة (LQ)، حقّق كلٌّ من AVT<span class="math inline">^{2}</span>-DWF وAVA-CL دقة بلغت <span class="nodecor">99.99%</span> و<span class="nodecor">100%</span> على التوالي، متفوِّقين بوضوح على الأساليب الأخرى. وفي مجموعة FakeAVCeleb الصعبة المصمَّمة للتزييفات المُعقّدة، أظهر AVA-CL المدعّم بالتعلّم التبايني للانتباه السمعي-البصري أداءً مُكافئًا لطريقتنا AVT<span class="math inline">^{2}</span>-DWF؛ غير أنّ منهجنا يُسجِّل موثوقية أعلى بفضل توازن بيانات الاختبار. أمّا في مجموعة DFDC الواسعة، فقد تفوّق AVT<span class="math inline">^{2}</span>-DWF على جميع أساليب الرؤية والرؤية-السمعية، مُحقِّقًا دقة <span class="nodecor">88.02%</span> ومساحة تحت المنحنى <span class="nodecor">89.20%</span>، مما يدل على أداء استثنائي.</p>

<h2 id="تقييم-البيانات-متقاطعة">تقييم البيانات المُتقاطِعة</h2>
<p>تُركِّز هذه التجربة على تقييم متانة AVT<span class="math inline">^{2}</span>-DWF عبر البيانات المُتقاطِعة. ولضمان التعميم، قارنّا منهجنا مع أربعة نماذج بارزة: Xception (<span class="nodecor">rossler2019faceforensics++</span>)، CViT (<span class="nodecor">wodajo2021deepfake</span>)، LipForensics (<span class="nodecor">haliassos2021lips</span>)، وMDS (<span class="nodecor">mittal2020emotions</span>). تمت التقييمات عبر ثلاث مجموعات بيانات معيارية: FakeAVCeleb التي تضم أربع طرق تزييف عميق، وDFDC التي تشمل ثماني تقنيات، وDF‑TIMIT التي تحتوي على طريقتين للتزييف. تُلخِّص نتائج هذا التقييم في الجدول [tab:tab2]. تُظهر الطرق التقليدية أداءً محدودًا عند مواجهة أنواع جديدة من التزييف العميق. وعلى الرغم من أن CViT، المُستفيد من بنية المُحَوِّل، قدّم نتائج مشجّعة، فإن AVT<span class="math inline">^{2}</span>-DWF تفوّق عليه، مما يؤكّد فعاليته المُحسَّنة في اكتشاف التزييف العميق.</p>

<h2 id="دراسة-الاستئصال">دراسة الاستئصال</h2>
<h3 id="فائدة-وحدة-dwf">فائدة وحدة DWF</h3>
<p>في تحليلٍ استبعاديّ لوحدة DWF ضمن إطار AVT<span class="math inline">^{2}</span>-DWF، قارَنّا ثلاثة تكوينات: المُشَفِّر البصري وحده، وتكوين AV البسيط (دمج الميزات الصوتية والبصرية من دون DWF)، والإطار الكامل الذي يشمل وحدة DWF (VA‑DWF). تُوضِّح نتائج الاختبار على مجموعتي DFDC وFakeAVCeleb في الجدول [tab:tab3] أثر وحدة DWF. في DFDC، حيث يبقى الصوت غير مُزوّر، أدّى الاعتماد على الدمج البسيط للميزات إلى تراجع ملحوظ في الدقة. بالمقابل، في FakeAVCeleb، التي تتضمّن تزييفًا صوتيًّا مع وجه حقيقي في بعض الحالات، حسَّن الدمج المعتمد على DWF الأداء بشكل واضح؛ إذ ارتفعت نسبة الكشف بمقدار <span class="nodecor">11.55%</span> و<span class="nodecor">12.89%</span> على التوالي، مما يدل على الفائدة الكبيرة لوحدة DWF في التقاط الميزات المشتركة بين الوسائط.</p>

<h3 id="فائدة-ترميز-الإطارات-n">فائدة ترميز الإطارات <span class="math inline">n</span></h3>
<p>لتقييم أثر استراتيجية ترميز الإطار<span class="math inline">–n</span>، نستخرج بقعًا عشوائية غير متكرّرة من تسلسل إطارات الوجه، ثم ندمجها لتكوين صور إدخال كاملة. تعرض نتائج الاختبار على مجموعتي DFDC وFakeAVCeleb في الجدول [tab:tab4] تحسّن الأداء بنسبة <span class="nodecor">22.45%</span> و<span class="nodecor">3.74%</span> على التوالي مقارنة بأسلوب البقع التقليدي. يُشير ذلك إلى فاعلية ترميز الإطار<span class="math inline">–n</span> في الحفاظ على المعلومات المستمرّة لملامح الوجه.</p>

<h1 id="الخلاصة">الخلاصة</h1>
<p>نُقدِّم في هذه الورقة إطار AVT<span class="math inline">^{2}</span>-DWF لمعالجة الفروق المكانية الدقيقة والاتساق الزمني داخل محتوى الفيديو. نُبرِز السمات الفريدة لكل وسيط عبر مُشَفِّرات مُحَوِّل الوجه والصوت باستخدام استراتيجية ترميز الإطار<span class="math inline">–n</span>، ثم نُطبِّق آلية الدمج الديناميكي (DWF) لاستخراج الخصائص المشتركة. تُشير نتائج تجاربنا إلى أن AVT<span class="math inline">^{2}</span>-DWF يتفوّق على الأساليب الحالية، سواء داخل البيانات نفسها أو عبر مجموعات بيانات مختلفة. تعكس هذه النتائج أهمية تحقيق تناغم شامل بين الوسائط المتعدّدة للكشف الفعّال عن التزييف العميق في السيناريوهات الواقعية.</p>
</main>
</body>
</html>