<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Yeda Song  , Dongwook Lee &amp; Gunhee Kim">
  <title>التحفُّظ التركيبي: نهج ترانزدكتيفي في التعلُّم بالتعزيز دون اتصال</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 22px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.8;
    }
    header {
      background: linear-gradient(90deg, #4e54c8 0%, #8f94fb 100%);
      color: #fff;
      padding: 40px 0 30px 0;
      text-align: center;
      margin-bottom: 40px;
      box-shadow: 0 2px 8px rgba(78,84,200,0.08);
    }
    h1.title {
      font-size: 2.5em;
      font-weight: 700;
      margin: 0 0 10px 0;
      letter-spacing: 1px;
    }
    .author {
      font-size: 1.2em;
      margin: 0;
      color: #e0e0e0;
    }
    h1, h2, h3 {
      color: #4e54c8;
      font-weight: 700;
      margin-top: 40px;
      margin-bottom: 18px;
      line-height: 1.3;
    }
    h1 {
      font-size: 2em;
      border-bottom: 2px solid #8f94fb;
      padding-bottom: 8px;
      margin-bottom: 24px;
    }
    h2 {
      font-size: 1.5em;
      border-bottom: 1px solid #d1d5fa;
      padding-bottom: 5px;
      margin-bottom: 18px;
    }
    p {
      margin: 0 0 18px 0;
      text-align: justify;
    }
    ul {
      margin: 0 0 18px 32px;
      padding: 0 20px 0 0;
    }
    ul li {
      margin-bottom: 12px;
      font-size: 1em;
    }
    a, a:visited {
      color: #4e54c8;
      text-decoration: underline;
      word-break: break-all;
    }
    a:hover {
      color: #222;
      background: #e0e7ff;
      border-radius: 3px;
      transition: background 0.2s;
    }
    code, .nodecor {
      font-family: 'Cairo', 'Consolas', 'monospace', Arial, sans-serif;
      background: #f3f3f3;
      color: #c7254e;
      padding: 2px 6px;
      border-radius: 4px;
      font-size: 0.95em;
    }
    em {
      color: #8f94fb;
      font-style: normal;
      font-weight: 700;
    }
    /* Responsive */
    @media (max-width: 700px) {
      body { font-size: 18px; }
      header { padding: 25px 0 18px 0; }
      h1.title { font-size: 1.5em; }
      h1, h2 { font-size: 1.1em; }
    }
    /* Light-touch enhancements */
    main {
      transition: box-shadow 0.2s ease;
    }
    main:hover {
      box-shadow: 0 4px 22px rgba(78,84,200,0.10);
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">التحفُّظ التركيبي: نهج ترانزدكتيفي في التعلُّم بالتعزيز دون اتصال</h1>
  <p class="author"><span class="nodecor">Yeda Song</span>  , <span class="nodecor">Dongwook Lee</span> &amp; <span class="nodecor">Gunhee Kim</span></p>
</header>
<main style="max-width: 900px; margin: auto; background: #fff; border-radius: 16px; box-shadow: 0 2px 16px rgba(78,84,200,0.07); padding: 40px 32px 32px 32px;">
  <h1 id="ملخص">مُلَخَّص</h1>
  <p>التعلُّم بالتعزيز دون اتصال هو إطارٌ واعد يهدف إلى تعلُّم السياسات المثلى من بياناتٍ سابقة فحسب، من دون أي تفاعل إضافي مع البيئة. غير أنّ هذا الإطار يعاني بطبيعته من مشكلة التحوّلات التوزيعيّة؛ إذ قد تختلف الحالات والأفعال التي تواجهها السياسة الجديدة عن تلك التي تُغطيها بياناتُ التدريب. وغالباً ما يُعالَج ذلك بفرض تحفُّظٍ على مستوى السياسة أو على مستوى دالّة القيمة، للحدّ من المخاطر والشكوك. في هذا العمل، نتبنّى منظوراً مختلفاً لتحقيق الأهداف نفسها للتحفُّظ. نقدِّم ما نسمّيه «التحفُّظ التركيبي مع البحث عن المرساة» (COCOA) للتعلُّم بالتعزيز دون اتصال، وهو نهج يُطبِّق فكرة إعادة المعايرة الترانزدكتيفيّة (<span class="nodecor">transd_aviv2023</span>) بطريقة تركيبيّة. في هذا الإطار، نُحلِّل الحالة (بوصفها المتغيِّر الإدخالي) إلى مكوّنَيْن: مرساة وفارقاً (دلتا) عنها. يَفرض التحفُّظ التركيبي أن يقع كلٌّ من المرساة والفارق ضمن نطاق التوزيع المعروف، وذلك باستخدام نموذج ديناميكياتٍ عكسيٍّ مُتعلَّم، ممّا يُعزِّز انضباط السياسة أو دالّة القيمة في فضاء المُدخلات التركيبي. ويبقى هذا التحفُّظ مستقلاً عن التحفُّظ السلوكي التقليدي في التعلُّم بالتعزيز دون اتصال. طبَّقنا COCOA على أربع خوارزميّات حديثة في التعلُّم بالتعزيز دون اتصال وقيَّمنا أداءها على معيار <span class="nodecor">D4RL</span>، وقد أظهرت النتائج تحسّناً عاماً في أداء كلٍّ منها. الشيفرة متاحة على: <a href="https://github.com/runamu/compositional-conservatism" class="uri">https://github.com/runamu/compositional-conservatism</a>.</p>
  <h1 id="introduction">مُقَدِّمَة</h1>
  <p>حقَّق التعلُّم بالتعزيز نجاحاتٍ ملحوظة في مجالاتٍ عدّة، من توجيه حركات الروبوتات (<span class="nodecor">dasari2020robonet</span>) وتحسين استراتيجيات الألعاب (<span class="nodecor">mnih2015human</span>) إلى تدريب نماذج اللغة (<span class="nodecor">rajpurkar2016squad</span>). وعلى الرغم من هذه الإنجازات، فقد دفعت التحدّياتُ المرتبطة بالتفاعلات الفعليّة في البيئات الحسّاسة والمعقّدة إلى تطوير التعلُّم بالتعزيز دون اتصال بوصفه مساراً عمليّاً. يتعلّم التعلُّم بالتعزيز دون اتصال (<span class="nodecor">wiering2012reinforcement, levine2020offline</span>)—أو ما يُعرَف بالتعلُّم الدفعي (<span class="nodecor">lange2012batch</span>)—السياساتِ من بياناتٍ مُسبقة فقط، من دون أي تفاعل مباشر مع البيئة. ويزداد هذا الاتّجاه شيوعاً في تطبيقاتٍ مثل القيادة الذاتيّة (<span class="nodecor">yu2020bdd100k</span>) والرعاية الصحيّة (<span class="nodecor">gottesman2019guidelines</span>) حيث تتوافر وفرةٌ من البيانات السابقة.</p>
  <p>في الجوهر، يكون التعلُّم بالتعزيز دون اتصال عُرضةً للتحوّلات التوزيعيّة. تنشأ هذه المشكلة عندما يختلف توزيع الحالات والأفعال التي تواجهها السياسة المُستخرَجة عن تلك الموجودة في مجموعة بيانات التدريب، وهي معضلةٌ معروفة في التعلُّم الآلي (<span class="nodecor">levine2020offline</span>). وقد عالجت خوارزميّاتٌ عديدة هذا الأمر عبر نهج التحفُّظ، سواءً بتقييد السياسة أو بتقدير مقدار عدم اليقين للتخفيف من الانحرافات التوزيعيّة (<span class="nodecor">count_kim2023, prdc_ran2023, iql_kostrikov2022, cql_kumar2020, brac_wu2019, bear_kumar2019, bcq_fujimoto2019, mobile_sun2023, rambo_rigter2022, romi_wang2021, combo_yu2021, mopo_yu2020, morel_kidambi2020</span>). تهدف هذه الاستراتيجيات إلى إبقاء الوكيل ضمن نطاق التوزيع المعروف وتقليل مخاطر السلوكيّات غير المتوقَّعة. في هذا العمل، نسعى أيضاً إلى ضبط سياسة الاختبار عبر مواءمة توزيعها مع البيانات المعروفة، ولكن من منظورٍ تركيبيٍّ مختلف.</p>
  <p>نُدرك أولاً أنّ مشكلة التحوُّل التوزيعي للحالة مرتبطةٌ ارتباطاً وثيقاً بكيفيّة تعاطي مُقارِّبات الدوالّ مع نقاط الإدخال الواقعة خارج «دعم التوزيع». وبناءً على ذلك، نستكشف إمكانيّة تحويل مشكلة «التعلُّم خارج الدعم» إلى مشكلة «خارج التركيب» عبر حقن تحيُّزاتٍ استقرائيّة في مُقارِّبات الدوالّ الخاصّة بالسياسة أو بدالّة القيمة Q. وقد اقترح (<span class="nodecor">transd_aviv2023</span>) منهجاً ترانزدكتيفيّاً يُسمّى <em>التحويل الثنائي</em>، حيث تُعاد معايرة الدالّة الهدف إلى صيغةٍ ثنائيةٍ بتحليل المتغيِّر الإدخالي إلى مكوّنَيْن: مرساة ودلتا. هنا، تمثّل المرساة نقطةً مرجعيّة مأخوذةً من بيانات التدريب، بينما تمثّل الدلتا الفرق بين المتغيِّر الإدخالي والمرساة. وتحت افتراضاتٍ محدّدة على توزيعي التدريب والاختبار وخصائص الدالّة الهدف، يمكن للتحويل الثنائي أن يُعالج مشكلة «خارج التركيب» ومن ثَمّ يخفِّف مسألة «خارج الدعم» للدالّة الأصليّة.</p>
  <p>في هذا السياق، نقدِّم إطار «التحفُّظ التركيبي مع البحث عن المرساة» (<em>COCOA</em>) للتعلُّم بالتعزيز دون اتصال. يستند COCOA إلى مبدأ إعادة المعايرة الترانزدكتيفيّة (<span class="nodecor">transd_aviv2023</span>)، ويضيف طبقةً تركيبيّة لتعزيز الانضباط. يُحوِّل هذا الإطار مسألة التحوُّل التوزيعي إلى مسألة «خارج التركيب» بنقل عناصر التعميم الرئيسة من البيانات إلى المكوّنات المُحلَّلة وآليّة ربطها؛ وبذلك يتعيّن اختيار المرساة والدلتا بحيث يكونا قريبَيْن من توزيع بيانات التدريب.</p>
  <p>لتحقيق ذلك، نقدِّم «سياسة البحث عن المرساة» بوصفها طبقةً إضافيّة تُلزم الوكيل بتعيين مرساةٍ تقع ضمن المنطقة المعروفة من فضاء الحالة. يشجِّع COCOA على اختيار مراسٍ قريبةٍ من أمثلة التدريب، مع تقييد الفارق (الدلتا) ضمن نطاقٍ ضيّق عبر وضع المرساة بين حالاتٍ متجاورة. يُسهم ذلك في تقليص فضاء الإدخال وتوجيهه نحو الجزء الذي تمّ استكشافه أثناء التدريب. باختصار، ومن خلال تعلُّم سياسةٍ تبحث عن مرساةٍ داخل التوزيع وتُقدِّر الفوارق باستخدام نموذج ديناميكياتٍ عكسيٍّ مُتعلَّم، نُحفِّز التحفُّظ في فضاء الإدخال التركيبي لكلٍّ من دالّة القيمة Q والسياسة. ويبقى هذا النهج مستقلاً وغير معتمد على التحفُّظ السلوكي التقليدي في التعلُّم بالتعزيز دون اتصال.</p>
  <p>ومن الناحية التجريبيّة، وجدنا أنّ COCOA يعزِّز أداء أربع خوارزميّات بارزة في التعلُّم بالتعزيز دون اتصال—CQL (<span class="nodecor">cql_kumar2020</span>) وIQL (<span class="nodecor">iql_kostrikov2022</span>) وMOPO (<span class="nodecor">mopo_yu2020</span>) وMOBILE (<span class="nodecor">mobile_sun2023</span>)—على معيار <span class="nodecor">D4RL</span> (<span class="nodecor">d4rl_fu2020</span>). وإضافةً إلى ذلك، تكشف دراسةُ الاستئصال عن أهمّيّة سياسة البحث عن المرساة في رفع مستوى الأداء. وتُجمَع مساهماتُنا الرئيسة كما يلي:</p>
  <ul>
    <li><p>نسعى إلى فرض التحفُّظ في <em>فضاء الإدخال التركيبي</em> لمُقارِّبات الدوالّ الخاصة بدالّة القيمة-‏Q والسياسة، بشكلٍ مستقلٍّ وغير معتمدٍ على التحفُّظ السلوكي السائد في التعلُّم بالتعزيز دون اتصال.</p></li>
    <li><p>نقدِّم التحفُّظ التركيبي مع البحث عن المرساة (<em>COCOA</em>) الذي يعثر على المرساة والدلتا ضمن التوزيع باستخدام نموذج ديناميكياتٍ عكسيٍّ مُتعلَّم، وهو أمرٌ حاسم للتعميم التركيبي.</p></li>
    <li><p>نُظهر تجريبيّاً أنّ COCOA يُحسِّن أداء أربع خوارزميّات حديثة للتعلُّم بالتعزيز دون اتصال على معيار D4RL. كما تُبيِّن دراسةُ الاستئصال فاعليّةَ سياسة البحث عن المرساة مقارنةً بالاختيار الاستدلالي للمرساة.</p></li>
  </ul>
  <h1 id="sec:prelim">المُقَدِّمات</h1>
  <h2 id="subsec:offlinerl">التعلُّم بالتعزيز دون اتصال</h2>
  <!-- بقية الوثيقة دون تغيير -->
</main>
</body>
</html>