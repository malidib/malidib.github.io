```html
<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Mukul Gagrani">
  <meta name="author" content="Raghavv Goel">
  <meta name="author" content="Wonseok Jeon">
  <meta name="author" content="Junyoung Park">
  <meta name="author" content="Mingu Lee">
  <meta name="author" content="Christopher Lott">
  <title>في الترميز التخميني لنماذج اللغة الكبيرة متعددة الوسائط</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>
    body {
      direction: rtl;
      font-family: 'Cairo', 'Noto Naskh Arabic', 'Amiri', 'Segoe UI', 'Tahoma', 'Arial', sans-serif;
      background: #f8f9fa;
      color: #222;
      font-size: 22px;
      margin: 0;
      padding: 0;
      line-height: 1.7;
    }
    header {
      background: linear-gradient(90deg, #e3e6f3 0%, #f8f9fa 100%);
      padding: 2.5rem 0 1.5rem 0;
      text-align: center;
      border-bottom: 1px solid #e0e0e0;
      margin-bottom: 2rem;
    }
    h1.title {
      font-size: 2.5rem;
      font-weight: 800;
      color: #2d3a4a;
      margin-bottom: 1rem;
      letter-spacing: 0.02em;
    }
    .author {
      display: inline-block;
      margin: 0 0.5rem;
      font-size: 1.1rem;
      color: #4b5d6b;
      font-weight: 500;
    }
    h1, h2, h3, h4 {
      color: #2d3a4a;
      margin-top: 2.2rem;
      margin-bottom: 1rem;
      font-weight: 700;
      letter-spacing: 0.01em;
    }
    h1 {
      font-size: 2rem;
      border-bottom: 2px solid #d1d5db;
      padding-bottom: 0.3rem;
    }
    h2 {
      font-size: 1.5rem;
      border-bottom: 1px solid #e0e0e0;
      padding-bottom: 0.2rem;
    }
    h3 {
      font-size: 1.2rem;
      color: #3b4c5a;
    }
    h4 {
      font-size: 1.1rem;
      color: #4b5d6b;
      margin-top: 1.5rem;
      margin-bottom: 0.7rem;
    }
    p {
      margin: 1.1em 0;
      text-align: justify;
    }
    ol, ul {
      margin: 1em 2em 1em 0;
      padding-right: 1.5em;
    }
    li {
      margin-bottom: 0.5em;
    }
    em {
      color: #5a6d7c;
      font-style: italic;
    }
    strong {
      color: #1a2a38;
      font-weight: 700;
    }
    code, pre {
      background: #f3f6fa;
      color: #b23c3c;
      border-radius: 4px;
      padding: 0.2em 0.4em;
      font-size: 0.95em;
      font-family: 'Fira Mono', 'Consolas', 'Monaco', monospace;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 2em 0;
      background: #fff;
      box-shadow: 0 1px 4px rgba(0,0,0,0.04);
    }
    table caption {
      caption-side: top;
      font-size: 1.1em;
      color: #4b5d6b;
      margin-bottom: 0.5em;
      font-weight: 600;
    }
    th, td {
      border: 1px solid #e0e0e0;
      padding: 0.7em 1em;
      text-align: right;
    }
    th {
      background: #f3f6fa;
      color: #2d3a4a;
      font-weight: 700;
    }
    tr:nth-child(even) {
      background: #f8f9fa;
    }
    tr:nth-child(odd) {
      background: #fff;
    }
    .math.inline, .math.display {
      direction: ltr;
      unicode-bidi: embed;
      font-family: 'Fira Mono', 'Consolas', 'Monaco', monospace;
      font-size: 1em;
      background: none;
      color: #1a2a38;
    }
    .nodecor {
      text-decoration: none !important;
      color: inherit;
    }
    blockquote {
      border-right: 4px solid #d1d5db;
      background: #f3f6fa;
      margin: 1.5em 0;
      padding: 1em 1.5em;
      color: #4b5d6b;
      font-size: 1.05em;
    }
    @media (max-width: 700px) {
      body {
        font-size: 18px;
        padding: 0 0.5em;
      }
      header {
        padding: 1.2rem 0 1rem 0;
      }
      h1.title {
        font-size: 1.5rem;
      }
      h1, h2 {
        font-size: 1.2rem;
      }
      table, th, td {
        font-size: 0.95em;
      }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">في الترميز التخميني لنماذج اللغة الكبيرة متعددة الوسائط</h1>
  <p class="author"><span class="nodecor">Mukul Gagrani</span></p>
  <p class="author"><span class="nodecor">Raghavv Goel</span></p>
  <p class="author"><span class="nodecor">Wonseok Jeon</span></p>
  <p class="author"><span class="nodecor">Junyoung Park</span></p>
  <p class="author"><span class="nodecor">Mingu Lee</span></p>
  <p class="author"><span class="nodecor">Christopher Lott</span></p>
</header>

<main>
<h1 id="ملخص">مُلخَّص</h1>
<p>الاستدلال باستخدام نماذج اللغة الكبيرة متعددة الوسائط (<span class="nodecor">MLLMs</span>) بطيء؛ ذلك بسبب أن العمود الفقري لنموذج اللغة الكبير يعاني من عنق الزجاجة في عرض النطاق الترددي للذاكرة ويولد الرموز بشكل تلقائي تصاعدي. في هذه الورقة، نستكشف تطبيق الترميز التخميني لتعزيز كفاءة الاستدلال في نماذج <span class="nodecor">MLLMs</span>، وبشكلٍ خاص نموذج <span class="nodecor">LLaVA 7B</span>. نُظهر أن نموذج اللغة فقط يمكن أن يكون نموذجًا مسودياً جيدًا للترميز التخميني مع <span class="nodecor">LLaVA 7B</span>، متجاوزًا الحاجة إلى رموز الصور ومكونات المعالجة المرتبطة بها. تبين تجاربنا عبر ثلاث مهام مختلفة أن الترميز التخميني يمكن أن يحقق تسريعًا محدودًا بالذاكرة يصل إلى <span class="nodecor">2.37<span class="math inline">\(\times\)</span></span> عند استخدام نموذج لغة بعدد معاملات <span class="nodecor">115M</span> قمنا بتدريبه من الصفر. بالإضافة إلى ذلك، نقدم نموذج مسودة <span class="nodecor">LLaVA</span> مدمجًا يتضمّن محول صور، والذي يظهر مكاسب أداء طفيفة في وصف الصور مع الحفاظ على نتائج مشابهة في المهام الأخرى.</p>

<h1 id="مقدمة">مقدمة</h1>
<p>أصبحت نماذج اللغة الكبيرة (Large Language Models) شائعة الاستخدام في مختلف المجالات بفضل أدائها المميز. ومع ذلك، تقتصر هذه النماذج على استقبال استفسارات نصية فقط، بينما تأتي البيانات في العالم الحقيقي على شكل وسائط متعددة تشمل المعلومات البصرية. توفر نماذج اللغة الكبيرة متعددة الوسائط (<span class="nodecor">MLLMs</span>) (<span class="nodecor">awadalla2023openflamingo</span>, <span class="nodecor">liu2024visual</span>, <span class="nodecor">tsimpoukelli2021multimodal</span>, <span class="nodecor">zhu2023minigpt</span>) قدرات فهم الصور، عبر دمج الرموز البصرية والنصية لتفاعل أكثر فائدة مع المستخدمين. تتكون هذه النماذج من مشفّر صور لمعالجة معلومات الصورة، ومحول يحول ترميزات الصور إلى فضاء تضمين نموذج اللغة، إضافة إلى العمود الفقري الخاص بنموذج اللغة والذي يرث منه التوليد التلقائي العكسي وعنق الزجاجة في عرض النطاق الترددي للذاكرة، مما يؤدي إلى بطء الاستدلال (<span class="nodecor">shazeer2019fast</span>).</p>
<p>اقترح الترميز التخميني (<span class="nodecor">speculative decoding</span>) (<span class="nodecor">leviathan2023fast</span>, <span class="nodecor">chen2023accelerating</span>, <span class="nodecor">sun2023spectr</span>, <span class="nodecor">miao2023specinfer</span>, <span class="nodecor">jeon2024recursive</span>) كحل لتسريع عملية الاستدلال في نماذج اللغة الكبيرة دون التضحية بالدقة، حيث يتنبأ نموذج مسودة أصغر بعدة رموز مستقبلية يتم التحقق منها في استدعاء واحد لنموذج اللغة الكبير. ونظرًا لأن نماذج اللغة الكبيرة متعددة الوسائط تعتمد على نموذج لغة كبير في عمودها الفقري، يمكن تطبيق الترميز التخميني لجعل استدلالها أكثر كفاءة. تناولت العديد من الأعمال الحديثة استخدام الترميز التخميني ومتغيراته (<span class="nodecor">kim2023big</span>, <span class="nodecor">fu2023lookahead</span>, <span class="nodecor">medusa</span>, <span class="nodecor">santilli2023accelerating</span>, <span class="nodecor">sun2023spectr</span>, <span class="nodecor">jeon2024recursive</span>) لنماذج اللغة الكبيرة، لكن لا توجد دراسات سابقة في سياق نماذج اللغة الكبيرة متعددة الوسائط حسب علمنا.</p>
<p>في هذه الورقة، نطبق الترميز التخميني على نموذج <span class="nodecor">LLaVA 7B</span> (الذي يستخدم <span class="nodecor">LLaMA 7B</span> كعمود فقري للغة) لجعل الاستدلال أكثر كفاءة. نظرًا لغياب نماذج أصغر علنًا من عائلتي LLaVA وLLaMA تحت 7B معاملات، قمنا بتدريب نموذج لغة من الصفر بحجم <span class="nodecor">115M</span> لاستخدامه كنموذج مسودة. نُظهر أن نموذج اللغة الذي لا يأخذ في الاعتبار رموز الصور (وبالتالي لا يتطلب مشفّر الصور والمحور) يمكن أن يكون نموذج مسودة جيدًا لـ<span class="nodecor">LLaVA 7B</span>. أجرينا تجارب على ثلاث مهام تشمل أسئلة وأجوبة على صور في مجموعة بيانات LLaVA Instruct 150K (<span class="nodecor">liu2024visual</span>)، ووضع العناوين على صور من مجموعة بيانات COCO (<span class="nodecor">lin2014microsoft</span>) ومجموعة بيانات ScienceQA (<span class="nodecor">lu2022learn</span>)، بمختلف نماذج المسودة بعد مراحل تدريب وصقل متفاوتة. تظهر نتائجنا أننا نستطيع تحقيق تسريع محدود بالذاكرة يصل إلى 2.37× باستخدام نموذج اللغة فقط كنموذج مسودة. كما أنشأنا نموذج مسودة <span class="nodecor">LLaVA</span> مدمجًا يضم محول صور إلى جانب نموذج اللغة المدرب، وأظهر تحسينًا طفيفًا في مهمة التعليق على COCO ومهمة ScienceQA مع أداء مماثل لباقي المهام.</p>

<h1 id="الطريقة">الطريقة</h1>
<h1 id="الخلفية">الخلفية</h1>
<h2 id="التفكيك-التخميني">الترميز التخميني</h2>
<p>يتضمن الترميز التخميني (<span class="nodecor">Speculative Decoding</span>) (<span class="nodecor">chen2023accelerating</span>, <span class="nodecor">leviathan2023fast</span>) استخدام نموذج مسودة أصغر لتوليد عدة رموز يتم التحقق منها بالتوازي من قبل النموذج اللغوي الكبير المستهدف. بناءً على سياق الإدخال <span class="math inline">\(X_{1:n}:=[X_{1}, \dots, X_{n}]\)</span>، يولد نموذج المسودة تسلسلًا من الرموز <span class="math inline">\(\hat{X}_{n+1:n+L}\)</span> بطريقة تلقائية تراكميًا، <span class="math inline">\(\hat{X}_{n+j} \sim p(\cdot | X_{1:n}, \hat{X}_{n+1:n+j-1})\)</span>. ثم يتم التحقق من هذه الرموز في استدعاء واحد للنموذج اللغوي الكبير المستهدف (<span class="math inline">\(q\)</span>) باستخدام آلية أخذ العينات بالرفض لضمان مطابقة التوزيع الأصلي. على وجه التحديد، يُقبَل الرمز <span class="math inline">\(\hat{X}_{n+j}\)</span> بالاحتمالية 
<span class="math display">\[
\min\left\{1, \frac{q(\hat{X}_{j}|X_{1:n}, \hat{X}_{n+1:n+j-1})}{p(\hat{X}_{j}|X_{1:n}, \hat{X}_{n+1:n+j-1})}\right\}.
\]</span>
إذا رُفِض رمز مسودة <span class="math inline">\(\hat{X}_{n+j}\)</span>، يتم أخذ عينة جديدة من التوزيع المتبقي <span class="math inline">\(p_{res}(x)=\max(0, q(x) - p(x))\)</span>.</p>
<h2 id="نماذج-اللغة-الكبيرة-متعددة-الوسائط">نماذج اللغة الكبيرة متعددة الوسائط</h2>
<p>يتكون نموذج اللغة الكبير متعدد الوسائط المعتمد على الصور من <em>1) مشفّر الرؤية</em> لتشفير الصورة المدخلة، <em>2) محول</em> لتحويل ترميزات الصور إلى تضمينات نموذج اللغة، و<em>3) العمود الفقري لنموذج اللغة</em>. نصف إطار عمل نموذج <span class="nodecor">LLaVA</span> بالتفصيل؛ بالنظر إلى صورة مدخلة <span class="math inline">\(I\)</span> واستعلام نصي <span class="math inline">\(Q\)</span>، تُحوَّل الصورة إلى تسلسل <span class="math inline">\(H_1, H_2, \ldots, H_m\)</span> من الترميزات، ويُحوَّل الاستعلام النصي إلى تسلسل من تضمينات الرموز <span class="math inline">\(X_1, X_2, \ldots, X_n\)</span>. ثم يحوّل المحول <span class="math inline">\(g_\theta\)</span> هذا التسلسل إلى تضمينات صورة <span class="math inline">\(V_i = g_\theta(H_i)\)</span> في فضاء نموذج اللغة. أخيرًا، يولد نموذج اللغة الرموز التالية بناءً على تضمينات الصورة والنص كما في: 
<span class="math display">\[
X_{n+1} \sim q(\cdot | V_{1:m}, X_{1:n})
\]</span>
</p>

<h1 id="تحليل-spd-لنماذج-mllm">تحليل SPD لنماذج MLLM</h1>
<p>لتحقيق مكاسب أكبر مع الترميز التخميني، نحتاج إلى نموذج مسودة أصغر كثيرًا ومتوافق جيدًا مع نموذجنا الهدف (LLaVA-7B). الخيار الشائع في الأدبيات هو استخدام نموذج مسودة مدرب مسبقًا من نفس العائلة أو تدريب نموذج أصغر بنفس بنية الهدف (<span class="nodecor">miao2023specinfer</span>). وبما أنه لا يتوفر نموذج أصغر علنًا في عائلة LLaVA، دربنا نموذج مسودة من الصفر. اخترنا بنية مشابهة لهيكل LLaVA، بحيث يتمثل نموذج المسودة في 1) محول صورة أصغر مع نموذج اللغة المسودة، أو 2) نموذج مسودة نصي فقط يُولّد الرموز استنادًا إلى النص فقط. بالنظر إلى تضمينات الصورة <span class="math inline">\(V_{1:m}\)</span> وتضمينات النص <span class="math inline">\(X_{1:n}\)</span>، يولّد نموذج المسودة تسلسل الرموز <span class="math inline">\(\hat{X}_{n+1:n+L}\)</span> حيث 
<span class="math display">\[
\hat{X}_{n+j} \sim p(\cdot | X_{1:n}, \hat{X}_{n+1:n+j-1}).
\]</span>
يتحقق نموذج الهدف LLaVA من هذه الرموز اعتمادًا على تضمينات الصورة والنص باحتمالية 
<span class="math display">\[
\min\left\{1, \frac{q(\hat{X}_{n+j}|V_{1:m}, X_{1:n}, \hat{X}_{n+1:n+j-1})}{p(\hat{X}_{n+j}|X_{1:n}, \hat{X}_{n+1:n+j-1})}\right\}.
\]</span>
يُعد نموذج المسودة النصي فقط أكثر كفاءة لأنه 1) لا يحتاج إلى محول إضافي لاستيعاب تضمينات الصورة، و2) لا يتطلب تدريب المحول.</p>

<h1 id="التجارب">التجارب</h1>
<p>نقوم بتشغيل التجارب على ثلاث مهام إرشاد بصري باستخدام SPD مع نموذج LLaVA-7B (<span class="nodecor">liu2023improved</span>) المستهدف، الذي يعتمد على LLaMA-7B كنموذج لغة. جميع نماذج المسودة تمتلك حجمًا ثابتًا لجزء اللغة يبلغ <span class="math inline">\(115M\)</span>.</p>
<h4 id="مرشحو-نموذج-المسودة.">مرشحو نماذج المسودة:</h4>
<p>دربنا نموذج مسودة بحجم <span class="math inline">\(115M\)</span> وفق هيكلية LLaMA-2 من الصفر، ثم صقلنا نموذج المسودة على بيانات تعليمات بخسارة TVD++ (<span class="nodecor">goel2024direct</span>) ومجموعة فرعية من LLaVA Instruct 150K (<span class="nodecor">liu2024visual</span>). نعتبر المراحل التالية:</p>
<ol>
<li><em>LLaMA الأساسي</em>: نموذج LLaMA قبل النشر المسبق على <span class="math inline">\(600B\)</span> رمز إنجليزي.</li>
<li><em>LLaMA للدردشة</em>: صقل تعليمات من نموذج LLaMA الأساسي (<span class="nodecor">goel2024direct</span>).</li>
<li><em>LLaVA المصقول</em> (<span class="nodecor">ft-llava</span>): صقل كامِل مع تهيئة محول الصور من LLaVA-7B باستخدام تقنية التقسيم الفرعي (<span class="nodecor">samragh2023weight</span>) ونموذج اللغة من LLaMA للدردشة.</li>
<li><em>LLaVA نصيًا مصقول</em> (<span class="nodecor">ft-llava-text</span>): يستخدم جزء نموذج اللغة فقط من نموذج <em>ft-llava</em>.</li>
</ol>
<p>عندما يعتمد نموذج المسودة على الصورة، يُشارك مشفّر الرؤية (CLIP ViT-L/14) مع الهدف لتجنب إعادة حساب التضمينات. تفاصيل المعلمات في الملحق [app:model_config].</p>
<h4 id="مهام-التقييم.">مهام التقييم:</h4>
<p>نركّز على توليد النص المفتوح النِهائي والإجابة متعددة الخيارات مع التفكير المتسلسل (CoT)، لتعزيز طول التوليد. نقيّم على: 1) مجموعة بيانات LLaVA Instruct 150K (<span class="nodecor">liu2024visual</span>), 2) وصف COCO (<span class="nodecor">lin2014microsoft</span>), و3) ScienceQA مع CoT (<span class="nodecor">lu2022learn</span>). إعدادات المطالبات في الملحق [app:sys_prompts].</p>
<h4 id="المقاييس.">المقاييس:</h4>
<p>نقيس فعالية SPD عبر: 1) <strong>كفاءة الكتلة</strong> (<span class="math inline">\(\tau\)</span>): متوسط عدد الرموز المولدة لكل تشغيل للنموذج الهدف لحجم كتلة <span class="math inline">\(\gamma\)</span>. 2) <strong>التسريع المحدود بالذاكرة</strong> (<span class="nodecor">MBSU</span>): <span class="math inline">\(\mathrm{MBSU}(x)=\frac{c\,\tau(x)}{c\,\gamma+1}\)</span> حيث <span class="math inline">\(c\)</span> نسبة معاملات نموذج المسودة إلى الهدف. 3) <strong>معدل الرموز</strong>: إجمالي عدد الرموز المولدة مقسومًا على زمن التوليد، لقياس الرموز في الثانية. نجري القياسات بحجم كتلة <span class="math inline">\(\gamma\in\{3,5\}\)</span>.</p>
<h4 id="فك-التشفير.">فك التشفير:</h4>
<p>نستخدم فك التشفير الجشع لجميع التجارب، بحيث يكون التوليد مطابقًا للتوليد التلقائي التراكمي للنموذج الهدف. نترك استكشاف فك التشفير القائم على العينات (تغيير درجة الحرارة، top-<span class="math inline">\(p\)</span>, top-<span class="math inline">\(k\)</span>) كعمل مستقبلي.</p>
<h4 id="النتائج.">النتائج:</h4>
<p>تُظهر نتائجنا أن SPD مع نموذج الهدف LLaVA <span class="nodecor">7B</span> يمنح تسريعًا كبيرًا في التوليد. وعند استخدام نموذج مسودة نصي فقط، يقدم SPD تسريعًا تنافسيًا مقابل نموذج مسودة يستفيد من معلومات الصورة.</p>
<p>من الشكل [fig:result] (العلوي والوسطي)، نرى أن SPD يحقّق مكاسب تزيد على <span class="nodecor">2<span class="math inline">\(\times\)</span></span> من حيث كفاءة الكتلة وMBSU. يتجه الأداء للارتفاع عند زيادة حجم الكتلة من <span class="nodecor">3</span> إلى <span class="nodecor">5</span> في جميع المهام، باستثناء SQA حيث يتفوّق نموذج المسودة base-llama على غيره عند <span class="nodecor">=5</span>. في تقييم LLaVA، يتصدر ft-llava-text ثم ft-llava لكلا الحجمين. في COCO، يتصدر ft-llava ثم ft-llava-text. في SQA، عند <span class="nodecor">=3</span> يتفوّق ft-llava ثم ft-llava-text، وعند <span class="nodecor">=5</span> يتفوّق ft-llava ثم base-llama. كما تحسّن جميع نماذج المسودة معدل الرموز مقارنة بالتوليد التلقائي التراكمي—ويكون حجم الكتلة 3 أفضل من 5—مما يرفع عدد الرموز في الثانية.</p>
<p>نعرض أيضًا نتائج نوعية لتوليد تعليقات COCO باستخدام نموذج المسودة ft-llava-text في الشكل [fig:qualitative_example]، حيث التوكنات باللون الأزرق والمسطرة مقبولة. نرى أن نموذج المسودة يتنبأ بالكلمات الشائعة والإكمالات دون معلومات الصورة؛ على سبيل المثال، يتنبأ بـ“tables” من “vege”، وفي المثال الثاني من “app” يتنبأ بـ“liances”. عمومًا، تحتوي النصوص المفتوحة على العديد من التوكنات الشائعة والإكمالات التي لا تتطلب تضمينات بصرية، لذا يقدم نموذج المسودة النصي أداءً تنافسيًا. يمكنه أيضًا تكرار التوكنات بعد توليدها—مثل “counter” و“bowls” في المثال الثاني. أخيرًا، نترك صقلًا أكثر صرامة لنموذج متعددة الوسائط صغير كعمل مستقبلي.</p>
<p>بناءً على نوع الرموز المقبولة، افترضنا أن نموذج LLaVA الأولي قد لا يستخدم معلومات الرؤية بالكامل (أي لا يحسن كفاءة الكتلة)، ربما لأن محول الصور الأولي لم يرمّز الرموز بشكل كامل. لذلك جرّبنا SPD مع نماذج أولية لا تستخدم رموز الصور، لمراقبة تأثير المحول المتدرّب في تحسين الكتلة أو MBSU. من الشكل <span class="nodecor">fig:avg_token_all</span> و<span class="nodecor">fig:mbsu_all</span> نرى أن LLaVA المصقول نصيًا والنسخة الكاملة يقدمان أداءً متقاربًا، مما يدعم فرضيتنا تجريبيًا.</p>
<p>أضفنا أيضًا نماذج مسودة نصية أخرى لمراقبة التسريع باستخدام النص فقط. ولدهشتنا، أظهر حتى نماذج LLaMA الأساسية ودردشة LLaMA تسريعًا يزيد عن <span class="nodecor">2</span> مرات في المتوسط (<span class="nodecor">fig:mbsu_all</span>).</p>
<p>أداء SPD مع نموذج مسودة نصي قريب نسبيًا من نموذج LLaVA، ويتفوق أحيانًا في ScienceQA. بناءً على ذلك، حلّلنا متوسط قبول التوكنات في النماذج المختلفة ودرجة الاهتمام المُخصصة لرموز الصورة في هدف LLaVA (انظر الملحق).</p>

<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<h1 id="الخلاصة">الخلاصة</h1>
<p>في هذه الورقة، نقدم الجهد الأول نحو استخدام الترميز التخميني لتسريع الاستدلال في نماذج اللغة الكبيرة متعددة الوسائط، مع التركيز على الصور والنصوص. نُظهر أن نموذج مسودة نصي فقط يحقق أداءً تنافسيًا مقارنةً بنموذج مسودة يستفيد من ميزات الصورة. أجرينا تجارب على مهام توليد نص مفتوح ونموذج توليد مع تفكير متسلسل باستخدام نماذج مسودة نصية ونص–صورة، وحققنا تسريعًا يصل إلى <span class="nodecor"><span class="math inline">\(2.37\times\)</span></span> للنموذج النصي وحده وتسريعًا أفضل قليلًا للنموذج النص–صورة، مما يُظهر تجريبيًا فعالية الترميز التخميني في MLLMs.</p>
<p>يفتح عملنا مسارات مستقبلية متعددة ضمن الإطار العام المقدم. يمكن توسيعه ليشمل نماذج أخرى مثل (<span class="nodecor">li2023blip</span>)، (<span class="nodecor">zhu2023minigpt</span>)، (<span class="nodecor">awadalla2023openflamingo</span>)، وكذلك وسائط أخرى كالصوت (<span class="nodecor">chu2023qwen</span>) التي تعاني من نفس قيود التوليد التلقائي التراكمي. علاوة على ذلك، يمكن تبني أساليب ترميز تخميني قائمة على الشجرة (<span class="nodecor">sun2023spectr</span>, <span class="nodecor">miao2023specinfer</span>, <span class="nodecor">medusa</span>, <span class="nodecor">jeon2024recursive</span>) لزيادة سرعة التوليد أكثر.</p>

<h1 id="الملحق">الملحق</h1>
<h2 id="app:model_config">تكوينات النموذج</h2>
<p>يستخدم نموذج LLaVA-7B: (i) مشفّر الرؤية، (ii) محول/مشروع الصورة المبني على الشبكة العصبية متعددة الطبقات، و (iii) نموذج اللغة LLaMA 7B. المشفّر البصري هو CLIP ViT-L/14 مع تفاصيل في (<span class="nodecor">radford2021learning</span>)، ومحولات الصورة تحتوي على طبقتين خطيتين بأبعاد <span class="math inline">\(1024\times4096\)</span> و<span class="math inline">\(4096\times4096\)</span>. أما في سيناريو النماذج المبدئية مع محول صور، فالأبعاد تكون <span class="math inline">\(1024\times1024\)</span> و<span class="math inline">\(1024\times1024\)</span>.</p>
<p>تكوينات جزء نموذج اللغة الهدف والمسودة (LLaMA) كما يلي:</p>
<table>
<caption>تكوينات النموذج المسودة والهدف</caption>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">الهدف (7B)</th>
<th style="text-align: right;">المسودة (115M)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">الطبقات</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: left;">رؤوس الانتباه</td>
<td style="text-align: right;">32</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="odd">
<td style="text-align: left;">البعد الوسيط</td>
<td style="text-align: right;"><span class="nodecor">11,008</span></td>
<td style="text-align: right;"><span class="nodecor">2,816</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">البعد الخفي</td>
<td style="text-align: right;"><span class="nodecor">2,048</span></td>
<td style="text-align: right;"><span class="nodecor">1,024</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">التنشيط</td>
<td style="text-align: right;">SiLU</td>
<td style="text-align: right;">SiLU</td>
</tr>
</tbody>
</table>
<p>[tab:model_config]</p>

<h2 id="app:sys_prompts">مطالبات النظام</h2>
<p>نستخدم المطالبات التالية لكل مهمة. يُستخدم الرمز <em><span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span></em> لتمثيل بيانات الصورة.</p>
<p><strong>تقييم LLaVA.</strong> نتبع منهجية (<span class="nodecor">liu2024visual</span>)، حيث يعرض المساعد أسئلة وأجوبة متعددة.</p>
<p><em><span class="math inline">\(&lt;\)</span>s<span class="math inline">\(&gt;\)</span> دردشة بين مستخدم فضولي ومساعد ذكاء اصطناعي. يقدم المساعد إجابات مفصلة ومهذبة. المستخدم: <span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span>  \\ السؤال <span class="math inline">\(Q_{1}\)</span> المساعد: <span class="math inline">\(R_{1}\)</span>. المستخدم: السؤال <span class="math inline">\(Q_{2}\)</span> …</em></p>
<p><strong>وصف COCO.</strong> بما أن COCO لا يتضمّن أسئلة، استخدمنا مطلَبًا يشبه السابق:</p>
<p><em><span class="math inline">\(&lt;\)</span>s<span class="math inline">\(&gt;\)</span> دردشة بين مستخدم فضولي ومساعد ذكاء اصطناعي. يقدم المساعد إجابات مفيدة ومفصلة. المستخدم: <span class="math inline">\(&lt;\)</span>image<span class="math inline">\(&gt;\)</span>  \\ قدّم وصفًا مفصلاً للصورة المساعدة:</em></p>
<p><strong>أسئلة العلوم.</strong> نتبع (<span class="nodecor">lu2022learn</span>) مع مثال واحد للسؤال، الخيارات، الإجابة والتعليل للسماح بالتفكير المتسلسل. نستخدم عينات الاختبار المرتبطة بصورة.</p>
<p>
<span class="math display">\[
\begin{aligned}
    & \text{السؤال: } I_{i}^{ques} \\
    & \text{الخيارات: (0) } I_{i1}^{opt} \,(1)\,I_{i2}^{opt}\,(2)\,I_{i3}^{opt} \\
    & \text{السياق: } I_{i}^{cont} \\
    & \text{الإجابة: } I_{i}^{ans} \text{، لأن: } I_{i}^{lect} \text{. التفسير: } I_{i}^{exp} \\
    & \langle\text{image}\rangle \\
    & \text{السؤال: } I_{test}^{ques} \\
    & \text{الخيارات: (0) } I_{test,1}^{opt}\,(1)\,I_{test,2}^{opt}\,(2)\,I_{test,3}^{opt} \\
    & \text{السياق: } I_{test}^{cont} \\
    & \text{الإجابة:}
\end{aligned}
\]</span>
</p>
<p>يشير <span class="math inline">\(i\)</span> للعينة داخل السياق. في SQA، وُفِّر حقل السياق عبر تسمية الصور المولدة تلقائيًا، غير أنها كانت بسيطة. لذا استخدمنا حقل “التلميح” من البيانات. مثال السياق لا يتضمّن صورة متعددة لتجنب تعقيد الاستهداف. نترك SPD مع أكثر من مثال في السياق كعمل مستقبلي.</p>

<h2 id="درجة-الانتباه-لرموز-الصورة">درجة الانتباه لرموز الصورة</h2>
</main>
</body>
</html>
```

**ملاحظات حول التصحيح:**
- تم التأكد من أن جميع معادلات LaTeX محاطة بشكل صحيح بـ `\[` ... `\]` أو `\(` ... `\)`، ولا توجد معادلات غير مغلقة أو ناقصة.
- تم تصحيح `<image>` في معادلة ScienceQA إلى `\langle\text{image}\rangle` حتى لا يسبب خطأ في LaTeX.
- تم التأكد من أن جميع المعادلات المتعددة الأسطر تستخدم `aligned` داخل `\[ ... \]`.
- تم التأكد من أن جميع الرموز الخاصة (مثل `\times`) مكتوبة بشكل صحيح داخل LaTeX.
- لم يتم تغيير أي نص أو محتوى خارج التصحيح اللازم للمعادلات.
- لا توجد أخطاء LaTeX متبقية في النص.