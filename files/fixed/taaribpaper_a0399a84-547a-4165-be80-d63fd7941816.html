<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Yeda Song  , Dongwook Lee &amp; Gunhee Kim">
  <title>التحفظ التركيبي: نهج توصيلي في التعلم التعزيزي دون اتصال</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">التحفظ التركيبي: نهج توصيلي في التعلم التعزيزي دون اتصال</h1>
<p class="author"><span class="nodecor">Yeda Song</span>  , <span class="nodecor">Dongwook Lee</span> &amp; <span class="nodecor">Gunhee Kim</span></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>التعلم التعزيزي دون اتصال هو إطار عمل جذاب لتعلم السياسات المثلى من التجارب السابقة دون تفاعل إضافي مع البيئة. ومع ذلك، يواجه التعلم التعزيزي دون اتصال بشكل حتمي مشكلة التحولات التوزيعية، حيث قد لا تكون الحالات والأفعال التي تمت مواجهتها أثناء تنفيذ السياسة ضمن توزيع بيانات التدريب. الحل الشائع يتضمن دمج التحفظ في السياسة أو في دالة القيمة للحماية من الشكوك والمجهولات. في هذا العمل، نركز على تحقيق نفس أهداف التحفظ ولكن من منظور مختلف. نقترح التحفظ التركيبي مع البحث عن المرساة للتعلم التعزيزي دون اتصال، وهو نهج يسعى للتحفظ بطريقة <em>تركيبية</em> على رأس إعادة المعايرة التوصيلية (<span class="nodecor">transd_aviv2023</span>)، والتي تقوم بتحليل المتغير الداخلي (الحالة في حالتنا) إلى مرساة والفارق عن المدخل الأصلي. يسعى التحفظ التركيبي لدينا إلى كل من المراسي والفروقات داخل التوزيع باستخدام نموذج الديناميكيات العكسية المتعلم، مما يشجع على التحفظ في فضاء المدخلات التركيبي للسياسة أو دالة القيمة. هذا التحفظ التركيبي مستقل وغير مدرك للتحفظ <em>السلوكي</em> السائد في التعلم التعزيزي دون اتصال. نطبق التحفظ التركيبي على أربع خوارزميات من أحدث خوارزميات التعلم التعزيزي دون اتصال ونقيمها على معيار <span class="nodecor">D4RL</span>، حيث يحسن التحفظ التركيبي عموماً أداء كل خوارزمية. الكود متاح في <a href="https://github.com/runamu/compositional-conservatism" class="uri">https://github.com/runamu/compositional-conservatism</a>.</p>
<h1 id="introduction">مُقَدِّمَة</h1>
<p>حقق التعلم بالتعزيز نجاحات ملحوظة في مجالات متعددة، من توجيه حركات الروبوتات (<span class="nodecor">dasari2020robonet</span>) وتحسين استراتيجيات الألعاب (<span class="nodecor">mnih2015human</span>) إلى التدريب الواعد لنماذج اللغة (<span class="nodecor">rajpurkar2016squad</span>). على الرغم من هذه الإنجازات، فقد دفعت التحديات التي تفرضها التفاعلات الزمنية الفعلية في البيئات المعقدة والحساسة إلى تطوير التعلم بالتعزيز دون اتصال كاتجاه قابل للتطبيق. يتعلم التعلم بالتعزيز دون اتصال (<span class="nodecor">wiering2012reinforcement, levine2020offline</span>) أو التعلم بالتعزيز الدفعي (<span class="nodecor">lange2012batch</span>) السياسات فقط من البيانات الموجودة مسبقاً، دون أي تفاعل مباشر مع البيئة. يزداد شعبية التعلم بالتعزيز دون اتصال في التطبيقات العملية مثل القيادة الذاتية (<span class="nodecor">yu2020bdd100k</span>) أو الرعاية الصحية (<span class="nodecor">gottesman2019guidelines</span>) حيث تكون البيانات السابقة وفيرة.</p>
<p>بطبيعته، يكون التعلم بالتعزيز دون اتصال عرضة للتحولات التوزيعية. تنشأ هذه المشكلة عندما يختلف توزيع الحالات والإجراءات التي تواجهها أثناء تنفيذ السياسة عن تلك الموجودة في مجموعة البيانات التدريبية، وهي حالة تشكل تحدياً خاصاً في التعلم الآلي (<span class="nodecor">levine2020offline</span>). تتناول العديد من خوارزميات التعلم بالتعزيز دون اتصال الحالية هذه المشكلة من خلال تقليل التحولات التوزيعية عبر نهج التحفظ، بما في ذلك تقييد السياسة أو تقدير الشكوك لقياس الانحرافات التوزيعية (<span class="nodecor">count_kim2023, prdc_ran2023, iql_kostrikov2022, cql_kumar2020, brac_wu2019, bear_kumar2019, bcq_fujimoto2019, mobile_sun2023, rambo_rigter2022, romi_wang2021, combo_yu2021, mopo_yu2020, morel_kidambi2020</span>). تهدف هذه الاستراتيجيات إلى الحفاظ على الوكيل ضمن التوزيعات المعروفة، مما يقلل من مخاطر السلوكيات غير المتوقعة. في هذا العمل، نسعى أيضاً لتحقيق نفس هدف الحفاظ على الاستقرار، مع التركيز على مواءمة توزيع بيانات الاختبار مع التوزيع المعروف، ولكن من منظور مختلف.</p>
<p>نبدأ بالاعتراف بأن مشكلة التحول التوزيعي للحالة ترتبط ارتباطاً وثيقاً بكيفية التعامل مع نقاط الإدخال خارج الدعم لمقاربات الدالة. نستكشف إمكانية تحويل مشكلة التعلم خارج الدعم إلى مشكلة خارج التركيب من خلال حقن التحيزات الاستقرائية في مقاربات الدالة للسياسة أو دالة القيمة-Q. تم اقتراح مثل هذا التحويل سابقاً بواسطة (<span class="nodecor">transd_aviv2023</span>)، حيث يقدم نهجاً توصلياً يسمى <em>التحويل الثنائي</em> يقوم بالتنبؤات من خلال هندسة ثنائية بعد إعادة معايرة الدالة المستهدفة. تقوم هذه إعادة المعايرة بتحليل المتغير الإدخالي إلى مكونين، هما <em>المرساة</em> و<em>الدلتا</em>، حيث المرساة هي متغير في فضاء الإدخال والدلتا هي الفرق بين المتغير الإدخالي والمرساة. إذا استوفت توزيعات البيانات التدريبية والاختبارية المعاد معايرتها افتراضات معينة، وإذا كانت الدالة المستهدفة تتمتع بخصائص معينة، فإن التحويل الثنائي يمكن أن يعالج مشكلة خارج التركيب، والتي بدورها قد تحل مشكلة خارج الدعم مع الدالة المستهدفة الأصلية.</p>
<p>نقترح إطاراً للحفاظ على التركيب مع البحث عن المرساة (<em>COCOA</em>) للتعلم بالتعزيز دون اتصال، وهو إطار يعتمد نهجاً تركيبياً للحفاظ على الاستقرار، بناءً على إعادة المعايرة التوصيلية (<span class="nodecor">transd_aviv2023</span>). يحول نهجنا مشكلة التحول التوزيعي إلى مشكلة خارج التركيب. ينقل هذا العوامل الرئيسية للتعميم من البيانات إلى المكونات المحللة والعلاقات بينها، مما يتطلب اختيار المرساة والدلتا بالقرب من توزيع مجموعة البيانات التدريبية.</p>
<p>نقترح نهجاً جديداً للبحث عن المرساة مع سياسة إضافية، تسمى <em>سياسة البحث عن المرساة</em>، والتي تفرض على الوكيل العثور على المراسي ضمن المنطقة المعروفة من فضاء الحالة. وبالتالي، يشجع COCOA المراسي على أن تكون قريبة من مجموعة البيانات دون اتصال مع تقييد الدلتا في نطاق ضيق من خلال تحديد المراسي بين الحالات المجاورة. يمكن لهذا النهج تقليل فضاء الإدخال وتوجيهه نحو الفضاء الذي تم استكشافه بشكل رئيسي خلال مرحلة التدريب. باختصار، من خلال تعلم سياسة للبحث عن المراسي داخل التوزيع والاختلافات من الديناميكيات المتعلمة، يمكننا تشجيع الحفاظ على الاستقرار في فضاء الإدخال التركيبي لمقاربات الدالة لوظيفة القيمة-Q والسياسة. هذا النهج مستقل وغير مدرك للحفاظ على السلوك السائد في التعلم بالتعزيز دون اتصال.</p>
<p>وجدنا تجريبياً أن طريقتنا تحسن أداء أربع طرق تمثيلية للتعلم بالتعزيز دون اتصال، بما في ذلك CQL (<span class="nodecor">cql_kumar2020</span>), IQL (<span class="nodecor">iql_kostrikov2022</span>), MOPO (<span class="nodecor">mopo_yu2020</span>), و MOBILE (<span class="nodecor">mobile_sun2023</span>) على معيار D4RL (<span class="nodecor">d4rl_fu2020</span>). كما نظهر من خلال دراسة استئصال أن تعلم سياسة البحث عن المرساة فعال في تحسين أداء طريقتنا. يمكن تلخيص مساهماتنا الرئيسية على النحو التالي:</p>
<ul>
<li><p>نسعى إلى الحفاظ على الاستقرار في <em>فضاء الإدخال التركيبي</em> لمقاربات الدالة لوظيفة القيمة-Q والسياسة، بشكل مستقل وغير مدرك للحفاظ على السلوك السائد في التعلم بالتعزيز دون اتصال.</p></li>
<li><p>نقدم الحفاظ على التركيب مع البحث عن المرساة (<em>COCOA</em>) الذي يجد المراسي والدلتا داخل التوزيع مع نموذج الديناميكيات المتعلم، وهو أمر حاسم للتعميم التركيبي.</p></li>
<li><p>نظهر تجريبياً أن COCOA يحسن أداء أربع خوارزميات حديثة للتعلم بالتعزيز دون اتصال على معيار D4RL. بالإضافة إلى ذلك، تظهر دراستنا الاستئصالية فعالية سياسة البحث عن المرساة مقارنة بتحديد المرساة الاستنتاجي.</p></li>
</ul>
<h1 id="sec:prelim">المُقَدِّمات</h1>
<h2 id="subsec:offlinerl">تعلم التعزيز دون اتصال</h2>
<p>نفترض مشكلة عملية اتخاذ القرار ماركوف <span class="math inline">\((\mathcal{S}, \mathcal{A}, T, R)\)</span> مع فضاء حالة مستمر <span class="math inline">\(\mathcal{S}\)</span>، وفضاء عمل مستمر <span class="math inline">\(\mathcal{A}\)</span>، ودالة انتقال <span class="math inline">\(T: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{S}\)</span>، ودالة مكافأة <span class="math inline">\(R: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span>. الهدف هو إيجاد سياسة <span class="math inline">\(\pi: \mathcal{S} \rightarrow \mathcal{A}\)</span> تعظم العائد المتوقع <span class="math inline">\(J(\pi)=\mathbb{E}_{\pi}\left[\sum_{t=0}^{\infty} \gamma^{t} R\left(s_{t}, a_{t}\right)\right]\)</span>، حيث <span class="math inline">\(\gamma \in[0,1)\)</span> هو عامل التخفيض.</p>
<p>في تعلم التعزيز دون اتصال، المعروف أيضاً بتعلم التعزيز الدفعي، نعطى مجموعة بيانات <span class="math inline">\(\mathcal{D}_{\text{env}}=\left\{\left(s_{i}, a_{i}, s_{i+1}, r_{i}\right)\right\}_{i=1}^{N}\)</span> تم إنشاؤها بواسطة سياسة سلوكية. الهدف في تعلم التعزيز دون اتصال هو إيجاد سياسة <span class="math inline">\(\pi\)</span> تعظم العائد المتوقع <span class="math inline">\(J(\pi)\)</span> باستخدام مجموعة البيانات الثابتة <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span> فقط. مثل معظم خوارزميات تعلم التعزيز دون اتصال المبنية على النموذج، نتعلم نموذج ديناميكيات <span class="math inline">\(\widehat{T}(s_{i+1}| s_{i}, a_{i})\)</span> يتنبأ بالحالة التالية <span class="math inline">\(s_{i+1}\)</span> بناءً على الحالة الحالية <span class="math inline">\(s_{i}\)</span> والفعل <span class="math inline">\(a_{i}\)</span>. بالإضافة إلى نموذج الديناميكيات الأمامي، نتعلم أيضاً نموذج ديناميكيات عكسي <span class="math inline">\(\widehat{T}(s_{i}| s_{i+1}, a_{i})\)</span> يتنبأ بالحالة الحالية <span class="math inline">\(s_{i}\)</span> بناءً على الحالة التالية <span class="math inline">\(s_{i+1}\)</span> والفعل <span class="math inline">\(a_{i}\)</span>.</p>
<h2 id="subsec:bilinear_transduction">التحويل الثنائي الخطي</h2>
<p>نتبع صياغة (<span class="nodecor">transd_aviv2023</span>) حول مشكلة التعميم. بدون افتراض على توزيع التدريب والاختبار، يقتصر أداء التعميم لمقرب الدالة. تحدث هذه المشكلة بشكل خاص عندما لا يكون توزيع الاختبار متضمناً في توزيع التدريب، والمعروف أيضاً بمشكلة التعلم خارج الدعم. كحالة خاصة من التعلم خارج الدعم، تحدث مشكلة خارج التركيب عندما يتم تقسيم فضاء الإدخال إلى مكونين، ويشمل هامش توزيع التدريب لكل مكون ذلك الخاص بتوزيع الاختبار بينما لا يحتوي توزيع التدريب المشترك بالضرورة على توزيع الاختبار المشترك. تحت افتراضات معينة، يقترح (<span class="nodecor">transd_aviv2023</span>) طريقة إعادة معايرة توصيلية تسمى <em>التحويل الثنائي الخطي</em> لتحويل مشكلة خارج الدعم إلى مشكلة خارج التركيب.</p>
<p><strong>التحويل الثنائي الخطي.</strong> يحل التقدير تحت افتراضات معينة. أولاً، يتم إعادة معايرة الدالة الهدف <span class="math inline">\(f(x)\)</span> كما يلي: <span class="math display">\[\label{eq:transductive_reparameterization}
   f(x) := \bar{f}(x-\tilde{x}, \tilde{x}),\]</span> حيث يُعرف <span class="math inline">\(\tilde{x}\)</span> بأنه <em>مرساة</em>، يتم اختيارها من مجموعة البيانات التدريبية. الفرق (<span class="math inline">\(x-\tilde{x}\)</span>) بين المتغير الإدخالي <span class="math inline">\(x\)</span> والمرساة <span class="math inline">\(\tilde{x}\)</span> يعرف بـ <em>الدلتا</em>. يتم تقريب الدالة الهدف المعاد معايرتها <span class="math inline">\(\bar{f}\)</span> كدالة ثنائية خطية للتضمينات <span class="math inline">\(\boldsymbol{\varphi_{1}}\)</span> و <span class="math inline">\(\boldsymbol{\varphi_{2}}\)</span>: <span class="math display">\[\label{eq:bilinear_representation}
   \bar{f_{\boldsymbol{\theta}}}(x) = \boldsymbol{\varphi_{1}}(x - \tilde{x}) \cdot \boldsymbol{\varphi_{2}}(\tilde{x}).\]</span> بديهياً، يسهل ذلك خاصية الرتبة المنخفضة للتضمينات <span class="math inline">\(\boldsymbol{\varphi_{1}}\)</span> و <span class="math inline">\(\boldsymbol{\varphi_{2}}\)</span>، مما يمكن مقرب الدالة من التعميم إلى نقاط خارج التركيب.</p>
<p><strong>الشروط الكافية للتحويل الثنائي الخطي.</strong> يقدم (<span class="nodecor">transd_aviv2023</span>) شروطاً كافية لتطبيق التحويل الثنائي الخطي. الافتراضات تتعلق بكل من مجموعة البيانات والدالة الهدف <span class="math inline">\(f\)</span>. الافتراض الأول يتعلق بـ <em>التغطية التركيبية</em> لمجموعة البيانات. يجب أن يكون لمجموعة البيانات الاختبارية نسبة كثافة تركيبية محدودة بالنسبة لمجموعة البيانات التدريبية. يعني ذلك أن دعم التوزيع المشترك لتوزيعات التدريب للمكونات يجب أن يشمل دعم التوزيع المشترك لتوزيعات الاختبار للمكونات. ثانياً، يجب أن تكون الدالة الهدف <span class="math inline">\(f\)</span> <em>قابلة للتحويل الثنائي الخطي</em>، أي يجب أن توجد دالة حتمية <span class="math inline">\(\bar{f}\)</span> بحيث <span class="math inline">\(f(x)=\bar{f}(x-\tilde{x}, \tilde{x})\)</span> لجميع <span class="math inline">\(x, \tilde{x} \in \mathcal{X}\)</span>. أخيراً، يجب ألا يتدهور توزيع المراسي التدريبية (<span class="nodecor">sample_shah2020</span>). تحت هذه الشروط الثلاثة، من الممكن تعميم الدالة الهدف إلى نقاط خارج التركيب بحد أدنى نظري مضمون للمخاطر.</p>
<p><strong>الصلة بالتعميم التركيبي.</strong> في ضوء الأدبيات حول <em>التعميم التركيبي</em> (<span class="nodecor">compositional_wiedemer2023</span>)، نفسر التحويل الثنائي الخطي كحالة خاصة من التعميم التركيبي، حيث تعمل نماذج <span class="math inline">\(\boldsymbol{\varphi_{1}}, \boldsymbol{\varphi_{2}}\)</span> كـ <em>وظائف المكونات</em>، مستخرجة للميزات ذات الرتبة المنخفضة للإدخال، ويعمل الجداء الداخلي كـ <em>وظيفة التركيب</em>.</p>
<h1 id="الحفاظ-التركيبي-مع-البحث-عن-المرساة-cocoa">الحفاظ التركيبي مع البحث عن المرساة (COCOA)</h1>
<h2 id="subsec:offline_rl_bilinear_transduction">تعلم التعزيز دون اتصال مع التحويل الثنائي الخطي</h2>
<p>تستخدم الخوارزميات الأساسية في تعلم التعزيز دون اتصال، مثل الشبكات العصبية العميقة (Deep Q-Networks) (<span class="nodecor">mnih2015human</span>) وطرق الممثل-الناقد (<span class="nodecor">mnih2016asynchronous, haarnoja2018soft</span>)، شبكات عصبية عميقة كمقربات للدالة. لذلك، نستخدم التحويل الثنائي الخطي (§ [subsec:bilinear_transduction]) لمقربات الدالة للسياسة ودالة الجودة. في كل من مرحلتي التدريب والاختبار، نقوم بتحليل الحالة الحالية <span class="math inline">\(s\)</span> إلى مرساة <span class="math inline">\(\tilde{s}\)</span> وفارق <span class="math inline">\(\mathit{\Delta} s = s - \tilde{s}\)</span>، حيث <span class="math inline">\(\tilde{s} \sim \mathcal{D}_{\text{env}}\)</span>. ثم تكون السياسة ودالة الجودة <span class="math display">\[\label{eq:policy_qfunction}
   \begin{aligned}
      \bar{\pi}_{\boldsymbol{\theta}}(s) &amp;= \boldsymbol{\varphi_{\boldsymbol{\theta},1}}(\mathit{\Delta} s) \cdot \boldsymbol{\varphi_{\boldsymbol{\theta},2}}(\tilde{s}),  \hspace{12pt}
      \bar{Q}_{\boldsymbol{\phi}}(s,a) &amp;= \boldsymbol{\varphi_{\boldsymbol{\phi},1}}(\mathit{\Delta} s, a) \cdot \boldsymbol{\varphi_{\boldsymbol{\phi},2}}(\tilde{s}, a).
   \end{aligned}\]</span> يتم تدريب السياسة <span class="math inline">\(\pi(a|s)\)</span> لتعظيم العائد المتوقع <span class="math inline">\(J(\pi)\)</span>، ويتم تدريب دالة الجودة <span class="math inline">\(Q(s, a)\)</span> لتقليل دالة الخسارة <span class="math inline">\(\mathcal{L}_{\text{Q}}\)</span> المحددة في خوارزمية تعلم التعزيز دون اتصال الأساسية.</p>
<p>يمكن أن تؤدي تحليلات الحالة المختلفة إلى فضاءات إدخال تركيبية مختلفة، مما يؤدي إلى قدرات تعميم مختلفة. من أجل تلبية افتراضات التحويل الثنائي الخطي في § [subsec:bilinear_transduction]، النهج المثالي هو إيجاد التحليل الذي يلبي هذين المعيارين: مرساة داخل التوزيع وفارق داخل التوزيع. يتم توضيح هذه الحالة المثالية في الشكل [fig:anchor_delta]. ومع ذلك، على عكس الأعمال السابقة (<span class="nodecor">transd_aviv2023, pinneri2023equivariant</span>) التي تركز فقط على خاصية التحويل للحالة الهدف، نحاول التعامل مع كل حالة في كل خطوة، ومن غير العملي فرض هذه القيود باستخدام طرق القوة الغاشمة مثل مقارنة الحالات الحالية بجميع النقاط الأخرى. لذلك، نقدم سياسة جديدة للبحث عن المراسي داخل التوزيع والفروقات واستغلال نموذج الديناميكيات المتعلم الذي يمنع التحليل التعسفي، لاستغلال قوة التحويل الثنائي الخطي. نفرض أيضاً أن يكون كل فارق ضمن مسافة خطوات قليلة من نموذج الديناميكيات لتقييد توزيع الفروق في كل من مرحلتي التدريب والاختبار إلى نطاق مماثل. هذا النهج يقلل من فضاء الإدخال ويوجهه نحو الفضاء الذي تم استكشافه بشكل رئيسي خلال مرحلة التدريب، مما يعزز التعميم بشكل أكبر.</p>
<h2 id="تعلم-البحث-عن-التحليل-داخل-التوزيع">تعلم البحث عن التحليل داخل التوزيع</h2>
<p>نصف نموذج الديناميكيات العكسية، مسار البحث عن النقطة المرجعية، والسياسة العكسية العشوائية المتباينة، والتي تعتبر مكونات ضرورية قبل تدريب سياسة البحث عن النقطة المرجعية.</p>
<p>[subsec:learning_to_seek]</p>
<h3 id="subsec:anchor_seeking_trajectory">مسار البحث عن المرساة</h3>
<p><strong>تدريب نموذج ديناميكيات العكس.</strong> بناءً على انتقال <span class="math inline">\((s, a, s&#39;)\)</span> مأخوذ من مجموعة البيانات <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span>، نقوم بتدريب نموذج ديناميكيات الانتقال العكسي <span class="math inline">\(\widehat{T}_{r}(s|s&#39;, a)\)</span> (<span class="nodecor">romi_wang2021</span>, <span class="nodecor">lai2020bidirectional</span>, <span class="nodecor">goyal2018recall</span>, <span class="nodecor">edwards2018forward</span>, <span class="nodecor">holyoak1999bidirectional</span>) للتنبؤ بالحالة <span class="math inline">\(s\)</span> بناءً على الحالة التالية <span class="math inline">\(s&#39;\)</span> والفعل <span class="math inline">\(a\)</span>. بمعنى آخر، يتنبأ نموذج الديناميكيات العكسية <span class="math inline">\(T(s&#39;, a)\)</span> بـ "من أي حالة <span class="math inline">\(s\)</span> نأتي إذا وصلنا إلى <span class="math inline">\(s&#39;\)</span> بأخذ الفعل <span class="math inline">\(a\)</span>؟". يتم ذلك من خلال تقليل دالة الخسارة مع الحالة <span class="math inline">\(s\)</span> لمجموعة البيانات المحددة كما يلي <span class="math display">\[\label{eq:reverse_dynamics_loss}
   \mathcal{L}_{\text{r}} = \mathbb{E}_{(s, a, s&#39;) \sim \mathcal{D}_{\text{env}}} \left[ \left\| \widehat{T}_{r}(s&#39;, a) - s \right\|_{2}^{2} \right].\]</span></p>
<p><strong>سياسة عكسية عشوائية متباينة.</strong> لا نستخدم سياسة عكسية مدربة ولكن بدلاً من ذلك نستخدم سياسة عكسية ارتجالية تختار فعلاً عشوائياً من مجموعة البيانات <span class="math inline">\(\mathcal{D}_{\text{env}}\)</span>. الأفعال اللاحقة في التدحرجات العكسية، بعد الفعل الأول، تتبع نفس الاتجاه كالفعل الأول ولكنها تُخفض قليلاً مع إضافة ضوضاء غاوسية صغيرة. هذا يضمن أن التدحرج العكسي يبتعد عن مجموعة البيانات. نظراً لأننا نستخدم أفعالاً عشوائية ونحافظ على اتجاه ثابت طوال التدحرج العكسي، فمن المرجح أن نخوض في مناطق غير مستكشفة خارج مجموعة البيانات دون اتصال. باختصار، تعطي السياسة العكسية فعلاً <span class="math inline">\(a_{j}\)</span> في كل خطوة تدحرج <span class="math inline">\(j\)</span> كما يلي: <span class="math display">\[a_j = \phi a + \epsilon_j, \ \ \ \text{حيث } \ a \sim \mathcal{D}_{\text{env}}, \epsilon_j \sim \mathcal{N}(0, \sigma^2), \quad j=1,2,\ldots, h.\]</span> <span class="math inline">\(h\)</span> هو طول الأفق، <span class="math inline">\(\phi\)</span> هو معامل النطاق، و<span class="math inline">\(\sigma\)</span> هو معامل الضوضاء. نحدد <span class="math inline">\(\phi=0.8\)</span> و<span class="math inline">\(\sigma=0.1\)</span> عندما يكون الحد الأقصى لقيمة الفعل هو <span class="nodecor">1.0</span>.</p>
<p><strong>مسار البحث عن المرساة.</strong> نستخدم تدحرجات نموذج العكس لصنع مسارات البحث عن المرساة لتدريب سياسة البحث عن المرساة. أولاً، نأخذ حالة مرساة من مجموعة البيانات ونولد انتقالاً عكسياً <span class="math inline">\(\mathcal{D}_{\text{reverse}}=\left\{\left(s_{i+1}, a_{i}, s_{i}, r_{i}\right)\right\}_{i=1}^{j}\)</span> من حالة المرساة باستخدام نموذج الديناميكيات العكسية والسياسة العكسية العشوائية المتباينة. لاحظ أن اتجاه مسار البحث عن المرساة يكون عكسياً لذلك الانتقال العكسي، <span class="math inline">\(\mathcal{D}_{\text{reverse}}\)</span>. بهذه الطريقة، يمكننا توليد مسارات البحث عن المرساة بفعالية لتدريب سياسة البحث عن المرساة. استخدام تدحرجات نموذج العكس لمعالجة مشكلة البيانات خارج التوزيع تم اقتراحه لأول مرة بواسطة (<span class="nodecor">romi_wang2021</span>)، الذين يعززون مجموعة البيانات دون اتصال بالانتقال العكسي، يدربون سياسة باستخدام هذه المجموعة المعززة من البيانات، ويظهرون فعالية مثل هذا النهج في إعداد التعلم بالتعزيز دون اتصال. وأخيراً، يتم تلخيص تفاصيل توليد مسار البحث عن المرساة في الخوارزمية [algorithm:generate_anchor_seeking_trajactory].</p>
<h3 id="subsec:dynamics_aware_anchor_seeking">تدريب سياسة البحث عن المرساة المدركة للديناميكيات</h3>
<p>نقوم بتدريب سياسة البحث عن المرساة <span class="math inline">\(\tilde{\pi}(a|s)\)</span> قبل تدريب السياسة الرئيسية. نستخدم مسارات البحث عن المرساة في § [subsec:anchor_seeking_trajectory]، والتي تكون في الاتجاه المعاكس لمجموعة البيانات <span class="math inline">\(D_{\text{reverse}}\)</span>. من خلال اتباع مسار البحث عن المرساة، يتم تدريب سياسة البحث عن المرساة لاختيار الأفعال <span class="math inline">\(\eta\)</span> التي توجه العامل في اتجاه يتحرك من الحدود الخارجية نحو المنطقة المرئية. بما أن المسار المتدحرج للبحث عن المرساة يتم إنشاؤه بواسطة سياسة البحث عن المرساة <span class="math inline">\(\tilde{\pi}(a|s)\)</span> ونموذج الديناميكيات <span class="math inline">\(\widehat{T}(s, a)\)</span>، فإن الانتقال العكسي <span class="math inline">\(D_{\text{reverse}}\)</span> يعتمد على <span class="math inline">\(\widehat{T}_r(s, r|s&#39;, a)\)</span>. نظراً لأن الانتقال العكسي صُمم لينحرف عن مجموعة البيانات دون اتصال بالإنترنت، فإن مسار البحث عن المرساة، باتجاهه المعكوس، يضمن أن الانتقال يتقارب مرة أخرى مع مجموعة البيانات من الحالات غير المألوفة. نتيجة لذلك، نقوم بتدريب سياسة البحث عن المرساة لتقليل خسارة متوسط الخطأ المربع بين الفعل المتوقع والفعل في مجموعة البيانات <span class="math inline">\(D_{\text{reverse}}\)</span>. تعرف دالة الخسارة كما يلي: <span class="math display">\[\mathcal{L}_{\text{anchor}}(\theta)
   = \mathbb{E}_{\substack{\scriptscriptstyle (s&#39;, a, s) \sim \mathcal{D}_{\text{reverse}},  \scriptscriptstyle \eta \sim \tilde{\pi}_{\theta}(a|s)}} \left[ (\eta - a)^2 \right].\]</span> بهذه الطريقة، يمكن لسياسة البحث عن المرساة <span class="math inline">\(\tilde{\pi}(a|s)\)</span> أن توفر فعلاً مناسباً للتحرك نحو مرساة داخل التوزيع. يُعطى هذا الفعل لنموذج الديناميكيات <span class="math inline">\(\widehat{T}(s, a)\)</span> للتنبؤ بالحالة التالية. وبالتالي، كما هو موضح، يكون المسار المتدحرج للبحث عن المرساة سلسلة من الانتقالات التي تبدأ من الحالة الحالية <span class="math inline">\(s\)</span> وتنتهي عند حالة المرساة <span class="math inline">\(\tilde{s}\)</span>.</p>
<h2 id="ملخص-الطريقة">مُلَخَّص الطريقة</h2>
<p>نوضح دمج نموذج البحث عن المرساة في إطار التحويل الثنائي الخطي. نختار خوارزمية الناقد-الممثل اللين (<span class="nodecor">Soft Actor-Critic (SAC)</span>) (<span class="nodecor">haarnoja2018soft</span>) كخوارزمية تعلم تعزيز تمثيلية تستخدم مقاربات الدالة. نستخدم التحويل الثنائي الخطي المعزز بالبحث عن المرساة لشبكات الناقد والممثل في <span class="nodecor">SAC</span>.</p>
<p>بالنظر إلى حالة الإدخال <span class="math inline">\(s_n\)</span>، نستخدم البحث للحصول على إجراء من سياسة البحث عن المرساة. يتم استنتاج المرساة <span class="math inline">\(\tilde{s}\)</span> بهذا الإجراء من خلال خطوة الديناميكيات الأمامية ثم يتم تحديثها لتكون الحالة التالية <span class="math inline">\(s_{n+1}\)</span>. بعد عدة محاولات من البحث، يمكننا تحديد المرساة النهائية واستخدامها لتحليل الحالة إلى المرساة والفارق. يتم حساب الفارق بين الحالة الأولية <span class="math inline">\(s_n\)</span> وهذه المرساة <span class="math inline">\(\tilde{s}\)</span>، الفارق، كما يلي: <span class="math inline">\(\Delta s = \tilde{s} - s_{n}\)</span>.</p>
<p>ثم نقوم بتنفيذ التحويل الثنائي الخطي كما هو موضح في المعادلة ([eq:bilinear_representation]). نقوم بتضمين <span class="math inline">\(\Delta s\)</span> و <span class="math inline">\(\tilde{s}\)</span> على التوالي كـ <span class="math inline">\(\boldsymbol{\varphi_1}(\Delta s)\)</span> و <span class="math inline">\(\boldsymbol{\varphi_2}(\tilde{s})\)</span>، ونحسب الجداء الداخلي بينهما. ثم يتم إدخال الناتج في طبقة <span class="nodecor">MLP</span> صغيرة لزيادة مرونة مقرب الدالة. هذه الخطوة تقدم لاخطية، حيث قد لا تكون السياسة أو دالة <span class="nodecor">Q</span> خطية لمدخلاتها.</p>
<p>يلخص الخوارزم ([algorithm:bilinear_transduction_with_anchor_seeking]) العملية بأكملها في وحدة الممثل. في وحدة الناقد، نقوم بدمج الإجراء مع كل من المرساة والفارق في العملية الأمامية قبل تنفيذ التحويل الثنائي الخطي. بعد ذلك، يتم استخدام قيم الإجراء وقيمة <span class="nodecor">Q</span>، المشتقة كـ <span class="math inline">\(\bar{f}_\text{actor}\)</span> و <span class="math inline">\(\bar{f}_\text{critic}\)</span> على التوالي، لتحديث شبكات الناقد والممثل في سياسة <span class="nodecor">SAC</span>.</p>
<h1 id="experiments">التجارب</h1>
<p>في تجاربنا، نهدف إلى الإجابة التجريبية على السؤالين التاليين: (i) ما مقدار تحسين أداء الخوارزميات السابقة الخالية من النموذج والمبنية على النموذج بواسطة طريقتنا؟ و (ii) ما هو تأثير البحث عن النقاط المرجعية على الأداء؟</p>
<h2 id="subsec:d4rl_benchmark_tasks">نتائج مهام معيار D4RL</h2>
<p>نقوم بتقييم طريقتنا على مهام Gym-MuJoCo في معيار D4RL (<span class="nodecor">d4rl_fu2020</span>)، والذي يتكون من <span class="nodecor">12</span> مهمة من بيئات OpenAI Gym (<span class="nodecor">gym_brockman2016</span>) و MuJoCo (<span class="nodecor">mujoco_todorov2012</span>). يرجى الرجوع إلى  [appendix:d4rl_benchmark_tasks] للتفاصيل حول المهام.</p>
<p><strong>الخطوط الأساسية.</strong> نطبق COCOA على عدة خوارزميات تعلم تعزيز دون اتصال سابقة، سواء كانت تعتمد على النماذج أو لا تعتمد عليها. تشمل هذه (i) CQL (<span class="nodecor">cql_kumar2020</span>) التي تفرض عقوبات على قيم Q على العينات خارج التوزيع للأمان، (ii) IQL (<span class="nodecor">iql_kostrikov2022</span>) التي تستفيد من قدرة التعميم لمقرب الدالة من خلال النظر إلى دالة قيمة الحالة كمتغير عشوائي، (iii) MOPO (<span class="nodecor">mopo_yu2020</span>) كنهج يعتمد على النموذج يفرض عقوبات على المكافآت بناءً على الشكوك من التنبؤ بالحالات اللاحقة، و (iv) MOBILE (<span class="nodecor">mobile_sun2023</span>) التي تقيس الشكوك من خلال عدم الاتساق في تقدير بيلمان باستخدام مجموعة من نماذج الديناميكيات. نقوم أيضاً بتقديم نتائج (v) تقليد السلوك (Behavior Cloning)، والذي يتعلم المهام من خلال تقليد بيانات الخبراء. لتحقيق استقرار التدريب، يتم إعادة إنتاج جميع خوارزميات الخط الأساسي مع تطبيق تطبيع الطبقة.</p>
<p><strong>النتائج.</strong> الجدول [tab:d4rl_benchmark] يلخص نتائج تجاربنا. تشير خوارزميات الخط الأساسي بـ "منفردة"، وتشير طريقتنا بـ "+COCOA". نقدم متوسط العائد لآخر <span class="nodecor">10</span> فترات تدريب عبر <span class="nodecor">4</span> بذور، مع الانحراف المعياري. لجميع الخوارزميات، نقوم بإعادة إنتاج النتائج باستخدام قاعدة الكود الموصوفة في الملحق [appendix:codebase]. تعزز طريقتنا أداء جميع خوارزميات الخط الأساسي، كما يتم قياسه بالتحسين المتوسط عبر المهام باستثناء المهام العشوائية لـ IQL كما يفعل الورق الأصلي لـ IQL. باختصار، تحسن COCOA أداء الطرق الأصلية في <span class="nodecor">10</span> من <span class="nodecor">12</span> مهمة لـ CQL، <span class="nodecor">3</span> من <span class="nodecor">9</span> مهام لـ IQL، <span class="nodecor">7</span> من <span class="nodecor">12</span> مهمة لـ MOPO، و <span class="nodecor">9</span> من <span class="nodecor">12</span> مهمة لـ MOBILE.</p>
<h2 id="subsec:ablation_study">دراسة الاستئصال: تأثير البحث عن المرساة</h2>
<p>لفحص تأثير اختيار المرساة على الأداء، نجري تجربة باستخدام نسخة من طريقتنا لا تستخدم البحث عن المرساة. لهذه الدراسة الاستئصالية، نستخدم خوارزمية الاستعلام المستمر (<span class="nodecor">CQL</span>) (<span class="nodecor">cql_kumar2020</span>) كخوارزمية أساسية بسبب كفاءتها الحسابية كخوارزمية خالية من النموذج واكتمالها في دعم جميع أنواع المهام، بما في ذلك المهام "العشوائية".</p>
<p><strong>المعيار الأساسي.</strong> المعيار الأساسي لهذه الدراسة الاستئصالية مشار إليه بـ "+<span class="nodecor">COCOA</span> (بدون <span class="nodecor">A.S.</span>)" في الجدول [tab:ablation_study_anchor_seeking]. في هذا المعيار الأساسي، نتبنى إجراء اختيار المرساة الاستدلالي من <span class="nodecor">transd_aviv2023</span>، مع إدخال تعديلات رئيسية لسياقنا. على عكس الطريقة الأصلية، التي تختار المراسي بناءً على حالات الهدف، يختار معيارنا الأساسي المراسي بناءً على الحالة الحالية، معالجاً غياب حالة الهدف في إعدادنا. للتخفيف من المتطلبات الحسابية لهذه الطريقة، نحد اختيارنا إلى مجموعة فرعية من المرشحين للمراسي، يتم أخذ عينات منها عشوائياً من مجموعة البيانات.</p>
<p>يعمل اختيار المرساة الاستدلالي كما يلي. نقوم في البداية بسحب <span class="math inline">\(N\)</span> مرشحين للمراسي <span class="math inline">\(s_{i}\)</span> من مجموعة البيانات ونحسب الفارق <span class="math inline">\(\mathit{\Delta} s\)</span> بين المرشحين والحالة الحالية، المعرفة بـ <span class="math display">\[\mathit{\Delta} s_{n} = s - s_{n}, \quad n \in \{1, \ldots, N\}, \quad s_{n} \in D_{\text{env}}.
   \label{eq:eq7}\]</span> بعد ذلك، نقيم كل فارق زوجي بين <span class="math inline">\(N\)</span> حالة أخرى مأخوذة عينات منها من <span class="math inline">\(D_{\text{env}}\)</span> كما يلي <span class="math display">\[\mathit{\Delta} s_{i,j} = s_{i} - s_{j}, \quad i, j \in \{1, \ldots, N\}, \quad i \neq j, \quad s_{i}, s_{j} \in D_{\text{env}}.
   \label{eq:eq8}\]</span> أخيراً، نختار المرساة المرشحة التي تقلل المسافة إلى الحالة الحالية: <span class="math display">\[\tilde{s} = s_{\tilde{n}}, \quad \text{with} \quad \tilde{n} = \underset{n}{\arg\min} \left\{ \underset{i,j}{\min} \left\| \mathit{\Delta} s_{n} - \mathit{\Delta} s_{i,j} \right\| \right\}.
   \label{eq:eq9}\]</span></p>
<p>يفرض هذا المعيار الأساسي نتائج تحليل الحالة لتكون قريبة من بيانات التوزيع من خلال حساب المسافة المباشرة. بينما يمكن أن تكون فعالة وقابلة للتطبيق إذا كانت مجموعة البيانات صغيرة و<span class="math inline">\(N\)</span> كبير بما فيه الكفاية، فإن قابليتها للتوسع محدودة حيث تتزايد كمية الحساب المطلوبة تربيعياً مع حجم البيانات. نظراً لأن تكلفة الحساب تتصاعد تكعيبياً مع حجم العينة، نضع <span class="math inline">\(N\)</span> عند 30، مطابقين ميزانيتنا الحسابية مع "+<span class="nodecor">COCOA</span>".</p>
<p><strong>النتائج.</strong> نفحص ما إذا كانت هذه النسخة تحسن أداء <span class="nodecor">CQL</span>. تلخص النتائج في الجدول [tab:ablation_study_anchor_seeking]. نقوم بالإبلاغ عن العائد المتوسط لآخر 10 فترات تدريب عبر 4 بذور، مع الانحراف المعياري. يحقق المعيار الأساسي "+<span class="nodecor">COCOA</span> (بدون <span class="nodecor">A.S.</span>)" أداء أعلى في مهمتين فقط، "hopper-random" و "walker2d-random"، وأداء مماثل أو أقل في المهام الأخرى مقارنة بالمعيار الأساسي الأصلي "Alone". في المقابل، تحسن طريقتنا "+<span class="nodecor">COCOA</span>" أداء نماذج <span class="nodecor">CQL</span> في 10 من أصل 12 مهمة. تشير هذه النتيجة إلى أن البحث عن المرساة هو مكون حاسم لنجاح طريقتنا.</p>
<h1 id="related_work">الأعمال ذات الصلة</h1>
<p><strong>التعلم المعزز دون اتصال.</strong> في التعلم المعزز دون اتصال، يستخدم العوامل مجموعة بيانات محددة مسبقاً دون تفاعلات إضافية مع البيئة، وعادة ما يتبعون إما الاستراتيجية المعتمدة على النموذج أو الاستراتيجية المستقلة عن النموذج. تعمل خوارزميات التعلم المعزز المستقلة عن النموذج (<span class="nodecor">count_kim2023</span>, <span class="nodecor">prdc_ran2023</span>, <span class="nodecor">iql_kostrikov2022</span>, <span class="nodecor">cql_kumar2020</span>, <span class="nodecor">brac_wu2019</span>, <span class="nodecor">bear_kumar2019</span>, <span class="nodecor">bcq_fujimoto2019</span>) على تحسين السياسة مباشرة باستخدام التجارب السابقة في ذاكرة الإعادة، مع تطبيق التحفظ على دالة القيمة أو السياسة. في المقابل، تستخدم طرق التعلم المعزز دون اتصال المعتمدة على النموذج (<span class="nodecor">mobile_sun2023</span>, <span class="nodecor">rambo_rigter2022</span>, <span class="nodecor">romi_wang2021</span>, <span class="nodecor">combo_yu2021</span>, <span class="nodecor">mopo_yu2020</span>, <span class="nodecor">morel_kidambi2020</span>) نموذجاً مدرباً في البيئة لإنشاء بيانات إضافية تستخدم لتعلم السياسة. من خلال هذه البيانات المركبة، تصبح هذه الطريقة أقوى في التعميم وأكثر متانة حتى في الحالات غير المرئية.</p>
<p><strong>التعميم خارج التوزيع في التعلم المعزز دون اتصال.</strong> تم إجراء العديد من الدراسات لتحسين التعميم خارج التوزيع لخوارزميات التعلم المعزز دون اتصال. يتناول (<span class="nodecor">plas_lou2022</span>) مشكلة تحول توزيع الأفعال من خلال تقديم نهج قائم على المعلومات المتبادلة لتعلم نموذج تضمين الأفعال. في مسعى مماثل، يقترح (<span class="nodecor">merlion_gu2022</span>) طريقة تعلم تمثيل الأفعال الزائفة التي تقيس العلاقات السلوكية والتوزيعية بين الأفعال. يطور (<span class="nodecor">pbrl_bai2022</span>) طريقة مدفوعة بالشكوك تستخدم الاختلاف في وظائف Q المعززة. يزيد من مجموعة البيانات ببيانات خارج التوزيع التي يفرض عليها عقوبة أكثر دقة. يقترح (<span class="nodecor">mocoda_pitis2022</span>) تحليلاً محلياً لديناميكيات الانتقال وتوسيع الحالة لتحسين التعميم لخوارزميات التعلم المعزز دون اتصال. كما يقدمون براهين نظرية لتعقيد العينة وقدرة التعميم. تشابه طريقتنا طريقتهم في أننا نستخدم أيضاً الهندسة المعمارية المحللة للسياسة ووظيفة Q. ومع ذلك، على عكسهم، لا نستخدم نموذج ديناميكيات محلل وبدلاً من ذلك نستفيد من إطار عمل التحويل الثنائي الخطي.</p>
<p><strong>التعميم التركيبي والاستقراء.</strong> يتم استكشاف التعميم التركيبي، الذي يسعى للتعميم على تركيبات غير مرئية من المكونات، من خلال دراسات مختلفة. يسلط (<span class="nodecor">compositional_wiedemer2023</span>) الضوء على إجراء توليدي من خطوتين كأساس لمعالجة مجموعة واسعة من المشكلات التركيبية. يتضمن هذا الإجراء توليداً معقداً للمكونات الفردية ودمجها ببساطة في مخرج واحد. يقدمون مجموعة من الشروط الكافية التي يمكن من خلالها للنماذج المدربة على البيانات أن تعمم بشكل تركيبي. في ملاحظة ذات صلة، يقدم (<span class="nodecor">sample_shah2020</span>) خوارزمية تعلم معزز فعالة من حيث العينات تستغل البنية منخفضة الرتبة لوظيفة Q المثلى، وهي دالة ثنائية خطية للحالات والأفعال. يثبتون تحسيناً كمياً في تعقيد العينة للتعلم المعزز مع فضاءات حالة وفعل مستمرة عبر البنية منخفضة الرتبة. يستكشف (<span class="nodecor">first_dong2023</span>) استقراء النماذج غير الخطية لتحول المجال المنظم. يثبتون أن عائلة معينة من النماذج غير الخطية يمكن أن تستقرئ بنجاح إلى توزيعات غير مرئية، شريطة أن تكون تغطية الميزات جيدة الشروط. يقترح (<span class="nodecor">transd_aviv2023</span>) استراتيجية استقراء تعتمد على التضمينات الثنائية الخطية لتمكين التعميم التركيبي، مما يعالج مشكلة خارج الدعم تحت شروط معينة.</p>
<h1 id="sec:conclusion">الخلاصة</h1>
<p>لقد استكشفنا منظوراً جديداً للتحفظ في التعلم المعزز خارج الخط لا يعتمد على مساحة سلوك العامل ولكن على مساحة الإدخال التركيبية للسياسة ووظيفة الجودة. اقترحنا إطار عمل عملياً، COCOA، لإيجاد تحليل أفضل للحالات لتشجيع هذا التحفظ. COCOA هو نهج بسيط ولكنه فعال يمكن تطبيقه على أي خوارزمية تعلم معزز خارج الخط تستخدم مقرب دالة. وجدنا من خلال تجاربنا عبر مهام متنوعة في بيئة Gym-MuJoCo لمعيار D4RL أن طريقتنا عموماً عززت أداء خوارزميات التعلم المعزز خارج الخط.</p>
<p>بما أن دراستنا تركز بشكل أساسي على الاستكشاف التجريبي، قد يكون من الضروري إجراء مزيد من التحقيق لفهم أكثر شمولاً للآلية وراء تحسين الأداء أو خصائص مساحة الإدخال التركيبية. علاوة على ذلك، نظراً لأن تجاربنا كانت محدودة ببيئات الروبوتات التي تعتمد على التحكم مع مساحات حالة وفعل مستمرة، يمكن أن يكون توسيع نطاق عملنا لتطبيق إطار عمل التحفظ التركيبي على مجالات أخرى، بما في ذلك البيئات ذات المساحات الفعلية المنفصلة، أو الملاحظات المبنية على الصور، أو الديناميكيات المعقدة للغاية، امتداداً قيماً لهذا العمل.</p>
<h1 id="الشكر-والتقدير">الشكر والتقدير</h1>
<p>نشكر جايكيوم كيم، سوتشان لي، سيوهونغ بارك، أفيف نتنياهو، والمراجعين المجهولين على مناقشاتهم القيمة وتعليقاتهم. لقد تم دعم هذا العمل من قبل معهد تخطيط وتقييم تكنولوجيا المعلومات والاتصالات (<span class="nodecor">IITP</span>) بتمويل من الحكومة الكورية (<span class="nodecor">MSIT</span>) (<span class="nodecor">No. 2019-0-01082</span>, <span class="nodecor">SW StarLab</span>), معهد تخطيط وتقييم تكنولوجيا المعلومات والاتصالات (<span class="nodecor">IITP</span>) بتمويل من الحكومة الكورية (<span class="nodecor">MSIT</span>) (<span class="nodecor">No. 2022-0-00156</span>, البحث الأساسي في التعلم المستمر لتحسين جودة الفيديوهات العرضية وتحويلها إلى ميتافيرس ثلاثي الأبعاد), معهد تخطيط وتقييم تكنولوجيا المعلومات والاتصالات (<span class="nodecor">IITP</span>) بتمويل من الحكومة الكورية (<span class="nodecor">MSIT</span>) [<span class="nodecor">NO.2021-0-01343</span>, برنامج الدراسات العليا في الذكاء الاصطناعي (جامعة سيول الوطنية)], ومنحة مركز البحوث التطبيقية في الذكاء الاصطناعي (<span class="nodecor">CARAI</span>) بتمويل من إدارة برنامج الاقتناء الدفاعي (<span class="nodecor">DAPA</span>) ووكالة تطوير الدفاع (<span class="nodecor">ADD</span>) (<span class="nodecor">UD190031RD</span>). جونهي كيم هو المؤلف المراسل.</p>
<h1 id="sec:reproducibility">بيان القابلية للتكرار</h1>
<p>لضمان القابلية للتكرار، نوفر شفرة طريقتنا على <a href="https://github.com/runamu/compositional-conservatism" class="uri">https://github.com/runamu/compositional-conservatism</a>. للحصول على قاعدة الشفرات لخوارزميات الأساس، يرجى الرجوع إلى الملحق [appendix:codebase]. يتم وصف المعلمات الفائقة وهيكلية النموذج في الملحق [appendix:hyperparameters] والملحق [appendix:model_architecture]، على التوالي.</p>
<h1 id="إعدادات-التجربة-وتفاصيل-التنفيذ">إعدادات التجربة وتفاصيل التنفيذ</h1>
<h2 id="appendix:d4rl_benchmark_tasks">مهام معيار <span class="nodecor">D4RL</span></h2>
<p><strong>نصف الفهد</strong>: نصف الفهد هو روبوت ثنائي الأبعاد ثنائي الأرجل مكون من <span class="nodecor">8</span> وصلات صلبة، تشمل الساقين والجذع، مقترنة بـ <span class="nodecor">6</span> مفاصل محركة. فضاء الحالة ذو <span class="nodecor">17</span> بعداً، يشمل زوايا المفاصل والسرعات. يقوم خصم بزعزعة استقراره من خلال ممارسة فعل ذو <span class="nodecor">6</span> أبعاد مع قوى ثنائية الأبعاد على الجذع وكل قدم.</p>
<p><strong>القافز</strong>: القافز هو روبوت أحادي القدم مسطح، مكون من <span class="nodecor">4</span> وصلات صلبة تمثل الجذع، الساق العلوية، الساق السفلية، والقدم، ويشمل <span class="nodecor">3</span> مفاصل محركة. لديه فضاء حالة ذو <span class="nodecor">11</span> بعداً، يتضمن زوايا المفاصل والسرعات. يستخدم خصم قوة ثنائية الأبعاد على القدم لتعطيل استقراره.</p>
<p><strong>المشاة ثنائي الأبعاد</strong>: يعمل المشاة كروبوت ثنائي الأبعاد ثنائي الأرجل بتركيبة من <span class="nodecor">7</span> وصلات، تمثل الساقين والجذع، إلى جانب <span class="nodecor">6</span> مفاصل محركة. ضمن فضاء حالته الذي يبلغ <span class="nodecor">17</span> بعداً، تتضمن زوايا المفاصل والسرعات. يستخدم خصم فعلاً ذو <span class="nodecor">4</span> أبعاد مع قوى ثنائية الأبعاد على كلا القدمين لزعزعة توازنه.</p>
<p><strong>المهارة</strong>: المهارة هي مهمة معقدة حيث يستخدم يد روبوتية محاكاة بـ <span class="nodecor">24</span> درجة حرية لمهام مثل دق مسمار، فتح باب، تدوير قلم، أو تحريك كرة. نستخدم نوعين من مجموعات البيانات لهذا: مجموعة البيانات "البشرية"، التي تشمل <span class="nodecor">25</span> مساراً توضيحياً بشرياً، ومجموعة البيانات "المستنسخة"، وهي مزيج متساوٍ من بيانات التوضيح والسلوك المستنسخ من سياسة التوضيح.</p>
<p><strong><span class="nodecor">NeoRL</span></strong>(<span class="nodecor">qin2022neorl</span>): <span class="nodecor">NeoRL</span> هو معيار مصمم ليعكس الظروف الواقعية من خلال جمع مجموعات البيانات باستخدام سياسة أكثر حذراً، متماشية بشكل وثيق مع طرق جمع البيانات الواقعية. ندرة وتحديد البيانات يشكل تحدياً كبيراً لخوارزميات التعلم الآلي خارج الخط. تدرس أبحاثنا تسع مجموعات بيانات، تشمل ثلاث بيئات مختلفة (<span class="nodecor">HalfCheetah-v3</span>, <span class="nodecor">Hopper-v3</span>, <span class="nodecor">Walker2d-v3</span>) وثلاث مستويات من جودة البيانات (<span class="nodecor">L</span>, <span class="nodecor">M</span>, <span class="nodecor">H</span>)، تشير إلى جودة منخفضة، متوسطة، وعالية، على التوالي. بشكل لافت، يقدم <span class="nodecor">NeoRL</span> كميات متفاوتة من مسارات البيانات التدريبية (<span class="nodecor">100</span>, <span class="nodecor">1000</span>, <span class="nodecor">10000</span>) لكل بيئة. لتجاربنا، اخترنا بشكل موحد <span class="nodecor">1000</span> مسار.</p>
<h2 id="appendix:model_architecture">هندسة النموذج</h2>
<p><strong>هندسة نموذج الديناميكيات</strong>: كما في الأعمال السابقة، استخدمنا شبكة عصبية كأساس لنموذج الديناميكيات لدينا، والذي يخرج توزيعاً غاوسياً للحالة التالية والمكافأة. من خلال تجميع هذه الشبكات، حققنا استقراراً أكبر وأداء محسناً. من تجميع سبعة، اخترنا أفضل خمسة نماذج بناءً على خطأ التحقق. يتألف العمود الفقري لنموذج الديناميكيات من أربع طبقات، كل منها ببعد مخفي يبلغ <span class="nodecor">200</span>.</p>
<p><strong>هندسة الممثل والناقد</strong>: يتألف إطار عمل الممثل والناقد مثل SAC (<span class="nodecor">haarnoja2018soft</span>) من وحدات الممثل والناقد. عادة ما يمتلك الممثل عموداً فقرياً مكوناً من شبكة عصبية. يتم نقل الخصائص المضمنة داخل هذا العمود الفقري من خلال طبقة أخيرة تخرج توزيعاً غاوسياً، مما يؤدي إلى نتيجة غير حتمية. على الرغم من أن MOPO، MOBILE، CQL، و IQL (<span class="nodecor">mopo_yu2020</span>, <span class="nodecor">mobile_sun2023</span>, <span class="nodecor">iql_kostrikov2022</span>, <span class="nodecor">cql_kumar2020</span>)، تستخدم تقليدياً <span class="nodecor">2</span>، <span class="nodecor">2</span>، <span class="nodecor">3</span>، و <span class="nodecor">2</span> طبقات عمود فقري ببعد <span class="nodecor">256</span> على التوالي، عند دمج COCOA، قمنا بتوحيد استخدام طبقتين عمود فقري ببعد مخفي يبلغ <span class="nodecor">100</span>.</p>
<p><strong>هندسة سياسة البحث عن المرساة</strong>: تعمل سياسة البحث عن المرساة كوحدة إضافية مشتركة بين الممثل والناقد. يتم تضمين البيانات المدخلة، المكونة من الدلتا والمرساة، من خلال شبكة عصبية ومن ثم تتم معالجتها بواسطة هندسة ثنائية الخط. في البداية، يتم تضمين المدخلات إلى بعد <span class="nodecor">4</span> مع شبكتين عصبيتين ب<span class="nodecor">64</span> قناة، وتنتج الهندسة الثنائية الخطية ناتجاً ببعد <span class="nodecor">64</span> باستخدام تلك الخصائص المضمنة. ثم يتم تمرير نواتج الهندسة الثنائية الخطية من خلال هندسات العمود الفقري للممثل والناقد، مما يؤدي إلى تحديد الفعل وقيمة Q على التوالي.</p>
<p><strong>حجم المعلمة</strong>: تم بناء سياسة البحث عن المرساة على شبكة عصبية مدمجة. بالنسبة للخوارزميات المبنية على النموذج مثل MOPO و MOBILE، يبلغ حجم معلمة الديناميكيات حوالي <span class="nodecor">1.9M</span>، مماثل لذلك في COCOA. ومع ذلك، فإن حجم المعلمة اللازم لتدريب الممثل والناقد لـ MOPO و MOBILE يعادل <span class="nodecor">0.21M</span>. ومع ذلك، عند إضافة COCOA إلى هذه الخوارزميات، ينخفض حجم المعلمة إلى <span class="nodecor">0.19M</span>. نظراً للحجم الكبير لمعلمات الديناميكيات، فإن متطلبات المعلمة الإجمالية للتدريب عبر خوارزميات النموذج المضافة إلى COCOA تظل ثابتة عند <span class="nodecor">2.2M</span>. في المقابل، IQL+COCOA و CQL+COCOA، التي تعمل بدون نموذج ديناميكيات، لكل منها حجم معلمة يبلغ <span class="nodecor">2.0M</span>.</p>
<h2 id="appendix:codebase">تنفيذ الشيفرة</h2>
<p>تم تصميم طريقتنا كتحسين إضافي لخوارزميات التعلم المعزز دون اتصال القائمة. ونتيجة لذلك، بدلاً من تطوير تنفيذ جديد، قمنا بتكييف قواعد الشيفرات الأساسية المعمول بها. لضمان تكييف الشيفرة بشكل متسق وموثوق، اعتمدنا على (<span class="nodecor">offinerlkit</span>) كأساس لجميع الخوارزميات الأساسية، بما في ذلك (<span class="nodecor">cql_kumar2020</span>)، (<span class="nodecor">iql_kostrikov2022</span>)، (<span class="nodecor">mopo_yu2020</span>) و (<span class="nodecor">mobile_sun2023</span>). يدعم موثوقية هذه القاعدة الشيفرية بسجلات تدريب مفصلة ونتائج تتماشى مع تلك الموجودة في الأوراق الأصلية. بالإضافة إلى ذلك، يقدم (<span class="nodecor">offinerlkit</span>) نتائج لمجموعات بيانات Gym-MuJoCo-v2 التي لم تكن موجودة في أوراق CQL وMOPO الأصلية، مما يلبي احتياجاتنا. لاحظ أن أحد مؤلفي MOBILE (<span class="nodecor">mobile_sun2023</span>) يوفر هذه القاعدة الشيفرية. تم مشاركة تكييفاتنا للشيفرة كعرض توضيحي في المواد التكميلية.</p>
<h2 id="appendix:hyperparameters">المعلمات الفائقة لكل خوارزمية</h2>
<p><strong>CQL.</strong> لكل من CQL و CQL+COCOA، نستخدم <span class="math inline">\(\alpha=5.0\)</span> لجميع مهام D4RL-Gym لأن قاعدة الكود المستنسخة (<span class="nodecor">offinerlkit</span>) التي توفر النتائج لمهام MuJoCo-v2، والتي لم تدرج في الورقة الأصلية (<span class="nodecor">cql_kumar2020</span>)، تستخدم هذه القيمة. بالنسبة لـ COCOA، تم تحديد طول أفق البحث عن المرساة <span class="math inline">\(h\)</span> إلى <span class="nodecor">1</span> لمعظم المهام، باستثناء “halfcheetah-medium-expert-v2”، “hopper-medium-expert-v2”، و“walker2d-medium-expert-v2”، حيث تم تحديد <span class="math inline">\(h\)</span> إلى <span class="nodecor">3</span>.</p>
<p><strong>IQL.</strong> لكل من IQL، نستخدم نفس المعلمات الفائقة الموصوفة في الورقة الأصلية (<span class="nodecor">iql_kostrikov2022</span>)، <span class="math inline">\(\tau=0.7\)</span> و <span class="math inline">\(\beta=3.0\)</span>، والتي تستخدم أيضاً في قاعدة الكود المستنسخة (<span class="nodecor">offinerlkit</span>). بالنسبة لـ IQL+COCOA، استخدمنا <span class="math inline">\(\tau=0.6\)</span> و <span class="math inline">\(\beta=3.0\)</span>. بالنسبة لـ COCOA، حددنا طول أفق البحث عن المرساة <span class="math inline">\(h\)</span> إلى <span class="nodecor">1</span> لجميع المهام. لقد أعدنا إنتاج القيمة العشوائية لـ halfcheetah، hopper، walker2d، والتي هي <span class="nodecor">6.62</span> إلى <span class="nodecor">6</span>، <span class="nodecor">8.1</span> إلى <span class="nodecor">7</span>، <span class="nodecor">6.1</span> إلى <span class="nodecor">6.5</span> على التوالي.</p>
<p><strong>MOPO.</strong> لـ MOPO، نستخدم المعلمات الفائقة المستخدمة في قاعدة الكود المستنسخة (<span class="nodecor">offinerlkit</span>)، والتي توفر النتائج لمهام MuJoCo-v2 غير المدرجة في الورقة الأصلية (<span class="nodecor">mopo_yu2020</span>). كما في الورقة الأصلية، نستخدم عدم اليقين العشوائي لـ MOPO و MOPO+COCOA. بالنسبة لـ MOPO+COCOA، بحثنا عن أفضل معامل العقوبة <span class="math inline">\(\lambda\)</span> وطول التدحرج <span class="math inline">\(h_r\)</span> لكل مهمة في النطاقات التالية: <span class="math inline">\(\lambda \in \{0.1, 0.5, 1.0, 5.0, 10.0\}\)</span>، <span class="math inline">\(h_r \in \{1, 5, 7, 10\}\)</span> باستثناء حالة halfcheetah-medium-expert. تم وصف أفضل المعلمات الفائقة في الجدول [tab:mopo_mobile_hyperparameters]. بالنسبة لـ COCOA، حددنا طول أفق البحث عن المرساة <span class="math inline">\(h\)</span> إلى <span class="nodecor">1</span> لجميع المهام.</p>
<p><strong>MOBILE.</strong> نستخدم نفس المعلمات الفائقة الموصوفة في الورقة الأصلية (<span class="nodecor">mobile_sun2023</span>) لـ MOBILE. بالنسبة لـ MOBILE+COCOA، بحثنا عن أفضل معامل العقوبة <span class="math inline">\(\lambda\)</span> وطول التدحرج <span class="math inline">\(h_r\)</span> لكل مهمة في النطاقات التالية: <span class="math inline">\(\lambda \in \{0.1, 1.0, 1.5, 2.0\}\)</span>، <span class="math inline">\(h_r \in \{1, 5, 10\}\)</span> باستثناء حالة walker-medium-replay. تم وصف أفضل المعلمات الفائقة في الجدول [tab:mopo_mobile_hyperparameters]. بالنسبة لـ COCOA، حددنا طول أفق البحث عن المرساة <span class="math inline">\(h\)</span> إلى <span class="nodecor">1</span> لجميع المهام. بالإضافة إلى ذلك، بعد التحقق من التقارب، قصرنا تدريبنا على أقصى <span class="nodecor">2000</span> عصر وحصلنا على النتائج من هذا النطاق الزمني المحدد.</p>
<h2 id="appendix:more_experimental_results">نتائج تجريبية إضافية</h2>
<p>لقد قمنا بتجربة معيارين إضافيين - D4RL Adroit و NeoRL. تم تلخيص نتائج هذه التجارب في الجدول [tab:adroit] و [tab:neorl]. تكشف هذه التحليلات الأوسع أن COCOA يعزز أداء IQL و MOBILE في معظم المهام. تم إجراء جميع التجارب على المعايير الإضافية دون تطبيق تطبيع الطبقات للسماح بالمقارنة المباشرة مع الأداء المبلغ عنه في أوراقهم الأصلية.</p>
<p>أظهرت طريقتنا تحسينات متسقة في الأداء عبر ست مهام D4RL Adroit التي اختبرناها، مما يظهر قوتها وقابليتها للتكيف. بينما واجهت COCOA تحديات في المهام المعقدة مثل الباب والمطرقة، مشابهة لخوارزميتها الأصلية، يعكس هذا صعوبة هذه المهام بسبب المكافآت المتفرقة. بشكل ملحوظ، في مهام مثل القلم، حققت طريقتنا تحسينات ملحوظة في الأداء.</p>
<p>في Adroit، يقتصر عصر التدريب على <span class="nodecor">200</span> كما وصف في (<span class="nodecor">mobile_sun2023</span>). بالإضافة إلى ذلك، استخدمنا نفس المعلمات الفائقة لـ MOBILE+COCOA على Adroit كما وصف في الورقة. تم وصف المعلمات الفائقة لـ Adroit و NeoRL في الجدول [tab:combined].</p>
<h2 id="appendix:combo">مقارنة مع خوارزمية <span class="nodecor">COMBO</span></h2>
<p>تظهر <span class="nodecor">CQL+COCOA</span> و<span class="nodecor">COMBO</span> بعض التشابهات، لا سيما في استخدامهما للديناميكيات ونهج أقل تحفظاً تجاه فضاء الحالة-الفعل. ومع ذلك، تختلف منهجياتهما في متابعة التحفظ بشكل كبير: تركز <span class="nodecor">COCOA</span> على التحفظ في فضاء الإدخال التركيبي، بينما يؤكد <span class="nodecor">COMBO</span> على تنظيم القيم للأفعال غير المألوفة. وبالتالي، <span class="nodecor">COCOA</span> و<span class="nodecor">COMBO</span> متعامدان، وسيكون من المفيد مقارنة دمج <span class="nodecor">COCOA</span> مع <span class="nodecor">COMBO</span>، حيث يمكن أن تكون <span class="nodecor">COCOA</span> إضافة إلى أي خوارزمية.</p>
<p>مماثلة لـ<span class="nodecor">COMBO</span>، تظهر الطرق المبنية على <span class="nodecor">MBPO</span> مثل <span class="nodecor">MOPO</span> و<span class="nodecor">RAMBO</span> أيضاً ميلاً للتفوق على الطرق الخالية من النماذج في الإعدادات العشوائية والمتوسطة. يبدو أن توسيع البيانات من خلال <span class="nodecor">MBPO</span> مفيد بشكل خاص في هذه المهام. سيكون من المثير للاهتمام مقارنة الدوال القيمية المحددة للحالة نظرياً أو تجريبياً بين <span class="nodecor">CQL</span>، <span class="nodecor">CQL+COCOA</span>، و<span class="nodecor">COMBO</span> لمزيد من التحليل.</p>
<h1 id="النتائج-التفصيلية">النتائج التفصيلية</h1>
<p><span>width=0.4</span></p>
<table>
<caption>معلمات التحكم الخاصة بـ MOBILE+COCOA لمعيار Adroit و MOPO+COCOA لمعيار NeoRL.<span data-label="tab:combined"></span></caption>
<thead>
<tr class="header">
<th style="text-align: left;">المهمة</th>
<th style="text-align: center;"><span class="math inline">\(\lambda\)</span></th>
<th style="text-align: center;"><span class="math inline">\(h_r\)</span></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">door-cloned-v1</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">door-human-v1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">hammer-cloned-v1</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">hammer-human-v1</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">pen-cloned-v1</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">pen-human-v1</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">HalfCheetah-v3-low</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Hopper-v3-low</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Walker2d-v3-low</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">HalfCheetah-v3-medium</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Hopper-v3-medium</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Walker2d-v3-medium</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">HalfCheetah-v3-high</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">Hopper-v3-high</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Walker2d-v3-high</td>
<td style="text-align: center;">2.5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h2 id="appendix:theoretical_analysis">التحليل النظري</h2>
<p>في هذا القسم، نوضح ببساطة كيف يمكن تقريب المتنبئ التحويلي الخاص بنا في عملية ثنائية الخطية باستخدام نظرية ستون-فايرشتراس.</p>
<p>[thm:stone_weierstrass_lch] لنفترض أن <span class="math inline">\( X \)</span> هو فضاء هاوسدورف مضغوط محلياً وأن <span class="math inline">\( A \)</span> هو جبر فرعي من <span class="math inline">\( C_0(X, \mathbb{R}) \)</span>. إذا <span class="math inline">\( A \)</span> كثيف في <span class="math inline">\( C_0(X, \mathbb{R}) \)</span> بالنسبة لتوبولوجيا التقارب الموحد إذا وفقط إذا كان يفصل النقاط ولا يختفي في أي مكان.</p>
<p>لتكن <span class="math inline">\(\mathcal{X}\)</span> و <span class="math inline">\(\mathcal{Y}\)</span> فضاءات هاوسدورف مضغوطة محلياً (LCH). بالإضافة إلى ذلك، لتكن <span class="math inline">\(\mathcal{F} \subset C(\mathcal{X}; \mathbb{R})\)</span> و <span class="math inline">\(\mathcal{G} \subset C(\mathcal{Y}; \mathbb{R})\)</span> فضاءات فرعية متجهية كثيفة في توبولوجيا التقارب الموحد على الكومباكتا. ثم تخبرنا النظرية [thm:stone_weierstrass_lch] أن <span class="math display">\[\left\{
\sum_{k=1}^{d} f_k(x)g_k(y) \, \middle|\, f_1, \ldots, f_k \in \mathcal{F}, g_1, \ldots, g_k \in \mathcal{G}, d \in \mathbb{N}
\right\}
\subseteq C(\mathcal{X} \times \mathcal{Y}; \mathbb{R})\]</span> ، والذي يشكل جبراً، كثيف في توبولوجيا التقارب الموحد على الكومباكتا. بعبارة أخرى، إذا كان لدينا تضمين مشترك <span class="math inline">\(f_\theta \colon \mathcal{X} \to \mathbb{R}^d\)</span> و <span class="math inline">\(g_\phi \colon \mathcal{Y} \to \mathbb{R}^d\)</span>، فإن <span class="math inline">\(h_{\theta,\phi}(x, y) = f_\theta(x) \cdot g_\phi(y)\)</span> هو مقرب عالمي، بحيث أن <span class="math inline">\((d, width) \to (\infty, \infty)\)</span> و <span class="math inline">\(f_\theta(x)\)</span>، <span class="math inline">\(g_\phi(y)\)</span> لهما عمق <span class="math inline">\(\geq 2\)</span>. نظراً لأننا نستخدم شبكة معلمة لتقريب المتنبئ التحويلي ومساحة الإدخال لدينا <span class="math inline">\((s, a)\)</span> هي جزء من <span class="math inline">\(\mathbb{R}^m \times \mathbb{R}^n\)</span>، حيث <span class="math inline">\(m\)</span> و <span class="math inline">\(n\)</span> تشير إلى أبعادهما على التوالي، <span class="math inline">\(\boldsymbol{\varphi_{\boldsymbol{\theta},1}}\)</span> و <span class="math inline">\(\boldsymbol{\varphi_{\boldsymbol{\theta},2}}\)</span> والتي تم وصفها في القسم [subsec:offline_rl_bilinear_transduction]، يمكن أن تتوافق مع <span class="math inline">\(f_\theta\)</span> و <span class="math inline">\(g_\phi\)</span>، على التوالي.</p>
<h2 id="subsec:d4rl_performance_graph">رسوم بيانية لأداء مهام معيار <span class="nodecor">D4RL</span></h2>
<p>في هذا القسم، نقدم الرسوم البيانية لأداء كل خوارزمية على مهام معيار <span class="nodecor">D4RL</span>. نحن نشمل فقط <span class="nodecor">9</span> المهام التي ليست مهام "عشوائية" لأن نقاط التحقق من الطرق الأساسية للمهام "العشوائية" لم تقدم.</p>
</body>
</html>
