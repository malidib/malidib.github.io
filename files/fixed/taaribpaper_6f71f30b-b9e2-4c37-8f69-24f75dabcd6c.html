<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Gengming Zhang, Hao Cao, Kewei Hu, Yaoqiang Pan, Yuqin Deng, Hongjun Wang, Hanwen Kang">
  <title>تقدير نقاط القطع الدقيقة لحصاد الليتشي الآلي من خلال التعلم المدرك للهندسة</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">تقدير نقاط القطع الدقيقة لحصاد الليتشي الآلي من خلال التعلم المدرك للهندسة</h1>
<p class="author"><span class="nodecor">Gengming Zhang</span>, <span class="nodecor">Hao Cao</span>, <span class="nodecor">Kewei Hu</span>, <span class="nodecor">Yaoqiang Pan</span>, <span class="nodecor">Yuqin Deng</span>,<br />
<span class="nodecor">Hongjun Wang</span>, <span class="nodecor">Hanwen Kang</span></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>يُعَدُّ تحديد نقاط قطف الليتشي بدقة في بيئات البساتين غير المنظمة والحصول على مواقعها الإحداثية أمراً حاسماً لنجاح روبوتات قطف الليتشي. ومع ذلك، غالباً ما تواجه طرق الكشف عن الأجسام المعتمدة على الصور ثنائية الأبعاد (2D) صعوبات بسبب الهياكل الهندسية المعقدة للفروع والأوراق والثمار، مما يؤدي إلى تحديد خاطئ لنقاط قطف الليتشي. في هذه الدراسة، نقترح نموذج شبكة <span class="nodecor">Fcaf3d-lychee</span> المصمم خصيصاً لتحديد مواقع نقاط قطف الليتشي بدقة. يتم الحصول على بيانات سحابة النقاط لنقاط قطف الليتشي في البيئات الطبيعية باستخدام كاميرا <span class="nodecor">Microsoft’s Azure Kinect DK</span> لقياس الوقت (<span class="nodecor">TOF</span>) من خلال التصوير المتعدد الزوايا. نقوم بتعزيز نموذج الكشف عن الأجسام ثلاثية الأبعاد بدون مرساة والكامل التلافيفي (<span class="nodecor">Fcaf3d</span>) بوحدة الضغط والإثارة (<span class="nodecor">SE</span>)، والتي تستغل آليات الانتباه البصري البشري لتحسين استخراج الميزات لنقاط قطف الليتشي. يتم تقييم النموذج المدرب على مجموعة اختبار من مواقع قطف الليتشي ويحقق درجة <span class="nodecor"><span class="math inline">\(F_{1}\)</span></span> مثيرة للإعجاب بنسبة <span class="nodecor">88.57%</span>، متفوقاً بشكل كبير على النماذج الحالية. يؤدي الكشف عن الموقع ثلاثي الأبعاد لنقاط القطف في بيئات بساتين الليتشي الحقيقية إلى دقة عالية، حتى في ظل درجات متفاوتة من التغطية. أخطاء تحديد مواقع نقاط قطف الليتشي تكون ضمن <span class="nodecor">±1.5 cm</span> في جميع الاتجاهات، مما يدل على قوة وعمومية النموذج.</p>
<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>الزراعة الدقيقة أو الزراعة الذكية هي مفهوم حديث لإدارة المزرعة بالكامل، يستخدم مجموعة متنوعة من التقنيات تتراوح من الاستشعار عن بعد وجمع البيانات القريبة إلى الأتمتة والروبوتات. في هذه الدراسة، نركز على حصاد الليتشي الطازج. لقد ثبت أن هذا العمل يمثل تحدياً كبيراً، ليس فقط من حيث التصميم الميكانيكي للروبوت، ولكن أيضاً من حيث نظام الرؤية، وطرق التنقل، والتحكم ونظام التلاعب (<span class="nodecor">r65</span>). في هذه الحالة، يتركز اهتمامنا الرئيسي على اكتشاف نقاط القطف على سيقان الليتشي في بيانات السحابة النقطية لتحسين الحصاد الذاتي لروبوتات القطف التقليدية.</p>
<p>يمكن قطف الفواكه الفردية مثل التفاح والبرتقال مباشرة، بينما تحتاج الفواكه العنقودية مثل الليتشي والعنب إلى قطف العنقود بالكامل. يمكن قطف الليتشي فقط عن طريق تحديد نقطة القطف الرئيسية للفرع الحامل للفاكهة (MFBB) ثم قطع الـ MFBB لمنع تلف الفاكهة (<span class="nodecor">r2</span>). إذا كانت منطقة العمل واضحة والفواكه غير محجوبة بالعوائق، فليس من الصعب فصل هذه الفواكه عن الأشجار (<span class="nodecor">r63</span>). ومع ذلك، فإن الحقل غير منظم، وقد تكون عناقيد فاكهة الليتشي مختلفة الأحجام في الشكل وتظهر في ارتفاعات ومواقع مختلفة، بالإضافة إلى ذلك، فإن نقاط قطف الليتشي في السيناريوهات غير المنظمة عرضة للازدحام الشديد (أو أن الموقع المستهدف ليس واضحاً بما فيه الكفاية)، والأهداف الصغيرة صعبة الكشف، مما يؤثر على دقة الكشف. لذلك، أصبح البحث عن تحديد دقيق وموجه لنقاط قطف عناقيد الليتشي محور البحث، وهناك تقنيتان رئيسيتان مطلوبتان لتحقيق هذا الهدف: (1) بيانات السحابة النقطية ثلاثية الأبعاد (3D) التي تقاوم الازدحام، و(2) نموذج كشف السحابة النقطية ثلاثي الأبعاد (3D) CNN الذي يمتلك قدرة تحديد دقيقة بناءً على ما سبق.</p>
<p>في السنوات الأخيرة، تم استخدام تقنيات الرؤية الحاسوبية على نطاق واسع لتحديد نقاط القطف للفواكه (<span class="nodecor">r3,r42,r4</span>). يتم استخدام خصائص الهدف نفسه (مثل اللون، الشكل، النسيج، إلخ) للكشف عن الليتشي وتحديده من خلال معالجة الصور (مثل تصفية الصور، تجزئة الصور، المعالجة المورفولوجية) وخوارزميات التعلم الآلي وتمثيل هذه العمليات (<span class="nodecor">r5,r43,r6,r7,r44</span>). مع التطور العميق للتعلم العميق، وخاصة الشبكات العصبية التلافيفية (CNN)، التي تعزز بشكل أكبر قدرة التعميم وكذلك متانة أنظمة إدراك الرؤية الآلية، تحول المزيد والمزيد من الباحثين إلى طرق كشف الصور ثنائية الأبعاد المبنية على التعلم العميق. خوارزميات الكشف عن الأجسام، بما في ذلك You Only Look Once (YOLO) وشبكات التعلم العميق المحسنة على YOLO (<span class="nodecor">r11,r12,r45,r46,r47</span>)، والكشف ذو المرحلتين الممثل بسلسلة RCNN (الشبكات العصبية التلافيفية المعتمدة على المنطقة) (<span class="nodecor">r50</span>). حققت الدراسات المذكورة أعلاه نتائج جيدة لكشف نقاط القطف، ولكن لا يزال من الصعب مواجهة الوضع بسبب تقلبات الضوء، والازدحام بواسطة الأوراق والفروع، إلخ.</p>
<p>تتميز بيانات السحابة النقطية ثلاثية الأبعاد بدقتها في تمثيل الأشكال الهندسية، وسهولة الوصول إلى معلومات العمق، وتساعد إعادة البناء ثلاثي الأبعاد على تحسين متانة ودقة الكشف، بالإضافة إلى التطوير المكثف للشبكات العصبية التلافيفية ثلاثية الأبعاد، مما حفز الباحثين على استكشاف طرق تعتمد على السحابة النقطية لحصاد الفاكهة وكشف نقاط القطف (<span class="nodecor">r51,r16,r52,r53,r14,r54,r66,r57</span>). على الرغم من أن الطرق البحثية المذكورة أعلاه قد حققت نتائج مرضية، لا تزال هناك فجوات بحثية كبيرة في كشف نقاط قطف الليتشي.</p>
<p>في هذا العمل، تم اقتراح نموذج معالجة البيانات المساعد بذراع الروبوت استناداً إلى دمج سحابة نقاط الكاميرا TOF ونموذج شبكة Fcaf3d-lychee لنقاط قطف الليتشي، والذي يمكن استخدامه لتحديد مواقع نقاط قطف الليتشي بدقة في بيئة البستان الطبيعية، وتم اختباره ميدانياً. يدمج النهج المقترح بيانات السحابة النقطية متعددة الزوايا من كاميرا TOF، ثم يكتشف بيانات الاستشعار المدمجة بصرياً للعثور على الموقع الدقيق لنقطة قطف الليتشي. تظهر النتائج أن نتائج هذه الدراسة تتمتع بدقة تحديد مواقع أعلى من نموذج الكشف عن الأهداف النموذجي للسحابة النقطية ثلاثية الأبعاد. باختصار، يتم تقديم المساهمات التالية في هذه الورقة:</p>
<ul>
<li><p>اقتراح نموذج Fcaf3d-lychee لكشف نقاط قطف الليتشي، مما يحسن بشكل كبير دقة تحديد مواقع نقاط القطف.</p></li>
<li><p>استخدام كاميرا TOF لخياطة السحابة النقطية متعددة الزوايا، مما يحل بفعالية مشكلة الازدحام أحادي الزاوية وقلة المعلومات الحسية.</p></li>
<li><p>عرض روبوت قطف الليتشي المجهز بنموذج Fcaf3d-lychee على التعرف وتحديد مواقع نقاط القطف في بيئات البستان الطبيعية.</p></li>
</ul>
<p>يتم تنظيم بقية هذه الورقة على النحو التالي. يستعرض القسم [section: review] الأعمال ذات الصلة. يتم تقديم نظرة عامة على النظام والمنهجيات الخاصة بنهجنا في القسم [section: method]. تُعرض نتائج التجربة والمناقشة في القسم [section: experiment]، يلي ذلك الخاتمة في القسم [section: conclusion].</p>
<h1 id="section: review">الأعمال ذات الصلة</h1>
<h2 id="مراجعة-عن-الكشف-عن-الأهداف-ثنائية-الأبعاد-المبنية-على-الصور-في-الليتشي">مراجعة حول الكشف عن الأهداف ثنائية الأبعاد المبنية على الصور في الليتشي</h2>
<p>حالياً، يغطي مجال تطبيق الروبوتات مجالات تكنولوجية أساسية ومتعلقة عديدة (<span class="nodecor">r17</span>). من بينها، في تطوير روبوتات الحصاد الذكية، كانت خوارزميات الرؤية عاملاً رئيسياً يؤثر على الأداء العام للنظام. يتضمن تطوير خوارزميات الرؤية لروبوتات حصاد الفاكهة مهمتين رئيسيتين: تحديد موقع الهدف واستخراج نقطة القطف، وتشمل التحديات تشوه لون الهدف الناتج عن البيئات الضوئية الطبيعية المتغيرة، تداخل أعضاء النبات المتشابكة على خوارزميات التعرف، وتشوهات داخل الفئة لثمار نفس النوع (<span class="nodecor">r18</span>). استخدام الرؤية الآلية وخوارزمياتها المتعلقة قد استهدف حلولاً لتحسين الكفاءة والدقة والذكاء والتفاعل عن بعد أثناء عمليات الروبوت (<span class="nodecor">r19</span>). لتصنيفها، كانت الأولى الرئيسية هي طرق تحليل الميزة الفردية، طرق تحليل دمج الميزات المتعددة، وطرق التعرف على الأنماط (<span class="nodecor">r20</span>, <span class="nodecor">r21</span>). (<span class="nodecor">r24</span>) اقترح طريقة تعرف تعتمد على مصنف تحليل التمييز الخطي المحسن (LDA) لمعالجة مشكلة معدل النجاح المنخفض لروبوتات قطف الليتشي في التعرف على الليتشي الأخضر (الليتشي غير الناضج بالكامل) بسبب تداخل الخلفية. الجوهر هو طريقة LDA لاستخراج ميزات التحويل الصوري، ويتم تقديم فكرة “الهامش الأقصى” في خوارزمية SVM لتحديد قيمة العتبة لـ LDA، والتي يتم دمجها أخيراً في نظام تصنيف LDA متعدد بواسطة طريقة Adaboost. من خلال التجارب، كانت دقة هذا المصنف لليتشي غير الناضج <span class="nodecor">80.4%</span>، ويمكن أيضاً تطبيق الخوارزمية على تحديد نضج الفاكهة. تستند الأبحاث المذكورة أعلاه إلى معالجة الصور، والتي لها متطلبات عالية للضوء ولا تزال بحاجة إلى المرور بعملية حسابية معينة عند التعامل مع الموضع ثلاثي الأبعاد لنقطة القطف، مما يؤدي إلى كفاءة قطف ضعيفة وقلة القوة.</p>
<h2 id="مراجعة-على-طرق-الكشف-عن-الأهداف-ثنائية-الأبعاد-لثمره-الليتشي-باستخدام-التعلم-العميق">مراجعة حول طرق الكشف عن الأهداف ثنائية الأبعاد لثمرة الليتشي باستخدام التعلم العميق</h2>
<p>في الأبحاث الحالية، تم اعتماد تقنيات قياس الرؤية المجسمة وتقنيات معالجة الصور الرقمية التقليدية من قبل العديد من الباحثين، بينما يوفر التطور المتزايد للتعلم العميق في مجال معالجة الصور حلاً أفضل لتمييز الفواكه، مما يمكن روبوتات قطف الفواكه من التكيف مع بيئات البساتين المعقدة (<span class="nodecor">r25</span>). لتحسين قدرة روبوت القطف على العمل طوال اليوم، تم اقتراح خوارزمية للكشف عن ثمار الليتشي وسيقان الفاكهة للبيئات الليلية (<span class="nodecor">r26</span>). تم اختبار أداء الخوارزمية تحت شدات ضوء صناعية مختلفة بشكل منفصل، وتم استخدام شبكة العصبونات التلافيفية YOLOv3 وشبكة U-Net للكشف عن الفواكه وسيقان الفواكه، وأخيراً أدى النموذج بأفضل أداء تحت السطوع الطبيعي، حيث حقق دقة كشف متوسطة قدرها <span class="nodecor">99.57</span>% بالإضافة إلى تأثير تجزئة قدره <span class="nodecor">84.33</span>% MIoU. ومع ذلك، فإن الطريقة تستخدم أولاً YOLOv3 للحصول على موقع الفاكهة المتسلسلة ثم تستخدم منطقة ROI لتمييز سيقان الفواكه، وهو ما لا يعد فعالاً في التنفيذ. (<span class="nodecor">r28</span>) قام بتحسين هيكل الشبكة لـ YOLOv5s لسيناريوهات الاحتجاب الخلفي واحتجاب الفواكه التي يسهل مواجهتها أثناء قطف التفاح بحيث يمكنه تحديد الأشياء التي يمكن التقاطها وتلك التي لا يمكن التقاطها أثناء عملية القطف. ومع ذلك، فإن الطريقة تنطبق غالباً على الفواكه الفردية، أو لتقدير الغلات، ولا تنطبق على سيناريوهات حصاد الليتشي. لقد أحرزت الدراسات المذكورة أعلاه تقدماً كبيراً في الحصول على نقاط قطف الفواكه من خلال الكشف عن الأهداف بواسطة الصور ثنائية الأبعاد، ولكن معظم الدراسات لم تبحث في الكشف المباشر عن نقاط قطف الفرع الأم.</p>
<h2 id="مراجعة-على-طرق-الكشف-عن-الأهداف-ثلاثية-الأبعاد-باستخدام-التعلم-العميق-للفواكه">مراجعة حول طرق الكشف عن الأهداف ثلاثية الأبعاد باستخدام التعلم العميق للفواكه</h2>
<p>على الرغم من أن الكشف عن الأهداف باستخدام الصور ثنائية الأبعاد قد أحرز تقدماً كبيراً في تحديد نقاط القطف لروبوتات قطف الفواكه، إلا أن معظم الدراسات لم تبحث في الكشف المباشر عن نقاط قطف الفرع الأم. بينما قد تفتقر الصور ثنائية الأبعاد إلى معلومات توزيع مكاني مفصلة، فإن السحب النقطية ثلاثية الأبعاد التي يتم الحصول عليها بواسطة طرق كاميرا العمق توفر تمثيلاً أكثر شمولاً لتوزيع الأجسام، خاصة في المشاهد المعقدة مع التداخلات والتغطيات. لقد تم التعرف بشكل متزايد على إمكانيات السحب النقطية ثلاثية الأبعاد، مما أدى إلى ظهور طرق جديدة لكشف الأجسام. (<span class="nodecor">r33</span>) اقترح استراتيجية لتحديد نقاط القطف بدقة للعنب المزروع صناعياً من خلال الجمع بين القريب والبعيد واستخدام مزيج من بيانات السحب النقطية للعمق. في البداية، تم الكشف عن عناقيد العنب بشكل تقريبي لتحديد المواقع في الصورة البعيدة، ثم في الصورة القريبة، تم استخدام ميزات بيانات السحب النقطية للعمق والميزات الهيكلية لعناقيد العنب الأفقية بشكل كامل، وتم تحديد المواقع بنجاح لـ <span class="nodecor">95</span> من <span class="nodecor">100</span> عينة، بدقة <span class="nodecor">95</span>%. كانت دقة تحديد الموقع تصل إلى <span class="nodecor">95</span>%. بالإضافة إلى ذلك، (<span class="nodecor">r36</span>) اقترح طريقة تصنيف أعضاء شجرة الرمان وعد الفواكه استناداً إلى دمج متعدد الميزات وآلة الدعم الناقل (SVM) لمعالجة قيود التداخل في طريقة التعرف المعتمدة على الصور ثنائية الأبعاد التقليدية، أولاً، يتم الحصول على سحابة نقطية ثلاثية الأبعاد، ثم تتم معالجة السحابة النقطية مسبقاً، واستخراج ميزات اللون والشكل للتدريب، والحصول على نموذج التصنيف، وتظهر التجارب أن هذه الطريقة يمكن أن تكتشف معظم الفواكه على الشجرة. ومع ذلك، لم تتم دراسة طرق كشف الأهداف للسحب النقطية لنقاط القطف.</p>
<p>عملنا مبني على نموذج Fcaf3d، الذي تم تحسينه للحصول على نموذج الكشف ثلاثي الأبعاد Fcafd-lychee لنقاط قطف الليتشي، والذي يمكنه استخدام تدفق السحب النقطية لكاميرا العمق للكشف المباشر عن نقاط قطف الليتشي والحصول على إحداثياتها ثلاثية الأبعاد، مما يسرع وينفذ بكفاءة كشف نقاط القطف.</p>
<h1 id="section: method">المواد والطرق</h1>
<h2 id="نظرة-عامة-على-النظام">نظرة عامة على النظام</h2>
<p>تُعرض عملية اكتشاف نقطة قطف الليتشي المقترحة في الشكل [fig:graph1]. يمكن تقسيم العملية إلى خطوتين، الحصول على سحابة النقاط وأداء التطريز متعدد الزوايا، ونموذج <span class="nodecor">Fcaf3d-lychee</span> لتحديد الموقع بدقة. أولاً، يتجول الروبوت القاطف، الذي يحمل معلومات خريطة البستان، بشكل مستقل في البستان وسيحصل على الموقع الأولي للهدف من تدفق بيانات سحابة النقاط. بعد ذلك، سيقوم الروبوت القاطف بأداء الحصول على سحابة النقاط ثلاثية الزوايا المحددة على مسافة قريبة من الهدف وأداء التطريز والتنقيص. ستتم تغذية سحابة النقاط المعالجة مسبقاً إلى نموذج <span class="nodecor">Fcaf3d-lychee</span> للتعرف الدقيق وتحديد الموقع. أخيراً، سيتم توجيه المؤثر النهائي لأداء حركة قطف الليتشي.</p>
<h2 id="نموذج-الرؤية-اليد-عين-وطريقة-المعايرة-المغلقة">نموذج الرؤية يد-عين وطريقة المعايرة المغلقة</h2>
<p>تم تشغيل حساس الكاميرا على ذراع الروبوت aubo ذي الست درجات حرية وجمع البيانات، والتي في نهاية المطاف حولت الإحداثيات ثلاثية الأبعاد لنقطة قطف الليتشي بدقة إلى نظام إحداثيات قاعدة الذراع الروبوتية. قبل الحصول على البيانات، تم إكمال معايرة الكاميرا ومعايرة اليد-عين على التوالي. تم تحقيق الأولى في هذه الدراسة بواسطة طريقة معايرة Zhang الكلاسيكية المقترحة بواسطة (<span class="nodecor">r59</span>). في أعمالنا المبكرة، تم إجراء بعض الأبحاث على المعايرة بين الكاميرا وذراع الروبوت، واقترح (<span class="nodecor">r60</span>) طريقة معايرة يد-عين مغلقة الحلقة قوية ودقيقة لتحديد الارتباط الإحداثي بين الحافة النهائية لذراع الروبوت وحساس الكاميرا لروبوت بعين في اليد. مصفوفة اليد-عين كما في المعادلة (1). <span class="math display">\[\begin{gathered}
    \widehat{{_{C}^{F}}T} =\frac{1}{N_c} \sum_{i=1}^{N_C} {({_{B}^{C}}T{^{(i)}}{_{R}^{B}}T{_{F}^{R}}T{^{(i)}})^{-1}}\end{gathered}\]</span> في هذه الحالة، <span class="math inline">\(_R^BT\)</span> هي مصفوفة التحويل الثابتة {R} بالنسبة لـ {B} نظراً للثبات النسبي بين قاعدة ذراع الروبوت ولوحة الفحص خلال عملية المعايرة، والتي يمكن معايرتها بواسطة القرب المسيطر لأصل {G} إلى الأصل وكذلك محوري X و Y لـ {B}. <span class="math inline">\(_B^CT^{(i)}\)</span> هي مصفوفة التحويل {B} بالنسبة لـ {C}، المقابلة للوضع i للروبوت والتي يمكن الحصول عليها بواسطة طريقة المربعات الصغرى (<span class="nodecor">r61</span>). <span class="math inline">\(_F^RT^{(i)}\)</span> هي مصفوفة التحويل {F} بالنسبة لـ {R}، المقابلة لموضع i للروبوت والتي يمكن الحصول عليها بسهولة بواسطة الحركات الأمامية. <span class="math inline">\(N{_c}\)</span> هو عدد الأوضاع المختلفة. في هذه الدراسة، <span class="math inline">\(N{_c}\)</span>=<span class="nodecor">16</span>.</p>
<p>عندما تحركت ذراع الروبوت في مواقع معينة (<span class="nodecor">r64</span>)، تم في نهاية المطاف تحويل الميزات المحلية في كل موقع من نقاط قطف الليتشي إلى نفس نظام الإحداثيات الأساسي لتكوين الميزة العالمية (على الرغم من أنه يجب أن يظل يطلق عليها “الميزة المحلية” لأن الذراع مع حساس الكاميرا كان محدوداً بمنطقة محلية عند المسح).</p>
<h2 id="اكتساب-سحابه-النقاط">اكتساب سحابة النقاط</h2>
<h3 id="التصفية">التصفية</h3>
<p>تتأثر دقة السحب النقطية الخام المحصلة من نظام الرؤية متعددة الزوايا بعوامل عديدة مثل التغيرات في الإضاءة الطبيعية، الاهتزاز في البيئات الديناميكية، خطأ معايرة اليد-العين، وأخطاء الأجهزة الأصلية للكاميرا. تولد هذه العوامل ضوضاء فوضوية وهياكل غير متساوية بين السحب النقطية المكتسبة. كما نوقش في هذا القسم، قمنا بدمج مرشح إحصائي ومرشح لوني لتنقية الشوائب المتناثرة خارج الجسم الرئيسي وذات فجوة لونية واسعة من نقطة الاختيار لتوفير حالة أولية جيدة للخياطة اللاحقة.</p>
<p>كما اقترح (<span class="nodecor">r62</span>) للتصفية اللونية في إعادة بناء <span class="nodecor">3D</span> لأشجار الفاكهة، نقترح أداء مرشحات لونية بسيطة في السحب النقطية ذات الميزات اللونية لتقليل عدد كبير من الأوراق الخضراء في البداية وتوفير ظروف أولية جيدة لكشف السحب النقطية اللاحق. <span class="math display">\[\begin{gathered}
\left\{
\begin{aligned}
    R_s &amp; &gt; &amp; \sigma_1 \\
    G_s &amp; \leq &amp; \sigma_2
\end{aligned}
\right.\end{gathered}\]</span> حيث <span class="math inline">\(R_s\)</span>، <span class="math inline">\(G_s\)</span> هما قناتا الأحمر والأخضر في الميزات اللونية للسحب النقطية على التوالي، عندما <span class="math inline">\(R_s\leq\sigma_1\)</span>، <span class="math inline">\(G_s\)</span>&gt;<span class="math inline">\(\sigma_2\)</span>، سيتم إزالة السحب النقطية من السحب النقطية بالكامل.</p>
<p>يمكن للمرشح الإحصائي إزالة أي نقاط شاذة متناثرة خارج الهيكل الرئيسي للسحب النقطية (<span class="nodecor">r58</span>). بالنسبة للعناصر العشوائية، يتم البحث عن جميع نقاط المجال لكل نقطة ويتم حساب المسافة من كل نقطة إلى جارتها. يتم حساب المتوسط <span class="math inline">\(\mu\)</span> والانحراف المعياري <span class="math inline">\(\sigma\)</span> للمسافات من جميع النقاط من جيرانها، وأي نقطة يكون متوسط قيمتها خارج الفترة القياسية سيتم اعتبارها كضوضاء. <span class="math display">\[\begin{gathered}
\left[ 
    \mu-\alpha_v\times\sigma,\ \mu+\alpha_v\times\sigma
\right]\end{gathered}\]</span> حيث <span class="math inline">\(\alpha_v\)</span>&lt;3 يحدد عرض الفترة القياسية.</p>
<h3 id="الخياطة">الخياطة</h3>
<p>وفقاً لنظرية معايرة اليد-العين، يمكن الحصول على توجيه كل مجموعة من النقاط السحابية (ثلاث زوايا محددة) في نظام إحداثيات قاعدة الذراع الروبوتية، مما يوفر توجيهاً أولياً جيداً لخياطة النقاط السحابية التالية.</p>
<p>لنفترض أن عدد نقاط السحابة التي حصلت عليها الكاميرا في زوايا محددة هو <span class="nodecor"><span class="math inline">\(n_1\)</span></span>، <span class="nodecor"><span class="math inline">\(n_2\)</span></span> و <span class="nodecor"><span class="math inline">\(n_3\)</span></span> على التوالي. إحداثيات النقطة الكلية الخاصة بالنظام الأساسي هي <span class="nodecor"><span class="math inline">\({^R}P_A^{(k)}\)</span></span>، <span class="nodecor"><span class="math inline">\({^R}P_B^{(k)}\)</span></span> و <span class="nodecor"><span class="math inline">\({^R}P_C^{(k)}\)</span></span> (A، B، C تمثل فقط مواقع مختلفة للكاميرا) تلك المتعلقة بنظام إحداثيات الكاميرا هي <span class="nodecor"><span class="math inline">\({^{CA}}P^{(k)}\)</span></span>، <span class="nodecor"><span class="math inline">\({^{CB}}P^{(k)}\)</span></span> و <span class="nodecor"><span class="math inline">\({^{CC}}P^{(k)}\)</span></span> (CA هو تمثيل نظام إحداثيات الكاميرا في الموقع A، CB، CC مماثلة)، على التوالي. استبدل جميع قيم k للحصول على النقاط النسبية لنظام الإحداثيات الأساسي: <span class="math display">\[\left[
\begin{array}{c}
    {^R}P_A^{(1)} \\
    {^R}P_A^{(2)} \\
    \vdots \\
    {^R}P_A^{(\textnormal{$n_1$})} 
\end{array}
\right]
= \ 
^{\ R}_{CA}T
\left[
\begin{array}{c}
    ^{CA}P^{(1)} \\
    ^{CA}P^{(2)} \\
    \vdots \\
    ^{CA}P^{(\textnormal{$n_1$})} 
\end{array}
\right]\]</span></p>
<p><span class="math display">\[\left[
\begin{array}{c}
    {^R}P_B^{(1)} \\
    {^R}P_B^{(2)} \\
    \vdots \\
    {^R}P_B^{(\textnormal{$n_2$})} 
\end{array}
\right]
= \ 
^{\ R}_{CB}T
\left[
\begin{array}{c}
    ^{CB}P^{(1)} \\
    ^{CB}P^{(2)} \\
    \vdots \\
    ^{CB}P^{(\textnormal{$n_2$})} 
\end{array}
\right]\]</span></p>
<p><span class="math display">\[\left[
\begin{array}{c}
    {^R}P_C^{(1)} \\
    {^R}P_C^{(2)} \\
    \vdots \\
    {^R}P_C^{(\textnormal{$n_3$})} 
\end{array}
\right]
= \ 
^{\ R}_{CC}T
\left[
\begin{array}{c}
    ^{CC}P^{(1)} \\
    ^{CC}P^{(2)} \\
    \vdots \\
    ^{CC}P^{(\textnormal{$n_3$})} 
\end{array}
\right]\]</span></p>
<p>في هذا السياق، <span class="nodecor"><span class="math inline">\(_{CA}^{\ R}T\)</span></span> = <span class="nodecor"><span class="math inline">\(_{CB}^{\ R}T\)</span></span> = <span class="nodecor"><span class="math inline">\(_{CC}^{\ R}T\)</span></span> = <span class="nodecor"><span class="math inline">\(_{C}^{R}T\)</span></span> = <span class="nodecor"><span class="math inline">\(_{F}^{R}T^{(i)}\)</span></span> <span class="nodecor"><span class="math inline">\(\widehat{{_{C}^{F}}T}\)</span></span>، هي مصفوفة التحويل بين نظام إحداثيات الكاميرا ونظام إحداثيات القاعدة للذراع الروبوتية المستنتجة في القسم III-B. وبالتالي، يمكن كتابة السحابة النقطية المخيطة <span class="nodecor"><span class="math inline">\(P_s\)</span></span> كما يلي: <span class="math display">\[\setlength{\arraycolsep}{1.8pt}
    P_s
=
\left[
\begin{array}{c|c|c}
    \left[
    \begin{array}{c} 
    {^R}P_A^{(1)} \\
    {^R}P_A^{(2)} \\
    \vdots \\
    {^R}P_A^{(\textnormal{$n_1$})} 
    \end{array}
    \right]^T
    \!&amp; 
    \left[
    \begin{array}{c} 
    {^R}P_B^{(1)} \\
    {^R}P_B^{(2)} \\
    \vdots \\
    {^R}P_B^{(\textnormal{$n_3$})} 
    \end{array}
    \right]^T
    &amp; 
    \left[
    \begin{array}{c} 
    {^R}P_C^{(1)} \\
    {^R}P_C^{(2)} \\
    \vdots \\
    {^R}P_C^{(\textnormal{$n_3$})} 
    \end{array} 
    \right]^T
\end{array}
\right]^T\]</span></p>
<!-- ... باقي النص كما هو ... -->
</body>
</html>
