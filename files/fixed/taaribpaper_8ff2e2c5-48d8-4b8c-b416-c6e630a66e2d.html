<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque, University of Montreal &amp; Mila firstname.lastname@umontreal.ca Shunichi Akatsuka, Hitachi, Ltd. shunichi.akatsuka.bo@hitachi.com Aaron Courville University of Montreal &amp; Mila aaron.courville@umontreal.ca">
  <title>تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">تَشْكِيلُ الاِسْتِجَابَةِ الأَمْثَلِ</h1>
<p class="author"><span class="nodecor">Milad Aghajohari</span>, <span class="nodecor">Tim Cooijmans</span>, <span class="nodecor">Juan Agustin Duque</span>,<br />
<span class="nodecor">University of Montreal &amp; Mila</span><br />
<code>firstname.lastname@umontreal.ca</code><br />
<strong><span class="nodecor">Shunichi Akatsuka</span></strong>,<br />
<span class="nodecor">Hitachi, Ltd.</span><br />
<span class="nodecor">shunichi.akatsuka.bo@hitachi.com</span><br />
<strong><span class="nodecor">Aaron Courville</span></strong><br />
<span class="nodecor">University of Montreal &amp; Mila</span><br />
<code>aaron.courville@umontreal.ca</code><br /></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>نستكشف تحدي تعلم التعزيز العميق متعدد الوكلاء في بيئات تنافسية جزئية، حيث تواجه الأساليب التقليدية صعوبات في تعزيز التعاون المبني على المعاملة بالمثل. يتعلم وكلاء <span class="nodecor">LOLA</span> و<span class="nodecor">POLA</span> سياسات تعاونية قائمة على المعاملة بالمثل عن طريق التفاضل عبر عدد محدود من خطوات التحسين المستقبلية التي يقوم بها الخصم. إلا أن لهذه التقنيات قيداً أساسياً: نظراً لاعتمادها على عدد قليل من خطوات التحسين، يمكن أن يستغلها خصم قادر على اتخاذ خطوات إضافية لتعظيم عائده. استجابةً لذلك، نقدم نهجاً جديداً يسمى تشكيل الاستجابة المثلى (<span class="nodecor">BRS</span>)، الذي يوظف خصماً يحاكي الاستجابة المثلى، ويطلق عليه "المحقِّق". لتكييف المحقِّق مع سياسة الوكيل في الألعاب المعقدة، نقترح آلية تكيف قابلة للتفاضل تعتمد على الحالة، ميسرة عبر "الإجابة على الأسئلة" لاستخراج تمثيل للوكيل بناءً على سلوكه في مواقف بيئية محددة. للتحقق من صحة طريقتنا تجريبيًّا، نعرض أدائها المحسَّن مقابل خصم <span class="nodecor">Monte Carlo Tree Search (MCTS)</span>، الذي يعمل كتقريب للاستجابة المثلى في لعبة القطع النقدية. يوسّع هذا العمل نطاق تطبيق تعلم التعزيز متعدد الوكلاء في البيئات التنافسية الجزئية ويمهد طريقًا جديدًا نحو تحقيق رفاهية اجتماعية أفضل في الألعاب ذات المنفعة الجماعية.</p>
<h1 id="sec:intro">مُقَدِّمَة</h1>
<p>مكنت خوارزميات التعلم المعزز متعدد الوكلاء من تحقيق أداء متميز في ألعاب معقدة وعالية الأبعاد مثل لعبة الذهاب (<span class="nodecor">AlphaGo</span>) وستار كرافت (<span class="nodecor">AlphaStar</span>). الهدف الأسمى من التعلم المعزز هو تدريب وكلاء قادرين على مساعدة البشر في حل المشكلات الصعبة. لا محالة، سيحتاج هؤلاء الوكلاء إلى الاندماج في سيناريوهات الحياة الواقعية التي تتطلب التفاعل مع البشر ووكلاء تعلم آخرين. فعلى الرغم من تفوق التدريب متعدد الوكلاء في البيئات التعاونية أو التنافسية الكاملة، غالبًا ما يفشل في اكتشاف تعاون قائم على المعاملة بالمثل في البيئات التنافسية الجزئية. مثال بارز على ذلك انكفاء وكلاء التعلم المعزز متعدد الوكلاء عن تعلم سياسات مثل المعاملة بالمثل في معضلة السجين المتكررة (<span class="nodecor">LOLA</span>).</p>
<p>رغم الطابع التمثيلي لألعاب المنفعة الجماعية الشائعة مثل معضلة السجين، فإن مثل هذه المشكلات تتكرر في المجتمع والطبيعة. فكِّر في سيناريو تحاول فيه دولتان (وكلاء) تعظيم إنتاجهما الصناعي، مع ضمان مناخ عمل مناسب يقلل الانبعاثات الكربونية. من ناحية، ترغب كل دولة في أن تفي الدولة الأخرى بالتزاماتها البيئية؛ ومن ناحية أخرى، يغريهما إصدار المزيد من الكربون لتحقيق عوائد صناعية أكبر. ستجبر المعاهدة الفعالة كل دولة—من خلال تهديد بالعقوبات—على الالتزام بالحدود المتفق عليها للانبعاثات. وإذا أخفق الوكلاء في تطوير استراتيجيات كالمعاملة بالمثل، فمن المرجح أن ينتهي بهم المطاف بتصعيد متبادل مؤسف في استهلاك الطاقة والانبعاثات.</p>
<p>قدمت (<span class="nodecor">LOLA</span>) خوارزمية تعلم مع مراعاة تعلم الخصم، نجحت في استكشاف سلوك المعاملة بالمثل في معضلة السجين المتكررة عبر التفاضل عبر خطوة واحدة بسيطة يتخذها الخصم. بناءً على ذلك، قدمت (<span class="nodecor">POLA</span>) تحديث سياسة الخصم القريب، الذي يعزز تعامل LOLA عبر افتراض تحديث بسيط لسياسة الخصم وتمكين تدريب الشبكات العصبية في ألعاب أكثر تعقيدًا، مثل لعبة القطع النقدية. وإلى حد علمنا، يعدّ تعلم مع مراعاة تعلم الخصم القريب الأسلوب الوحيد الذي يدرب موثوقًا وكلاء يتبعون تعاونًا قائمًا على المعاملة بالمثل في هذه اللعبة.</p>
<p>وعلى الرغم من نجاحه في لعبة القطع النقدية، فإن لتعلم مع مراعاة تعلم الخصم القريب حدودًا. فاستمداده لمعرفة الخصم يقتصر على عدد محدود من خطوات التحسين المستقبلية، مما يجعله عرضة للاستغلال من قبل خصوم يمكنهم اتخاذ مزيد من التحسينات. تُظهر تحليلاتنا أن الخصم الذي يتعلم خصيصًا لتعظيم عائده ضد سياسة ثابتة مدربة بواسطة هذا الأسلوب يستغل الوكيل القائم على LOLA/POLA. كما أن هذه القيود تعيق قابلية التوسع حين يكون الخصم شبكة عصبية معقدة تتطلب العديد من خطوات التحسين لمحاكاة تعلمها الحقيقي.</p>
<p>في هذه الورقة نقدم نهجًا جديدًا نسميه تشكيل الاستجابة المثلى. يرتكز على بناء خصم يقترب من سياسة الاستجابة المثلى ضد وكيل معين، وهو ما نسميه "المحقِّق". يوضّح الشكل [fig:cobalt] الإطار العام: يخضع المحقِّق للتدريب ضد مجموعة متنوعة من وكلاء التدريب، ثم ندرِّب الوكيل عبر التفاضل عبر المحقِّق. على عكس LOLA وPOLA اللتين تفترضان عددًا قليلاً من خطوات التحسين المستقبلية للخصم، يعتمد أسلوبنا على أن يولد المحقِّق استجابة مثلى للوكيل الحالي من خلال التكيف الديناميكي للسياسة.</p>
<p>نعتمد على التجارب في معضلة السجين المتكررة ولعبة القطع النقدية. وبما أن نتائج الوكيل تعتمد على قدرته في مواجهة خصم معتدل الاستجابة، فإن المقارنة المنطقية تكون مقابل خصم يستجيب بأفضل شكل ممكن، وهو ما نقربه عبر بحث شجرة مونت كارلو. نظهر أنه بينما لا يتعاون MCTS بالكامل مع وكلاء LOLA/POLA، فإنه يتعاون بالكامل مع وكلاء تشكيل الاستجابة المثلى.</p>
<p><strong>المُسَاهَمَات الرَئِيسِيَّة</strong>: نُلَخِّص مساهماتنا فيما يلي:</p>
<ul>
<li><p>نُبيِّن أن الخصم المُحسَّن بواسطة بحث شجرة مونت كارلو لا يتعاون مع وكلاء LOLA/POLA، بل يستغلهم لتحقيق عوائد أعلى من التعاون الكامل.</p></li>
<li><p>لتجاوز هذا الضعف، نقدم أسلوب تشكيل الاستجابة المثلى، الذي يدرب وكيلًا عبر التفاضل عبر خصم يحاكي الاستجابة المثلى ("المحقِّق"). نُثبت تجريبيًا أن وكلاء BRS يتعاونون بصورة كاملة كما هو مبين في الشكل [fig:coin_main_compare].</p></li>
<li><p>نقترح كذلك آلية تكيف قابلة للتفاضل وواعية بالحالة للمحقِّق، تُمكّنه من التكيف مع سياسة الوكيل.</p></li>
</ul>
<h1 id="sec:background">الخَلْفِيَّة</h1>
<h2 id="تعلم-التعزيز-المتعدد-العوامل">تَعَلُّم التَّعْزِيز المُتَعَدِّد العَوامِل</h2>
<p>تُعَرَّف لعبة ماركوف متعددة العوامل بالرمز <span class="math inline">\(\bm{(} N, \mathcal{S},\left\{\mathcal{A}^i\right\}_{i=1}^N, \mathbb{P},\left\{r^i\right\}_{i = 1}^N, \gamma \bm{)}\)</span>. هنا، <span class="math inline">\(N\)</span> تمثل عدد العوامل، <span class="math inline">\(\mathcal{S}\)</span> فضاء الحالات، و<span class="math inline">\(\mathcal{A}:=\mathcal{A}^1 \times \cdots \times \mathcal{A}^N\)</span> مجموعة الأفعال لكل عامل. احتمالات انتقال الحالة ممثلة بـ <span class="math inline">\(\mathbb{P}: \mathcal{S} \times \mathcal{A} \rightarrow \Delta(\mathcal{S})\)</span> والمكافأة بـ <span class="math inline">\(r^i: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}\)</span>. أخيرًا، <span class="math inline">\(\gamma \in [0,1]\)</span> هو عامل الخصم. يحاول كل عامل تعظيم عائده <span class="math inline">\(R^i = \sum_{t=0}^\infty \gamma^t r^i_t\)</span>. تمثل سياسة العامل <span class="math inline">\(i\)</span> بـ <span class="math inline">\(\pi^{i}_{\theta_{i}}\)</span> حيث <span class="math inline">\(\theta_i\)</span> معاملات الشبكة العصبية. يتم تدريب هذه السياسات باستخدام مقدرات التدرج مثل REINFORCE (<span class="nodecor">reinforce</span>).</p>
<h2 id="المعضلات-الاجتماعية-ومعضلة-السجين-المتكررة">المُعْضِلات الاِجْتِمَاعِيَّة ومُعْضِلَة السَّجِين المُتَكَرِّرَة</h2>
<p>في الألعاب ذات المنفعة الجماعية تظهر معضلات اجتماعية عندما يسعى كل وكيل لتعظيم مكافأته الشخصية فيقوّض الناتج الجماعي أو الرفاهية الاجتماعية. يصبح هذا جليًا حين تكون النتيجة الجماعية أدنى من تلك التي يمكن تحقيقها بالتعاون الكامل. توضح نماذج نظرية مثل معضلة السجين كيف أن كل مشارك، رغم أنه أفضل حالًا حين يعترف، يؤدي اعترافه إلى مكافأة جماعية أقل مما لو بقي صامتًا.</p>
<p>في معضلة السجين المتكررة (<span class="nodecor">IPD</span>)، لا يعود الانسحاب المطلق الاستراتيجية المثلى. فعند مواجهة خصم يتبع استراتيجية الرد بالمثل (<span class="nodecor">TFT</span>)، يؤدي التعاون المستمر إلى عوائد أعلى. قد نتوقع أن يكتشف وكلاء MARL—المصممون لتعظيم عائدهم الفردي—استراتيجية <span class="nodecor">TFT</span> باعتبارها توازن ناش الذي يعزز كلاً من العوائد الفردية والجماعية، ولا يضفي حافزًا لتغيير السياسة. ومع ذلك، أظهرت الملاحظات التجريبية أن الوكلاء المدربين لتعظيم عائدهم الخاص يميلون عادة إلى الانسحاب المطلق.</p>
<p>يمثل هذا أحد التحديات الرئيسية لـ MARL في بيئات المنفعة الجماعية: يتجاهل الوكلاء أثناء التدريب أن الوكلاء الآخرين يتعلمون أيضًا. فإذا كان الهدف الرفاهية الاجتماعية، يمكن مشاركة المكافآت بين الوكلاء أثناء التدريب، وهو ما يضمن تعاونًا كاملًا في إعداد IPD. لكن هذا لا يكفي عندما نبتغي التعاون المبني على المعاملة بالمثل—من الضروري ابتكار خوارزميات قادرة على اكتشاف سياسات تحفز الخصم على التعاون من أجل تعظيم عائده الخاص.</p>
<h1 id="sec:relatedworks">الأَعْمَال ذات الصِّلَة</h1>
<p>تحاول (<span class="nodecor">LOLA</span>) تشكيل الخصم عبر أخذ تدرج لقيمة الوكيل بالنظر إلى خطوة واحدة للأمام من معاملات الخصم. بدلًا من تحسين <span class="math inline">\(V^1(\theta_i^1, \: \theta_i^2)\)</span> فقط، تهدف (

... باقي النص كما هو ... -->
</body>
</html>