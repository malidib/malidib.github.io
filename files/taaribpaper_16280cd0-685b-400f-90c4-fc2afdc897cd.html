<!DOCTYPE html>
<html lang="ar" dir="rtl">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Yihua Shao*, Hongyi Cai*, Wenxin Long, Weiyi Lang, Zhe Wang, Haoran Wu, Yan Wang, Yang Yang^{1}, Member, IEEE, Zhen Lei^{3}, Fellow, IEEE">
  <title>AccidentBlip2: كشف الحوادث باستخدام Multi-View MotionBlip2</title>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <style>
    @import url('https://fonts.googleapis.com/css2?family=Cairo:wght@400;700&display=swap');
    body {
      direction: rtl;
      font-family: 'Cairo', 'Segoe UI', Tahoma, Geneva, Verdana, Arial, sans-serif;
      font-size: 22px;
      background: #f8f9fa;
      color: #222;
      margin: 0;
      padding: 0;
      line-height: 1.7;
    }
    header {
      background: linear-gradient(90deg, #0d6efd 0%, #6c63ff 100%);
      color: #fff;
      padding: 40px 0 30px 0;
      text-align: center;
      box-shadow: 0 2px 8px rgba(0,0,0,0.04);
      margin-bottom: 40px;
    }
    h1.title {
      font-size: 2.5em;
      font-weight: 700;
      margin-bottom: 0.3em;
      letter-spacing: 1px;
    }
    .author {
      font-size: 1.1em;
      margin-top: 0.5em;
      color: #e0e0e0;
    }
    h1, h2, h3 {
      color: #0d6efd;
      font-weight: 700;
      margin-top: 1.5em;
      margin-bottom: 0.7em;
      line-height: 1.2;
    }
    h1 {
      font-size: 2em;
      border-bottom: 2px solid #0d6efd;
      padding-bottom: 0.2em;
      margin-bottom: 1em;
    }
    h2 {
      font-size: 1.4em;
      border-right: 4px solid #6c63ff;
      padding-right: 0.5em;
      margin-bottom: 0.7em;
    }
    h3 {
      font-size: 1.1em;
      color: #495057;
      margin-bottom: 0.5em;
    }
    p {
      margin: 0 0 1.2em 0;
      text-align: justify;
    }
    ul, ol {
      margin: 0 0 1.2em 2em;
      padding-right: 1.5em;
    }
    ul li, ol li {
      margin-bottom: 0.5em;
    }
    a {
      color: #0d6efd;
      text-decoration: underline;
      word-break: break-all;
    }
    a:hover {
      color: #6c63ff;
      text-decoration: none;
    }
    code, pre {
      background: #f1f3f4;
      color: #d63384;
      font-family: 'Cairo', 'Consolas', 'monospace';
      font-size: 0.95em;
      border-radius: 4px;
      padding: 2px 6px;
    }
    .math.inline {
      color: #198754;
      font-weight: 600;
      font-size: 1em;
      background: #e9f7ef;
      border-radius: 3px;
      padding: 0 4px;
    }
    .math.display {
      display: block;
      background: #e9f7ef;
      color: #198754;
      border-radius: 5px;
      padding: 12px 18px;
      margin: 18px 0;
      font-size: 1.1em;
      direction: ltr;
      text-align: left;
      overflow-x: auto;
    }
    .nodecor {
      text-decoration: none !important;
      color: inherit;
      font-weight: 600;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 1.5em 0;
      background: #fff;
      box-shadow: 0 1px 4px rgba(0,0,0,0.03);
    }
    th, td {
      border: 1px solid #dee2e6;
      padding: 0.7em 1em;
      text-align: center;
    }
    th {
      background: #e3e7fd;
      color: #0d6efd;
      font-weight: 700;
    }
    blockquote {
      border-right: 5px solid #0d6efd;
      background: #f1f3f4;
      color: #495057;
      margin: 1.5em 0;
      padding: 1em 1.5em;
      border-radius: 6px;
      font-size: 1.05em;
    }
    footer {
      margin: 40px auto 20px auto;
      max-width: 900px;
      color: #6c757d;
      text-align: center;
      font-size: 0.95em;
    }
    @media (max-width: 700px) {
      body { font-size: 18px; }
      header { padding: 25px 0 18px 0; }
      h1.title { font-size: 1.5em; }
      h1 { font-size: 1.2em; }
      h2 { font-size: 1em; }
      .author { font-size: 0.95em; }
      table, th, td { font-size: 0.95em; }
    }
  </style>
</head>
<body>
<header>
  <h1 class="title">AccidentBlip2: كشف الحوادث باستخدام <span class="nodecor">Multi-View MotionBlip2</span></h1>
  <p class="author"><span class="nodecor">Yihua Shao*</span>, <span class="nodecor">Hongyi Cai*</span>, <span class="nodecor">Wenxin Long</span>, <span class="nodecor">Weiyi Lang</span>, <span class="nodecor">Zhe Wang</span>, <span class="nodecor">Haoran Wu</span>, <span class="nodecor">Yan Wang</span>, <span class="nodecor">Yang Yang<span class="math inline">\(^{1}, Member, IEEE\)</span></span>, <span class="nodecor">Zhen Lei<span class="math inline">\(^{3}, Fellow, IEEE\)</span></span></p>
</header>

<main style="max-width: 900px; margin: auto; background: #fff; border-radius: 12px; box-shadow: 0 2px 12px rgba(0,0,0,0.04); padding: 2.5em 2.5em 2em 2.5em;">

<section>
<h1 id="ملخص">مُلَخَّص</h1>
<p>أظهرت النماذج اللغويّة الكبيرة مُتعدِّدة الوسائط (MLLMs) قدراتٍ مميّزة في العديد من مهام الفهم مُتعدِّد الوسائط. لذلك، نستفيد هنا من قدرة هذه النماذج على توصيف البيئة وفهم المشاهد في بيئات النقل المعقّدة. في هذه الورقة، نقترح <span class="nodecor">AccidentBlip2</span>، نموذجًا لغويًّا كبيرًا مُتعدِّد الوسائط قادرًا على التنبّؤ في الوقت الحقيقي باحتماليّة وقوع حادث. يتضمّن نهجُنا استخراج السمات اعتمادًا على التسلسل الزمني لصور الإحاطة المكوّنة من ستّة اتجاهات، ثم إجراء الاستدلال الزمني باستخدام إطار <span class="nodecor">BLIP‑2</span> عبر مُحوِّل الرؤية. بعد ذلك، نُدخِل التمثيل الزمني الناتج إلى نماذج اللغة الكبيرة مُتعدِّدة الوسائط للاستدلال وتحديد ما إذا كان من المحتمل وقوع حادث. ونظرًا لأن <span class="nodecor">AccidentBlip2</span> لا يعتمد على صور <span class="nodecor">BEV</span> أو بيانات <span class="nodecor">LiDAR</span>، فإن عدد المعلمات اللازمة للاستدلال وتكلفة المعالجة ينخفضان بشكلٍ كبير، كما لا يتطلّب موارد تدريب عالية. يتفوّق <span class="nodecor">AccidentBlip2</span> على الحلول الحاليّة في مجموعة بيانات <span class="nodecor">DeepAccident</span>، ويمكن أن يشكّل أيضًا معيارًا مرجعيًّا للتنبّؤ بحوادث القيادة الذاتيّة من الطرف إلى الطرف. سيُصدر الكود على: <a href="https://github.com/YihuaJerry/AccidentBlip2.git" class="uri">https://github.com/YihuaJerry/AccidentBlip2.git</a></p>
</section>

<section>
<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>تُعدّ النماذجُ اللغويّة الكبيرة مُتعدِّدة الوسائط القادرة على اكتشاف الحوادث وتحديدها بدقّة ذات أهميّة بالغة لمجال السلامة في القيادة الذاتيّة. أجرى عددٌ من الباحثين أعمالًا سابقة في كشف سلوك المركبات. عادةً ما تتصرّف المركبات وفقًا للمحيط المروري وقواعد المرور، حيث قد تتوقّف أو تُغيِّر مساراتها أو حتى تتراجع إلى الخلف في أنظمة المرور المعقّدة. نأخذ هذه الظواهر جميعًا بعين الاعتبار عند نمذجة إدراك بيئة المركبة. ومع ذلك، فإن التعقيد الشديد لنظام المرور يُفضي إلى صعوبات في نمذجة الإدراك، ما يجعل طرق الإدراك التقليديّة تعتمد في كثيرٍ من الأحيان على استنتاجات خاطئة.</p>
<p>مع ذلك، ما تزال الطرائق القائمة على الرؤية فقط لكشف الحوادث في المشاهد المعقّدة محدودة. تتفوّق النماذج اللغويّة الكبيرة مُتعدِّدة الوسائط في فهم المشاهد المعقّدة، ما يجعلها ملائمة تمامًا لمهام القيادة الذاتيّة في البيئات المعقّدة. تعتمد الأعمال الحاليّة عادةً على نماذج مُتعدِّدة الوسائط لكشف المركبات والمشاة وغيرها في البيئة، ممّا يُعزّز الاعتماديّة في القيادة الذاتيّة. غير أنّه في سيناريوهات المرور المعقّدة، تحدث الحوادث بشكلٍ متكرّر؛ لذا يمكن لوكيل MLLM المثبّت داخل المركبة أن يستخدم قدرته على فهم السياق لتحديد معلومات الحوادث المحيطة وتقديم تنبيهات مبكّرة لتعزيز السلامة.</p>
<p>في هذه الورقة، نقترح <span class="nodecor">AccidentBlip2</span>، نموذجًا لغويًّا كبيرًا مُتعدِّد الوسائط مُخصّصًا للاستدلال على الحوادث في سياق السلاسل الزمنيّة. نهدف من خلاله إلى تعزيز التطبيق العملي لهذه النماذج في بيئات المرور المعقّدة. باعتمادنا على MLLMs، أنشأنا إطارًا لتجميع مدخلات صور الكاميرا ذات الاتّجاهات الستّة ضمن إدخال زمني مُتعدِّد الوسائط، ثم يستخرج مُحوِّل الرؤية الرموزَ الزمنيّة الخاصّة بكل إطار. تُنقَل هذه الرموز بعد ذلك إلى مُشفِّر الرؤية المدمج ضمن MLLM، حيث تُستثمر قدرات التفكير مُتعدِّد الوسائط للتنبّؤ بما إذا كان قد وقع حادث. كما يمكن للنموذج التفاعلُ مع السائق عبر واجهة لغويّة لاستشعار بيئة الطريق بشكلٍ أدقّ وتنبيه السائق لأيّ مخاطر محتملة.</p>
<p>إلى جانب تحليل المشهد المحيط لمركبةٍ واحدة، طوّرنا نظامًا تعاونيًّا لإدراك بيئة عدّة مركبات من الطرف إلى الطرف، لتعويض النقاط العمياء والنواقص في إدراك المركبة الوحيدة. مدّدنا اختبارات بيئة المركبة الواحدة إلى سيناريوهات القيادة الشاملة من الطرف إلى الطرف، وقسنا دقّة التنبّؤ بالحوادث وقدرة النظام على الربط بين الذات ومركباتٍ متعدّدة. عمومًا، تبرز مساهماتُنا الرئيسيّة في النقاط التالية:</p>
<ul>
  <li>نقترح وكيلًا جديدًا للاستدلال على حوادث المرور الرؤيويّة، يُعنى بالتنبّؤ بالحوادث المحتملة وتنبيه السائقين في بيئات القيادة المعقّدة.</li>
  <li>نقدّم إطارًا للتنبّؤ بالحوادث من الطرف إلى الطرف قائمًا على النماذج اللغويّة الكبيرة مُتعدِّدة الوسائط، يُمكّن هذه النماذج من تحديد وقوع حادث أو وجود خطر في أو حول نظام مركباتٍ متعدّدة بالاستناد إلى معلومات المناظير لكل مركبة.</li>
</ul>
</section>

<section>
<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<h2 id="نموذج-اللغة-الكبير-متعدد-الوسائط">نموذج اللغة الكبير مُتعدِّد الوسائط</h2>
<p>مع ظهور GPT‑4، بدأت العديد من النماذج اللغويّة الكبيرة في استكشاف قدرات المعالجة مُتعدِّدة الوسائط. تستفيد هذه النماذج من المعلومات البصريّة واللفظيّة معًا لتعزيز قدراتها في الفهم والاندماج بين وسائط مختلفة. على سبيل المثال، قاد كلٌّ من GPT‑4V وLLaVA‑v1.5 تطويرَ نماذج لغويّة كبيرة بصريّة‑لفظيّة مُتعدِّدة الوسائط تناسب سيناريوهات متنوّعة. علاوةً على ذلك، تمّ تحسين نماذج مثل OWL‑ViT وQwen‑VL—المتخصّصة في كشف الأهداف—لمعالجة مشاهد معيّنة عبر دمج مجموعات بيانات موجهة، ما يُمكّنها من تنفيذ مهام بصريّة محدّدة بأوامر المستخدم. وفيما يتعلّق بالبيانات الزمنيّة، قدّم الباحثون أيضًا Video‑LLaVA وغيرها من النماذج التي تجمع بين معالجة الصور والفيديو في نموذجٍ لغوي كبير. وهناك أيضًا جهود لإدماج الوسائط الصوتيّة مثل Qwen‑Audio، ما يمهّد الطريق لتطوير نماذج لغويّة كبيرة سمعيّة مُتعدِّدة الوسائط. تُشكّل هذه التكاملات المتقاطعة للوسائط في النماذج اللغويّة الكبيرة حلولًا عالية الجودة لمهام متنوّعة في العالم الحقيقي.</p>
<h2 id="نماذج-اللغة-الكبيرة-للقيادة-الآلية">نماذج اللغة الكبيرة للقيادة الآليّة</h2>
<p>مؤخّرًا ومع التطوّر السريع في تقنيات القيادة الذاتيّة، بدأ ظهور تطبيقات النماذج اللغويّة الكبيرة في مجالات القيادة الذكيّة والتليماتيكس. في عام <span class="nodecor">2023</span>، ظهر نموذج <span class="nodecor">UniAD</span> الذي يُعدّ أوّل تطبيقٍ شامل للنماذج اللغويّة الكبيرة في القيادة الذاتيّة، إذ يدمج ثلاثة مكوّنات أساسيّة: الإدراك، واتّخاذ القرار، والتخطيط ضمن بنية شبكة واحدة، ما يُقلّل فقدان المعلومات بين الوحدات المنفصلة. كما طُرح نموذج مُتعدِّد الوسائط يُسمّى <span class="nodecor">CAVG</span>، يركّز على نيّة السائق، ويتكوّن من خمسة مُشفِّرات متخصّصة: نصّي، عاطفي، بصري، سياقي وعبر الوسائط، مع مفكِّكاتها، لتمكين النموذج من التعامل مع وسائط وأبعاد مختلفة لمهام القيادة الذاتيّة. في مهام الإدراك، يستخدم <span class="nodecor">DriveGPT4</span> خوارزمية <span class="nodecor">YOLOv8</span> لاكتشاف الأهداف الشائعة مثل المركبات في كلّ إطار فيديو، ثم يمرّر الإحداثيات إلى <span class="nodecor">ChatGPT</span> كمعلوماتٍ لغويّة. ومع ذلك، نظرًا لأن <span class="nodecor">DriveGPT4</span> يقتصر على وعيٍ بيئي لمركبةٍ واحدة، فإنه غير مناسب للتطبيقات من الطرف إلى الطرف في بيئات المركبات المتعدّدة.</p>
<h2 id="حكم-الحوادث">الاستدلال على الحوادث</h2>
<p>يُعدّ التنبّؤ بالحوادث المروريّة من أكثر مجالات البحث نشاطًا في سلامة القيادة الذاتيّة. اعتمد العديد من الباحثين على أساليب رؤية تقليديّة ومستشعرات أماميّة للمركبة مُقترنة بشبكات عصبيّة زمنيّة—مثل LSTM أو RNN—لتحذير السائقين من احتماليّة وقوع حادث (<span class="nodecor">c10</span>). ومع ذلك، تقتصر هذه الطرائق على استشعار سلوك المركبات القريبة فقط، وتفشل في تقدير المخاطر ضمن بيئات المرور المعقّدة، خصوصًا في ظروف الطقس القاسية. مع ظهور النماذج اللغويّة الكبيرة، حاول بعض الباحثين استخدامها في إدراك الحوادث، مثل “النموذج العملاق للحوادث”، ولكن نظرًا لاعتماد تلك النماذج على مجموعات بيانات مُعَدّة مسبقًا، فإنها لا تمتلك قدرةً كافية على تعميم التفكير في بيئاتٍ جديدة تمامًا تحت ظروفٍ معقّدة.</p>
</section>

<section>
<h1 id="المنهجية">المنهجيّة</h1>
<p>لا يستطيع <span class="nodecor">BLIP‑2</span> معالجة مدخلات الصور المكوّنة من ستّة اتجاهات مباشرةً، ولا يمتلك القدرة على استدلال السلاسل الزمنيّة المستخلصة من هذه المدخلات. في هذا القسم، نستخدم بيانات المحاكاة وتقنيات الإسقاط/تحويل الرؤية من المحاكي (<span class="nodecor">c11</span>) لتعزيز قدرة <span class="nodecor">BLIP‑2</span> على معالجة الصور الزمنيّة مُتعدِّدة الاتجاهات. نأمل من خلال ذلك أن يصبح <span class="nodecor">BLIP‑2</span> المدرَّب قادرًا على التفكير في التسلسلات الزمنيّة المكوّنة من ستّة اتجاهات للاستدلال. تجدر الإشارة إلى أنّ إطارنا قابلٌ للتوسعة ليشمل سيناريوهات محاكاة القيادة الذاتيّة الطرفية لمركباتٍ متعدّدة.</p>
<h2 id="مدخلات-متعددة-الوجهات-والاستدلال-الزمني">مدخلات مُتعدِّدة الاتجاهات والاستدلال الزمني</h2>
<p>نقدّم إطارًا إدراكيًّا يستفيد من النماذج اللغويّة الكبيرة مُتعدِّدة الوسائط، ويتكوّن من مكوّنين أساسيّين: استخراج السمات من مدخلات الصور مُتعدِّدة الاتجاهات والاستدلال الزمني. في المرحلة الأولى، نستخدم <span class="nodecor">ViT‑g/14</span> للتعامل مع ستّة اتجاهات واستخراج الميزات البصريّة ذات الصلة من كلّ منظور. في المرحلة الثانية، نعتمد على استعلاماتٍ تحمل معلوماتٍ سياقيّة وزمنيّة مرتبطة بالتسلسل الزمني للإطارات. من خلال دمج هاتين المرحلتين، يُتيح إطارُنا القدرةَ على الإدراك والاستدلال الشامل مُتعدِّد الوسائط على امتداد الزمن.</p>
<p>يعتمد مُشفِّرُنا البصري على <span class="nodecor">ViT‑g/14</span> المأخوذ من <span class="nodecor">EVA‑CLIP</span> (<span class="nodecor">c13</span>). يقرأ مُحوِّل الرؤية أوّلًا صور الاتّجاهات الستّ المُلتقطة من الكاميرات، ويُغيِّر حجم كلٍّ منها لاستخراج الميزات بشكلٍ منفصل. نستخدم مُحوِّل الرؤية المدرَّب مسبقًا كعمودٍ فقري لاستخراج مجموعة الميزات <span class="math inline">\(f_t\)</span> للاتجاهات الستّ عند كل إطارٍ زمني <span class="math inline">\(t\)</span>، ثم نُدخِل هذه الميزات إلى الطبقة الخطيّة في <span class="nodecor">Q‑Former</span> لإجراء عملية الاستدلال. قبل ذلك، يقوم مُحوِّل الرؤية بتسطيح ميزات كلّ صورة على بعدٍ واحد، ثم يدمج ميزات الاتّجاهات الستّ ليُشكّل التمثيل الزمني <span class="math inline">\(f_t\)</span> الذي يُدخَل في آلية الانتباه المتقاطع (<em>Cross‑Attention</em>) في <span class="nodecor">Q‑Former</span>. في الوقت نفسه، يُدخَل الاستعلامُ القابل للتعلّم <span class="math inline">\(Q_t\)</span> إلى طبقة الانتباه الذاتي في <span class="nodecor">Q‑Former</span>، ثم يُمرَّر الخرجُ الجديد عبر طبقة التغذية الأماميّة.</p>
<p>نتيجةً لخصوصيّة بيانات القيادة الذاتيّة الزمنيّة، نقترح آلية الانتباه الذاتي الزمني (<em>Temporal Self‑Attention</em>)، حيث يلتقط الاستعلامُ الزمني <span class="math inline">\(Q_{n}\)</span> معلوماتِ الصور مُتعدِّدة الاتجاهات لكلّ إطار، ثم يُدخَل في <span class="nodecor">Q‑Former</span> المقابل للحصول على الاستعلام التالي <span class="math inline">\(Q_{n+1}\)</span> لإدخال الإطار التالي. بفضل آلية الانتباه في <span class="nodecor">Q‑Former</span>، يتفاعل الاستعلامُ الجديد مع الميزات المستخرجة من آخر مخرجات <span class="nodecor">Q‑Former</span>، كما هو مُبيَّن في [fig:pic2]. وتُعرض عملية الانتباه المتقاطع بين ميزات الاتّجاهات المتعدّدة لكلّ إطار <span class="math inline">\(f_t\)</span> والاستعلام الزمني <span class="math inline">\(Q_n\)</span> في المعادلة الآتية:</p>
<p><span class="math display">\[
\mathrm{Attn}\!\left(Q_{n},\, K_{\mathrm{Car}},\, V_{\mathrm{Car}}\right)
= \mathrm{Softmax}\!\left( \frac{Q_{n}\, K_{\mathrm{Car}}^{T}}{\sqrt{d_{k}}} \right) V_{\mathrm{Car}}
\]</span></p>
<p>حيث <span class="math inline">\(Q_{n-1} \in \mathbb{R}^{N_Q \times D}\)</span> يُشير إلى ميزة الاستعلام من الطابع الزمني السابق، والتي تؤدّي (بعد التحديث الزمني) إلى توليد إخراج الحالة الحاليّة <span class="math inline">\(Q_n\)</span>. بعد ذلك، ومن أجل تقديم منظور الصورة الحالي، يجمعُ الاستعلامُ الحالي <span class="math inline">\(Q_n\)</span> ميزات الصورة من <span class="nodecor">ViT‑g/14</span>، المشار إليها بـ <span class="math inline">\(f_t\)</span>. ويمكن وصف آلية الانتباه المتقاطع كما يلي:</p>
<p><span class="math display">\[
Q = Q_n\, W^{Q} \quad\;\; K = f_t\, W^{K} \quad\;\; V = f_t\, W^{V}
\]</span></p>
<p><span class="math display">\[
\mathrm{CrossAttn}(Q_n, f_t) \;=\; \mathrm{Softmax}\!\left( \frac{Q\, K^{T}}{\sqrt{d_{k}}} \right) V
\]</span></p>
<h2 id="الاستشعار-المتكامل-من-طرف-إلى-طرف-لعدة-مركبات">الاستشعار المتكامل من طرفٍ إلى طرف لعدّة مركبات</h2>
<p>لتعزيز موثوقيّة إدراك بيئة المركبة، قد لا يكفي الاعتمادُ على مركبةٍ واحدة بسبب وجود مناطق عمياء. ولمواجهة ذلك، طوّرنا نظام إدراك بصري تعاوني يربط بين عدّة مركبات. وفي ظل بيئات المرور المعقّدة، صمّمنا شبكةً عصبيّة تُقيِّم ليس فقط الحوادث المحتملة التي تنطوي على المركبة الذاتيّة، بل أيضًا مخاطر الاصطدام فيما بينها. وقد حققنا ذلك عبر تدريبٍ مشترك مُتعدِّد المركبات، ما مكّننا من بناء وكيلٍ مُدرِك للحوادث يجمع البيانات من عدّة مركبات باستخدام مجموعة بيانات <span class="nodecor">DeepAccident</span>.</p>
<p>في نظام المركبات المتعدّدة، اعتمدنا على النسخة المدرَّبة مسبقًا من <span class="nodecor">ViT‑g/14</span> لكلّ مركبة <span class="math inline">\(Car_n\)</span> لاستخراج الميزات <span class="math inline">\(f\)</span> من صور الإطار الحالي مُتعدِّد المناظير. تُغذّى هذه الميزات بعد ذلك إلى كتلة <span class="nodecor">MotionBlip2</span> المخصّصة. وبإضافة الاستعلامات الناتجة من <span class="nodecor">AccidentBlip2</span>، نجمعها مع استعلامات باقي المركبات وندخلها إلى <span class="nodecor">Motion Q‑Former</span> في الخطوة الزمنيّة التالية. وأخيرًا، نحوِّل هذا الاستعلام مُتعدِّد الأبعاد إلى متّجه أحاديّ البُعد ليكون مدخلًا لشبكة الإدراك مُتعدِّدة الطبقات (MLP)، كما توضحه المعادلة الآتية:</p>
<p><span class="math display">\[
\mathbf{X_i} = \mathrm{MLP}\!\left(\mathrm{concat}(Q_1, Q_2, Q_3, Q_4, \ldots, \mathrm{dim}=0)\right)
\]</span></p>
<p>يُدخَل المتّجه الناتج <span class="math inline">\(X_i\)</span> للإطار <span class="math inline">\(i\)</span> في طبقة التضمين، ثم يُجرى عليه انحدارٌ ذاتي لتحديد احتمال وقوع حادث بين المركبات. كلّ استعلام <span class="math inline">\(Q_n\)</span> يُمثّل منظورًا واحدًا لمركبة <span class="math inline">\(Car_n\)</span> ضمن النظام، ما يؤدّي إلى دمج 24 منظورًا في تنسيق الميزات النهائي.</p>
</section>

<section>
<h1 id="التجربة">التجربة</h1>
<p>في هذا القسم، نقيم نظامنا على مجموعة بيانات المحاكاة <span class="nodecor">DeepAccident</span>. نختبره بشكلٍ منفصل لإدراك المركبة الفرديّة مقابل إدراك المركبات المتعدّدة. وبشكلٍ أكثر تحديدًا، نُركّز على الإجابة عن السؤالين التاليين:</p>
<ul>
  <li>هل يمكن لـ <span class="nodecor">AccidentBlip2</span> تحديد الحوادث في أنظمة المرور المعقّدة بكفاءة أعلى من الحلول الأخرى المعتمدة على الرؤية في سياق السلاسل الزمنيّة؟</li>
  <li>هل يستطيع <span class="nodecor">AccidentBlip2</span> استشعار الحوادث المحيطة بدقّة أكبر عند استخدام نظام مركباتٍ متعدّدة من الطرف إلى الطرف مقارنةً بإدراك المركبة الواحدة؟</li>
</ul>
<h2 id="بيانات-التدريب">بيانات التدريب</h2>
<p>للتدريب والتقييم، اعتمدنا مجموعة البيانات مفتوحة المصدر (<span class="nodecor">c2</span>) التي توفّر معلومات بصريّة شاملة عن المركبات والبُنى التحتيّة. تحتوي هذه المجموعة على سيناريوهات تُحاكي الاصطدامات المحتملة في العالم الحقيقي، مع وجود أربع مركبات وبنية تحتيّة واحدة، بالإضافة إلى بيانات سحابة نقاط من منظور علوي. وعلى عكس تقنيات مُتعدِّدة الوسائط الأخرى، يقتصر <span class="nodecor">AccidentBlip2</span> على أساليب الرؤية حصريًّا، فيستخدم فقط الصور البصريّة المُلتقطة من الكاميرات المثبّتة على المركبات.</p>
<p>وفقًا لوصف مجموعة البيانات (<span class="nodecor">c2</span>)، تتألف من 12 نوعًا من سيناريوهات الحوادث عند التقاطعات المنظّمة وغير المنظّمة. تضمّ المجموعة نحو 57 ألف إطار V2X موسوم، مُوزّعة بنسبة 0.7 للتدريب، و0.15 للتحقّق، و0.15 للاختبار. يلزم جهازٌ مُزوّد بأربع وحدات معالجة رسوميّات من طراز <span class="nodecor">A6000</span>، وتستغرق عملية التدريب أقلّ من 24 ساعة.</p>
<h2 id="المعايير-الأساسية">المعايير الأساسيّة</h2>
<p>نستخدم كنماذج أساسيّة عدّة نماذج شائعة للغات الفيديو الكبيرة. أوّلها النموذج اللغوي الكبير للفيديو (<em>Video‑LLM</em>)، الذي يُعتَمَد غالبًا في مهام معالجة الفيديو. كما نقارنه بنسخةٍ مُحسّنة منه تُسمّى <span class="nodecor">Video‑Vicuna</span>. بالإضافة إلى ذلك، شملنا أيضًا مقارنةً مع نموذج <span class="nodecor">Video‑LAVA</span> ونموذج الاستدلال الكبير للغة الفيديو قبل الإسقاط (<em>Pre‑Projection Inference</em>). نُلاحظ أن النماذج المُحسَّنة للرؤية مثل LLaVA وGPT‑4V تُواجه صعوبةً في التعامل مع مدخلات زمنيّة كبيرة، وليس أداؤها أفضل من النموذج اللغوي الكبير للفيديو، لذا لم تُضمّن في المقارنات النهائيّة.</p>
<h2 id="تفاصيل-التنفيذ">تفاصيل التنفيذ</h2>
<p>يتألّف نموذجُنا المُقترَح من مُشفِّر بصري <span class="nodecor">ViT‑g/14</span> بالإضافة إلى وحدة <span class="nodecor">Motion Q‑Former</span> لاستخراج ودمج الميزات من الصور والنصوص. ولضمان أفضل دقّة في التنبّؤ بالحوادث، نُدرِّب <span class="nodecor">AccidentBlip2</span> على صورٍ بحجم <span class="math inline">\(224 \times 224\)</span>.</p>
<p>في إعداداتنا، نجمد معلمات <span class="nodecor">ViT‑g/14</span> والنموذج اللغوي <span class="nodecor">OPT‑2.7B</span> (<span class="nodecor">c14</span>)، حيث أُهيئت بأوزان مُدرَّبة مسبقًا مماثلة لـ <span class="nodecor">BLIP‑2</span> (<span class="nodecor">c21</span>). تهدف وحدة <span class="nodecor">Motion Q‑Former</span>، بالتعاون مع <span class="nodecor">Q‑Former</span>، إلى دمج وجهات النظر المتعدّدة ضمن نظام المركبات التعاوني، وهي تُحمَّل وتُدرَّب على مهام التنبّؤ والمخرجات اللغويّة معًا. تمرُّر مدخلاتُ الصور عبر طبقة <span class="nodecor">ViT</span> التي تُمكّن <span class="nodecor">Motion Q‑Former</span> من التقاط الميزات الديناميكيّة عبر الإطارات الزمنيّة المختلفة.</p>
<p>خلال التدريب، اعتمدنا فترة تسخين لمُعدّل التعلّم للثلاث خطوات الأولى، مع استخدام مُحسّن <span class="nodecor">Adam</span> بـ <span class="math inline">\(\beta_1=0.9\)</span> و<span class="math inline">\(\beta_2=0.999\)</span>. درّبنا <span class="nodecor">AccidentBlip2</span> على مدى 8 حِقَب، مُستخدمين جدولةً جيبيّة لمُعدّل التعلّم يصل فيها الحد الأدنى إلى <span class="math inline">\(1e^{-5}\)</span>. تمتد فترة التدريب على 6 مراحل بحجم دفعةٍ قدرُه 8، يختلف عدد المركبات فيها باختلاف المهمة. وبالنسبة للمدخلات مُتعدِّدة السيناريوهات، حيث تُؤخَذ عيّنات من الإطارات الزمنيّة من 0 إلى <span class="math inline">\(T\)</span>، يكون شكلُها: <span class="math inline">\(X_S \in \mathbb{R}^{T \times N_V \times V_C \times C \times H \times W}\)</span>، حيث يشير <span class="math inline">\(T\)</span> إلى طول السلسلة الزمنيّة، و<span class="math inline">\(N_V\)</span> إلى عدد المركبات، و<span class="math inline">\(V_C\)</span> إلى ستّ مناظير منفصلة (أمامي، أمامي يسار، أمامي يمين، خلفي، خلفي يسار، خلفي يمين)، و<span class="math inline">\(H\)</span> و<span class="math inline">\(W\)</span> إلى أبعاد الصورة. يتفاعل <span class="nodecor">Motion Q‑Former</span> تِكراريًّا مع مخرجات الاستعلام <span class="math inline">\(Q_{T-1} \in \mathbb{R}^{N_V \times N_Q \times D}\)</span> من آخر خطوة زمنيّة، ما يُمكّن من التقاط الميزات مُتعدِّدة الاتجاهات عبر الزمن حتى الوصول إلى <span class="math inline">\(Q_T\)</span>، ويقوم النموذجُ اللغوي بعد ذلك بجمعه كمُجمّع للتضمين الشامل. يعتمد حساب دالة الخسارة <span class="math inline">\(\mathrm{Loss}(p_t)\)</span> على Focal Loss، حيث يُرمز <span class="math inline">\(p_t\)</span> إلى احتمال الفئة الصحيحة، وتُستخدم <span class="math inline">\(\alpha\)</span> لموازنة أوزان العيّنات الإيجابيّة والسلبيّة، بينما تتحكّم <span class="math inline">\(\gamma\)</span> في معدل العيّنات التي يتم التركيز عليها، كما في المعادلة الآتية:</p>
<p><span class="math display">\[
\mathrm{Loss}(p_t) = -\,\alpha\,\bigl(1 - p_t\bigr)^{\gamma}\,\log(p_t)
\]</span></p>
<p>ونظرًا لتوزّع فئات الحوادث غير المتكافئ، ضبطنا <span class="math inline">\(\alpha\)</span> على 0.25، وتركنا <span class="math inline">\(\gamma\)</span> عند القيمة الافتراضيّة 2.0، لتعزيز تركيز النموذج على العيّنات الصعبة أو المشكوك في تصنيفها.</p>
<h2 id="التقييم">التقييم</h2>
<p>يقدّم الجدول [Table 1] نتائج أداء تكوينات مختلفة لنموذجنا. ومن خلال تصميم إصداراتٍ متباينة، نقيم <span class="nodecor">AccidentBlip2</span> في سيناريو مركبة واحدة وسيناريو مركبات متعدّدة، بما في ذلك المركبة الذاتيّة، والمركبات الأخرى، والرؤية من البنية التحتيّة. وبالاستفادة من ستّ كاميرات مُثبّتة عبر أربع مركبات، تمكّن <span class="nodecor">Motion Q‑Former</span> من التقاط الميزات الزمنيّة لعدّة مركبات بفعاليّة.</p>
<p>أدّى ذلك إلى تحسين الدقّة بنسبة <span class="nodecor">2%</span> مقارنةً بسيناريو المركبة الواحدة. علاوةً على ذلك، لوحظت زيادة واضحة بنسبة <span class="nodecor">6.6%</span> عند استخدام عدّة مناظير، ما يؤكّد قدرة النموذج على الاستدلال الشامل للحوادث.</p>
<p>يتفوّق نموذجُنا المُقترح على النماذج الأساسيّة في كلا التكوينين. مقارنةً بالنماذج اللغويّة الكبيرة للفيديو الموضّحة في الجدول [Table 2]، يحقّق <span class="nodecor">AccidentBlip2</span> دقّة <span class="nodecor">66.5%</span> عند مدخلات مركبةٍ واحدة. وبالمقارنة مع إعدادات <span class="nodecor">V2XFormer</span> الثلاثة، يُظهر نموذجُنا زيادة تقارب <span class="nodecor">3%</span> في الدقّة.</p>
<p>عالجنا تحدّي انفجار التدرّج أثناء التدريب باستخدام فترة تسخين لمُعدّل التعلّم، ما زاد من استقرار العملية. يعمل نموذجُنا مُتعدِّد المركبات على أربع وحدات معالجة رسوميّات، ما يُتيح جمع استعلامات <span class="nodecor">Motion Q‑Former</span> من كلّ مركبة. كما استكشفنا تأثير الوزن الأوّلي لـ <span class="nodecor">Q‑Former</span>، فتبيّن أن التهيئة بأوزان <span class="nodecor">BLIP‑2</span> المدرَّبة مسبقًا تُحسّن الأداء على مجموعة بيانات <span class="nodecor">DeepAccident</span>.</p>
</section>

<section>
<h1 id="الاستنتاجات">الاستنتاجات</h1>
<p>في هذه الورقة، قدّمنا إطارَ عملٍ لكشف الحوادث قائمًا على <span class="nodecor">Motion Q‑Former</span> أطلقنا عليه <span class="nodecor">AccidentBlip2</span>، ويعتمد حصريًّا على مدخلات الرؤية لتحليل معلومات الطرق. يُدخِل هذا الإطار آلية الانتباه الزمني ضمن <span class="nodecor">BLIP‑2</span> عبر تعزيز آلية الانتباه الذاتي التقليديّة، حيث تُستخدم الاستعلامات كحاملاتٍ للسمات الزمنيّة، لتُشفّر المعلومات من كلّ إطار ثم تُضمّن في إطارٍ لاحق. ويُتيح هذا التصميم إمكانيّة الاستدلال التلقائي باستخدام شبكة <span class="nodecor">MLP</span> لتحديد وقوع الحوادث وتقديم توصيفٍ دقيق للبيئة المحيطة.</p>
<p>وعند مقارنة دقّة كشف الحوادث مع نماذج اللغة الكبيرة الأخرى المعتمدة على الفيديو، يبرز <span class="nodecor">AccidentBlip2</span> بدقّةٍ لافتة بلغت <span class="nodecor">66.5%</span>، مُتجاوزًا أداء جميع النماذج الأساسيّة. ويُسلّط ذلك الضوء على فعاليّة إطار عملِنا في أنظمة النقل مُتعدِّدة المركبات المعقّدة. وعلى وجه التحديد، يحقّق نظامُنا المُكوَّن من أربع مركبات دقّة بلغت <span class="nodecor">72.2%</span> في كشف الحوادث البيئيّة، ما يُشير إلى تحسّنٍ كبير مقارنةً بدقّة كشف حوادث المركبة الواحدة. علاوةً على ذلك، يُظهر <span class="nodecor">AccidentBlip2</span> مزايا واضحة في التحقّق من الحوادث مقارنةً بالنماذج اللغويّة الكبيرة الأخرى المعتمدة فقط على الفيديو في أنظمة المركبات المتعدّدة، ما يؤكّد إمكانات نهجِنا في تعزيز كشف الحوادث في سيناريوهات المرور المعقّدة.</p>
</section>

<blockquote>
  لمزيد من التفاصيل والتجارب المكرّرة، يُرجى متابعة المستودع الرسمي للمشروع: 
  <a href="https://github.com/YihuaJerry/AccidentBlip2.git">github.com/YihuaJerry/AccidentBlip2</a>
</blockquote>

</main>

<footer>
  © 2025. جميع الحقوق محفوظة للمؤلفين. النص مُعرَّب ومُنقَّح لغويًا وتقنيًا مع الحفاظ على البُنى الرياضيّة (LaTeX).
</footer>
</body>
</html>