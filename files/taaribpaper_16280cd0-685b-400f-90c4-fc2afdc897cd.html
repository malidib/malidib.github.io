<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Yihua Shao*, Hongyi Cai*, Wenxin Long, Weiyi Lang, Zhe Wang, Haoran Wu, Yan Wang, Yang Yang^{1}, Member, IEEE, Zhen Lei^{3}, Fellow, IEEE">
  <title>AccidentBlip2: كشف الحوادث باستخدام Multi-View MotionBlip2</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">AccidentBlip2: كشف الحوادث باستخدام <span class="nodecor">Multi-View MotionBlip2</span></h1>
<p class="author"><span class="nodecor">Yihua Shao*</span>, <span class="nodecor">Hongyi Cai*</span>, <span class="nodecor">Wenxin Long</span>, <span class="nodecor">Weiyi Lang</span>, <span class="nodecor">Zhe Wang</span>, <span class="nodecor">Haoran Wu</span>, <span class="nodecor">Yan Wang</span>, <span class="nodecor">Yang Yang<span class="math inline">\(^{1}, Member, IEEE\)</span></span>, <span class="nodecor">Zhen Lei<span class="math inline">\(^{3}, Fellow, IEEE\)</span></span></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>أظهرت نماذج اللغة الكبيرة متعددة الوسائط (MLLMs) قدرات مميزة في العديد من مجالات التفكير متعدد الوسائط. لذلك، نستفيد من قدرة التفكير لدى هذه النماذج في وصف البيئة وفهم المشهد في بيئات النقل المعقدة. في هذه الورقة، نقترح <span class="nodecor">AccidentBlip2</span>، وهو نموذج لغة كبير متعدد الوسائط يمكنه التنبؤ في الوقت الفعلي بإمكانية وقوع حادث. يتضمن نهجنا استخراج الخصائص بناءً على المشهد الزمني لصور العرض المحيطة ذات الستة اتجاهات والاستدلال الزمني باستخدام إطار العمل الزمني <span class="nodecor">blip</span> عبر محول الرؤية. ثم ندخل الترميز الزمني الناتج في <span class="nodecor">MLLMs</span> للاستدلال وتحديد ما إذا كان سيقع حادث أم لا. ونظرًا لأن <span class="nodecor">AccidentBlip2</span> لا يعتمد على صور <span class="nodecor">BEV</span> أو <span class="nodecor">LiDAR</span>، يمكن تقليل عدد معلمات الاستدلال وتكلفة استدلال <span class="nodecor">MLLMs</span> بشكل كبير، كما لا يتطلب تكاليف تدريب كبيرة أثناء التدريب. يتفوق <span class="nodecor">AccidentBlip2</span> على الحلول الحالية في مجموعة بيانات <span class="nodecor">DeepAccident</span> ويمكن أن يوفر أيضًا حلاً مرجعيًا للتنبؤ بحوادث القيادة الذاتية من البداية إلى النهاية. سيتم إصدار الكود على: <a href="https://github.com/YihuaJerry/AccidentBlip2.git" class="uri">https://github.com/YihuaJerry/AccidentBlip2.git</a></p>
<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>تعد النماذج اللغوية الكبيرة متعددة الوسائط القادرة على اكتشاف وتحديد وقوع الحوادث بدقة ذات أهمية كبيرة لمجال السلامة في القيادة الذاتية. هناك بعض الأعمال السابقة للأشخاص في كشف سلوك المركبة. عادةً ما يتم قيادة المركبة وفقًا لمحيطها ولقواعد المرور. بالإضافة إلى ذلك، في أنظمة المرور المعقدة، يمكن للمركبات التوقف أو تغيير المسارات أو حتى الرجوع للخلف. لذلك نحن نأخذ في الاعتبار كل هذه الظواهر في محيط المركبة. وعلى الرغم من أن هذه الظواهر جميعها مشمولة في سياق استشعار بيئة المركبة، إلا أن التعقيد الكامن في نظام المرور يؤدي إلى صعوبات في نمذجة استشعار الطريق. ولهذا غالبًا ما تؤدي طرق الإدراك العصبي التقليدية إلى أحكام سلبية.</p>
<p>ومع ذلك، فإن الطرق المخصصة لكشف حالات الحوادث البصرية البحتة للمشاهد المعقدة لا تزال ناقصة. لكن قدرة النماذج اللغوية الكبيرة متعددة الوسائط على فهم المشاهد المعقدة تجعلها تبرز في البيئات المعقدة لمهام القيادة الذاتية. الأعمال التمثيلية الحالية تستخدم عادة نماذج بايغرام متعددة الوسائط لكشف المركبات والأشخاص وما إلى ذلك في البيئة، مما يعزز الاعتمادية في القيادة الذاتية. ومع ذلك، في سيناريوهات المرور المعقدة، تحدث الحوادث بشكل متكرر، لذا يمكن للنموذج اللغوي الكبير متعدد الوسائط داخل المركبة استخدام قدرته على فهم السيناريو لتحديد معلومات الحوادث المحيطة بالمركبة ذاتية القيادة ومساعدة مدير المرور على معرفة ما إذا كان هناك أي خطر أو تصادم في البيئة المحيطة.</p>
<p>في هذه الورقة، نقترح (<span class="nodecor">AccidentBlip2</span>)، نموذجًا لغويًا كبيرًا متعدد الوسائط للحكم على الحوادث في سيناريوهات السلاسل الزمنية. نهدف إلى تعزيز التطبيق العملي للنماذج الكبيرة متعددة الوسائط في سيناريوهات المرور المعقدة. باستخدام النماذج الكبيرة متعددة الوسائط، قمنا بتنفيذ إطار عمل لتجميع صور الكاميرا ذات الستة اتجاهات في إدخال مؤقت متعدد الوسائط مع محولات الرؤية. بعد ذلك، يمكن استخدام الرموز الزمنية التي يولدها محول الرؤية لتدريب المحول الزمني. في التطبيق العملي، يتم نقل رمز الصور الزمني من المحول الزمني إلى مشفر الرؤية للنماذج اللغوية الكبيرة متعددة الوسائط، حيث يتم استخدام قوة التفكير للنماذج اللغوية الكبيرة متعددة الوسائط للتنبؤ وتحديد ما إذا كان قد وقع حادث. يمكن للنماذج اللغوية الكبيرة متعددة الوسائط أيضًا التفاعل مع السائق من خلال اللغة لاستشعار البيئة على الطريق بدقة أكبر واكتشاف أي مخاطر في الطريق.</p>
<p>بالإضافة إلى تحليل إدراك المشهد المحيط لمركبة واحدة، قمنا أيضًا بتطوير نظام إدراك تعاوني لعدة مركبات من البداية إلى النهاية لتعويض بعض النقاط العمياء والنواقص في إدراك المركبة الواحدة. لقد مددنا تجارب البيئة للمركبة الواحدة إلى سيناريوهات القيادة من البداية إلى النهاية، واختبرنا دقة الحكم على الحوادث والإدراك بالإضافة إلى الترابط بين الذات والمركبات المتعددة، على التوالي. بشكل عام، إليكم مساهماتنا الرئيسية:</p>
<ul>
<li><p>نقترح وكيلاً جديدًا للحكم على حوادث المرور البصرية، والذي يعالج التنبؤ بالحوادث المحددة وينبه السائقين إلى إمكانية وقوع حادث في بيئات القيادة المعقدة.</p></li>
<li><p>نقدم إطارًا للتنبؤ بالحوادث من البداية إلى النهاية بناءً على النماذج اللغوية الكبيرة متعددة الوسائط، مما يمكّن النماذج الكبيرة من تحديد ما إذا كان قد وقع حادث أو هناك خطر حادث في أو حول نظام المركبات المتعددة بناءً على معلومات المركبة الجانبية.</p></li>
</ul>
<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<h2 id="نموذج-اللغة-الكبير-متعدد-الوسائط">نموذج اللغة الكبير متعدد الوسائط</h2>
<p>مع ظهور GPT4، بدأ عدد كبير من نماذج اللغة الكبيرة في استكشاف القدرات متعددة الوسائط. تُستخدم هذه النماذج لمعلومات متعددة الوسائط لتحسين نموذج اللغة وتعزيز قدرة النماذج الكبيرة على الاندماج والفهم في وسائط مختلفة. من بينها، تقود النماذج الكبيرة متعددة الوسائط البصرية-اللفظية، بقيادة GPT-4V وLlava-v1.5، إلى تطوير نماذج لغوية كبيرة متعددة الوسائط تنطبق على سيناريوهات مختلفة. بالإضافة إلى ذلك، تم تحسين نماذج مثل owlViT وQwen-VL، وهي نماذج لغوية كبيرة متعددة الوسائط لكشف الأهداف، لمشاهد محددة من خلال الجمع بين مجموعات بيانات محددة، مما يتيح لها إنجاز مهام بصرية محددة بأوامر موجهة من المستخدم. بالنسبة للمعلومات متعددة الوسائط الزمنية، قدم الباحثون أيضًا Video-Llava وVideo-Llava، والتي تدمج قدرات معالجة الصور والفيديوهات الزمنية في نموذج لغوي كبير. هناك أيضًا باحثون يعملون حاليًا على تحسين البيانات من وسائط الصوت لنماذج اللغة الكبيرة، مثل Qwen-Audio، مما يسهل تطوير نماذج لغوية كبيرة لوسائط الصوت. هذه الاندماجات المتقاطعة للوسائط في نماذج اللغة الكبيرة، التي تسهلها نماذج اللغة الكبيرة في سيناريوهات مختلفة، توفر حلولاً عالية الجودة لمهام متنوعة في العالم المفتوح.</p>
<h2 id="نماذج-اللغة-الكبيرة-للقيادة-الآلية">نماذج اللغة الكبيرة للقيادة الآلية</h2>
<p>مؤخرًا، مع التطور السريع لنماذج القيادة الذاتية، بدأت نماذج اللغة الكبيرة تظهر تدريجيًا في مجال القيادة الذكية والتليماتيكس. شهد عام <span class="nodecor">2023</span> ظهور <span class="nodecor">uniad</span>، الذي يطبق نماذج اللغة الكبيرة بشكل كامل في مجال القيادة الذاتية لأول مرة، حيث يدمج <span class="nodecor">uniad</span> الشخصيات الثلاث المهمة للقيادة الذاتية: الإدراك، واتخاذ القرار، والتخطيط في هندسة شبكية موحدة، مما يقلل بشكل فعال من فقدان المعلومات بين الوحدات المستقلة المختلفة. كما طور الباحثون نموذج لغة كبير متعدد الوسائط يمكنه التركيز على نية السائق يسمى <span class="nodecor">CAVG</span>. يتكون <span class="nodecor">CAVG</span> من خمسة مشفرات متخصصة: مشفر نصي، مشفر عاطفي، مشفر بصري، مشفر سياقي، ومشفر عبر الوسائط. تسمح هذه المشفرات، بالاشتراك مع مفككاتها المقابلة، لنموذج اللغة الكبير بالتعامل مع وسائط مختلفة من مهام القيادة الذاتية. في مهمة الإدراك، يستخدم <span class="nodecor">DRIVEGPT4</span> <span class="nodecor">YOLOv8</span> لكشف الأهداف الشائعة مثل السيارات في كل إطار من الفيديو وترسل الإحداثيات الحدودية الناتجة إلى <span class="nodecor">ChatGPT</span> كمعلومات لغوية. ومع ذلك، بما أن <span class="nodecor">DriveGPT4</span> يقوم فقط بمهام الوعي بالبيئة لمركبة واحدة، فلا يمكن نشره عمليًا لمهام من طرف إلى طرف مع ربط متعدد المركبات.</p>
<h2 id="حكم-الحوادث">حكم الحوادث</h2>
<p>يعد حكم الحوادث المرورية من أكثر المجالات بحثًا في مجال سلامة القيادة الذاتية، حيث قام العديد من الباحثين بأعمال كثيرة في هذا المجال. استنادًا إلى الطرق التقليدية للكشف، استخدم الباحثون الرؤية الأمامية للمركبة بالتزامن مع شبكة عصبية للتنبؤ الزمني، مثل شبكة الذاكرة طويلة الأمد أو الشبكة العصبية المتكررة، لتحذير السائقين من وقوع حادث (<span class="nodecor">c10</span>). ومع ذلك، فإن هذه الطريقة لا تستطيع إلا أن تستشعر حالة المركبة المحيطة بالدراجة، ولا تستطيع الإحساس بالخطر في بيئة المرور المعقدة. تقل احتمالية التنبؤ الصحيح بالحوادث إذا ما واجهت أحوال جوية معقدة. مع التطور السريع لنماذج اللغة الكبيرة، كان هناك باحثون يستخدمون نماذج اللغة الكبيرة في إدراك الحوادث، مثل نموذج الحوادث العملاق. ومع ذلك، نظرًا لأن مجموعة البيانات الخاصة به مسترجعة مباشرة من خلال نماذج اللغة الكبيرة، فإنه لا يستطيع التفكير في إدراك البيئات الجديدة كليًا في الحالات المعقدة.</p>
<h1 id="المنهجية">المنهجية</h1>
<p>Blip2 غير قادر على معالجة مدخلات الصور ذات <span class="nodecor">6</span> اتجاهات بشكل مباشر، ولا يمكنه استخدام استدلال الصور ذات <span class="nodecor">6</span> اتجاهات الزمنية بشكل مباشر. في هذا القسم، نستخدم مجموعة بيانات المحاكاة وتحويل الرؤية من محاكي (<span class="nodecor">c11</span>) لتعزيز قدرة Blip2 على معالجة الصور الزمنية ذات <span class="nodecor">6</span> اتجاهات. وبالتالي، نأمل أن يكون Blip2 المدرب قادرًا على التفكير في الصور الزمنية ذات <span class="nodecor">6</span> اتجاهات التي يمكن استخدامها للاستدلال. من الجدير بالذكر أن إطار عملنا يمكن توسيعه ليشمل سيناريوهات محاكاة القيادة الذاتية من طرف إلى طرف لعدة مركبات.</p>
<h2 id="مدخلات-متعددة-الوجهات-والاستدلال-الزمني">مدخلات متعددة الاتجاهات والاستدلال الزمني</h2>
<p>نقدم إطارًا إدراكيًا يستفيد من نماذج اللغة الكبيرة متعددة الوسائط، ويتألف من مكونين رئيسيين: معالجة الصور متعددة الاتجاهات والاستدلال الزمني. في مرحلة معالجة الاتجاهات المتعددة، نستخدم ViT-<span class="nodecor">14g</span> للتعامل مع الاتجاهات المتعددة واستخراج الميزات الصورية ذات الصلة. تلتقط هذه الميزات معلومات بصرية مهمة من وجهات نظر مختلفة. في مرحلة الاستدلال الزمني، نستخدم الاستعلامات كحاملات للمعلومات الزمنية للنقل الأمامي. تحتوي هذه الاستعلامات على إشارات سياقية وزمنية تسهل الفهم والاستدلال عبر الزمن. من خلال دمج هاتين المرحلتين، يمكن لإطارنا الإدراك والاستدلال الشامل بطريقة متعددة الوسائط.</p>
<p>يستخدم مشفرنا البصري ViT-<span class="nodecor">14g</span> من EVA-CLIP(<span class="nodecor">c13</span>). يقرأ محول الرؤية أولاً الصور متعددة الاتجاهات التي توفرها الكاميرا ويستخرج الميزات بشكل منفصل بعد تغيير حجم كل صورة. نستخدم محول الرؤية المدرب مسبقًا كعمود فقري لاستخراج الميزات <span class="math inline">\(f_t\)</span> للاتجاهات المتعددة لإطارات <span class="math inline">\(t\)</span>، وندخل ميزات كل إطار متعدد الاتجاهات في الطبقة الخطية لـ Qformer للقيام بالاستدلال. قبل إدخال ميزات الاتجاهات المتعددة لإطار <span class="math inline">\(t\)</span> في Qformer، سيقوم محول الرؤية أولاً بنشر ميزات كل صورة في بعد واحد، ثم يجمع ميزات الصور الست، ويدخل الميزات المجمعة <span class="math inline">\(f_t\)</span> في انتباه متقاطع لـ Qformer. في الوقت نفسه، يتم إدخال الاستعلام القابل للتعلم <span class="math inline">\(Q_t\)</span> في طبقة الانتباه الذاتي لـ Qformer فقط، ويتم إخراج الاستعلام الجديد من خلال طبقة التغذية الأمامية.</p>
<p>اعتمادًا على نوع البيانات في القيادة الذاتية، نحتاج إلى اقتراح إطار يمكنه التعامل مع الصور الزمنية. نقترح الانتباه الذاتي الزمني، الذي يدخل معلومات الصور متعددة الاتجاهات لكل إطار من خلال الاستعلام الزمني <span class="math inline">\(Q_n\)</span>، يدخل <span class="math inline">\(Q_n\)</span> في Qformer المقابل، ويحصل على الاستعلام <span class="math inline">\(Q_{n+1}\)</span> المقابل لإدخال الإطار التالي. باستخدام مزايا آلية الانتباه في Qformer، يتفاعل مع الميزات المستخرجة من آخر إخراج لـ Qformer، كما هو موضح في [fig:pic2]. تظهر ميزات الاتجاهات المتعددة لكل إطار <span class="math inline">\(f_t\)</span> في الانتباه مع الاستعلام الزمني <span class="math inline">\(Q_n\)</span>، كما هو مكتوب في [eq1], <span class="math display">\[Attn(Q_{n-1}, K_{Car}, V_{Car} )=Softmax(\frac{Q_{n}\cdot K_{Car}^{T}}{\sqrt{d_{k} } } ) V  
\label{eq1}\]</span></p>
<p>حيث <span class="math inline">\(Q_{n-1} \in \mathbb{R} ^ {N_Q \times D}\)</span> يشير إلى ميزة الاستعلام من الطابع الزمني الأخير، والتي تؤدي إلى توليد إخراج الحالة الحالية <span class="math inline">\(Q_n\)</span>. بعد ذلك، من أجل تقديم وجهة النظر الصورية الحالية، يجمع الحالة الحالية <span class="math inline">\(Q_n\)</span> ميزات الصورة من ViT-g، المشار إليها بـ <span class="math inline">\(f_t\)</span>. يمكن وصف آلية الانتباه المتقاطع كما يلي</p>
<p><span class="math display">\[Q=Q_nW^Q  \hspace{1em} K=V=f_tW^K\]</span></p>
<p><span class="math display">\[CrossAttn(Q_n, f_t) = Softmax\left ( \frac {QK^T}{\sqrt {d_2}}\right ) V\]</span></p>
<h2 id="الاستشعار-المتكامل-من-طرف-إلى-طرف-لعدة-مركبات">الاستشعار المتكامل من طرف إلى طرف لعدة مركبات</h2>
<p>لضمان إدراك موثوق لبيئة المركبة، قد لا يكون الاعتماد فقط على مركبة واحدة كافيًا بسبب وجود مناطق عمياء في الإدراك البصري. لمواجهة هذا التحدي، قمنا بتنفيذ نظام إدراك يعتمد فقط على الرؤية للتفاعل بين عدة مركبات. في أنظمة المرور المعقدة، تم تصميم الشبكة العصبية الخاصة بمركبتنا ليس فقط لتقييم الحوادث المحتملة التي تشمل المركبة الذاتية ولكن أيضًا لتقييم مخاطر الحوادث التي تشمل مركبات أخرى. وقد تم تحقيق ذلك من خلال طريقة التدريب المشترك لعدة مركبات. ونتيجة لذلك، نجحنا في تدريب وكيل واعٍ بالحوادث لعدة مركبات باستخدام بيانات من مجموعة بيانات DeepAccident.</p>
<p>في نظامنا لعدة مركبات، نستخدم نموذج ViT-14g المدرب مسبقًا على كل مركبة، المشار إليها بـ <span class="math inline">\(Car_n\)</span>، لاستخراج الميزات <span class="math inline">\(f\)</span> من صور العرض المتعدد للإطار الحالي للمركبة الذاتية. ثم يتم إدخال هذه الميزات في وحدة MotionBlip2 المصممة خصيصًا. بالإضافة إلى ذلك، بالنسبة للاستعلامات التي تم إنشاؤها بواسطة AccidentBlip2، نقوم بدمجها مع استعلامات المركبات المتعددة وإدخالها في Motion Qformer في الخطوة الزمنية التالية. أخيرًا، نحول هذا الاستعلام عالي الأبعاد إلى موتر أحادي البعد لاستخدامه كمدخلات للشبكة العصبية متعددة الطبقات (Multi-Layer Perceptron)، كما هو موضح في المعادلة [eq4]. <span class="math display">\[\mathbf{X_i} = \text{MLP}(\text{concat}(Q_1, Q_2, Q_3, Q_4, ... \text{dim}=0))
\label{eq4}\]</span></p>
<p>ندخل الرمز الناتج من الشبكة العصبية متعددة الطبقات في الإطار <span class="math inline">\(i\)</span> إلى التضمين، ويخضع الرمز المضمن <span class="math inline">\(X_i\)</span> للانحدار الذاتي، ومن خلاله نحدد ما إذا كان هناك حادث بين أنظمة المركبات المتعددة. كل استعلام <span class="math inline">\(Q_n\)</span> يشير إلى وجهة نظر واحدة من مركبة واحدة <span class="math inline">\(Car_n\)</span> في نظام المركبات المتعددة، متصلة ببعضها البعض، مما يؤدي إلى تجميع 24 وجهة نظر لتنسيق الميزات.</p>
<h1 id="التجربة">التجربة</h1>
<p>في هذا القسم، نقوم بتقييم نظامنا على مجموعة بيانات المحاكاة DeepAccident. نختبره بشكل منفصل لإدراك المركبة الفردية مقابل إدراك المركبات المتعددة. بشكل أكثر تحديدًا، سنركز على شرح النقاط التالية:</p>
<ul>
<li><p>هل يمكن لـ AccidentBlip2 تحديد الحوادث في أنظمة المرور المعقدة بكفاءة أكبر من الحلول الأخرى التي تعتمد فقط على الرؤية في أنظمة السلاسل الزمنية؟</p></li>
<li><p>هل يمكن لـ AccidentBlip2 الإحساس بالحوادث التي تحدث حوله بدقة أكبر من نظام المركبة الفردية في نظام نهاية إلى نهاية للمركبات المتعددة؟</p></li>
</ul>
<h2 id="بيانات-التدريب">بيانات التدريب</h2>
<p>للتدريب والتقييم، اعتمدنا مجموعة بيانات مفتوحة المصدر (<span class="nodecor">c2</span>)، مستفيدين من معلوماتها البصرية الكاملة عن المركبات والبنى التحتية. في مجموعة بيانات (<span class="nodecor">c2</span>)، تتكون من مجموعات من السيناريوهات التي تحاكي تلك الاصطدامات المحتملة في العالم الحقيقي، مرفقة بأربع مركبات، بنية تحتية واحدة وتوضيحات سحابة نقاط الرؤية العلوية. ومع ذلك، على عكس تلك التقنيات متعددة الوسائط، يعتمد (<span class="nodecor">AccidentBlip2</span>) على أساليب حصرية للرؤية، وبالتالي يشمل فقط الصور البصرية المأخوذة من الأجسام.</p>
<p>كما اقترحته (<span class="nodecor">c2</span>)، تتكون من <span class="nodecor">12</span> نوعًا من سيناريوهات الحوادث التي تحدث عند التقاطعات المنظمة وغير المنظمة. تحتوي مجموعة البيانات على <span class="nodecor">57</span> ألف إطار موضح <span class="nodecor">V2X</span>، مقسمة بنسبة <span class="nodecor">0.7</span>، <span class="nodecor">0.15</span> و <span class="nodecor">0.15</span> لمجموعات التدريب، التحقق والاختبار على التوالي. يمكن أن يتم تدريبنا على جهاز بأربع وحدات معالجة رسومات من نوع <span class="nodecor">A6000</span>، والذي يتطلب أقل من <span class="nodecor">24</span> ساعة للنتيجة.</p>
<h2 id="المعايير-الأساسية">المعايير الأساسية</h2>
<p>المعيار الأساسي لدينا هو نموذج اللغة الكبير للفيديو الشائع (نموذج اللغة الكبير للفيديو). يُرى استخدام نموذج اللغة الكبير للفيديو غالبًا عند التعامل مع مهام الفيديو. نقوم أيضًا بمقارنته مع نموذج اللغة الكبير للفيديو-فيكونا، وهو نسخة محسنة من نموذج اللغة الكبير للفيديو. بالإضافة إلى عائلة نموذج اللغة الكبير للفيديو من نماذج اللغة الكبيرة للفيديو، استخدمنا أيضًا نموذج اللغة الكبير للفيديو-لافا، ونموذج الاستدلال الكبير للغة الفيديو للمحاذاة قبل الإسقاط، كمقارنة. النماذج الكبيرة للغة الفيديو المحسنة بصريًا مثل لافا وGPT-4V لا تتسع بسهولة لمدخلات البيانات الزمنية، وليست أفضل من نموذج اللغة الكبير للفيديو، لذا لم يتم تضمينها في المقارنة.</p>
<h2 id="تفاصيل-التنفيذ">تفاصيل التنفيذ</h2>
<p>يتكون نموذجنا المقترح من مشفر بصري ViT-14g، ويستخدم Motion Qformer لاستخراج ودمج الميزات من النصوص والصور. مع مراعاة التجربة المثلى للتنبؤ بالحوادث، نقوم بتدريب نموذج AccidentBlip2 بحجم صورة <span class="math inline">\(224 \times 224\)</span>.</p>
<p>في إعداداتنا، يتم تجميد معلمات نموذجنا للنموذج البصري ViT-14g ونموذج اللغة OPT-2.7B(<span class="nodecor">c14</span>)، والتي تم تهيئتها بأوزان مدربة مسبقًا، مماثلة لـ Blip-2(<span class="nodecor">c21</span>). يهدف Motion Qformer، بما أنه يتعاون مع ميزات Qformer، إلى دمج عدة وجهات نظر مع المركبات الجماعية معًا، يتم تحميله وتدريبه مع المهام التي تدعم كل من التنبؤ ومخرجات نموذج اللغة. يتم تمرير مدخلات الصورة من خلال طبقة ViT إلى Motion Qformer من أجل التقاط الميزات الديناميكية الزمنية داخل إطارات مختلفة.</p>
<p>أثناء التدريب، اخترنا تسخين معدل التعلم للفترات الثلاث الأولى، بالاقتران مع محسن Adam[1] بـ <span class="math inline">\(\beta_1\)</span> = 0.9 و<span class="math inline">\(\beta_2\)</span> = 0.999. تم تدريب نموذج AccidentBlip2 لمدة 8 فترات، باستخدام تحليل معدل التعلم الجيبي كمجدول لمعدل التعلم حتى <span class="math inline">\(1e-5\)</span>. فترة التدريب هي 6 وحجم الدفعة هو 8، تختلف من مهمة إلى أخرى وتشمل أعدادًا مختلفة من المركبات. من حيث السيناريوهات المتعددة، مع أخذ عينات من الطابع الزمني 0 إلى T(الطابع الزمني النهائي)، شكل المدخلات هو <span class="math inline">\(X_S \in \mathbb{R} ^ {T \times N_V \times V_C \times C \times H \times W}\)</span>، حيث <span class="math inline">\(T\)</span> يدل على طول الزمن، <span class="math inline">\(N_V\)</span> يمثل عدد المركبات، <span class="math inline">\(V_C\)</span> يدل على 6 وجهات نظر منفصلة للمركبة، تشمل الكاميرات الأمامية، الأمامية اليسرى، الأمامية اليمنى، الخلفية، الخلفية اليسرى والخلفية اليمنى، و<span class="math inline">\(H\)</span> و<span class="math inline">\(W\)</span> ترمزان لحجم صورة الإدخال. يتفاعل Motion Qformer بشكل تكراري مع مخرجات الاستعلام <span class="math inline">\(Q_{T-1} \in \mathbb{R} ^ {N_V \times N_Q \times D}\)</span>، التي تنبثق من وجهات النظر في الطابع الزمني الأخير. يأخذ ويخرج الميزات متعددة الاتجاهات الزمنية بشكل متكرر حتى يصل إلى نهايته، ثم تكون وحدة LLM مسؤولة عن جمع مخرج الاستعلام النهائي <span class="math inline">\(Q_T\)</span> كمجمع تضمين صورة شاملة. يتم حساب خسارة التدريب <span class="math inline">\(Loss(p_t)\)</span>، الموضحة في المعادلة التالية [eq2]، بناءً على Focal Loss، حيث <span class="math inline">\(p_t\)</span> يدل على احتمال الفئة الصحيحة، <span class="math inline">\(\alpha\)</span> يستخدم لموازنة أوزان العينات الإيجابية والسلبية و<span class="math inline">\(\gamma\)</span> يتحكم في معدل العينات المكتشفة.</p>
<p>نظرًا للتخفيف من فئات الخلل حيث يتنبأ نموذجنا بالحوادث، يتم ضبط <span class="math inline">\(\alpha\)</span> على 0.25، بينما تظل <span class="math inline">\(\gamma\)</span> عند قيمتها الافتراضية 2.0، مما يساعد على تدريب النموذج للتركيز بشكل أكبر على العينات المصنفة بشكل خاطئ.</p>
<h2 id="التقييم">التقييم</h2>
<p>يقدم الجدول [Table 1] نتائج أداء مدخلات تكوينات مختلفة لنموذجنا. من خلال تصميم نماذج تكوين مختلفة لإظهار أداء النموذج، قمنا بتقييم AccidentBlip2 في سيناريو سيارة واحدة، وسيناريو عدة سيارات، بما في ذلك السيارة الذاتية، والسيارات الأخرى والسيارات خلفها، بالإضافة إلى سيناريو الرؤية الكاملة من خلال إدخال رؤية البنية التحتية. وبفضل الاستفادة من ستة مستشعرات كاميرا مثبتة عبر أربع سيارات، يلتقط نموذج Motion Qformer بفعالية الميزات الزمنية لعدة سيارات.</p>
<p>هذا يؤدي إلى تحسين الدقة بنسبة <span class="nodecor">2%</span> مقارنة بسيناريو السيارة الواحدة. علاوة على ذلك، يمكن ملاحظة زيادة واضحة بنسبة <span class="nodecor">6.6%</span> مع وجود عدة وجهات نظر، مما يظهر أن النموذج يأخذ بعين الاعتبار نظرة عامة على الحوادث بفعالية.</p>
<p>يتفوق نموذجنا المقترح على النماذج الأساسية في كل من تكوينات السيارة الواحدة والسيارات المتعددة. مقارنة بنماذج اللغة الكبيرة للفيديو هذه، المعروضة في الجدول [Table 2]، يحقق AccidentBlip2 نسبة <span class="nodecor">66.5%</span> بمدخلات سيارة واحدة فقط. بالمقارنة مع V2XFormer، المقترح مع ثلاثة إعدادات اندماج وكيل V2X مختلفة، يمكن ملاحظة زيادة كبيرة في نموذجنا، بتحسين حوالي <span class="nodecor">3%</span> في الدقة.</p>
<p>تم تصميم نهجنا لمعالجة تحدي تحولات التدرج أثناء التدريب، والتي يمكن أن تؤدي إلى انفجار التدرج في العصور الأولى. للتخفيف من هذه المشكلة، نستخدم جدولًا زمنيًا للتسخين يقوم بتعديلات طفيفة على معدل التعلم ضمن نطاق ضئيل. هذا يساعد على استقرار عملية التدريب. يعمل نموذجنا متعدد السيارات على أربع وحدات معالجة رسومات منفصلة، مما يتيح لنا جمع نتائج الاستعلام من كل Motion Qformer. بالإضافة إلى ذلك، أجرينا تجارب لاستكشاف تأثير الوزن الأولي لـ Qformer، مقارنة بالأوزان المدربة مسبقًا من Blip2 مع التهيئة الافتراضية. كشفت النتائج عن انخفاض في الأداء العام لنموذج اللغة الخاص بنا عند تقييمه على مجموعة بيانات DeepAccident. للتغلب على ذلك، نستخدم الأوزان المدربة مسبقًا من Blip2 لتهيئة AccidentBlip2، مما يحسن فعالية النموذج في مهام كشف الحوادث.</p>
<h1 id="الاستنتاجات">الاستنتاجات</h1>
<p>في هذه الورقة البحثية، نقترح إطار عمل لكشف الحوادث يعتمد على نموذج <span class="nodecor">Motion Qformer</span> ويسمى <span class="nodecor">AccidentBlip2</span>، والذي يعتمد فقط على مدخلات الرؤية لتحليل معلومات الطريق. يقدم هذا الإطار آلية الانتباه الزمني في <span class="nodecor">Blip2</span> من خلال استبدال آليات الانتباه الذاتي. يستخدم الاستعلامات كحاملات للميزات الزمنية، مشفرًا المعلومات من كل إطار في الانتباه الزمني للإطار التالي. يتيح ذلك الاستدلال التلقائي باستخدام <span class="nodecor">MLP</span> لتحديد ما إذا كان قد وقع حادث ولتقديم وصف للبيئة المحيطة.</p>
<p>عند مقارنة دقة كشف الحوادث مع نماذج اللغة الكبيرة الأخرى المعتمدة على الفيديو، يبرز <span class="nodecor">AccidentBlip2</span> بدقة مثيرة للإعجاب بلغت <span class="nodecor">66.5%</span>، متجاوزًا أداء جميع النماذج الأساسية. يسلط ذلك الضوء على فعالية إطار عملنا في أنظمة النقل متعددة المركبات المعقدة. على وجه التحديد، يحقق نظامنا المكون من أربع مركبات دقة بلغت <span class="nodecor">72.2%</span> في كشف الحوادث البيئية، مما يشير إلى تحسن كبير مقارنة بدقة كشف حوادث المركبة الواحدة. علاوة على ذلك، يظهر <span class="nodecor">AccidentBlip2</span> مزايا واضحة في التحقق من الحوادث مقارنة بنماذج اللغة الكبيرة الأخرى المعتمدة فقط على الفيديو المستخدمة في أنظمة المركبات المتعددة. تؤكد هذه النتائج فعالية نهجنا وإمكاناته لتعزيز قدرات كشف الحوادث في سيناريوهات المرور المعقدة.</p>
</body>
</html>
