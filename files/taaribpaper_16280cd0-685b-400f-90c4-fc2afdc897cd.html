<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Yihua Shao*, Hongyi Cai*, Wenxin Long, Weiyi Lang, Zhe Wang, Haoran Wu, Yan Wang, Yang Yang^{1}, Member, IEEE, Zhen Lei^{3}, Fellow, IEEE">
  <title>AccidentBlip2: كشف الحوادث باستخدام Multi-View MotionBlip2</title>
  <style type="text/css">code{white-space: pre;}</style>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <style>body { direction: rtl; font-size: 22px; }</style>
</head>
<body>
<header>
<h1 class="title">AccidentBlip2: كشف الحوادث باستخدام <span class="nodecor">Multi-View MotionBlip2</span></h1>
<p class="author"><span class="nodecor">Yihua Shao*</span>, <span class="nodecor">Hongyi Cai*</span>, <span class="nodecor">Wenxin Long</span>, <span class="nodecor">Weiyi Lang</span>, <span class="nodecor">Zhe Wang</span>, <span class="nodecor">Haoran Wu</span>, <span class="nodecor">Yan Wang</span>, <span class="nodecor">Yang Yang<span class="math inline">\(^{1}, Member, IEEE\)</span></span>, <span class="nodecor">Zhen Lei<span class="math inline">\(^{3}, Fellow, IEEE\)</span></span></p>
</header>
<h1 id="ملخص">مُلَخَّص</h1>
<p>أظهرت نماذج اللغة الكبيرة متعددة الوسائط (MLLMs) قدرات مميزة في العديد من مهام الفهم متعدد الوسائط. لذلك، نستفيد هنا من قدرة هذه النماذج على وصف البيئة وفهم المشاهد في بيئات النقل المعقدة. في هذه الورقة، نقترح <span class="nodecor">AccidentBlip2</span>، نموذجًا لغويًا كبيرًا متعدد الوسائط قادرًا على التنبؤ في الوقت الفعلي بإمكانية وقوع حادث. يتضمن نهجنا استخراج السمات بناءً على التسلسل الزمني لصور المحيط المكوّن من ستة اتجاهات، ثم إجراء الاستدلال الزمني باستخدام إطار عمل <span class="nodecor">blip</span> عبر محول الرؤية. بعد ذلك، ندخل التمثيل الزمني الناتج إلى نماذج اللغة الكبيرة متعددة الوسائط للاستدلال وتحديد ما إذا كان من المحتمل وقوع حادث. ونظرًا لأن <span class="nodecor">AccidentBlip2</span> لا يعتمد على صور <span class="nodecor">BEV</span> أو بيانات <span class="nodecor">LiDAR</span>، فإن عدد معلمات الاستدلال وتكلفة المعالجة ينخفضان بشكل كبير، كما لا يتطلب موارد تدريب عالية. يتفوق <span class="nodecor">AccidentBlip2</span> على الحلول الحالية في مجموعة بيانات <span class="nodecor">DeepAccident</span>، ويمكن أن يشكل أيضًا معيارًا مرجعيًا للتنبؤ بحوادث القيادة الذاتية من البداية إلى النهاية. سيتم إصدار الكود على: <a href="https://github.com/YihuaJerry/AccidentBlip2.git" class="uri">https://github.com/YihuaJerry/AccidentBlip2.git</a></p>
<h1 id="مقدمة">مُقَدِّمَة</h1>
<p>تُعد النماذج اللغوية الكبيرة متعددة الوسائط القادرة على اكتشاف وتحديد وقوع الحوادث بدقة ذات أهمية بالغة لمجال السلامة في القيادة الذاتية. أجرى عدد من الباحثين أعمالًا سابقة في كشف سلوك المركبات. عادةً ما تتصرف المركبات وفقًا للمحيط المروري وقواعد المرور، حيث قد تتوقف أو تغير مساراتها أو حتى تتراجع إلى الخلف في أنظمة المرور المعقدة. نأخذ هذه الظواهر جميعًا بعين الاعتبار عند نمذجة إدراك بيئة المركبة. ومع ذلك، فإن التعقيد الشديد لنظام المرور يؤدي إلى صعوبات في نمذجة الاستشعار، مما يجعل طرق الإدراك التقليدية تعتمد في كثير من الأحيان على استنتاجات خاطئة.</p>
<p>مع ذلك، ما تزال الطرق الخاصة بكشف الحوادث البصرية البحتة في المشاهد المعقدة محدودة. تتفوق النماذج اللغوية الكبيرة متعددة الوسائط في فهم المشاهد المعقدة، مما يجعلها ملائمة تمامًا لمهام القيادة الذاتية في البيئات المعقدة. تعتمد الأعمال الحالية عادةً على نماذج متعددة الوسائط لكشف المركبات والمشاة وغيرها في البيئة، مما يعزز الاعتمادية في القيادة الذاتية. ومع ذلك، في سيناريوهات المرور المعقدة، تحدث الحوادث بشكل متكرر، لذا يمكن لوكيل MLLM المثبت داخل المركبة أن يستخدم قدرته على فهم السياق لتحديد معلومات الحوادث المحيطة وتقديم تنبيهات مبكرة لتعزيز السلامة.</p>
<p>في هذه الورقة، نقترح <span class="nodecor">AccidentBlip2</span>، نموذجًا لغويًا كبيرًا متعدد الوسائط مخصصًا للحكم على الحوادث في سياق السلاسل الزمنية. نهدف من خلاله إلى تعزيز التطبيق العملي لهذه النماذج في بيئات المرور المعقدة. باعتمادنا على MLLMs، أنشأنا إطارًا لتجميع مدخلات صور الكاميرا ذات الستة اتجاهات ضمن إدخال زمني متعدد الوسائط، ثم يستخرج محول الرؤية الرموز الزمنية الخاصة بكل إطار. تُنقل هذه الرموز بعد ذلك إلى مشفر الرؤية المدمج ضمن MLLM، حيث تُستخدم قدرات التفكير متعدد الوسائط للتنبؤ بما إذا كان قد وقع حادث. كما يمكن للنموذج التفاعل مع السائق عبر واجهة لغوية لاستشعار بيئة الطريق بشكل أدق وتنبيه السائق لأي مخاطر محتملة.</p>
<p>إلى جانب تحليل المشهد المحيط لمركبة واحدة، طورنا نظامًا تعاونيًّا لإدراك بيئة عدة مركبات من طرف إلى طرف، لتعويض النقاط العمياء والنواقص في إدراك المركبة الوحيدة. قمنا بتمديد اختبارات بيئة المركبة الواحدة إلى سيناريوهات القيادة الشاملة من البداية إلى النهاية، وقياسنا دقة التنبؤ بالحوادث وقدرة النظام على الربط بين الذات ومركبات متعددة. بشكل عام، تبرز مساهماتنا الرئيسية في النقاط التالية:</p>
<ul>
<li><p>نقترح وكيلاً جديدًا للحكم على حوادث المرور البصرية، يُعنى بالتنبؤ بالحوادث المحتملة وتنبيه السائقين في بيئات القيادة المعقدة.</p></li>
<li><p>نقدم إطارًا للتنبؤ بالحوادث من البداية إلى النهاية قائمًا على نماذج اللغة الكبيرة متعددة الوسائط، يمكّن هذه النماذج من تحديد وقوع حادث أو وجود خطر في أو حول نظام مركبات متعددة بالاستناد إلى المعلومات الجانبية لكل مركبة.</p></li>
</ul>
<h1 id="الأعمال-ذات-الصلة">الأعمال ذات الصلة</h1>
<h2 id="نموذج-اللغة-الكبير-متعدد-الوسائط">نموذج اللغة الكبير متعدد الوسائط</h2>
<p>مع ظهور GPT-4، بدأت العديد من نماذج اللغة الكبيرة في استكشاف قدرات المعالجة متعددة الوسائط. تستفيد هذه النماذج من المعلومات البصرية واللفظية معًا لتعزيز قدراتها في الفهم والاندماج بين وسائط مختلفة. على سبيل المثال، قاد كل من GPT-4V وLlava-v1.5 تطوير نماذج لغوية كبيرة متعددة الوسائط بصرية-لفظية تناسب سيناريوهات متنوعة. علاوةً على ذلك، تم تحسين نماذج مثل owlViT وQwen-VL—المتخصصة في كشف الأهداف—لمعالجة مشاهد معينة عبر دمج مجموعات بيانات موجهة، مما يمكّنها من تنفيذ مهام بصرية محددة بأوامر المستخدم. فيما يتعلق بالبيانات الزمنية، قدّم الباحثون أيضًا Video-Llava وغيرها من النماذج التي تجمع بين معالجة الصور والفيديو في نموذج لغوي كبير. وهناك أيضًا جهود لتحسين إدماج الوسائط الصوتية مثل Qwen-Audio، مما يمهد الطريق لتطوير نماذج لغوية كبيرة متعددة الوسائط الصوتية. تشكّل هذه التكاملات المتقاطعة للوسائط في نماذج اللغة الكبيرة حلولًا عالية الجودة لمهام متنوعة في العالم الحقيقي.</p>
<h2 id="نماذج-اللغة-الكبيرة-للقيادة-الآلية">نماذج اللغة الكبيرة للقيادة الآلية</h2>
<p>مؤخرًا ومع التطور السريع في تقنيات القيادة الذاتية، بدأ ظهور تطبيقات نماذج اللغة الكبيرة في مجالات القيادة الذكية والتليماتيكس. في عام <span class="nodecor">2023</span>، ظهر نموذج <span class="nodecor">uniad</span> الذي يعد أول تطبيق كامل لنماذج اللغة الكبيرة في القيادة الذاتية، حيث يدمج ثلاثة مكونات أساسية: الإدراك، اتخاذ القرار، والتخطيط ضمن بنية شبكة واحدة، مما يقلل من فقدان المعلومات بين الوحدات المنفصلة. كما طرح الباحثون نموذجًا متعدد الوسائط يُسمى <span class="nodecor">CAVG</span>، يركز على نية السائق، ويتكون من خمسة مشفرات متخصصة: نصي، عاطفي، بصري، سياقي وعبر الوسائط، بمرافقها من مفكِّكات، لتمكين النموذج من التعامل مع وسائط وأبعاد مختلفة لمهام القيادة الذاتية. في مهام الإدراك، يستخدم <span class="nodecor">DRIVEGPT4</span> خوارزمية <span class="nodecor">YOLOv8</span> لاكتشاف الأهداف الشائعة مثل المركبات في كل إطار فيديو، ثم يمرر الإحداثيات إلى <span class="nodecor">ChatGPT</span> كمعلومات لغوية. ومع ذلك، نظرًا لأن <span class="nodecor">DRIVEGPT4</span> يقتصر على وعي بيئي لمركبة واحدة، فإنه غير مناسب للتطبيقات الطرفية من البداية إلى النهاية في بيئات السيارات المتعددة.</p>
<h2 id="حكم-الحوادث">حكم الحوادث</h2>
<p>يُعد التنبؤ بالحوادث المرورية من أكثر مجالات البحث نشاطًا في سلامة القيادة الذاتية. اعتمد العديد من الباحثين على أساليب رؤية تقليدية ومستشعرات أمامية للمركبة مقترنة بشبكات عصبية زمنية—مثل LSTM أو RNN—لتحذير السائقين من احتمالية وقوع حادث (<span class="nodecor">c10</span>). ومع ذلك، تقصر هذه الطرق على استشعار سلوك المركبات القريبة فقط، وتفشل في تقدير المخاطر ضمن بيئات المرور المعقدة، خاصة في ظروف الطقس القاسية. مع ظهور نماذج اللغة الكبيرة، حاول بعض الباحثين استخدامها في إدراك الحوادث، مثل "نموذج الحوادث العملاق"، ولكن نظرًا لاعتماد تلك النماذج على مجموعات بيانات مُعَدَّة مسبقًا، فإنها لا تملك القدرة الكافية على تعميم التفكير في بيئات جديدة تمامًا تحت ظروف معقدة.</p>
<h1 id="المنهجية">المنهجية</h1>
<p>لا يستطيع Blip2 معالجة مدخلات الصور المكوّنة من ستة اتجاهات مباشرةً، ولا يملك القدرة على استدلال السلاسل الزمنية المستخلصة من هذه المدخلات. في هذا القسم، نستخدم بيانات المحاكاة وتقنيات تحويل الرؤية من المحاكي (<span class="nodecor">c11</span>) لتعزيز قدرة Blip2 على معالجة الصور الزمنية متعددة الاتجاهات. نأمل من خلال ذلك أن يصبح Blip2 المدرب قادرًا على التفكير في التسلسلات الزمنية المكونة من ستة اتجاهات للاستدلال. تجدر الإشارة إلى أن إطارنا قابل للتوسعة ليشمل سيناريوهات محاكاة القيادة الذاتية الطرفية المتعددة المركبات.</p>
<h2 id="مدخلات-متعددة-الوجهات-والاستدلال-الزمني">مدخلات متعددة الاتجاهات والاستدلال الزمني</h2>
<p>نقدم إطارًا إدراكيًا يستفيد من نماذج اللغة الكبيرة متعددة الوسائط، ويتكوّن من مكونين أساسيين: استخراج السمات من مدخلات الصور متعددة الاتجاهات والاستدلال الزمني. في المرحلة الأولى، نستخدم ViT-<span class="nodecor">14g</span> للتعامل مع ستة اتجاهات واستخراج الميزات البصرية ذات الصلة من كل وجهة. في المرحلة الثانية، نعتمد على استعلامات تحمل المعلومات السياقية والزمنية المتعلقة بالتسلسل الزمني للإطارات. من خلال دمج هاتين المرحلتين، يتيح إطارنا القدرة على الإدراك والاستدلال الشامل متعدد الوسائط على امتداد الزمن.</p>
<p>يعتمد مشفّرنا البصري على ViT-<span class="nodecor">14g</span> المأخوذ من EVA-CLIP (<span class="nodecor">c13</span>). يقرأ محول الرؤية أولًا صور الست اتجاهات الملتقطة من الكاميرات، ويغير حجم كل منها لاستخراج الميزات بشكل منفصل. نستخدم محول الرؤية المدرب مسبقًا كعمود فقري لاستخراج مجموعة الميزات <span class="math inline">\(f_t\)</span> للاتجاهات الست عند كل إطار زمني <span class="math inline">\(t\)</span>، ثم ندخل هذه الميزات إلى الطبقة الخطية في Qformer لإجراء عملية الاستدلال. قبل ذلك، يقوم محول الرؤية بربط ميزات كل صورة في بعد واحد، ثم يدمج ميزات الست جهات ليشكّل التمثيل الزمني <span class="math inline">\(f_t\)</span> الذي يُدخل في آلية الانتباه المتقاطع (Cross-Attention) في Qformer. في الوقت نفسه، يُدخل الاستعلام القابل للتعلم <span class="math inline">\(Q_t\)</span> إلى طبقة الانتباه الذاتي في Qformer، ثم يُمرر الخرج الجديد عبر طبقة التغذية الأمامية.</p>
<p>نتيجةً لخصوصية بيانات القيادة الذاتية الزمنية، نقترح آلية الانتباه الذاتي الزمني (Temporal Self-Attention)، حيث يلتقط الاستعلام الزمني <span class="math inline">\(Q_n\)</span> معلومات الصور متعددة الاتجاهات لكل إطار، ثم يُدخل في Qformer المقابل للحصول على الاستعلام التالي <span class="math inline">\(Q_{n+1}\)</span> لإدخال الإطار التالي. بفضل آلية الانتباه في Qformer، يتفاعل الاستعلام الجديد مع الميزات المستخرجة من آخر مخرجات Qformer، كما هو موضح في [fig:pic2]. تُعرض عملية الانتباه المتقاطع بين ميزات الاتجاهات المتعددة لكل إطار <span class="math inline">\(f_t\)</span> والاستعلام الزمني <span class="math inline">\(Q_n\)</span> في المعادلة [eq1]: <span class="math display">\[Attn(Q_{n-1}, K_{Car}, V_{Car} )=Softmax(\frac{Q_{n}\cdot K_{Car}^{T}}{\sqrt{d_{k} } } ) V  
\label{eq1}\]</span></p>
<p>حيث <span class="math inline">\(Q_{n-1} \in \mathbb{R} ^ {N_Q \times D}\)</span> يشير إلى ميزة الاستعلام من الطابع الزمني الأخير، والتي تؤدي إلى توليد إخراج الحالة الحالية <span class="math inline">\(Q_n\)</span>. بعد ذلك، من أجل تقديم وجهة النظر الصورية الحالية، يجمع الحالة الحالية <span class="math inline">\(Q_n\)</span> ميزات الصورة من ViT-g، المشار إليها بـ <span class="math inline">\(f_t\)</span>. يمكن وصف آلية الانتباه المتقاطع كما يلي</p>
<p><span class="math display">\[Q=Q_nW^Q  \hspace{1em} K=V=f_tW^K\]</span></p>
<p><span class="math display">\[CrossAttn(Q_n, f_t) = Softmax\left ( \frac {QK^T}{\sqrt {d_2}}\right ) V\]</span></p>
<h2 id="الاستشعار-المتكامل-من-طرف-إلى-طرف-لعدة-مركبات">الاستشعار المتكامل من طرف إلى طرف لعدة مركبات</h2>
<p>لتعزيز موثوقية إدراك بيئة المركبة، قد لا يكفي الاعتماد على مركبة واحدة بسبب وجود مناطق عمياء. لمواجهة ذلك، طورنا نظام إدراك بصري تعاوني يربط بين عدة مركبات. في ظل بيئات المرور المعقدة، صممنا شبكة عصبية تقيم ليس فقط الحوادث المحتملة التي تنطوي على المركبة الذاتية، بل أيضًا مخاطر الاصطدام فيما بينها. حققنا ذلك عبر تدريب مشترك متعدد المركبات، مما مكّننا من بناء وكيل مدرك للحوادث يجمع البيانات من عدة مركبات باستخدام مجموعة بيانات DeepAccident.</p>
<p>في نظام المركبات المتعددة، اعتمدنا على النسخة المدربة مسبقًا من ViT-<span class="nodecor">14g</span> لكل مركبة <span class="math inline">\(Car_n\)</span> لاستخراج الميزات <span class="math inline">\(f\)</span> من صور الإطار الحالي متعدد الجوانب. تُغذى هذه الميزات بعد ذلك إلى كتلة <span class="nodecor">MotionBlip2</span> المخصصة. بإضافة الاستعلامات الناتجة من <span class="nodecor">AccidentBlip2</span>، نجمعها مع استعلامات باقي المركبات وندخلها إلى <span class="nodecor">Motion Qformer</span> في الخطوة الزمنية التالية. أخيرًا، نحول هذا الاستعلام متعدد الأبعاد إلى متجه أحادي الأبعاد ليكون مدخلاً لشبكة الإدراك متعددة الطبقات (MLP)، كما يوضح المعادلة [eq4]: <span class="math display">\[\mathbf{X_i} = \text{MLP}(\text{concat}(Q_1, Q_2, Q_3, Q_4, ... \text{dim}=0))
\label{eq4}\]</span></p>
<p>يُدخل المتجه الناتج <span class="math inline">\(X_i\)</span> للإطار <span class="math inline">\(i\)</span> في طبقة التضمين، ثم يجري عليه الانحدار الذاتي لتحديد احتمال وقوع حادث بين المركبات. كل استعلام <span class="math inline">\(Q_n\)</span> يمثل وجهة نظر واحدة لمركبة <span class="math inline">\(Car_n\)</span> ضمن النظام، مما يؤدي إلى دمج 24 وجهة نظر في تنسيق الميزات النهائي.</p>
<h1 id="التجربة">التجربة</h1>
<p>في هذا القسم، نقيم نظامنا على مجموعة بيانات المحاكاة <span class="nodecor">DeepAccident</span>. نختبره بشكل منفصل لإدراك المركبة الفردية مقابل إدراك المركبات المتعددة. بشكل أكثر تحديدًا، نركز على الإجابة عن الأسئلة التالية:</p>
<ul>
<li><p>هل يمكن لـ <span class="nodecor">AccidentBlip2</span> تحديد الحوادث في أنظمة المرور المعقدة بكفاءة أعلى من الحلول الأخرى المعتمدة على الرؤية في سياق السلاسل الزمنية؟</p></li>
<li><p>هل يستطيع <span class="nodecor">AccidentBlip2</span> استشعار الحوادث المحيطة بدقة أكبر عند استخدام نظام مركبات متعددة نهاية إلى نهاية مقارنة بإدراك المركبة الواحدة؟</p></li>
</ul>
<h2 id="بيانات-التدريب">بيانات التدريب</h2>
<p>للتدريب والتقييم، اعتمدنا مجموعة البيانات مفتوحة المصدر (<span class="nodecor">c2</span>) التي توفر معلومات بصرية شاملة عن المركبات والبُنى التحتية. تحتوي هذه المجموعة على سيناريوهات تحاكي الاصطدامات المحتملة في العالم الحقيقي، مع وجود أربع مركبات وبنية تحتية واحدة، بالإضافة إلى بيانات سحابة نقاط من وجهة النظر العلوية. على عكس التقنيات متعددة الوسائط الأخرى، يقتصر <span class="nodecor">AccidentBlip2</span> على أساليب الرؤية حصريًا، فتستخدم فقط الصور البصرية الملتقطة من الكاميرات المثبتة على المركبات.</p>
<p>وفقًا لوصف مجموعة البيانات (<span class="nodecor">c2</span>)، تتألف من 12 نوعًا من سيناريوهات الحوادث عند التقاطعات المنظمة وغير المنظمة. تضم المجموعة نحو 57 ألف إطار V2X موسوم، موزعة بنسبة 0.7 للتدريب، 0.15 للتحقق، و0.15 للاختبار. يلزم جهاز مزود بأربع وحدات معالجة رسومات من طراز <span class="nodecor">A6000</span>، وتستغرق عملية التدريب أقل من 24 ساعة.</p>
<h2 id="المعايير-الأساسية">المعايير الأساسية</h2>
<p>نستخدم كنماذج أساسية عدة نماذج شائعة للغات الفيديو الكبيرة. أولها نموذج اللغة الكبير للفيديو (Video Large Language Model)، الذي يُعتمد غالبًا في مهام معالجة الفيديو. كما نقارنه بنسخة محسّنة منه تُسمى فيديو-فيكونا (Video-Falcon). بالإضافة لذلك، شملنا أيضًا مقارنة مع نموذج فيديو-لافا (Video-LAVA) ونموذج الاستدلال الكبير للغة الفيديو قبل الإسقاط (Pre-Projection Inference). نلاحظ أن النماذج المحسّنة بصريًا مثل لافا وGPT-4V تواجه صعوبة في التعامل مع مدخلات زمنية كبيرة، وليس أداؤها أفضل من نموذج اللغة الكبير للفيديو، لذا لم تُضمّن في المقارنات النهائية.</p>
<h2 id="تفاصيل-التنفيذ">تفاصيل التنفيذ</h2>
<p>يتألف نموذجنا المقترح من مشفّر بصري ViT-<span class="nodecor">14g</span> بالإضافة إلى وحدة Motion Qformer لاستخراج ودمج الميزات من الصور والنصوص. ولضمان أفضل دقة في التنبؤ بالحوادث، ندرب <span class="nodecor">AccidentBlip2</span> على صور بحجم <span class="math inline">\(224 \times 224\)</span>.</p>
<p>في إعداداتنا، نجمد معلمات ViT-<span class="nodecor">14g</span> والموديل اللغوي <span class="nodecor">OPT-2.7B</span> (<span class="nodecor">c14</span>), حيث أُهيأت بأوزان مدربة مسبقًا مماثلة لـ <span class="nodecor">Blip-2</span> (<span class="nodecor">c21</span>). تهدف وحدة Motion Qformer، بالتعاون مع محول Qformer، إلى دمج وجهات النظر المتعددة ضمن نظام المركبات التعاوني، وهي تُحمّل وتُدرّب على مهام التنبؤ والمخرجات اللغوية معًا. تمرُّر مدخلات الصور عبر طبقة ViT تُمكّن Motion Qformer من التقاط الميزات الديناميكية عبر الإطارات الزمنية المختلفة.</p>
<p>خلال التدريب، اعتمدنا فترة تسخين معدل التعلم للثلاثة خطوات الأولى، مع استخدام محسن Adam[1] بـ <span class="math inline">\(\beta_1=0.9\)</span> و<span class="math inline">\(\beta_2=0.999\)</span>. درّبنا <span class="nodecor">AccidentBlip2</span> على مدى 8 حقبات، مستخدمين جدولة جيبية لمعدل التعلم يصل فيها الحد الأدنى إلى <span class="math inline">\(1e^{-5}\)</span>. تمتد فترة التدريب على 6 مراحل بحجم دفعة قدره 8، يختلف عدد المركبات فيها باختلاف المهمة. بالنسبة للمدخلات متعددة السيناريوهات، حيث تُؤخذ عينات من الإطارات الزمنية من 0 إلى <span>T</span>, يكون شكلها: <span class="math inline">\(X_S \in \mathbb{R}^{T \times N_V \times V_C \times C \times H \times W}\)</span>، حيث يشير <span>T</span> إلى طول السلسلة الزمنية، و<span>N_V</span> إلى عدد المركبات، و<span>V_C</span> إلى ست وجهات نظر منفصلة (أمامية، أمامية يسرى، أمامية يمنى، خلفية، خلفية يسرى، خلفية يمنى)، و<span>H</span> و<span>W</span> إلى أبعاد الصورة. يتفاعل <span class="nodecor">Motion Qformer</span> تكراريًا مع مخرجات الاستعلام <span class="math inline">\(Q_{T-1} \in \mathbb{R}^{N_V \times N_Q \times D}\)</span> من آخر خطوة زمنية، مما يمكّن من التقاط الميزات المتعددة الاتجاهات عبر الزمن حتى الوصول إلى <span>\(Q_T\)</span>، ويقوم نموذج اللغة بعد ذلك بجمعه كمُجمّع للتضمين الشامل. يعتمد حساب دالة الخسارة <span class="math inline">\(Loss(p_t)\)</span>—المعروضة في [eq2]—على Focal Loss، حيث يرمز <span>\(p_t\)</span> إلى احتمال الفئة الصحيحة، ويستخدم <span>\(\alpha\)</span> لموازنة أوزان العينات الإيجابية والسلبية، بينما يتحكم <span>\(\gamma\)</span> في معدل العينات التي يتم التركيز عليها.</p>
<p>نظراً لتوزع فئات الحوادث غير المتكافئ، ضبطنا <span class="math inline">\(\alpha\)</span> على 0.25، وتركنا <span class="math inline">\(\gamma\)</span> عند القيمة الافتراضية 2.0، لتعزيز تركيز النموذج على العينات الصعبة أو المشكوك في تصنيفها.</p>
<h2 id="التقييم">التقييم</h2>
<p>يقدم الجدول [Table 1] نتائج أداء تكوينات مختلفة لنموذجنا. من خلال تصميم إصدارات متباينة، نقيم <span class="nodecor">AccidentBlip2</span> في سيناريو مركبة واحدة وسيناريو مركبات متعددة، بما في ذلك المركبة الذاتية، المركبات الأخرى والرؤية من البنية التحتية. بفضل الاستفادة من ستة كاميرات مثبتة عبر أربع مركبات، تمكن <span class="nodecor">Motion Qformer</span> من التقاط الميزات الزمنية لعدة مركبات بفعالية.</p>
<p>أدى ذلك إلى تحسين الدقة بمقدار <span class="nodecor">2%</span> مقارنة بسيناريو المركبة الواحدة. علاوة على ذلك، لوحظت زيادة واضحة بنسبة <span class="nodecor">6.6%</span> عند استخدام عدة وجهات نظر، مما يؤكد قدرة النموذج على الاستدلال الشامل للحوادث.</p>
<p>يتفوق نموذجنا المقترح على النماذج الأساسية في كلا التكوينين. مقارنةً بنماذج اللغة الكبيرة للفيديو الموضحة في الجدول [Table 2]، يحقق <span class="nodecor">AccidentBlip2</span> دقة <span class="nodecor">66.5%</span> عند مدخلات مركبة واحدة. وبالمقارنة مع إعدادات V2XFormer الثلاثة، يُظهر نموذجنا زيادة تقارب <span class="nodecor">3%</span> في الدقة.</p>
<p>عالجنا تحدي انفجار التدرج أثناء التدريب باستخدام فترة تسخين لمعدل التعلم، مما زاد من استقرار العملية. يعمل نموذجنا متعدد المركبات على أربع وحدات معالجة رسومات، مما يتيح جمع استعلامات Motion Qformer من كل مركبة. كما استكشفنا تأثير الوزن الأولي لـ Qformer، فتبين أن التهيئة بأوزان Blip2 المدربة مسبقًا تحسّن الأداء على مجموعة بيانات <span class="nodecor">DeepAccident</span>.</p>
<h1 id="الاستنتاجات">الاستنتاجات</h1>
<p>في هذه الورقة، قدمنا إطار عمل لكشف الحوادث قائمًا على <span class="nodecor">Motion Qformer</span> أطلقنا عليه <span class="nodecor">AccidentBlip2</span>، ويعتمد حصريًا على مدخلات الرؤية لتحليل معلومات الطرق. يُدخل هذا الإطار آلية الانتباه الزمني ضمن <span class="nodecor">Blip2</span> عبر استبدال آلية الانتباه الذاتي التقليدية، حيث تُستخدم الاستعلامات كحاملات للسمات الزمنية، ليتم تشفير المعلومات من كل إطار ثم تضمينها في إطار لاحق. يتيح هذا التصميم إمكانية الاستدلال التلقائي باستخدام شبكة <span class="nodecor">MLP</span> لتحديد وقوع الحوادث وتقديم توصيف دقيق للبيئة المحيطة.</p>
<p>عند مقارنة دقة كشف الحوادث مع نماذج اللغة الكبيرة الأخرى المعتمدة على الفيديو، يبرز <span class="nodecor">AccidentBlip2</span> بدقة مثيرة للإعجاب بلغت <span class="nodecor">66.5%</span>، متجاوزًا أداء جميع النماذج الأساسية. يسلط ذلك الضوء على فعالية إطار عملنا في أنظمة النقل متعددة المركبات المعقدة. على وجه التحديد، يحقق نظامنا المكون من أربع مركبات دقة بلغت <span class="nodecor">72.2%</span> في كشف الحوادث البيئية، مما يشير إلى تحسن كبير مقارنة بدقة كشف حوادث المركبة الواحدة. علاوةً على ذلك، يظهر <span class="nodecor">AccidentBlip2</span> مزايا واضحة في التحقق من الحوادث مقارنة بنماذج اللغة الكبيرة الأخرى المعتمدة فقط على الفيديو في أنظمة المركبات المتعددة، مما يؤكد إمكانات نهجنا لتعزيز كشف الحوادث في سيناريوهات المرور المعقدة.</p>
</body>
</html>